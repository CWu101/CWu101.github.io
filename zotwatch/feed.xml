<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sun, 04 Jan 2026 02:51:43 +0000</lastBuildDate><item><title>GrowSP++: Growing Superpoints and Primitives for Unsupervised 3D Semantic Segmentation</title><link>https://doi.org/10.1109/tpami.2025.3650165</link><guid>10.1109/tpami.2025.3650165</guid><pubDate>Fri, 02 Jan 2026 18:16:22 +0000</pubDate><dc:creator>Zihui Zhang</dc:creator><dc:creator>Weisheng Dai</dc:creator><dc:creator>Bing Wang</dc:creator><dc:creator>Bo Li</dc:creator><dc:creator>Bo Yang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3650165</prism:doi><description>We study the problem of 3D semantic segmentation from raw point clouds. Unlike existing methods which primarily rely on a large amount of human annotations for training neural networks, we proposes GrowSP++, an unsupervised method to successfully identify complex semantic classes for every point in 3D scenes, without needing any type of human labels. Our method is composed of three major components: 1) a feature extractor incorporating 2D-3D feature distillation, 2) a superpoint constructor featuring progressively growing superpoints, and 3) a semantic primitive constructor with an additional growing strategy. The key to our method is the superpoint constructor together with the progressive growing strategy on both super points and semantic primitives, driving the feature extractor to progressively learn similar features for 3D points belonging to the same semantic class. We extensively evaluate our method on five challenging indoor and outdoor datasets, demonstrating state of-the-art performance over all unsupervised baselines. We hope our work could inspire more advanced methods for unsupervised 3D semantic learning.
Published: 2026-01-02T18:16:22+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.566 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zihui Zhang; Weisheng Dai; Bing Wang; Bo Li; Bo Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3650165"&gt;10.1109/tpami.2025.3650165&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.566 (consider)&lt;/p&gt;
&lt;p&gt;We study the problem of 3D semantic segmentation from raw point clouds. Unlike existing methods which primarily rely on a large amount of human annotations for training neural networks, we proposes GrowSP++, an unsupervised method to successfully identify complex semantic classes for every point in 3D scenes, without needing any type of human labels. Our method is composed of three major components: 1) a feature extractor incorporating 2D-3D feature distillation, 2) a superpoint constructor featuring progressively growing superpoints, and 3) a semantic primitive constructor with an additional growing strategy. The key to our method is the superpoint constructor together with the progressive growing strategy on both super points and semantic primitives, driving the feature extractor to progressively learn similar features for 3D points belonging to the same semantic class. We extensively evaluate our method on five challenging indoor and outdoor datasets, demonstrating state of-the-art performance over all unsupervised baselines. We hope our work could inspire more advanced methods for unsupervised 3D semantic learning.&lt;/p&gt;</content:encoded></item><item><title>Regional Defeats Global: An Efficient Regional Feature Fusion via Convolutional Architecture for Multispectral Object Detection</title><link>https://doi.org/10.1016/j.inffus.2025.104110</link><guid>10.1016/j.inffus.2025.104110</guid><pubDate>Fri, 02 Jan 2026 07:42:39 +0000</pubDate><dc:creator>Zhenhao Wang</dc:creator><dc:creator>Tian Tian</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104110</prism:doi><description>Multispectral object detection continues to face significant challenges in achieving a balanced optimization between accuracy and efficiency. Most existing approaches rely heavily on global modeling, which, although capable of integrating multi-band information, incurs substantial computational overhead and fails to fully exploit the spatial correlations across spectral bands. To address this issue, this paper introduces a convolutional architecture-based region feature computation mechanism that leverages the inherent advantage of convolutional operations in preserving spatial structure, enabling spatial cues to be fully retained during feature representation learning and explicitly incorporated into multispectral feature interaction. Meanwhile, by reconstructing global attention computation into localized regional modeling, the proposed method markedly reduces computational cost while maintaining effective feature fusion, thereby facilitating a lightweight architectural design. Experimental results demonstrate that the proposed module achieves the lowest computational overhead while improving mAP@50 by 1.97% and 1.66% on the DroneVehicle and VEDAI remote-sensing datasets, respectively, compared with state-of-the-art methods. Moreover, it exhibits strong applicability on the pedestrian detection datasets FLIR and LLVIP. The code is available https://github.com/wzh326/LMFFM_CARFCOM.git .
Published: 2026-01-02T07:42:39+00:00
Venue: Information Fusion
Score: 0.562 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhenhao Wang; Tian Tian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104110"&gt;10.1016/j.inffus.2025.104110&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.562 (consider)&lt;/p&gt;
&lt;p&gt;Multispectral object detection continues to face significant challenges in achieving a balanced optimization between accuracy and efficiency. Most existing approaches rely heavily on global modeling, which, although capable of integrating multi-band information, incurs substantial computational overhead and fails to fully exploit the spatial correlations across spectral bands. To address this issue, this paper introduces a convolutional architecture-based region feature computation mechanism that leverages the inherent advantage of convolutional operations in preserving spatial structure, enabling spatial cues to be fully retained during feature representation learning and explicitly incorporated into multispectral feature interaction. Meanwhile, by reconstructing global attention computation into localized regional modeling, the proposed method markedly reduces computational cost while maintaining effective feature fusion, thereby facilitating a lightweight architectural design. Experimental results demonstrate that the proposed module achieves the lowest computational overhead while improving mAP@50 by 1.97% and 1.66% on the DroneVehicle and VEDAI remote-sensing datasets, respectively, compared with state-of-the-art methods. Moreover, it exhibits strong applicability on the pedestrian detection datasets FLIR and LLVIP. The code is available https://github.com/wzh326/LMFFM_CARFCOM.git .&lt;/p&gt;</content:encoded></item><item><title>Progressive Temporal Compensation and Semantic Enhancement for Exo-to-Ego Video Generation</title><link>https://doi.org/10.1016/j.inffus.2025.104117</link><guid>10.1016/j.inffus.2025.104117</guid><pubDate>Sat, 03 Jan 2026 16:24:00 +0000</pubDate><dc:creator>Xingyue Wang</dc:creator><dc:creator>Weipeng Hu</dc:creator><dc:creator>Jiun Tian Hoe</dc:creator><dc:creator>Jianhui Li</dc:creator><dc:creator>Ping Hu</dc:creator><dc:creator>Yap-Peng Tan</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104117</prism:doi><description>Transforming video perspectives from exocentric (third-person) to egocentric (first-person) is challenging due to limited overlap between two perspectives. Existing approaches often neglect the temporal dynamics—critical for capturing motion cues and reappearing objects—and do not fully exploit source-view inferred semantics. To address these limitations, we propose a Progressive Temporal Compensation and Semantic Enhancement (PCSE) framework for Exocentric-to-Egocentric Video Generation. The Progressive Temporal Compensation (PTC) module focuses on long-term temporal dependencies, progressively aligning exocentric temporal patterns with egocentric representations. By employing a reliance-shifting mechanism with a progression mask, PTC gradually reduces dependence on egocentric supervision, enabling more robust target-view learning. Moreover, to leverage high-level scene context, we introduce a Hierarchical Dual-channel Transformer (HDT), which jointly generates egocentric frames and their corresponding semantic layouts via dual encoder–decoder architectures with hierarchically processed transformer blocks. To further enhance structural coherence and semantic consistency, the generated semantic layouts guide frame refinement through an Uncertainty-aware Semantic Enhancement (USE) module. USE dynamically estimates uncertainty masks to locate and refine ambiguous regions, yielding more coherent and visually accurate results. Extensive experiments demonstrate that PCSE achieves leading performance among cue-free methods.
Published: 2026-01-03T16:24:00+00:00
Venue: Information Fusion
Score: 0.543 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xingyue Wang; Weipeng Hu; Jiun Tian Hoe; Jianhui Li; Ping Hu; Yap-Peng Tan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104117"&gt;10.1016/j.inffus.2025.104117&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.543 (consider)&lt;/p&gt;
&lt;p&gt;Transforming video perspectives from exocentric (third-person) to egocentric (first-person) is challenging due to limited overlap between two perspectives. Existing approaches often neglect the temporal dynamics—critical for capturing motion cues and reappearing objects—and do not fully exploit source-view inferred semantics. To address these limitations, we propose a Progressive Temporal Compensation and Semantic Enhancement (PCSE) framework for Exocentric-to-Egocentric Video Generation. The Progressive Temporal Compensation (PTC) module focuses on long-term temporal dependencies, progressively aligning exocentric temporal patterns with egocentric representations. By employing a reliance-shifting mechanism with a progression mask, PTC gradually reduces dependence on egocentric supervision, enabling more robust target-view learning. Moreover, to leverage high-level scene context, we introduce a Hierarchical Dual-channel Transformer (HDT), which jointly generates egocentric frames and their corresponding semantic layouts via dual encoder–decoder architectures with hierarchically processed transformer blocks. To further enhance structural coherence and semantic consistency, the generated semantic layouts guide frame refinement through an Uncertainty-aware Semantic Enhancement (USE) module. USE dynamically estimates uncertainty masks to locate and refine ambiguous regions, yielding more coherent and visually accurate results. Extensive experiments demonstrate that PCSE achieves leading performance among cue-free methods.&lt;/p&gt;</content:encoded></item><item><title>RGBT-Ground Benchmark: Visual Grounding Beyond RGB in Complex Real-World Scenarios</title><link>https://arxiv.org/abs/2512.24561v1</link><guid>http://arxiv.org/abs/2512.24561v1</guid><pubDate>Wed, 31 Dec 2025 02:01:02 +0000</pubDate><dc:creator>Tianyi Zhao</dc:creator><dc:creator>Jiawen Xi</dc:creator><dc:creator>Linhui Xiao</dc:creator><dc:creator>Junnan Li</dc:creator><dc:creator>Xue Yang</dc:creator><dc:creator>Maoxun Yuan</dc:creator><dc:creator>Xingxing Wei</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual Grounding (VG) aims to localize specific objects in an image according to natural language expressions, serving as a fundamental task in vision-language understanding. However, existing VG benchmarks are mostly derived from datasets collected under clean environments, such as COCO, where scene diversity is limited. Consequently, they fail to reflect the complexity of real-world conditions, such as changes in illumination, weather, etc., that are critical to evaluating model robustness and generalization in safety-critical applications. To address these limitations, we present RGBT-Ground, the first large-scale visual grounding benchmark built for complex real-world scenarios. It consists of spatially aligned RGB and Thermal infrared (TIR) image pairs with high-quality referring expressions, corresponding object bounding boxes, and fine-grained annotations at the scene, environment, and object levels. This benchmark enables comprehensive evaluation and facilitates the study of robust grounding under diverse and challenging conditions. Furthermore, we establish a unified visual grounding framework that supports both uni-modal (RGB or TIR) and multi-modal (RGB-TIR) visual inputs. Based on it, we propose RGBT-VGNet, a simple yet effective baseline for fusing complementary visual modalities to achieve robust grounding. We conduct extensive adaptations to the existing methods on RGBT-Ground. Experimental results show that our proposed RGBT-VGNet significantly outperforms these adapted methods, particularly in nighttime and long-distance scenarios. All resources will be publicly released to promote future research on robust visual grounding in complex real-world environments.
Published: 2025-12-31T02:01:02+00:00
Venue: arXiv
Score: 0.542 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianyi Zhao; Jiawen Xi; Linhui Xiao; Junnan Li; Xue Yang; Maoxun Yuan; Xingxing Wei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.542 (consider)&lt;/p&gt;
&lt;p&gt;Visual Grounding (VG) aims to localize specific objects in an image according to natural language expressions, serving as a fundamental task in vision-language understanding. However, existing VG benchmarks are mostly derived from datasets collected under clean environments, such as COCO, where scene diversity is limited. Consequently, they fail to reflect the complexity of real-world conditions, such as changes in illumination, weather, etc., that are critical to evaluating model robustness and generalization in safety-critical applications. To address these limitations, we present RGBT-Ground, the first large-scale visual grounding benchmark built for complex real-world scenarios. It consists of spatially aligned RGB and Thermal infrared (TIR) image pairs with high-quality referring expressions, corresponding object bounding boxes, and fine-grained annotations at the scene, environment, and object levels. This benchmark enables comprehensive evaluation and facilitates the study of robust grounding under diverse and challenging conditions. Furthermore, we establish a unified visual grounding framework that supports both uni-modal (RGB or TIR) and multi-modal (RGB-TIR) visual inputs. Based on it, we propose RGBT-VGNet, a simple yet effective baseline for fusing complementary visual modalities to achieve robust grounding. We conduct extensive adaptations to the existing methods on RGBT-Ground. Experimental results show that our proposed RGBT-VGNet significantly outperforms these adapted methods, particularly in nighttime and long-distance scenarios. All resources will be publicly released to promote future research on robust visual grounding in complex real-world environments.&lt;/p&gt;</content:encoded></item><item><title>Evolving, Not Training: Zero-Shot Reasoning Segmentation via Evolutionary Prompting</title><link>https://arxiv.org/abs/2512.24702v1</link><guid>http://arxiv.org/abs/2512.24702v1</guid><pubDate>Wed, 31 Dec 2025 08:10:03 +0000</pubDate><dc:creator>Kai Ye</dc:creator><dc:creator>Xiaotong You</dc:creator><dc:creator>Jianghang Lin</dc:creator><dc:creator>Jiayi Ji</dc:creator><dc:creator>Pingyang Dai</dc:creator><dc:creator>Liujuan Cao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reasoning Segmentation requires models to interpret complex, context-dependent linguistic queries to achieve pixel-level localization. Current dominant approaches rely heavily on Supervised Fine-Tuning (SFT) or Reinforcement Learning (RL). However, SFT suffers from catastrophic forgetting and domain dependency, while RL is often hindered by training instability and rigid reliance on predefined reward functions. Although recent training-free methods circumvent these training burdens, they are fundamentally limited by a static inference paradigm. These methods typically rely on a single-pass "generate-then-segment" chain, which suffers from insufficient reasoning depth and lacks the capability to self-correct linguistic hallucinations or spatial misinterpretations. In this paper, we challenge these limitations and propose EVOL-SAM3, a novel zero-shot framework that reformulates reasoning segmentation as an inference-time evolutionary search process. Instead of relying on a fixed prompt, EVOL-SAM3 maintains a population of prompt hypotheses and iteratively refines them through a "Generate-Evaluate-Evolve" loop. We introduce a Visual Arena to assess prompt fitness via reference-free pairwise tournaments, and a Semantic Mutation operator to inject diversity and correct semantic errors. Furthermore, a Heterogeneous Arena module integrates geometric priors with semantic reasoning to ensure robust final selection. Extensive experiments demonstrate that EVOL-SAM3 not only substantially outperforms static baselines but also significantly surpasses fully supervised state-of-the-art methods on the challenging ReasonSeg benchmark in a zero-shot setting. The code is available at https://github.com/AHideoKuzeA/Evol-SAM3.
Published: 2025-12-31T08:10:03+00:00
Venue: arXiv
Score: 0.538 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kai Ye; Xiaotong You; Jianghang Lin; Jiayi Ji; Pingyang Dai; Liujuan Cao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.538 (consider)&lt;/p&gt;
&lt;p&gt;Reasoning Segmentation requires models to interpret complex, context-dependent linguistic queries to achieve pixel-level localization. Current dominant approaches rely heavily on Supervised Fine-Tuning (SFT) or Reinforcement Learning (RL). However, SFT suffers from catastrophic forgetting and domain dependency, while RL is often hindered by training instability and rigid reliance on predefined reward functions. Although recent training-free methods circumvent these training burdens, they are fundamentally limited by a static inference paradigm. These methods typically rely on a single-pass &amp;quot;generate-then-segment&amp;quot; chain, which suffers from insufficient reasoning depth and lacks the capability to self-correct linguistic hallucinations or spatial misinterpretations. In this paper, we challenge these limitations and propose EVOL-SAM3, a novel zero-shot framework that reformulates reasoning segmentation as an inference-time evolutionary search process. Instead of relying on a fixed prompt, EVOL-SAM3 maintains a population of prompt hypotheses and iteratively refines them through a &amp;quot;Generate-Evaluate-Evolve&amp;quot; loop. We introduce a Visual Arena to assess prompt fitness via reference-free pairwise tournaments, and a Semantic Mutation operator to inject diversity and correct semantic errors. Furthermore, a Heterogeneous Arena module integrates geometric priors with semantic reasoning to ensure robust final selection. Extensive experiments demonstrate that EVOL-SAM3 not only substantially outperforms static baselines but also significantly surpasses fully supervised state-of-the-art methods on the challenging ReasonSeg benchmark in a zero-shot setting. The code is available at https://github.com/AHideoKuzeA/Evol-SAM3.&lt;/p&gt;</content:encoded></item><item><title>Improving Few-Shot Change Detection Visual Question Answering via Decision-Ambiguity-guided Reinforcement Fine-Tuning</title><link>https://arxiv.org/abs/2512.24591v1</link><guid>http://arxiv.org/abs/2512.24591v1</guid><pubDate>Wed, 31 Dec 2025 03:28:17 +0000</pubDate><dc:creator>Fuyu Dong</dc:creator><dc:creator>Ke Li</dc:creator><dc:creator>Di Wang</dc:creator><dc:creator>Nan Luo</dc:creator><dc:creator>Yiming Zhang</dc:creator><dc:creator>Kaiyu Li</dc:creator><dc:creator>Jianfei Yang</dc:creator><dc:creator>Quan Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Change detection visual question answering (CDVQA) requires answering text queries by reasoning about semantic changes in bi-temporal remote sensing images. A straightforward approach is to boost CDVQA performance with generic vision-language models via supervised fine-tuning (SFT). Despite recent progress, we observe that a significant portion of failures do not stem from clearly incorrect predictions, but from decision ambiguity, where the model assigns similar confidence to the correct answer and strong distractors. To formalize this challenge, we define Decision-Ambiguous Samples (DAS) as instances with a small probability margin between the ground-truth answer and the most competitive alternative. We argue that explicitly optimizing DAS is crucial for improving the discriminability and robustness of CDVQA models. To this end, we propose DARFT, a Decision-Ambiguity-guided Reinforcement Fine-Tuning framework that first mines DAS using an SFT-trained reference policy and then applies group-relative policy optimization on the mined subset. By leveraging multi-sample decoding and intra-group relative advantages, DARFT suppresses strong distractors and sharpens decision boundaries without additional supervision. Extensive experiments demonstrate consistent gains over SFT baselines, particularly under few-shot settings.
Published: 2025-12-31T03:28:17+00:00
Venue: arXiv
Score: 0.517 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fuyu Dong; Ke Li; Di Wang; Nan Luo; Yiming Zhang; Kaiyu Li; Jianfei Yang; Quan Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.517 (consider)&lt;/p&gt;
&lt;p&gt;Change detection visual question answering (CDVQA) requires answering text queries by reasoning about semantic changes in bi-temporal remote sensing images. A straightforward approach is to boost CDVQA performance with generic vision-language models via supervised fine-tuning (SFT). Despite recent progress, we observe that a significant portion of failures do not stem from clearly incorrect predictions, but from decision ambiguity, where the model assigns similar confidence to the correct answer and strong distractors. To formalize this challenge, we define Decision-Ambiguous Samples (DAS) as instances with a small probability margin between the ground-truth answer and the most competitive alternative. We argue that explicitly optimizing DAS is crucial for improving the discriminability and robustness of CDVQA models. To this end, we propose DARFT, a Decision-Ambiguity-guided Reinforcement Fine-Tuning framework that first mines DAS using an SFT-trained reference policy and then applies group-relative policy optimization on the mined subset. By leveraging multi-sample decoding and intra-group relative advantages, DARFT suppresses strong distractors and sharpens decision boundaries without additional supervision. Extensive experiments demonstrate consistent gains over SFT baselines, particularly under few-shot settings.&lt;/p&gt;</content:encoded></item><item><title>Text-to-video person re-identification benchmark: Dataset and dual-modal contextual alignment</title><link>https://doi.org/10.1016/j.neucom.2025.132596</link><guid>10.1016/j.neucom.2025.132596</guid><pubDate>Fri, 02 Jan 2026 16:07:05 +0000</pubDate><dc:creator>Jiajun Su</dc:creator><dc:creator>Simin Zhan</dc:creator><dc:creator>Pudu Liu</dc:creator><dc:creator>Jianqing Zhu</dc:creator><dc:creator>Huanqiang Zeng</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132596</prism:doi><description>Text-to-video person re-identification aims to identify individuals in video sequences based on textual descriptions, but it faces challenges such as insufficient large-scale annotated datasets and inadequate cross-modal alignment mechanisms. As a result, current state-of-the-art (SOTA) methods are typically trained and evaluated on small-scale benchmarks and rely on coarse cross-modal alignment strategies, which limits their ability to generalize and to fully exploit temporal information. Current methods struggle to bridge the semantic gap between static text and dynamic video content, particularly in capturing temporal dynamics and fine-grained spatial–temporal correspondences. These issues constitute key technological limitations of existing SOTA approaches and directly motivate the development of both a larger, more realistic benchmark and a more effective cross-modal alignment mechanism. To address these issues, we introduce TV-MARS, a benchmark built on the MARS dataset with 16,360 text–video pairs, enriched with natural language annotations describing motion states and environmental interactions. At approximately 4.8 " role="presentation"&gt; larger than existing text-to-video person re-identification benchmarks, TV-MARS provides a more comprehensive resource for research. Additionally, we propose a Dual-Modal Contextual Alignment (DMCA) method to bridge the modality gap between text and video sequences. DMCA employs a local contextualizer to extract fine-grained spatial features and a global integrator to synthesize temporal dynamics, adaptively fusing these features to create a unified representation that aligns static textual descriptions with dynamic video content, ensuring robust cross-modal semantic consistency. Experiments show that DMCA achieves a +8.88 % improvement in rank-1 accuracy, significantly advancing the state of the art in text-to-video person re-identification.
Published: 2026-01-02T16:07:05+00:00
Venue: Neurocomputing
Score: 0.516 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiajun Su; Simin Zhan; Pudu Liu; Jianqing Zhu; Huanqiang Zeng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132596"&gt;10.1016/j.neucom.2025.132596&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.516 (consider)&lt;/p&gt;
&lt;p&gt;Text-to-video person re-identification aims to identify individuals in video sequences based on textual descriptions, but it faces challenges such as insufficient large-scale annotated datasets and inadequate cross-modal alignment mechanisms. As a result, current state-of-the-art (SOTA) methods are typically trained and evaluated on small-scale benchmarks and rely on coarse cross-modal alignment strategies, which limits their ability to generalize and to fully exploit temporal information. Current methods struggle to bridge the semantic gap between static text and dynamic video content, particularly in capturing temporal dynamics and fine-grained spatial–temporal correspondences. These issues constitute key technological limitations of existing SOTA approaches and directly motivate the development of both a larger, more realistic benchmark and a more effective cross-modal alignment mechanism. To address these issues, we introduce TV-MARS, a benchmark built on the MARS dataset with 16,360 text–video pairs, enriched with natural language annotations describing motion states and environmental interactions. At approximately 4.8 &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; larger than existing text-to-video person re-identification benchmarks, TV-MARS provides a more comprehensive resource for research. Additionally, we propose a Dual-Modal Contextual Alignment (DMCA) method to bridge the modality gap between text and video sequences. DMCA employs a local contextualizer to extract fine-grained spatial features and a global integrator to synthesize temporal dynamics, adaptively fusing these features to create a unified representation that aligns static textual descriptions with dynamic video content, ensuring robust cross-modal semantic consistency. Experiments show that DMCA achieves a +8.88 % improvement in rank-1 accuracy, significantly advancing the state of the art in text-to-video person re-identification.&lt;/p&gt;</content:encoded></item><item><title>A Multi-Granularity Scene-Aware Graph Convolution Method for Weakly Supervised Person Search</title><link>https://doi.org/10.1007/s11263-025-02665-3</link><guid>10.1007/s11263-025-02665-3</guid><pubDate>Sat, 03 Jan 2026 09:25:59 +0000</pubDate><dc:creator>De Cheng</dc:creator><dc:creator>Haichun Tai</dc:creator><dc:creator>Nannan Wang</dc:creator><dc:creator>Xiangqian Zhao</dc:creator><dc:creator>Jie Li</dc:creator><dc:creator>Xinbo Gao</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02665-3</prism:doi><description>One-step Weakly Supervised Person Search (WSPS) addresses person detection and re-identification (ReID) within a unified framework, relying solely on pedestrian bounding box annotations for training, without requiring annotated identity labels. This approach enhances the practicality and efficiency of person search in real-world applications. However, WSPS faces two primary challenges: (1) the significant feature discrepancy between ReID and pedestrian detection tasks complicates shared representation learning, and (2) accurately estimating pseudo identity for each person image is challenging due to unrefined detections and significant intra-class variation in complex scenes. To address these challenges, we introduce a multi-granularity scene-aware graph convolution framework, which jointly optimizes task-specific features, improves pseudo-label estimation, and reduces the effects of label noise. Specifically, the Multi-granularity Feature Alignment (MFA) module in our designed two-branch network leverages bi-directional cluster-level interactions across multiple granularities to address the feature discrepancy. Building on MFA, we develop the Graph-convolution-based feature enhancement for more reliable Scene-aware pseudo-label Estimation (GSE). Meanwhile, the Label Refinement module, with its global-local Collaborative Learning (LCL) mechanism, addresses label noise by refining labels at both global and local levels, ensuring more robust weakly supervised learning. Extensive experimental evaluations demonstrate the effectiveness of the proposed method, achieving significant performance improvements over state-of-the-art approaches on the CUHK-SYSU and PRW datasets. Code is available at https://github.com/haichuntai/MSGM-main .
Published: 2026-01-03T09:25:59+00:00
Venue: International Journal of Computer Vision
Score: 0.512 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; De Cheng; Haichun Tai; Nannan Wang; Xiangqian Zhao; Jie Li; Xinbo Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02665-3"&gt;10.1007/s11263-025-02665-3&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.512 (consider)&lt;/p&gt;
&lt;p&gt;One-step Weakly Supervised Person Search (WSPS) addresses person detection and re-identification (ReID) within a unified framework, relying solely on pedestrian bounding box annotations for training, without requiring annotated identity labels. This approach enhances the practicality and efficiency of person search in real-world applications. However, WSPS faces two primary challenges: (1) the significant feature discrepancy between ReID and pedestrian detection tasks complicates shared representation learning, and (2) accurately estimating pseudo identity for each person image is challenging due to unrefined detections and significant intra-class variation in complex scenes. To address these challenges, we introduce a multi-granularity scene-aware graph convolution framework, which jointly optimizes task-specific features, improves pseudo-label estimation, and reduces the effects of label noise. Specifically, the Multi-granularity Feature Alignment (MFA) module in our designed two-branch network leverages bi-directional cluster-level interactions across multiple granularities to address the feature discrepancy. Building on MFA, we develop the Graph-convolution-based feature enhancement for more reliable Scene-aware pseudo-label Estimation (GSE). Meanwhile, the Label Refinement module, with its global-local Collaborative Learning (LCL) mechanism, addresses label noise by refining labels at both global and local levels, ensuring more robust weakly supervised learning. Extensive experimental evaluations demonstrate the effectiveness of the proposed method, achieving significant performance improvements over state-of-the-art approaches on the CUHK-SYSU and PRW datasets. Code is available at https://github.com/haichuntai/MSGM-main .&lt;/p&gt;</content:encoded></item><item><title>Robust and Generalizable Rumor Detection with Semantic Evolving Graph Masked Autoencoder</title><link>https://doi.org/10.1016/j.patcog.2025.112995</link><guid>10.1016/j.patcog.2025.112995</guid><pubDate>Fri, 02 Jan 2026 16:41:18 +0000</pubDate><dc:creator>Qiang Liu</dc:creator><dc:creator>Xiang Tao</dc:creator><dc:creator>Liang Wang</dc:creator><dc:creator>Shu Wu</dc:creator><dc:creator>Liang Wang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112995</prism:doi><description>Due to the rapid spread of rumors on social media, rumor detection has become an extremely important challenge. Recently, numerous rumor detection models that utilize textual information and the propagation structure of events have been proposed. However, these models overlook the importance of semantic evolving information of events in the propagation process, which is often challenging to truly learn in supervised training paradigms and conventional rumor detectors. This significantly limits the robustness and generalizability of rumor detectors. To address this issue, we propose a novel SE mantic E volving Graph M asked A uto E ncoder (SEE-GraphMAE) model in this paper. The model learns semantic evolving information of events by capturing local semantic changes and global semantic evolving information through specific graph masking and reconstruction strategies, which result in self-supervised objectives for model learning. By combining semantic evolving information and propagation structure information, the model achieves a comprehensive understanding of event propagation and performs accurate and robust detection, while also detecting rumors earlier by capturing semantic evolving information in the early stages. Meanwhile, with the help of graph masking and reconstruction objectives, we further propose a subgraph regularized test-time training strategy for bridging the gap between training and testing, and enhancing the rumor detector’s out-of-distribution generalization ability. Experimental results on several public datasets under both in-distribution and out-of-distribution settings demonstrate the superiority of our SEE-GraphMAE model over the state-of-the-art approaches in both overall performance and early rumor detection.
Published: 2026-01-02T16:41:18+00:00
Venue: Pattern Recognition
Score: 0.510 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qiang Liu; Xiang Tao; Liang Wang; Shu Wu; Liang Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112995"&gt;10.1016/j.patcog.2025.112995&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.510 (consider)&lt;/p&gt;
&lt;p&gt;Due to the rapid spread of rumors on social media, rumor detection has become an extremely important challenge. Recently, numerous rumor detection models that utilize textual information and the propagation structure of events have been proposed. However, these models overlook the importance of semantic evolving information of events in the propagation process, which is often challenging to truly learn in supervised training paradigms and conventional rumor detectors. This significantly limits the robustness and generalizability of rumor detectors. To address this issue, we propose a novel SE mantic E volving Graph M asked A uto E ncoder (SEE-GraphMAE) model in this paper. The model learns semantic evolving information of events by capturing local semantic changes and global semantic evolving information through specific graph masking and reconstruction strategies, which result in self-supervised objectives for model learning. By combining semantic evolving information and propagation structure information, the model achieves a comprehensive understanding of event propagation and performs accurate and robust detection, while also detecting rumors earlier by capturing semantic evolving information in the early stages. Meanwhile, with the help of graph masking and reconstruction objectives, we further propose a subgraph regularized test-time training strategy for bridging the gap between training and testing, and enhancing the rumor detector’s out-of-distribution generalization ability. Experimental results on several public datasets under both in-distribution and out-of-distribution settings demonstrate the superiority of our SEE-GraphMAE model over the state-of-the-art approaches in both overall performance and early rumor detection.&lt;/p&gt;</content:encoded></item><item><title>MoniRefer: A Real-world Large-scale Multi-modal Dataset based on Roadside Infrastructure for 3D Visual Grounding</title><link>https://arxiv.org/abs/2512.24605v1</link><guid>http://arxiv.org/abs/2512.24605v1</guid><pubDate>Wed, 31 Dec 2025 03:56:28 +0000</pubDate><dc:creator>Panquan Yang</dc:creator><dc:creator>Junfei Huang</dc:creator><dc:creator>Zongzhangbao Yin</dc:creator><dc:creator>Yingsong Hu</dc:creator><dc:creator>Anni Xu</dc:creator><dc:creator>Xinyi Luo</dc:creator><dc:creator>Xueqi Sun</dc:creator><dc:creator>Hai Wu</dc:creator><dc:creator>Sheng Ao</dc:creator><dc:creator>Zhaoxing Zhu</dc:creator><dc:creator>Chenglu Wen</dc:creator><dc:creator>Cheng Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>3D visual grounding aims to localize the object in 3D point cloud scenes that semantically corresponds to given natural language sentences. It is very critical for roadside infrastructure system to interpret natural languages and localize relevant target objects in complex traffic environments. However, most existing datasets and approaches for 3D visual grounding focus on the indoor and outdoor driving scenes, outdoor monitoring scenarios remain unexplored due to scarcity of paired point cloud-text data captured by roadside infrastructure sensors. In this paper, we introduce a novel task of 3D Visual Grounding for Outdoor Monitoring Scenarios, which enables infrastructure-level understanding of traffic scenes beyond the ego-vehicle perspective. To support this task, we construct MoniRefer, the first real-world large-scale multi-modal dataset for roadside-level 3D visual grounding. The dataset consists of about 136,018 objects with 411,128 natural language expressions collected from multiple complex traffic intersections in the real-world environments. To ensure the quality and accuracy of the dataset, we manually verified all linguistic descriptions and 3D labels for objects. Additionally, we also propose a new end-to-end method, named Moni3DVG, which utilizes the rich appearance information provided by images and geometry and optical information from point cloud for multi-modal feature learning and 3D object localization. Extensive experiments and ablation studies on the proposed benchmarks demonstrate the superiority and effectiveness of our method. Our dataset and code will be released.
Published: 2025-12-31T03:56:28+00:00
Venue: arXiv
Score: 0.507 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Panquan Yang; Junfei Huang; Zongzhangbao Yin; Yingsong Hu; Anni Xu; Xinyi Luo; Xueqi Sun; Hai Wu; Sheng Ao; Zhaoxing Zhu; Chenglu Wen; Cheng Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.507 (consider)&lt;/p&gt;
&lt;p&gt;3D visual grounding aims to localize the object in 3D point cloud scenes that semantically corresponds to given natural language sentences. It is very critical for roadside infrastructure system to interpret natural languages and localize relevant target objects in complex traffic environments. However, most existing datasets and approaches for 3D visual grounding focus on the indoor and outdoor driving scenes, outdoor monitoring scenarios remain unexplored due to scarcity of paired point cloud-text data captured by roadside infrastructure sensors. In this paper, we introduce a novel task of 3D Visual Grounding for Outdoor Monitoring Scenarios, which enables infrastructure-level understanding of traffic scenes beyond the ego-vehicle perspective. To support this task, we construct MoniRefer, the first real-world large-scale multi-modal dataset for roadside-level 3D visual grounding. The dataset consists of about 136,018 objects with 411,128 natural language expressions collected from multiple complex traffic intersections in the real-world environments. To ensure the quality and accuracy of the dataset, we manually verified all linguistic descriptions and 3D labels for objects. Additionally, we also propose a new end-to-end method, named Moni3DVG, which utilizes the rich appearance information provided by images and geometry and optical information from point cloud for multi-modal feature learning and 3D object localization. Extensive experiments and ablation studies on the proposed benchmarks demonstrate the superiority and effectiveness of our method. Our dataset and code will be released.&lt;/p&gt;</content:encoded></item><item><title>DGSSformer: Dynamically Global-aware Spatiotemporal Synchronous Transformer for Traffic Prediction</title><link>https://doi.org/10.1016/j.eswa.2025.131063</link><guid>10.1016/j.eswa.2025.131063</guid><pubDate>Fri, 02 Jan 2026 23:48:55 +0000</pubDate><dc:creator>Qingyong Zhang</dc:creator><dc:creator>Qian Shang</dc:creator><dc:creator>Quan Zhou</dc:creator><dc:creator>Mengpeng Yang</dc:creator><dc:creator>Bingrong Xu</dc:creator><dc:creator>Zhihui Yang</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.131063</prism:doi><description>Traffic prediction serves as a crucial component of the Intelligent Transportation System (ITS), playing a vital role in optimizing traffic resource allocation and travel scheduling. A fundamental challenge lies in effectively modeling the complex spatiotemporal patterns and dynamic heterogeneity of traffic data. Most existing studies focused on extracting features from temporal sequences and spatial topological structures independently, while overlooking the spatiotemporal coupling dependencies generated by the interaction between temporal and spatial dimensions and the informational complementarity between time series and spatial topology. In addition, although existing methods attempted spatiotemporal synchronous modeling to capture spatiotemporal features, such approaches faced two major challenges. On the one hand, extracting temporal and spatial features separately and then fusing them often leads to inconsistency in spatiotemporal semantics. On the other hand, reliance on local window–based synchronous modeling strategies results in delayed propagation of spatial dependencies, making it difficult to capture global cross-regional features and reducing responsiveness to sudden disturbances. To address these limitations, we propose a D ynamically G lobal-aware S patiotemporal S ynchronous Trans former ( DGSSformer ) to extract consistent spatiotemporal dependencies by constructing the coupling mechanism of spatiotemporal modeling. Specifically, traffic time-series information and graphs with correlation at nodes are taken as complementary inputs of heterogeneous information. Firstly, a global attention mechanism is introduced to enable correlation search across different features in the global temporal sequences, thereby avoiding the delayed transmission of local spatial information. Based on this, a spatiotemporal attention module is designed to align the spatiotemporal coupling features across heterogeneous information, mitigating semantic bias issues that arise in synchronous modeling and enhancing the model’s robustness against sudden disturbances. Experimental results on six real-world public traffic datasets demonstrate that the proposed model outperforms state-of-the-art methods while maintaining competitive computational efficiency.
Published: 2026-01-02T23:48:55+00:00
Venue: Expert Systems with Applications
Score: 0.506 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qingyong Zhang; Qian Shang; Quan Zhou; Mengpeng Yang; Bingrong Xu; Zhihui Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.131063"&gt;10.1016/j.eswa.2025.131063&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.506 (consider)&lt;/p&gt;
&lt;p&gt;Traffic prediction serves as a crucial component of the Intelligent Transportation System (ITS), playing a vital role in optimizing traffic resource allocation and travel scheduling. A fundamental challenge lies in effectively modeling the complex spatiotemporal patterns and dynamic heterogeneity of traffic data. Most existing studies focused on extracting features from temporal sequences and spatial topological structures independently, while overlooking the spatiotemporal coupling dependencies generated by the interaction between temporal and spatial dimensions and the informational complementarity between time series and spatial topology. In addition, although existing methods attempted spatiotemporal synchronous modeling to capture spatiotemporal features, such approaches faced two major challenges. On the one hand, extracting temporal and spatial features separately and then fusing them often leads to inconsistency in spatiotemporal semantics. On the other hand, reliance on local window–based synchronous modeling strategies results in delayed propagation of spatial dependencies, making it difficult to capture global cross-regional features and reducing responsiveness to sudden disturbances. To address these limitations, we propose a D ynamically G lobal-aware S patiotemporal S ynchronous Trans former ( DGSSformer ) to extract consistent spatiotemporal dependencies by constructing the coupling mechanism of spatiotemporal modeling. Specifically, traffic time-series information and graphs with correlation at nodes are taken as complementary inputs of heterogeneous information. Firstly, a global attention mechanism is introduced to enable correlation search across different features in the global temporal sequences, thereby avoiding the delayed transmission of local spatial information. Based on this, a spatiotemporal attention module is designed to align the spatiotemporal coupling features across heterogeneous information, mitigating semantic bias issues that arise in synchronous modeling and enhancing the model’s robustness against sudden disturbances. Experimental results on six real-world public traffic datasets demonstrate that the proposed model outperforms state-of-the-art methods while maintaining competitive computational efficiency.&lt;/p&gt;</content:encoded></item><item><title>Test-Time Adaptive Vision-Language Alignment for Zero-Shot Group Activity Recognition</title><link>https://doi.org/10.1016/j.patcog.2025.113033</link><guid>10.1016/j.patcog.2025.113033</guid><pubDate>Fri, 02 Jan 2026 16:03:03 +0000</pubDate><dc:creator>Runhao Zeng</dc:creator><dc:creator>Yirui Wang</dc:creator><dc:creator>Wenfu Peng</dc:creator><dc:creator>Xionglin Zhu</dc:creator><dc:creator>Ronghao Zhang</dc:creator><dc:creator>Zhihua Wang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.113033</prism:doi><description>Zero-shot group activity recognition (ZS-GAR) aims to identify activities unseen during training. However, conventional methods deploy models with parameters frozen at test time. This static nature prevents the model from adapting to the inherent distributional shift of unseen classes, severely impairing its generalization capability. To address this problem, we propose a test-time adaptation (TTA) framework that dynamically adapts the model during inference by employing two synergistic self-supervised mechanisms. First, an Actor-Drop Feature Augmentation strategy leverages group relational structure as a potent self-supervised signal by enforcing predictive consistency on samples where individuals are randomly masked. Second, our Label-Semantic Contrastive Learning mechanism generates pseudo-labels from high-confidence predictions and uses a dynamic memory bank, aligning features with their inferred semantic prototypes. This process not only enhances vision-language alignment for unseen classes but also demonstrates robustness against data corruptions, as validated on two new benchmarks, VD-C and CAD-C, featuring various corruption types. Extensive experiments on standard ZS-GAR benchmarks show our method significantly outperforms existing techniques, validating TTA’s effectiveness for this task.
Published: 2026-01-02T16:03:03+00:00
Venue: Pattern Recognition
Score: 0.501 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Runhao Zeng; Yirui Wang; Wenfu Peng; Xionglin Zhu; Ronghao Zhang; Zhihua Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.113033"&gt;10.1016/j.patcog.2025.113033&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.501 (consider)&lt;/p&gt;
&lt;p&gt;Zero-shot group activity recognition (ZS-GAR) aims to identify activities unseen during training. However, conventional methods deploy models with parameters frozen at test time. This static nature prevents the model from adapting to the inherent distributional shift of unseen classes, severely impairing its generalization capability. To address this problem, we propose a test-time adaptation (TTA) framework that dynamically adapts the model during inference by employing two synergistic self-supervised mechanisms. First, an Actor-Drop Feature Augmentation strategy leverages group relational structure as a potent self-supervised signal by enforcing predictive consistency on samples where individuals are randomly masked. Second, our Label-Semantic Contrastive Learning mechanism generates pseudo-labels from high-confidence predictions and uses a dynamic memory bank, aligning features with their inferred semantic prototypes. This process not only enhances vision-language alignment for unseen classes but also demonstrates robustness against data corruptions, as validated on two new benchmarks, VD-C and CAD-C, featuring various corruption types. Extensive experiments on standard ZS-GAR benchmarks show our method significantly outperforms existing techniques, validating TTA’s effectiveness for this task.&lt;/p&gt;</content:encoded></item><item><title>Few-Shot Harmful Meme Detection via Self-adaption Mixture-of-Experts</title><link>https://doi.org/10.1016/j.inffus.2026.104122</link><guid>10.1016/j.inffus.2026.104122</guid><pubDate>Sat, 03 Jan 2026 16:23:50 +0000</pubDate><dc:creator>Zou Li</dc:creator><dc:creator>Jinzhi Liao</dc:creator><dc:creator>Jiting Li</dc:creator><dc:creator>Ji Wang</dc:creator><dc:creator>Xiang Zhao</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104122</prism:doi><description>The automatic detection of harmful memes is essential for healthy online ecosystems but remains challenging due to the intricate interaction between visual and textual elements. Recently, the remarkable capabilities of multimodal large language models (MLLMs) have significantly enhanced the detection performance, yet scarce labeled data still limits their effectiveness. Although pioneering few-shot studies have explored this regime, they merely leverage surface-level capabilities while ignoring deeper complexities. To approach the core of the problem, we identify its notorious challenges: (1) heterogeneous multimodal features are complex and may exhibit negative correlations; (2) the semantic patterns underlying single modal are hard to uncover; and (3) the insufficient training samples render models more reliant on commonsense. To address the challenges, we propose a structural self-adaption mixture-of-experts framework (SSMoE) for few-shot harmful meme detection, including universal and specialized experts to foster more effective knowledge sharing, modal synergy, and expert specialization within the MLLM structure. Specifically, SSMoE integrates four novel components: (1) Semantic Data Clustering module aims to partition heterogeneous source data and mitigate negative transfer; (2) Targeted Prompt Injection module aims to employ a teacher model for providing cluster-specific external guidance; (3) Asymmetric Expert Specialization module aims to introduce shared and specialized experts for efficient parameter adaptation and knowledge specialization; and (4) Cluster-conditioned Routing module aims to dynamically direct inputs to the most relevant expert pathway based on semantic cluster identity. Extensive experiments on three benchmark datasets (FHM, MAMI, HarM) demonstrate that SSMoE significantly outperforms state-of-the-art baseline methods, particularly in extremely low-data scenarios.
Published: 2026-01-03T16:23:50+00:00
Venue: Information Fusion
Score: 0.500 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zou Li; Jinzhi Liao; Jiting Li; Ji Wang; Xiang Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104122"&gt;10.1016/j.inffus.2026.104122&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.500 (consider)&lt;/p&gt;
&lt;p&gt;The automatic detection of harmful memes is essential for healthy online ecosystems but remains challenging due to the intricate interaction between visual and textual elements. Recently, the remarkable capabilities of multimodal large language models (MLLMs) have significantly enhanced the detection performance, yet scarce labeled data still limits their effectiveness. Although pioneering few-shot studies have explored this regime, they merely leverage surface-level capabilities while ignoring deeper complexities. To approach the core of the problem, we identify its notorious challenges: (1) heterogeneous multimodal features are complex and may exhibit negative correlations; (2) the semantic patterns underlying single modal are hard to uncover; and (3) the insufficient training samples render models more reliant on commonsense. To address the challenges, we propose a structural self-adaption mixture-of-experts framework (SSMoE) for few-shot harmful meme detection, including universal and specialized experts to foster more effective knowledge sharing, modal synergy, and expert specialization within the MLLM structure. Specifically, SSMoE integrates four novel components: (1) Semantic Data Clustering module aims to partition heterogeneous source data and mitigate negative transfer; (2) Targeted Prompt Injection module aims to employ a teacher model for providing cluster-specific external guidance; (3) Asymmetric Expert Specialization module aims to introduce shared and specialized experts for efficient parameter adaptation and knowledge specialization; and (4) Cluster-conditioned Routing module aims to dynamically direct inputs to the most relevant expert pathway based on semantic cluster identity. Extensive experiments on three benchmark datasets (FHM, MAMI, HarM) demonstrate that SSMoE significantly outperforms state-of-the-art baseline methods, particularly in extremely low-data scenarios.&lt;/p&gt;</content:encoded></item><item><title>DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments</title><link>https://arxiv.org/abs/2512.24985v1</link><guid>http://arxiv.org/abs/2512.24985v1</guid><pubDate>Wed, 31 Dec 2025 17:31:29 +0000</pubDate><dc:creator>Yohan Park</dc:creator><dc:creator>Hyunwoo Ha</dc:creator><dc:creator>Wonjun Jo</dc:creator><dc:creator>Tae-Hyun Oh</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision Language Models (VLMs) are increasingly adopted as central reasoning modules for embodied agents. Existing benchmarks evaluate their capabilities under ideal, well-lit conditions, yet robust 24/7 operation demands performance under a wide range of visual degradations, including low-light conditions at night or in dark environments--a core necessity that has been largely overlooked. To address this underexplored challenge, we present DarkEQA, an open-source benchmark for evaluating EQA-relevant perceptual primitives under multi-level low-light conditions. DarkEQA isolates the perception bottleneck by evaluating question answering from egocentric observations under controlled degradations, enabling attributable robustness analysis. A key design feature of DarkEQA is its physical fidelity: visual degradations are modeled in linear RAW space, simulating physics-based illumination drop and sensor noise followed by an ISP-inspired rendering pipeline. We demonstrate the utility of DarkEQA by evaluating a wide range of state-of-the-art VLMs and Low-Light Image Enhancement (LLIE) models. Our analysis systematically reveals VLMs' limitations when operating under these challenging visual conditions. Our code and benchmark dataset will be released upon acceptance.
Published: 2025-12-31T17:31:29+00:00
Venue: arXiv
Score: 0.500 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yohan Park; Hyunwoo Ha; Wonjun Jo; Tae-Hyun Oh&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.500 (consider)&lt;/p&gt;
&lt;p&gt;Vision Language Models (VLMs) are increasingly adopted as central reasoning modules for embodied agents. Existing benchmarks evaluate their capabilities under ideal, well-lit conditions, yet robust 24/7 operation demands performance under a wide range of visual degradations, including low-light conditions at night or in dark environments--a core necessity that has been largely overlooked. To address this underexplored challenge, we present DarkEQA, an open-source benchmark for evaluating EQA-relevant perceptual primitives under multi-level low-light conditions. DarkEQA isolates the perception bottleneck by evaluating question answering from egocentric observations under controlled degradations, enabling attributable robustness analysis. A key design feature of DarkEQA is its physical fidelity: visual degradations are modeled in linear RAW space, simulating physics-based illumination drop and sensor noise followed by an ISP-inspired rendering pipeline. We demonstrate the utility of DarkEQA by evaluating a wide range of state-of-the-art VLMs and Low-Light Image Enhancement (LLIE) models. Our analysis systematically reveals VLMs&amp;#x27; limitations when operating under these challenging visual conditions. Our code and benchmark dataset will be released upon acceptance.&lt;/p&gt;</content:encoded></item><item><title>VIPER: Process-aware Evaluation for Generative Video Reasoning</title><link>https://arxiv.org/abs/2512.24952v1</link><guid>http://arxiv.org/abs/2512.24952v1</guid><pubDate>Wed, 31 Dec 2025 16:31:59 +0000</pubDate><dc:creator>Yifan Li</dc:creator><dc:creator>Yukai Gu</dc:creator><dc:creator>Yingqian Min</dc:creator><dc:creator>Zikang Liu</dc:creator><dc:creator>Yifan Du</dc:creator><dc:creator>Kun Zhou</dc:creator><dc:creator>Min Yang</dc:creator><dc:creator>Wayne Xin Zhao</dc:creator><dc:creator>Minghui Qiu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent breakthroughs in video generation have demonstrated an emerging capability termed Chain-of-Frames (CoF) reasoning, where models resolve complex tasks through the generation of continuous frames. While these models show promise for Generative Video Reasoning (GVR), existing evaluation frameworks often rely on single-frame assessments, which can lead to outcome-hacking, where a model reaches a correct conclusion through an erroneous process. To address this, we propose a process-aware evaluation paradigm. We introduce VIPER, a comprehensive benchmark spanning 16 tasks across temporal, structural, symbolic, spatial, physics, and planning reasoning. Furthermore, we propose Process-outcome Consistency (POC@r), a new metric that utilizes VLM-as-Judge with a hierarchical rubric to evaluate both the validity of the intermediate steps and the final result. Our experiments reveal that state-of-the-art video models achieve only about 20% POC@1.0 and exhibit a significant outcome-hacking. We further explore the impact of test-time scaling and sampling robustness, highlighting a substantial gap between current video generation and true generalized visual reasoning. Our benchmark will be publicly released.
Published: 2025-12-31T16:31:59+00:00
Venue: arXiv
Score: 0.499 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yifan Li; Yukai Gu; Yingqian Min; Zikang Liu; Yifan Du; Kun Zhou; Min Yang; Wayne Xin Zhao; Minghui Qiu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.499 (consider)&lt;/p&gt;
&lt;p&gt;Recent breakthroughs in video generation have demonstrated an emerging capability termed Chain-of-Frames (CoF) reasoning, where models resolve complex tasks through the generation of continuous frames. While these models show promise for Generative Video Reasoning (GVR), existing evaluation frameworks often rely on single-frame assessments, which can lead to outcome-hacking, where a model reaches a correct conclusion through an erroneous process. To address this, we propose a process-aware evaluation paradigm. We introduce VIPER, a comprehensive benchmark spanning 16 tasks across temporal, structural, symbolic, spatial, physics, and planning reasoning. Furthermore, we propose Process-outcome Consistency (POC@r), a new metric that utilizes VLM-as-Judge with a hierarchical rubric to evaluate both the validity of the intermediate steps and the final result. Our experiments reveal that state-of-the-art video models achieve only about 20% POC@1.0 and exhibit a significant outcome-hacking. We further explore the impact of test-time scaling and sampling robustness, highlighting a substantial gap between current video generation and true generalized visual reasoning. Our benchmark will be publicly released.&lt;/p&gt;</content:encoded></item><item><title>Video and Language Alignment in 2D Systems for 3D Multi-object Scenes with Multi-Information Derivative-Free Control</title><link>https://arxiv.org/abs/2512.24826v1</link><guid>http://arxiv.org/abs/2512.24826v1</guid><pubDate>Wed, 31 Dec 2025 12:39:03 +0000</pubDate><dc:creator>Jason Armitage</dc:creator><dc:creator>Rico Sennnrich</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Cross-modal systems trained on 2D visual inputs are presented with a dimensional shift when processing 3D scenes. An in-scene camera bridges the dimensionality gap but requires learning a control module. We introduce a new method that improves multivariate mutual information estimates by regret minimisation with derivative-free optimisation. Our algorithm enables off-the-shelf cross-modal systems trained on 2D visual inputs to adapt online to object occlusions and differentiate features. The pairing of expressive measures and value-based optimisation assists control of an in-scene camera to learn directly from the noisy outputs of vision-language models. The resulting pipeline improves performance in cross-modal tasks on multi-object 3D scenes without resorting to pretraining or finetuning.
Published: 2025-12-31T12:39:03+00:00
Venue: arXiv
Score: 0.499 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jason Armitage; Rico Sennnrich&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.499 (consider)&lt;/p&gt;
&lt;p&gt;Cross-modal systems trained on 2D visual inputs are presented with a dimensional shift when processing 3D scenes. An in-scene camera bridges the dimensionality gap but requires learning a control module. We introduce a new method that improves multivariate mutual information estimates by regret minimisation with derivative-free optimisation. Our algorithm enables off-the-shelf cross-modal systems trained on 2D visual inputs to adapt online to object occlusions and differentiate features. The pairing of expressive measures and value-based optimisation assists control of an in-scene camera to learn directly from the noisy outputs of vision-language models. The resulting pipeline improves performance in cross-modal tasks on multi-object 3D scenes without resorting to pretraining or finetuning.&lt;/p&gt;</content:encoded></item><item><title>A Hierarchical Information Policy Fusion Framework with Multimodal Large Language Models for Autonomous Guidewire Navigation in Endovascular Procedures</title><link>https://doi.org/10.1016/j.inffus.2025.104115</link><guid>10.1016/j.inffus.2025.104115</guid><pubDate>Sat, 03 Jan 2026 15:53:25 +0000</pubDate><dc:creator>Haoyu Wang</dc:creator><dc:creator>Taylor Yiu</dc:creator><dc:creator>Sijia Li</dc:creator><dc:creator>Ka Gao</dc:creator><dc:creator>Hangling Sun</dc:creator><dc:creator>Chenyu Zhou</dc:creator><dc:creator>Anji Li</dc:creator><dc:creator>Qiangqiang Fu</dc:creator><dc:creator>Yu Wang</dc:creator><dc:creator>Bin Chen</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104115</prism:doi><description>Robotic-assisted endovascular interventions promise to transform cardiovascular therapy by improving procedural precision and minimizing cardiologists’ exposure to occupational risks. However, current systems are limited by their reliance on manual control and lack of adaptability to complex vascular anatomies. To address these challenges, we propose a novel H ierarchical A utonomous G uidewire N avigation and D elivery ( HAG-ND ) framework that leverages the strengths of multimodal large language models (MLLMs) and a novel reinforcement learning module inspired by Deep Q-Networks (DQNs). The high-level MLLM is trained on diverse blood vessel and guidewire scenarios from various angles and positions, enabling it to assess the suitability and timing of substance release at the target location. Within the MLLM, a parliamentary mechanism is introduced, where multiple specialized models, each focusing on a specific aspect of the vascular environment, vote on the optimal course of action. The low-level reinforcement learning module focuses on optimizing autonomous guidewire navigation to the designated target site by learning from the rich semantic understanding provided by the MLLM. Experimental evaluations demonstrate that the HAG-ND framework significantly improves the accuracy and reliability of guidewire positioning and targeted delivery compared to existing methods. By harnessing the complementary capabilities of MLLMs and novel reinforcement learning techniques in a hierarchical architecture, HAG-ND represents a significant step towards fully autonomous and adaptive robotic-assisted endovascular interventions.
Published: 2026-01-03T15:53:25+00:00
Venue: Information Fusion
Score: 0.498 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haoyu Wang; Taylor Yiu; Sijia Li; Ka Gao; Hangling Sun; Chenyu Zhou; Anji Li; Qiangqiang Fu; Yu Wang; Bin Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104115"&gt;10.1016/j.inffus.2025.104115&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.498 (consider)&lt;/p&gt;
&lt;p&gt;Robotic-assisted endovascular interventions promise to transform cardiovascular therapy by improving procedural precision and minimizing cardiologists’ exposure to occupational risks. However, current systems are limited by their reliance on manual control and lack of adaptability to complex vascular anatomies. To address these challenges, we propose a novel H ierarchical A utonomous G uidewire N avigation and D elivery ( HAG-ND ) framework that leverages the strengths of multimodal large language models (MLLMs) and a novel reinforcement learning module inspired by Deep Q-Networks (DQNs). The high-level MLLM is trained on diverse blood vessel and guidewire scenarios from various angles and positions, enabling it to assess the suitability and timing of substance release at the target location. Within the MLLM, a parliamentary mechanism is introduced, where multiple specialized models, each focusing on a specific aspect of the vascular environment, vote on the optimal course of action. The low-level reinforcement learning module focuses on optimizing autonomous guidewire navigation to the designated target site by learning from the rich semantic understanding provided by the MLLM. Experimental evaluations demonstrate that the HAG-ND framework significantly improves the accuracy and reliability of guidewire positioning and targeted delivery compared to existing methods. By harnessing the complementary capabilities of MLLMs and novel reinforcement learning techniques in a hierarchical architecture, HAG-ND represents a significant step towards fully autonomous and adaptive robotic-assisted endovascular interventions.&lt;/p&gt;</content:encoded></item><item><title>OFL-SAM2: Prompt SAM2 with Online Few-shot Learner for Efficient Medical Image Segmentation</title><link>https://arxiv.org/abs/2512.24861v1</link><guid>http://arxiv.org/abs/2512.24861v1</guid><pubDate>Wed, 31 Dec 2025 13:41:16 +0000</pubDate><dc:creator>Meng Lan</dc:creator><dc:creator>Lefei Zhang</dc:creator><dc:creator>Xiaomeng Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The Segment Anything Model 2 (SAM2) has demonstrated remarkable promptable visual segmentation capabilities in video data, showing potential for extension to medical image segmentation (MIS) tasks involving 3D volumes and temporally correlated 2D image sequences. However, adapting SAM2 to MIS presents several challenges, including the need for extensive annotated medical data for fine-tuning and high-quality manual prompts, which are both labor-intensive and require intervention from medical experts. To address these challenges, we introduce OFL-SAM2, a prompt-free SAM2 framework for label-efficient MIS. Our core idea is to leverage limited annotated samples to train a lightweight mapping network that captures medical knowledge and transforms generic image features into target features, thereby providing additional discriminative target representations for each frame and eliminating the need for manual prompts. Crucially, the mapping network supports online parameter update during inference, enhancing the model's generalization across test sequences. Technically, we introduce two key components: (1) an online few-shot learner that trains the mapping network to generate target features using limited data, and (2) an adaptive fusion module that dynamically integrates the target features with the memory-attention features generated by frozen SAM2, leading to accurate and robust target representation. Extensive experiments on three diverse MIS datasets demonstrate that OFL-SAM2 achieves state-of-the-art performance with limited training data.
Published: 2025-12-31T13:41:16+00:00
Venue: arXiv
Score: 0.497 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Meng Lan; Lefei Zhang; Xiaomeng Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.497 (consider)&lt;/p&gt;
&lt;p&gt;The Segment Anything Model 2 (SAM2) has demonstrated remarkable promptable visual segmentation capabilities in video data, showing potential for extension to medical image segmentation (MIS) tasks involving 3D volumes and temporally correlated 2D image sequences. However, adapting SAM2 to MIS presents several challenges, including the need for extensive annotated medical data for fine-tuning and high-quality manual prompts, which are both labor-intensive and require intervention from medical experts. To address these challenges, we introduce OFL-SAM2, a prompt-free SAM2 framework for label-efficient MIS. Our core idea is to leverage limited annotated samples to train a lightweight mapping network that captures medical knowledge and transforms generic image features into target features, thereby providing additional discriminative target representations for each frame and eliminating the need for manual prompts. Crucially, the mapping network supports online parameter update during inference, enhancing the model&amp;#x27;s generalization across test sequences. Technically, we introduce two key components: (1) an online few-shot learner that trains the mapping network to generate target features using limited data, and (2) an adaptive fusion module that dynamically integrates the target features with the memory-attention features generated by frozen SAM2, leading to accurate and robust target representation. Extensive experiments on three diverse MIS datasets demonstrate that OFL-SAM2 achieves state-of-the-art performance with limited training data.&lt;/p&gt;</content:encoded></item><item><title>RankSAM: Lightweight adapters and prompt generation in zero-shot semantic segmentation</title><link>https://doi.org/10.1016/j.neucom.2025.132594</link><guid>10.1016/j.neucom.2025.132594</guid><pubDate>Fri, 02 Jan 2026 16:07:19 +0000</pubDate><dc:creator>Yue Zhuo</dc:creator><dc:creator>Zhaocheng Xu</dc:creator><dc:creator>Di Zhou</dc:creator><dc:creator>Pengpeng Xu</dc:creator><dc:creator>Yan Tian</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132594</prism:doi><description>Zero-shot segmentation plays a crucial role in neurocomputing, such as embodied intelligence systems and autonomous driving technologies. However, current approaches struggle to preserve the intrinsic generalization ability of SAM as input quality declines. In addition, prompt generation still faces an embarrassment in the balance between effectiveness and efficiency. Motivated by low-rank adaptation (LoRA), we design RankSAM, which integrates slim, adaptable modules into the middle layers of the frozen SAM framework. These modules dynamically fine-tune the operational rank of their weight matrices in response to input data, leveraging a trainable gating mechanism to selectively activate specific (rank-1) matrix components as needed. In addition, a learnable prompt predictor is designed to learn and generate prompt confidence maps and point prompts, and any remaining prompts that would produce the same mask are filtered out to enhance efficiency in prompt generation. The experimental results on multiple datasets indicate that our approach improves the mean intersection over union (mIoU) by a margin of 2.5%–2.8% compared to the prevailing approaches. Project page: https://messeyamumu.github.io/RankSAM .
Published: 2026-01-02T16:07:19+00:00
Venue: Neurocomputing
Score: 0.496 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yue Zhuo; Zhaocheng Xu; Di Zhou; Pengpeng Xu; Yan Tian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132594"&gt;10.1016/j.neucom.2025.132594&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.496 (consider)&lt;/p&gt;
&lt;p&gt;Zero-shot segmentation plays a crucial role in neurocomputing, such as embodied intelligence systems and autonomous driving technologies. However, current approaches struggle to preserve the intrinsic generalization ability of SAM as input quality declines. In addition, prompt generation still faces an embarrassment in the balance between effectiveness and efficiency. Motivated by low-rank adaptation (LoRA), we design RankSAM, which integrates slim, adaptable modules into the middle layers of the frozen SAM framework. These modules dynamically fine-tune the operational rank of their weight matrices in response to input data, leveraging a trainable gating mechanism to selectively activate specific (rank-1) matrix components as needed. In addition, a learnable prompt predictor is designed to learn and generate prompt confidence maps and point prompts, and any remaining prompts that would produce the same mask are filtered out to enhance efficiency in prompt generation. The experimental results on multiple datasets indicate that our approach improves the mean intersection over union (mIoU) by a margin of 2.5%–2.8% compared to the prevailing approaches. Project page: https://messeyamumu.github.io/RankSAM .&lt;/p&gt;</content:encoded></item><item><title>3D Semantic Segmentation for Post-Disaster Assessment</title><link>https://arxiv.org/abs/2512.24593v1</link><guid>http://arxiv.org/abs/2512.24593v1</guid><pubDate>Wed, 31 Dec 2025 03:30:58 +0000</pubDate><dc:creator>Nhut Le</dc:creator><dc:creator>Maryam Rahnemoonfar</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The increasing frequency of natural disasters poses severe threats to human lives and leads to substantial economic losses. While 3D semantic segmentation is crucial for post-disaster assessment, existing deep learning models lack datasets specifically designed for post-disaster environments. To address this gap, we constructed a specialized 3D dataset using unmanned aerial vehicles (UAVs)-captured aerial footage of Hurricane Ian (2022) over affected areas, employing Structure-from-Motion (SfM) and Multi-View Stereo (MVS) techniques to reconstruct 3D point clouds. We evaluated the state-of-the-art (SOTA) 3D semantic segmentation models, Fast Point Transformer (FPT), Point Transformer v3 (PTv3), and OA-CNNs on this dataset, exposing significant limitations in existing methods for disaster-stricken regions. These findings underscore the urgent need for advancements in 3D segmentation techniques and the development of specialized 3D benchmark datasets to improve post-disaster scene understanding and response.
Published: 2025-12-31T03:30:58+00:00
Venue: arXiv
Score: 0.495 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nhut Le; Maryam Rahnemoonfar&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.495 (consider)&lt;/p&gt;
&lt;p&gt;The increasing frequency of natural disasters poses severe threats to human lives and leads to substantial economic losses. While 3D semantic segmentation is crucial for post-disaster assessment, existing deep learning models lack datasets specifically designed for post-disaster environments. To address this gap, we constructed a specialized 3D dataset using unmanned aerial vehicles (UAVs)-captured aerial footage of Hurricane Ian (2022) over affected areas, employing Structure-from-Motion (SfM) and Multi-View Stereo (MVS) techniques to reconstruct 3D point clouds. We evaluated the state-of-the-art (SOTA) 3D semantic segmentation models, Fast Point Transformer (FPT), Point Transformer v3 (PTv3), and OA-CNNs on this dataset, exposing significant limitations in existing methods for disaster-stricken regions. These findings underscore the urgent need for advancements in 3D segmentation techniques and the development of specialized 3D benchmark datasets to improve post-disaster scene understanding and response.&lt;/p&gt;</content:encoded></item><item><title>Enhancing Semi-supervised Medical Image Segmentation via Semantic Transfer</title><link>https://doi.org/10.1016/j.patcog.2026.113039</link><guid>10.1016/j.patcog.2026.113039</guid><pubDate>Sat, 03 Jan 2026 16:19:14 +0000</pubDate><dc:creator>Shiyuan Huang</dc:creator><dc:creator>Shudong Wang</dc:creator><dc:creator>Kuijie Zhang</dc:creator><dc:creator>Wenhao Wu</dc:creator><dc:creator>Yingye Liu</dc:creator><dc:creator>Tiyao Liu</dc:creator><dc:creator>Shanchen Pang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113039</prism:doi><description>Semi-supervised learning has gained increasing attention in medical image segmentation due to its ability to alleviate the reliance on large-scale expert annotations. However, many existing SSL approaches focus on generic consistency constraints while lacking explicit mechanisms for semantic transfer between labeled and unlabeled data, limiting their effectiveness in regions with ambiguous or low-confidence predictions. To address this challenge, we propose STLU-Net, a dual-stream semi-supervised framework enhancing semantic interaction between labeled and unlabeled data via a fine-grained feature mixing module. This module performs channel-wise cross-sample fusion guided by feature similarity, encouraging the learning of transferable deep semantics while introducing controlled perturbations. Dual-stream supervision with structured feature perturbation penalizes predictions lacking consistent semantic support, mitigating confirmation bias on unlabeled data. Extensive experiments on multiple 3D medical image segmentation benchmarks demonstrate that STLU-Net achieves superior performance under limited supervision. Further analysis confirms that our method effectively extracts rich and generalizable semantic representations from limited annotations through hierarchical feature coordination, leading to notable performance gains in semi-supervised segmentation. Code is available at: https://github.com/Shiyuan-H/STLU-Net .
Published: 2026-01-03T16:19:14+00:00
Venue: Pattern Recognition
Score: 0.493 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shiyuan Huang; Shudong Wang; Kuijie Zhang; Wenhao Wu; Yingye Liu; Tiyao Liu; Shanchen Pang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113039"&gt;10.1016/j.patcog.2026.113039&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.493 (consider)&lt;/p&gt;
&lt;p&gt;Semi-supervised learning has gained increasing attention in medical image segmentation due to its ability to alleviate the reliance on large-scale expert annotations. However, many existing SSL approaches focus on generic consistency constraints while lacking explicit mechanisms for semantic transfer between labeled and unlabeled data, limiting their effectiveness in regions with ambiguous or low-confidence predictions. To address this challenge, we propose STLU-Net, a dual-stream semi-supervised framework enhancing semantic interaction between labeled and unlabeled data via a fine-grained feature mixing module. This module performs channel-wise cross-sample fusion guided by feature similarity, encouraging the learning of transferable deep semantics while introducing controlled perturbations. Dual-stream supervision with structured feature perturbation penalizes predictions lacking consistent semantic support, mitigating confirmation bias on unlabeled data. Extensive experiments on multiple 3D medical image segmentation benchmarks demonstrate that STLU-Net achieves superior performance under limited supervision. Further analysis confirms that our method effectively extracts rich and generalizable semantic representations from limited annotations through hierarchical feature coordination, leading to notable performance gains in semi-supervised segmentation. Code is available at: https://github.com/Shiyuan-H/STLU-Net .&lt;/p&gt;</content:encoded></item><item><title>EPSO-Net: A Multi-Objective Evolutionary Neural Architecture Search with PSO-Guided Mutation Fusion for Explainable Brain Tumor Segmentation</title><link>https://doi.org/10.1016/j.inffus.2025.104119</link><guid>10.1016/j.inffus.2025.104119</guid><pubDate>Sat, 03 Jan 2026 07:35:20 +0000</pubDate><dc:creator>Farhana Yasmin</dc:creator><dc:creator>Yu Xue</dc:creator><dc:creator>Mahade Hasan</dc:creator><dc:creator>Ghulam Muhammad</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104119</prism:doi><description>Accurate brain tumor segmentation from magnetic resonance imaging (MRI) remains a significant challenge due to early loss of spatial detail, inadequate contextual representation, and ineffective decoder fusion. In this paper, we propose EPSO-Net, a multi-objective evolutionary neural architecture search (NAS) framework that integrates three specialized modules: UTSA for preserving spatial encoding and enhancing low-level feature representation, Astra for capturing semantic abstraction and multi-scale context, and Revo for improving decoder refinement through attention-guided fusion of feature maps. These modules work synergistically within a flexible modular 3D search space, enabling dynamic architecture optimization during the evolutionary process. EPSO-Net utilizes a particle swarm optimization (PSO)-guided mutation fusion mechanism that enables efficient exploration of the search space, adjusting mutation behavior based on performance feedback. To the best of our knowledge, this is the first multi-objective evolutionary NAS framework employing PSO-guided mutation fusion to adapt mutation strategies, driving the search towards optimal solutions in a resource-efficient manner. Experiments on the BraTS 2021, BraTS 2020, and MSD Brain Tumor datasets demonstrate that EPSO-Net outperforms nine state-of-the-art methods, achieving high dice similarity coefficients (DSC) of 93.89%, 95.02%, and 91.25%, low Hausdorff distance (HD95) of 1.14 mm, 1.02 mm, and 1.44 mm, and strong Grad-CAM IoU (GIoU) of 89.32%, 90.12%, and 85.68%, respectively. EPSO-Net also demonstrates reliable generalization to the CHAOS, PROMISE12, and ACDC datasets. Furthermore, it significantly reduces model complexity, lowers FLOPS, accelerates inference, and enhances interpretability. The full code will be publicly available at: https://github.com/Farhana005/EPSO-Net .
Published: 2026-01-03T07:35:20+00:00
Venue: Information Fusion
Score: 0.493 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Farhana Yasmin; Yu Xue; Mahade Hasan; Ghulam Muhammad&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104119"&gt;10.1016/j.inffus.2025.104119&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.493 (consider)&lt;/p&gt;
&lt;p&gt;Accurate brain tumor segmentation from magnetic resonance imaging (MRI) remains a significant challenge due to early loss of spatial detail, inadequate contextual representation, and ineffective decoder fusion. In this paper, we propose EPSO-Net, a multi-objective evolutionary neural architecture search (NAS) framework that integrates three specialized modules: UTSA for preserving spatial encoding and enhancing low-level feature representation, Astra for capturing semantic abstraction and multi-scale context, and Revo for improving decoder refinement through attention-guided fusion of feature maps. These modules work synergistically within a flexible modular 3D search space, enabling dynamic architecture optimization during the evolutionary process. EPSO-Net utilizes a particle swarm optimization (PSO)-guided mutation fusion mechanism that enables efficient exploration of the search space, adjusting mutation behavior based on performance feedback. To the best of our knowledge, this is the first multi-objective evolutionary NAS framework employing PSO-guided mutation fusion to adapt mutation strategies, driving the search towards optimal solutions in a resource-efficient manner. Experiments on the BraTS 2021, BraTS 2020, and MSD Brain Tumor datasets demonstrate that EPSO-Net outperforms nine state-of-the-art methods, achieving high dice similarity coefficients (DSC) of 93.89%, 95.02%, and 91.25%, low Hausdorff distance (HD95) of 1.14 mm, 1.02 mm, and 1.44 mm, and strong Grad-CAM IoU (GIoU) of 89.32%, 90.12%, and 85.68%, respectively. EPSO-Net also demonstrates reliable generalization to the CHAOS, PROMISE12, and ACDC datasets. Furthermore, it significantly reduces model complexity, lowers FLOPS, accelerates inference, and enhances interpretability. The full code will be publicly available at: https://github.com/Farhana005/EPSO-Net .&lt;/p&gt;</content:encoded></item><item><title>LCF3D: A Robust and Real-Time Late-Cascade Fusion Framework for 3D Object Detection in Autonomous Driving</title><link>https://doi.org/10.1016/j.patcog.2026.113046</link><guid>10.1016/j.patcog.2026.113046</guid><pubDate>Sat, 03 Jan 2026 23:24:25 +0000</pubDate><dc:creator>Carlo Sgaravatti</dc:creator><dc:creator>Riccardo Pieroni</dc:creator><dc:creator>Matteo Corno</dc:creator><dc:creator>Sergio M. Savaresi</dc:creator><dc:creator>Luca Magri</dc:creator><dc:creator>Giacomo Boracchi</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113046</prism:doi><description>Accurately localizing 3D objects like pedestrians, cyclists, and other vehicles is essential in Autonomous Driving. To ensure high detection performance, Autonomous Vehicles complement RGB cameras with LiDAR sensors, but effectively combining these data sources for 3D object detection remains challenging. We propose LCF3D, a novel sensor fusion framework that combines a 2D object detector on RGB images with a 3D object detector on LiDAR point clouds. By leveraging multimodal fusion principles, we compensate for inaccuracies in the LiDAR object detection network. Our solution combines two key principles: (i) late fusion , to reduce LiDAR False Positives by matching LiDAR 3D detections with RGB 2D detections and filtering out unmatched LiDAR detections; and (ii) cascade fusion , to recover missed objects from LiDAR by generating new 3D frustum proposals corresponding to unmatched RGB detections. Experiments show that LCF3D is beneficial for domain generalization, as it turns out to be successful in handling different sensor configurations between training and testing domains. LCF3D achieves significant improvements over LiDAR-based methods, particularly for challenging categories like pedestrians and cyclists in the KITTI dataset, as well as motorcycles and bicycles in nuScenes. Code can be downloaded from: https://github.com/CarloSgaravatti/LCF3D .
Published: 2026-01-03T23:24:25+00:00
Venue: Pattern Recognition
Score: 0.493 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Carlo Sgaravatti; Riccardo Pieroni; Matteo Corno; Sergio M. Savaresi; Luca Magri; Giacomo Boracchi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113046"&gt;10.1016/j.patcog.2026.113046&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.493 (consider)&lt;/p&gt;
&lt;p&gt;Accurately localizing 3D objects like pedestrians, cyclists, and other vehicles is essential in Autonomous Driving. To ensure high detection performance, Autonomous Vehicles complement RGB cameras with LiDAR sensors, but effectively combining these data sources for 3D object detection remains challenging. We propose LCF3D, a novel sensor fusion framework that combines a 2D object detector on RGB images with a 3D object detector on LiDAR point clouds. By leveraging multimodal fusion principles, we compensate for inaccuracies in the LiDAR object detection network. Our solution combines two key principles: (i) late fusion , to reduce LiDAR False Positives by matching LiDAR 3D detections with RGB 2D detections and filtering out unmatched LiDAR detections; and (ii) cascade fusion , to recover missed objects from LiDAR by generating new 3D frustum proposals corresponding to unmatched RGB detections. Experiments show that LCF3D is beneficial for domain generalization, as it turns out to be successful in handling different sensor configurations between training and testing domains. LCF3D achieves significant improvements over LiDAR-based methods, particularly for challenging categories like pedestrians and cyclists in the KITTI dataset, as well as motorcycles and bicycles in nuScenes. Code can be downloaded from: https://github.com/CarloSgaravatti/LCF3D .&lt;/p&gt;</content:encoded></item><item><title>AI3D: Multimodal Verification System against Projective Attacks for Deep Learning Classifiers</title><link>https://doi.org/10.1016/j.patcog.2025.113036</link><guid>10.1016/j.patcog.2025.113036</guid><pubDate>Sat, 03 Jan 2026 16:19:14 +0000</pubDate><dc:creator>Imen Smati</dc:creator><dc:creator>Rania Khalsi</dc:creator><dc:creator>Faouzi Ghorbel</dc:creator><dc:creator>Mallek Mziou-Sallami</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.113036</prism:doi><description>Deep-learning verification tools primarily focus on handling image-type data, certifying the robustness of image classifiers against perturbations such as basic contrast, FGSM noise, and L ∞ . However, achieving geometric certification remains challenging, with limited studies devoted to this area. The robustness of neural network classifiers applied to contour-type data is a critical concern. To overcome this deficiency, our paper proposes an innovative formalism of Lower and Upper Bounds (LB and UB) for assessing the robustness of Deep Neural Networks (DNNs) against a broader range of geometric ’physical world’ attacks caused by a projective transformation. We extend the certification framework to a multimodal system that can certify the vision system with respect to both image and contour data. This tool for deep contour classifiers is compatible with any well-embedded 2D contour. As a preprocessing step, we focus on two arc-length reparameterization techniques and evaluate their impact on the model’s performance. Experiments were performed on MNIST and MPEG-7 datasets and then extended to the Swedish Leaf dataset as an application in leaf health classification. The results obtained serve as empirical evidence that affirms the crucial importance of incorporating diverse modalities into the certification system. The code for the proposed multimodal system is available on GitHub at the following link: https://github.com/ImenSmatiENSI/AI3D_Multimodal_system .
Published: 2026-01-03T16:19:14+00:00
Venue: Pattern Recognition
Score: 0.491 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Imen Smati; Rania Khalsi; Faouzi Ghorbel; Mallek Mziou-Sallami&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.113036"&gt;10.1016/j.patcog.2025.113036&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.491 (consider)&lt;/p&gt;
&lt;p&gt;Deep-learning verification tools primarily focus on handling image-type data, certifying the robustness of image classifiers against perturbations such as basic contrast, FGSM noise, and L ∞ . However, achieving geometric certification remains challenging, with limited studies devoted to this area. The robustness of neural network classifiers applied to contour-type data is a critical concern. To overcome this deficiency, our paper proposes an innovative formalism of Lower and Upper Bounds (LB and UB) for assessing the robustness of Deep Neural Networks (DNNs) against a broader range of geometric ’physical world’ attacks caused by a projective transformation. We extend the certification framework to a multimodal system that can certify the vision system with respect to both image and contour data. This tool for deep contour classifiers is compatible with any well-embedded 2D contour. As a preprocessing step, we focus on two arc-length reparameterization techniques and evaluate their impact on the model’s performance. Experiments were performed on MNIST and MPEG-7 datasets and then extended to the Swedish Leaf dataset as an application in leaf health classification. The results obtained serve as empirical evidence that affirms the crucial importance of incorporating diverse modalities into the certification system. The code for the proposed multimodal system is available on GitHub at the following link: https://github.com/ImenSmatiENSI/AI3D_Multimodal_system .&lt;/p&gt;</content:encoded></item><item><title>IGCDet: Independence Guided Co-Training for Sparsely Annotated Object Detection</title><link>https://doi.org/10.1016/j.knosys.2025.115217</link><guid>10.1016/j.knosys.2025.115217</guid><pubDate>Fri, 02 Jan 2026 07:39:05 +0000</pubDate><dc:creator>Jian-Xun Mi</dc:creator><dc:creator>Jiahui Feng</dc:creator><dc:creator>Haiyang Wang</dc:creator><dc:creator>Yanjun Wu</dc:creator><dc:creator>Ranzhi Zhao</dc:creator><dc:creator>Chang Liu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115217</prism:doi><description>Object detection models can achieve excellent detection performance with fully annotated instances. However, requiring complete annotations for every dataset is impractical due to high labor and time costs, as well as the inevitable occurrence of missing annotations. As a result, the absence of annotations can potentially provide misleading supervision and harm the training process. Recent methodologies have achieved remarkable effectiveness through the application of Co-Mining. However, the independence of each branch in Co-Mining cannot be guaranteed, overlooking valuable information during multi-perspective training. To address this issue, we introduce an Independence Guided Co-Training Model (IGCDet) that leverages Image Independence Decomposition to ensure the independence of each co-training branch. This model aims to capture diverse perspectives from images as extensively as possible, identifying missing annotations and incorporating them as positive supervision in the training process. Additionally, we propose the use of Joint-Confidence, derived from the combination of classification and regression, as pseudo-label scores, effectively mitigating issues associated with pseudo-label bias. Extensive experiments have verified the effectiveness of the proposed method.
Published: 2026-01-02T07:39:05+00:00
Venue: Knowledge-Based Systems
Score: 0.491 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jian-Xun Mi; Jiahui Feng; Haiyang Wang; Yanjun Wu; Ranzhi Zhao; Chang Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115217"&gt;10.1016/j.knosys.2025.115217&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.491 (consider)&lt;/p&gt;
&lt;p&gt;Object detection models can achieve excellent detection performance with fully annotated instances. However, requiring complete annotations for every dataset is impractical due to high labor and time costs, as well as the inevitable occurrence of missing annotations. As a result, the absence of annotations can potentially provide misleading supervision and harm the training process. Recent methodologies have achieved remarkable effectiveness through the application of Co-Mining. However, the independence of each branch in Co-Mining cannot be guaranteed, overlooking valuable information during multi-perspective training. To address this issue, we introduce an Independence Guided Co-Training Model (IGCDet) that leverages Image Independence Decomposition to ensure the independence of each co-training branch. This model aims to capture diverse perspectives from images as extensively as possible, identifying missing annotations and incorporating them as positive supervision in the training process. Additionally, we propose the use of Joint-Confidence, derived from the combination of classification and regression, as pseudo-label scores, effectively mitigating issues associated with pseudo-label bias. Extensive experiments have verified the effectiveness of the proposed method.&lt;/p&gt;</content:encoded></item><item><title>From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning</title><link>https://arxiv.org/abs/2512.24532v1</link><guid>http://arxiv.org/abs/2512.24532v1</guid><pubDate>Wed, 31 Dec 2025 00:36:03 +0000</pubDate><dc:creator>Amir Tahmasbi</dc:creator><dc:creator>Sadegh Majidi</dc:creator><dc:creator>Kazem Taram</dc:creator><dc:creator>Aniket Bera</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Spatial reasoning in large language models (LLMs) has gained increasing attention due to applications in navigation and planning. Despite strong general language capabilities, LLMs still struggle with spatial transformations and multi-step planning in structured environments. We propose a two-stage approach that decomposes spatial reasoning into atomic building blocks and their composition. First, we apply supervised fine-tuning on elementary spatial transformations, such as rotation, translation, and scaling, to equip the model with basic spatial physics. We then freeze this physics-aware model and train lightweight LoRA adapters within the GRPO framework to learn policies that compose these building blocks for multi-step planning in puzzle-based environments, in a closed-loop manner. To support this pipeline, we synthesize an ASCII-art dataset and construct a corresponding ASCII-based reinforcement learning environment. Our method consistently outperforms baselines, including the generic backbone, physics-aware model, and end-to-end RL models, under both Dynamic environments with explicit state updates and Static environments where the model must rely on its internal state across steps. In addition, the proposed approach converges faster and exhibits more stable training compared to end-to-end reinforcement learning from scratch. Finally, we analyze attention patterns to assess whether fine-tuning induces meaningful improvements in spatial understanding.
Published: 2025-12-31T00:36:03+00:00
Venue: arXiv
Score: 0.490 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Amir Tahmasbi; Sadegh Majidi; Kazem Taram; Aniket Bera&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.490 (consider)&lt;/p&gt;
&lt;p&gt;Spatial reasoning in large language models (LLMs) has gained increasing attention due to applications in navigation and planning. Despite strong general language capabilities, LLMs still struggle with spatial transformations and multi-step planning in structured environments. We propose a two-stage approach that decomposes spatial reasoning into atomic building blocks and their composition. First, we apply supervised fine-tuning on elementary spatial transformations, such as rotation, translation, and scaling, to equip the model with basic spatial physics. We then freeze this physics-aware model and train lightweight LoRA adapters within the GRPO framework to learn policies that compose these building blocks for multi-step planning in puzzle-based environments, in a closed-loop manner. To support this pipeline, we synthesize an ASCII-art dataset and construct a corresponding ASCII-based reinforcement learning environment. Our method consistently outperforms baselines, including the generic backbone, physics-aware model, and end-to-end RL models, under both Dynamic environments with explicit state updates and Static environments where the model must rely on its internal state across steps. In addition, the proposed approach converges faster and exhibits more stable training compared to end-to-end reinforcement learning from scratch. Finally, we analyze attention patterns to assess whether fine-tuning induces meaningful improvements in spatial understanding.&lt;/p&gt;</content:encoded></item><item><title>DAK-Pose: Dual-Augmentor Knowledge Fusion for Generalizable Video-Based 3D Human Pose Estimation</title><link>https://doi.org/10.1016/j.inffus.2025.104100</link><guid>10.1016/j.inffus.2025.104100</guid><pubDate>Sat, 03 Jan 2026 16:24:02 +0000</pubDate><dc:creator>Yachuan Wang</dc:creator><dc:creator>Bin Zhang</dc:creator><dc:creator>Hao Yuan</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104100</prism:doi><description>Real-world deployment of video-based 3D human pose estimation remains challenging, as limited annotated data collected in constrained lab settings cannot fully capture the complexity of human motion. While motion synthesis for data augmentation has emerged as a mainstream solution to enhance generalization, existing synthesis methods suffer from inherent trade-offs: kinematics-based motion synthesis approaches preserve anatomical plausibility but sacrifice temporal coherence, while coordinate-based methods ensure motion smoothness but violate biomechanical constraints. This results in persistent domain gaps when synthetic data is directly used in the observation space to train pose estimation models. To overcome this, we propose DAK-Pose, which shifts augmentation to the feature space. We disentangle motion into structural and dynamic features, and design two complementary augmentors: (1) A structure-prioritized module enforces kinematic constraints for anatomical validity, and (2) a dynamic-prioritized module generates diverse temporal patterns. Auxiliary encoders trained on synthetic motions generated by these augmentors transfer domain-invariant knowledge to the pose estimator through adversarial alignment. Experiments on Human3.6M, MPI-INF-3DHP, and 3DPW datasets show that DAK-Pose achieves state-of-the-art cross-dataset performance.
Published: 2026-01-03T16:24:02+00:00
Venue: Information Fusion
Score: 0.489 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yachuan Wang; Bin Zhang; Hao Yuan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104100"&gt;10.1016/j.inffus.2025.104100&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.489 (consider)&lt;/p&gt;
&lt;p&gt;Real-world deployment of video-based 3D human pose estimation remains challenging, as limited annotated data collected in constrained lab settings cannot fully capture the complexity of human motion. While motion synthesis for data augmentation has emerged as a mainstream solution to enhance generalization, existing synthesis methods suffer from inherent trade-offs: kinematics-based motion synthesis approaches preserve anatomical plausibility but sacrifice temporal coherence, while coordinate-based methods ensure motion smoothness but violate biomechanical constraints. This results in persistent domain gaps when synthetic data is directly used in the observation space to train pose estimation models. To overcome this, we propose DAK-Pose, which shifts augmentation to the feature space. We disentangle motion into structural and dynamic features, and design two complementary augmentors: (1) A structure-prioritized module enforces kinematic constraints for anatomical validity, and (2) a dynamic-prioritized module generates diverse temporal patterns. Auxiliary encoders trained on synthetic motions generated by these augmentors transfer domain-invariant knowledge to the pose estimator through adversarial alignment. Experiments on Human3.6M, MPI-INF-3DHP, and 3DPW datasets show that DAK-Pose achieves state-of-the-art cross-dataset performance.&lt;/p&gt;</content:encoded></item><item><title>Edit3r: Instant 3D Scene Editing from Sparse Unposed Images</title><link>https://arxiv.org/abs/2512.25071v1</link><guid>http://arxiv.org/abs/2512.25071v1</guid><pubDate>Wed, 31 Dec 2025 18:59:53 +0000</pubDate><dc:creator>Jiageng Liu</dc:creator><dc:creator>Weijie Lyu</dc:creator><dc:creator>Xueting Li</dc:creator><dc:creator>Yejie Guo</dc:creator><dc:creator>Ming-Hsuan Yang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present Edit3r, a feed-forward framework that reconstructs and edits 3D scenes in a single pass from unposed, view-inconsistent, instruction-edited images. Unlike prior methods requiring per-scene optimization, Edit3r directly predicts instruction-aligned 3D edits, enabling fast and photorealistic rendering without optimization or pose estimation. A key challenge in training such a model lies in the absence of multi-view consistent edited images for supervision. We address this with (i) a SAM2-based recoloring strategy that generates reliable, cross-view-consistent supervision, and (ii) an asymmetric input strategy that pairs a recolored reference view with raw auxiliary views, encouraging the network to fuse and align disparate observations. At inference, our model effectively handles images edited by 2D methods such as InstructPix2Pix, despite not being exposed to such edits during training. For large-scale quantitative evaluation, we introduce DL3DV-Edit-Bench, a benchmark built on the DL3DV test split, featuring 20 diverse scenes, 4 edit types and 100 edits in total. Comprehensive quantitative and qualitative results show that Edit3r achieves superior semantic alignment and enhanced 3D consistency compared to recent baselines, while operating at significantly higher inference speed, making it promising for real-time 3D editing applications.
Published: 2025-12-31T18:59:53+00:00
Venue: arXiv
Score: 0.488 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiageng Liu; Weijie Lyu; Xueting Li; Yejie Guo; Ming-Hsuan Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.488 (consider)&lt;/p&gt;
&lt;p&gt;We present Edit3r, a feed-forward framework that reconstructs and edits 3D scenes in a single pass from unposed, view-inconsistent, instruction-edited images. Unlike prior methods requiring per-scene optimization, Edit3r directly predicts instruction-aligned 3D edits, enabling fast and photorealistic rendering without optimization or pose estimation. A key challenge in training such a model lies in the absence of multi-view consistent edited images for supervision. We address this with (i) a SAM2-based recoloring strategy that generates reliable, cross-view-consistent supervision, and (ii) an asymmetric input strategy that pairs a recolored reference view with raw auxiliary views, encouraging the network to fuse and align disparate observations. At inference, our model effectively handles images edited by 2D methods such as InstructPix2Pix, despite not being exposed to such edits during training. For large-scale quantitative evaluation, we introduce DL3DV-Edit-Bench, a benchmark built on the DL3DV test split, featuring 20 diverse scenes, 4 edit types and 100 edits in total. Comprehensive quantitative and qualitative results show that Edit3r achieves superior semantic alignment and enhanced 3D consistency compared to recent baselines, while operating at significantly higher inference speed, making it promising for real-time 3D editing applications.&lt;/p&gt;</content:encoded></item><item><title>FoundationSLAM: Unleashing the Power of Depth Foundation Models for End-to-End Dense Visual SLAM</title><link>https://arxiv.org/abs/2512.25008v1</link><guid>http://arxiv.org/abs/2512.25008v1</guid><pubDate>Wed, 31 Dec 2025 17:57:45 +0000</pubDate><dc:creator>Yuchen Wu</dc:creator><dc:creator>Jiahe Li</dc:creator><dc:creator>Fabio Tosi</dc:creator><dc:creator>Matteo Poggi</dc:creator><dc:creator>Jin Zheng</dc:creator><dc:creator>Xiao Bai</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present FoundationSLAM, a learning-based monocular dense SLAM system that addresses the absence of geometric consistency in previous flow-based approaches for accurate and robust tracking and mapping. Our core idea is to bridge flow estimation with geometric reasoning by leveraging the guidance from foundation depth models. To this end, we first develop a Hybrid Flow Network that produces geometry-aware correspondences, enabling consistent depth and pose inference across diverse keyframes. To enforce global consistency, we propose a Bi-Consistent Bundle Adjustment Layer that jointly optimizes keyframe pose and depth under multi-view constraints. Furthermore, we introduce a Reliability-Aware Refinement mechanism that dynamically adapts the flow update process by distinguishing between reliable and uncertain regions, forming a closed feedback loop between matching and optimization. Extensive experiments demonstrate that FoundationSLAM achieves superior trajectory accuracy and dense reconstruction quality across multiple challenging datasets, while running in real-time at 18 FPS, demonstrating strong generalization to various scenarios and practical applicability of our method.
Published: 2025-12-31T17:57:45+00:00
Venue: arXiv
Score: 0.488 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuchen Wu; Jiahe Li; Fabio Tosi; Matteo Poggi; Jin Zheng; Xiao Bai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.488 (consider)&lt;/p&gt;
&lt;p&gt;We present FoundationSLAM, a learning-based monocular dense SLAM system that addresses the absence of geometric consistency in previous flow-based approaches for accurate and robust tracking and mapping. Our core idea is to bridge flow estimation with geometric reasoning by leveraging the guidance from foundation depth models. To this end, we first develop a Hybrid Flow Network that produces geometry-aware correspondences, enabling consistent depth and pose inference across diverse keyframes. To enforce global consistency, we propose a Bi-Consistent Bundle Adjustment Layer that jointly optimizes keyframe pose and depth under multi-view constraints. Furthermore, we introduce a Reliability-Aware Refinement mechanism that dynamically adapts the flow update process by distinguishing between reliable and uncertain regions, forming a closed feedback loop between matching and optimization. Extensive experiments demonstrate that FoundationSLAM achieves superior trajectory accuracy and dense reconstruction quality across multiple challenging datasets, while running in real-time at 18 FPS, demonstrating strong generalization to various scenarios and practical applicability of our method.&lt;/p&gt;</content:encoded></item><item><title>SliceLens: Fine-Grained and Grounded Error Slice Discovery for Multi-Instance Vision Tasks</title><link>https://arxiv.org/abs/2512.24592v1</link><guid>http://arxiv.org/abs/2512.24592v1</guid><pubDate>Wed, 31 Dec 2025 03:28:41 +0000</pubDate><dc:creator>Wei Zhang</dc:creator><dc:creator>Chaoqun Wang</dc:creator><dc:creator>Zixuan Guan</dc:creator><dc:creator>Sam Kao</dc:creator><dc:creator>Pengfei Zhao</dc:creator><dc:creator>Peng Wu</dc:creator><dc:creator>Sifeng He</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Systematic failures of computer vision models on subsets with coherent visual patterns, known as error slices, pose a critical challenge for robust model evaluation. Existing slice discovery methods are primarily developed for image classification, limiting their applicability to multi-instance tasks such as detection, segmentation, and pose estimation. In real-world scenarios, error slices often arise from corner cases involving complex visual relationships, where existing instance-level approaches lacking fine-grained reasoning struggle to yield meaningful insights. Moreover, current benchmarks are typically tailored to specific algorithms or biased toward image classification, with artificial ground truth that fails to reflect real model failures. To address these limitations, we propose SliceLens, a hypothesis-driven framework that leverages LLMs and VLMs to generate and verify diverse failure hypotheses through grounded visual reasoning, enabling reliable identification of fine-grained and interpretable error slices. We further introduce FeSD (Fine-grained Slice Discovery), the first benchmark specifically designed for evaluating fine-grained error slice discovery across instance-level vision tasks, featuring expert-annotated and carefully refined ground-truth slices with precise grounding to local error regions. Extensive experiments on both existing benchmarks and FeSD demonstrate that SliceLens achieves state-of-the-art performance, improving Precision@10 by 0.42 (0.73 vs. 0.31) on FeSD, and identifies interpretable slices that facilitate actionable model improvements, as validated through model repair experiments.
Published: 2025-12-31T03:28:41+00:00
Venue: arXiv
Score: 0.488 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wei Zhang; Chaoqun Wang; Zixuan Guan; Sam Kao; Pengfei Zhao; Peng Wu; Sifeng He&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.488 (consider)&lt;/p&gt;
&lt;p&gt;Systematic failures of computer vision models on subsets with coherent visual patterns, known as error slices, pose a critical challenge for robust model evaluation. Existing slice discovery methods are primarily developed for image classification, limiting their applicability to multi-instance tasks such as detection, segmentation, and pose estimation. In real-world scenarios, error slices often arise from corner cases involving complex visual relationships, where existing instance-level approaches lacking fine-grained reasoning struggle to yield meaningful insights. Moreover, current benchmarks are typically tailored to specific algorithms or biased toward image classification, with artificial ground truth that fails to reflect real model failures. To address these limitations, we propose SliceLens, a hypothesis-driven framework that leverages LLMs and VLMs to generate and verify diverse failure hypotheses through grounded visual reasoning, enabling reliable identification of fine-grained and interpretable error slices. We further introduce FeSD (Fine-grained Slice Discovery), the first benchmark specifically designed for evaluating fine-grained error slice discovery across instance-level vision tasks, featuring expert-annotated and carefully refined ground-truth slices with precise grounding to local error regions. Extensive experiments on both existing benchmarks and FeSD demonstrate that SliceLens achieves state-of-the-art performance, improving Precision@10 by 0.42 (0.73 vs. 0.31) on FeSD, and identifies interpretable slices that facilitate actionable model improvements, as validated through model repair experiments.&lt;/p&gt;</content:encoded></item></channel></rss>