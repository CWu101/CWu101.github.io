<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Wed, 24 Dec 2025 04:24:58 +0000</lastBuildDate><item><title>Boosting Faithful Multi-modal LLMs via Complementary Visual Grounding</title><link>https://doi.org/10.1109/tip.2025.3644140</link><guid>10.1109/tip.2025.3644140</guid><pubDate>Mon, 22 Dec 2025 18:44:25 +0000</pubDate><dc:creator>Zheren Fu</dc:creator><dc:creator>Zhendong Mao</dc:creator><dc:creator>Lei Zhang</dc:creator><dc:creator>Yongdong Zhang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3644140</prism:doi><description>Multimodal Large Language Models (MLLMs) exhibit impressive performance across vision-language tasks, but still face the hallucination challenges, where generated texts are factually inconsistent with visual input. Existing mitigation methods focus on surface symptoms of hallucination and heavily rely on post-hoc corrections, extensive data curation, or costly inference schemes. In this work, we identify two key factors of MLLM hallucination: Insufficient Visual Context, where ambiguous visual contexts lead to language speculation, and Progressive Textual Drift, where model attention strays from visual inputs in longer responses. To address these problems, we propose a novel Complementary Visual Grounding (CVG) framework. CVG exploits the intrinsic architecture of MLLMs, without requiring any external tools, models, or additional data. CVG first disentangles visual context into two complementary branches based on query relevance, then maintains steadfast visual grounding during the auto-regressive generation. Finally, it contrasts the output distributions of two branches to produce a faithful response. Extensive experiments on various hallucination and general benchmarks demonstrate that CVG achieves state-of-the-art performances across MLLM architectures and scales.
Published: 2025-12-22T18:44:25+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.544 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zheren Fu; Zhendong Mao; Lei Zhang; Yongdong Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3644140"&gt;10.1109/tip.2025.3644140&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.544 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal Large Language Models (MLLMs) exhibit impressive performance across vision-language tasks, but still face the hallucination challenges, where generated texts are factually inconsistent with visual input. Existing mitigation methods focus on surface symptoms of hallucination and heavily rely on post-hoc corrections, extensive data curation, or costly inference schemes. In this work, we identify two key factors of MLLM hallucination: Insufficient Visual Context, where ambiguous visual contexts lead to language speculation, and Progressive Textual Drift, where model attention strays from visual inputs in longer responses. To address these problems, we propose a novel Complementary Visual Grounding (CVG) framework. CVG exploits the intrinsic architecture of MLLMs, without requiring any external tools, models, or additional data. CVG first disentangles visual context into two complementary branches based on query relevance, then maintains steadfast visual grounding during the auto-regressive generation. Finally, it contrasts the output distributions of two branches to produce a faithful response. Extensive experiments on various hallucination and general benchmarks demonstrate that CVG achieves state-of-the-art performances across MLLM architectures and scales.&lt;/p&gt;</content:encoded></item><item><title>SynJAC: Synthetic-data-driven Joint-granular Adaptation and Calibration for Domain Specific Scanned Document Key Information Extraction</title><link>https://doi.org/10.1016/j.inffus.2025.104074</link><guid>10.1016/j.inffus.2025.104074</guid><pubDate>Tue, 23 Dec 2025 16:57:18 +0000</pubDate><dc:creator>Yihao Ding</dc:creator><dc:creator>Soyeon Caren Han</dc:creator><dc:creator>Zechuan Li</dc:creator><dc:creator>Hyunsuk Chung</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104074</prism:doi><description>Visually Rich Documents (VRDs), comprising elements such as charts, tables, and paragraphs, convey complex information across diverse domains. However, extracting key information from these documents remains labour-intensive, particularly for scanned formats with inconsistent layouts and domain-specific requirements. Despite advances in pretrained models for VRD understanding, their dependence on large annotated datasets for fine-tuning hinders scalability. This paper proposes SynJAC (Synthetic-data-driven Joint-granular Adaptation and Calibration), a method for key information extraction in scanned documents. SynJAC leverages synthetic, machine-generated data for domain adaptation and employs calibration on a small, manually annotated dataset to mitigate noise. By integrating fine-grained and coarse-grained document representation learning, SynJAC significantly reduces the need for extensive manual labelling while achieving competitive performance. Extensive experiments demonstrate its effectiveness in domain-specific and scanned VRD scenarios.
Published: 2025-12-23T16:57:18+00:00
Venue: Information Fusion
Score: 0.534 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yihao Ding; Soyeon Caren Han; Zechuan Li; Hyunsuk Chung&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104074"&gt;10.1016/j.inffus.2025.104074&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.534 (consider)&lt;/p&gt;
&lt;p&gt;Visually Rich Documents (VRDs), comprising elements such as charts, tables, and paragraphs, convey complex information across diverse domains. However, extracting key information from these documents remains labour-intensive, particularly for scanned formats with inconsistent layouts and domain-specific requirements. Despite advances in pretrained models for VRD understanding, their dependence on large annotated datasets for fine-tuning hinders scalability. This paper proposes SynJAC (Synthetic-data-driven Joint-granular Adaptation and Calibration), a method for key information extraction in scanned documents. SynJAC leverages synthetic, machine-generated data for domain adaptation and employs calibration on a small, manually annotated dataset to mitigate noise. By integrating fine-grained and coarse-grained document representation learning, SynJAC significantly reduces the need for extensive manual labelling while achieving competitive performance. Extensive experiments demonstrate its effectiveness in domain-specific and scanned VRD scenarios.&lt;/p&gt;</content:encoded></item><item><title>Text2Graph VPR: A Text-to-Graph Expert System for Explainable Place Recognition in Changing Environments</title><link>https://arxiv.org/abs/2512.18613v1</link><guid>http://arxiv.org/abs/2512.18613v1</guid><pubDate>Sun, 21 Dec 2025 06:16:20 +0000</pubDate><dc:creator>Saeideh Yousefzadeh</dc:creator><dc:creator>Hamidreza Pourreza</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual Place Recognition (VPR) in long-term deployment requires reasoning beyond pixel similarity: systems must make transparent, interpretable decisions that remain robust under lighting, weather and seasonal change. We present Text2Graph VPR, an explainable semantic localization system that converts image sequences into textual scene descriptions, parses those descriptions into structured scene graphs, and reasons over the resulting graphs to identify places. Scene graphs capture objects, attributes and pairwise relations; we aggregate per-frame graphs into a compact place representation and perform retrieval with a dual-similarity mechanism that fuses learned Graph Attention Network (GAT) embeddings and a Shortest-Path (SP) kernel for structural matching. This hybrid design enables both learned semantic matching and topology-aware comparison, and -- critically -- produces human-readable intermediate representations that support diagnostic analysis and improve transparency in the decision process. We validate the system on Oxford RobotCar and MSLS (Amman/San Francisco) benchmarks and demonstrate robust retrieval under severe appearance shifts, along with zero-shot operation using human textual queries. The results illustrate that semantic, graph-based reasoning is a viable and interpretable alternative for place recognition, particularly suited to safety-sensitive and resource-constrained settings.
Published: 2025-12-21T06:16:20+00:00
Venue: arXiv
Score: 0.521 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Saeideh Yousefzadeh; Hamidreza Pourreza&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.521 (consider)&lt;/p&gt;
&lt;p&gt;Visual Place Recognition (VPR) in long-term deployment requires reasoning beyond pixel similarity: systems must make transparent, interpretable decisions that remain robust under lighting, weather and seasonal change. We present Text2Graph VPR, an explainable semantic localization system that converts image sequences into textual scene descriptions, parses those descriptions into structured scene graphs, and reasons over the resulting graphs to identify places. Scene graphs capture objects, attributes and pairwise relations; we aggregate per-frame graphs into a compact place representation and perform retrieval with a dual-similarity mechanism that fuses learned Graph Attention Network (GAT) embeddings and a Shortest-Path (SP) kernel for structural matching. This hybrid design enables both learned semantic matching and topology-aware comparison, and -- critically -- produces human-readable intermediate representations that support diagnostic analysis and improve transparency in the decision process. We validate the system on Oxford RobotCar and MSLS (Amman/San Francisco) benchmarks and demonstrate robust retrieval under severe appearance shifts, along with zero-shot operation using human textual queries. The results illustrate that semantic, graph-based reasoning is a viable and interpretable alternative for place recognition, particularly suited to safety-sensitive and resource-constrained settings.&lt;/p&gt;</content:encoded></item><item><title>Object-Centric Framework for Video Moment Retrieval</title><link>https://arxiv.org/abs/2512.18448v1</link><guid>http://arxiv.org/abs/2512.18448v1</guid><pubDate>Sat, 20 Dec 2025 17:44:53 +0000</pubDate><dc:creator>Zongyao Li</dc:creator><dc:creator>Yongkang Wong</dc:creator><dc:creator>Satoshi Yamazaki</dc:creator><dc:creator>Jianquan Liu</dc:creator><dc:creator>Mohan Kankanhalli</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Most existing video moment retrieval methods rely on temporal sequences of frame- or clip-level features that primarily encode global visual and semantic information. However, such representations often fail to capture fine-grained object semantics and appearance, which are crucial for localizing moments described by object-oriented queries involving specific entities and their interactions. In particular, temporal dynamics at the object level have been largely overlooked, limiting the effectiveness of existing approaches in scenarios requiring detailed object-level reasoning. To address this limitation, we propose a novel object-centric framework for moment retrieval. Our method first extracts query-relevant objects using a scene graph parser and then generates scene graphs from video frames to represent these objects and their relationships. Based on the scene graphs, we construct object-level feature sequences that encode rich visual and semantic information. These sequences are processed by a relational tracklet transformer, which models spatio-temporal correlations among objects over time. By explicitly capturing object-level state changes, our framework enables more accurate localization of moments aligned with object-oriented queries. We evaluated our method on three benchmarks: Charades-STA, QVHighlights, and TACoS. Experimental results demonstrate that our method outperforms existing state-of-the-art methods across all benchmarks.
Published: 2025-12-20T17:44:53+00:00
Venue: arXiv
Score: 0.518 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zongyao Li; Yongkang Wong; Satoshi Yamazaki; Jianquan Liu; Mohan Kankanhalli&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.518 (consider)&lt;/p&gt;
&lt;p&gt;Most existing video moment retrieval methods rely on temporal sequences of frame- or clip-level features that primarily encode global visual and semantic information. However, such representations often fail to capture fine-grained object semantics and appearance, which are crucial for localizing moments described by object-oriented queries involving specific entities and their interactions. In particular, temporal dynamics at the object level have been largely overlooked, limiting the effectiveness of existing approaches in scenarios requiring detailed object-level reasoning. To address this limitation, we propose a novel object-centric framework for moment retrieval. Our method first extracts query-relevant objects using a scene graph parser and then generates scene graphs from video frames to represent these objects and their relationships. Based on the scene graphs, we construct object-level feature sequences that encode rich visual and semantic information. These sequences are processed by a relational tracklet transformer, which models spatio-temporal correlations among objects over time. By explicitly capturing object-level state changes, our framework enables more accurate localization of moments aligned with object-oriented queries. We evaluated our method on three benchmarks: Charades-STA, QVHighlights, and TACoS. Experimental results demonstrate that our method outperforms existing state-of-the-art methods across all benchmarks.&lt;/p&gt;</content:encoded></item><item><title>From Pixels to Predicates Structuring urban perception with scene graphs</title><link>https://arxiv.org/abs/2512.19221v1</link><guid>http://arxiv.org/abs/2512.19221v1</guid><pubDate>Mon, 22 Dec 2025 10:02:53 +0000</pubDate><dc:creator>Yunlong Liu</dc:creator><dc:creator>Shuyang Li</dc:creator><dc:creator>Pengyuan Liu</dc:creator><dc:creator>Yu Zhang</dc:creator><dc:creator>Rudi Stouffs</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Perception research is increasingly modelled using streetscapes, yet many approaches still rely on pixel features or object co-occurrence statistics, overlooking the explicit relations that shape human perception. This study proposes a three stage pipeline that transforms street view imagery (SVI) into structured representations for predicting six perceptual indicators. In the first stage, each image is parsed using an open-set Panoptic Scene Graph model (OpenPSG) to extract object predicate object triplets. In the second stage, compact scene-level embeddings are learned through a heterogeneous graph autoencoder (GraphMAE). In the third stage, a neural network predicts perception scores from these embeddings. We evaluate the proposed approach against image-only baselines in terms of accuracy, precision, and cross-city generalization. Results indicate that (i) our approach improves perception prediction accuracy by an average of 26% over baseline models, and (ii) maintains strong generalization performance in cross-city prediction tasks. Additionally, the structured representation clarifies which relational patterns contribute to lower perception scores in urban scenes, such as graffiti on wall and car parked on sidewalk. Overall, this study demonstrates that graph-based structure provides expressive, generalizable, and interpretable signals for modelling urban perception, advancing human-centric and context-aware urban analytics.
Published: 2025-12-22T10:02:53+00:00
Venue: arXiv
Score: 0.517 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yunlong Liu; Shuyang Li; Pengyuan Liu; Yu Zhang; Rudi Stouffs&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.517 (consider)&lt;/p&gt;
&lt;p&gt;Perception research is increasingly modelled using streetscapes, yet many approaches still rely on pixel features or object co-occurrence statistics, overlooking the explicit relations that shape human perception. This study proposes a three stage pipeline that transforms street view imagery (SVI) into structured representations for predicting six perceptual indicators. In the first stage, each image is parsed using an open-set Panoptic Scene Graph model (OpenPSG) to extract object predicate object triplets. In the second stage, compact scene-level embeddings are learned through a heterogeneous graph autoencoder (GraphMAE). In the third stage, a neural network predicts perception scores from these embeddings. We evaluate the proposed approach against image-only baselines in terms of accuracy, precision, and cross-city generalization. Results indicate that (i) our approach improves perception prediction accuracy by an average of 26% over baseline models, and (ii) maintains strong generalization performance in cross-city prediction tasks. Additionally, the structured representation clarifies which relational patterns contribute to lower perception scores in urban scenes, such as graffiti on wall and car parked on sidewalk. Overall, this study demonstrates that graph-based structure provides expressive, generalizable, and interpretable signals for modelling urban perception, advancing human-centric and context-aware urban analytics.&lt;/p&gt;</content:encoded></item><item><title>Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva</title><link>https://arxiv.org/abs/2512.20042v1</link><guid>http://arxiv.org/abs/2512.20042v1</guid><pubDate>Tue, 23 Dec 2025 04:21:15 +0000</pubDate><dc:creator>Nguyen Lam Phu Quy</dc:creator><dc:creator>Pham Phu Hoa</dc:creator><dc:creator>Tran Chi Nguyen</dc:creator><dc:creator>Dao Sy Duy Minh</dc:creator><dc:creator>Nguyen Hoang Minh Ngoc</dc:creator><dc:creator>Huynh Trung Kiet</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Real-world image captions often lack contextual depth, omitting crucial details such as event background, temporal cues, outcomes, and named entities that are not visually discernible. This gap limits the effectiveness of image understanding in domains like journalism, education, and digital archives, where richer, more informative descriptions are essential. To address this, we propose a multimodal pipeline that augments visual input with external textual knowledge. Our system retrieves semantically similar images using BEIT-3 (Flickr30k-384 and COCO-384) and SigLIP So-384, reranks them using ORB and SIFT for geometric alignment, and extracts contextual information from related articles via semantic search. A fine-tuned Qwen3 model with QLoRA then integrates this context with base captions generated by Instruct BLIP (Vicuna-7B) to produce event-enriched, context-aware descriptions. Evaluated on the OpenEvents v1 dataset, our approach generates significantly more informative captions compared to traditional methods, showing strong potential for real-world applications requiring deeper visual-textual understanding
Published: 2025-12-23T04:21:15+00:00
Venue: arXiv
Score: 0.507 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nguyen Lam Phu Quy; Pham Phu Hoa; Tran Chi Nguyen; Dao Sy Duy Minh; Nguyen Hoang Minh Ngoc; Huynh Trung Kiet&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.507 (consider)&lt;/p&gt;
&lt;p&gt;Real-world image captions often lack contextual depth, omitting crucial details such as event background, temporal cues, outcomes, and named entities that are not visually discernible. This gap limits the effectiveness of image understanding in domains like journalism, education, and digital archives, where richer, more informative descriptions are essential. To address this, we propose a multimodal pipeline that augments visual input with external textual knowledge. Our system retrieves semantically similar images using BEIT-3 (Flickr30k-384 and COCO-384) and SigLIP So-384, reranks them using ORB and SIFT for geometric alignment, and extracts contextual information from related articles via semantic search. A fine-tuned Qwen3 model with QLoRA then integrates this context with base captions generated by Instruct BLIP (Vicuna-7B) to produce event-enriched, context-aware descriptions. Evaluated on the OpenEvents v1 dataset, our approach generates significantly more informative captions compared to traditional methods, showing strong potential for real-world applications requiring deeper visual-textual understanding&lt;/p&gt;</content:encoded></item><item><title>Hypergraph Foundation Model</title><link>https://doi.org/10.1109/tpami.2025.3647504</link><guid>10.1109/tpami.2025.3647504</guid><pubDate>Tue, 23 Dec 2025 18:31:03 +0000</pubDate><dc:creator>Yue Gao</dc:creator><dc:creator>Yifan Feng</dc:creator><dc:creator>Shiquan Liu</dc:creator><dc:creator>Xiangmin Han</dc:creator><dc:creator>Shaoyi Du</dc:creator><dc:creator>Zongze Wu</dc:creator><dc:creator>Han Hu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3647504</prism:doi><description>Hypergraph neural networks (HGNNs) effectively model complex high-order relationships in domains like protein interactions and social networks by connecting multiple vertices through hyperedges, enhancing modeling capabilities, and reducing information loss. Developing foundation models for hypergraphs is challenging due to their distinct data, which includes both vertex features and intricate structural information. We present Hyper-FM, a Hypergraph Foundation Model for multi-domain knowledge extraction, featuring Hierarchical High-Order Neighbor Guided Vertex Knowledge Embedding for vertex feature representation and Hierarchical Multi-Hypergraph Guided Structural Knowledge Extraction for structural information. Additionally, we curate 11 text-attributed hypergraph datasets to advance research between HGNNs and LLMs. Experiments on these datasets show that Hyper-FM outperforms baseline methods by approximately 13.4%, validating our approach. Furthermore, we propose the first scaling law for hypergraph foundation models, demonstrating that increasing domain diversity significantly enhances performance, unlike merely augmenting vertex and hyperedge counts. This underscores the critical role of domain diversity in scaling hypergraph models.
Published: 2025-12-23T18:31:03+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.505 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yue Gao; Yifan Feng; Shiquan Liu; Xiangmin Han; Shaoyi Du; Zongze Wu; Han Hu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3647504"&gt;10.1109/tpami.2025.3647504&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.505 (consider)&lt;/p&gt;
&lt;p&gt;Hypergraph neural networks (HGNNs) effectively model complex high-order relationships in domains like protein interactions and social networks by connecting multiple vertices through hyperedges, enhancing modeling capabilities, and reducing information loss. Developing foundation models for hypergraphs is challenging due to their distinct data, which includes both vertex features and intricate structural information. We present Hyper-FM, a Hypergraph Foundation Model for multi-domain knowledge extraction, featuring Hierarchical High-Order Neighbor Guided Vertex Knowledge Embedding for vertex feature representation and Hierarchical Multi-Hypergraph Guided Structural Knowledge Extraction for structural information. Additionally, we curate 11 text-attributed hypergraph datasets to advance research between HGNNs and LLMs. Experiments on these datasets show that Hyper-FM outperforms baseline methods by approximately 13.4%, validating our approach. Furthermore, we propose the first scaling law for hypergraph foundation models, demonstrating that increasing domain diversity significantly enhances performance, unlike merely augmenting vertex and hyperedge counts. This underscores the critical role of domain diversity in scaling hypergraph models.&lt;/p&gt;</content:encoded></item><item><title>Bi-Grid Reconstruction for Image Anomaly Detection</title><link>https://doi.org/10.1109/tip.2025.3644787</link><guid>10.1109/tip.2025.3644787</guid><pubDate>Mon, 22 Dec 2025 18:44:25 +0000</pubDate><dc:creator>Aimin Feng</dc:creator><dc:creator>Huichuan Huang</dc:creator><dc:creator>Guangyu Wei</dc:creator><dc:creator>Wenlong Sun</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3644787</prism:doi><description>In the domain of image anomaly detection, significant progress has been made in un- and self-supervised methods with datasets containing only normal samples. Although these methods perform well in general industrial anomaly detection scenarios, they often struggle with over- or under-detection when faced with fine-grained anomalies in products. In this paper, we propose GRAD: Bi-Grid Reconstruction for Image Anomaly Detection, which utilizes two continuous grids to detect anomalies from both normal and abnormal perspectives. In this work: 1) Grids are served as feature repositories to assist in the reconstruction task, achieving stronger generalization compared to discrete storage, while also helping to avoid the Identical Shortcut (IS) problem common in general reconstruction methods. 2) An additional grid storing abnormal features is introduced alongside the normal grid storing normal features, which refines the boundaries of normal features, thereby enhancing GRAD’s detection performance for fine-grained defects. 3) The Feature Block Pasting (FBP) module is designed to synthesize a variety of anomalies at the feature level, enabling the rapid deployment of the abnormal grid. Additionally, benefiting from the powerful representation capabilities of grids, GRAD is suitable for a unified task setting, requiring only a single model to be trained for multiple classes. GRAD has been comprehensively tested on classic industrial datasets including MVTecAD, VisA, and the newest GoodsAD dataset, showing significant improvement over current state-of-the-art methods.
Published: 2025-12-22T18:44:25+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.501 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Aimin Feng; Huichuan Huang; Guangyu Wei; Wenlong Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3644787"&gt;10.1109/tip.2025.3644787&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.501 (consider)&lt;/p&gt;
&lt;p&gt;In the domain of image anomaly detection, significant progress has been made in un- and self-supervised methods with datasets containing only normal samples. Although these methods perform well in general industrial anomaly detection scenarios, they often struggle with over- or under-detection when faced with fine-grained anomalies in products. In this paper, we propose GRAD: Bi-Grid Reconstruction for Image Anomaly Detection, which utilizes two continuous grids to detect anomalies from both normal and abnormal perspectives. In this work: 1) Grids are served as feature repositories to assist in the reconstruction task, achieving stronger generalization compared to discrete storage, while also helping to avoid the Identical Shortcut (IS) problem common in general reconstruction methods. 2) An additional grid storing abnormal features is introduced alongside the normal grid storing normal features, which refines the boundaries of normal features, thereby enhancing GRAD’s detection performance for fine-grained defects. 3) The Feature Block Pasting (FBP) module is designed to synthesize a variety of anomalies at the feature level, enabling the rapid deployment of the abnormal grid. Additionally, benefiting from the powerful representation capabilities of grids, GRAD is suitable for a unified task setting, requiring only a single model to be trained for multiple classes. GRAD has been comprehensively tested on classic industrial datasets including MVTecAD, VisA, and the newest GoodsAD dataset, showing significant improvement over current state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Seg-LLaVA: A small-scale large vision-language model with external visual prompts</title><link>https://doi.org/10.1016/j.neucom.2025.132423</link><guid>10.1016/j.neucom.2025.132423</guid><pubDate>Mon, 22 Dec 2025 07:44:54 +0000</pubDate><dc:creator>Tianxing Guo</dc:creator><dc:creator>Huanyu Liu</dc:creator><dc:creator>Jiazheng Wen</dc:creator><dc:creator>Junbao Li</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132423</prism:doi><description>With recent significant advancements in large vision-language models (LVLMs), image-text understanding capabilities have substantially improved. However, a notable gap remains in fine-grained region understanding. Moreover, the resource consumption for training and testing large-scale LVLMs is immense, making them less accessible to researchers with limited resources. In this paper, we propose a small-scale LVLM, Seg-LLaVA, which employs a lightweight visual prompting method that leverages a semantic segmenter and a small-scale large language model (LLM). By integrating fine-grained knowledge generated by a specialized instance segmentation model with the original image into a multi-layer linear model, we enable the model to perceive object boundaries and types in the image without significantly increasing the number of training parameters, thereby greatly enhancing its visual understanding capabilities. Additionally, we adopt an efficient training approach, allowing Seg-LLaVA to achieve outstanding performance while further reducing resource requirements. Experimental results show that our model excels across multiple benchmarks and demonstrates strong fine-grained perception capabilities.
Published: 2025-12-22T07:44:54+00:00
Venue: Neurocomputing
Score: 0.497 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianxing Guo; Huanyu Liu; Jiazheng Wen; Junbao Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132423"&gt;10.1016/j.neucom.2025.132423&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.497 (consider)&lt;/p&gt;
&lt;p&gt;With recent significant advancements in large vision-language models (LVLMs), image-text understanding capabilities have substantially improved. However, a notable gap remains in fine-grained region understanding. Moreover, the resource consumption for training and testing large-scale LVLMs is immense, making them less accessible to researchers with limited resources. In this paper, we propose a small-scale LVLM, Seg-LLaVA, which employs a lightweight visual prompting method that leverages a semantic segmenter and a small-scale large language model (LLM). By integrating fine-grained knowledge generated by a specialized instance segmentation model with the original image into a multi-layer linear model, we enable the model to perceive object boundaries and types in the image without significantly increasing the number of training parameters, thereby greatly enhancing its visual understanding capabilities. Additionally, we adopt an efficient training approach, allowing Seg-LLaVA to achieve outstanding performance while further reducing resource requirements. Experimental results show that our model excels across multiple benchmarks and demonstrates strong fine-grained perception capabilities.&lt;/p&gt;</content:encoded></item><item><title>Multi-Part Object Representations via Graph Structures and Co-Part Discovery</title><link>https://arxiv.org/abs/2512.18192v1</link><guid>http://arxiv.org/abs/2512.18192v1</guid><pubDate>Sat, 20 Dec 2025 03:38:41 +0000</pubDate><dc:creator>Alex Foo</dc:creator><dc:creator>Wynne Hsu</dc:creator><dc:creator>Mong Li Lee</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Discovering object-centric representations from images can significantly enhance the robustness, sample efficiency and generalizability of vision models. Works on images with multi-part objects typically follow an implicit object representation approach, which fail to recognize these learned objects in occluded or out-of-distribution contexts. This is due to the assumption that object part-whole relations are implicitly encoded into the representations through indirect training objectives. We address this limitation by proposing a novel method that leverages on explicit graph representations for parts and present a co-part object discovery algorithm. We then introduce three benchmarks to evaluate the robustness of object-centric methods in recognizing multi-part objects within occluded and out-of-distribution settings. Experimental results on simulated, realistic, and real-world images show marked improvements in the quality of discovered objects compared to state-of-the-art methods, as well as the accurate recognition of multi-part objects in occluded and out-of-distribution contexts. We also show that the discovered object-centric representations can more accurately predict key object properties in a downstream task, highlighting the potential of our method to advance the field of object-centric representations.
Published: 2025-12-20T03:38:41+00:00
Venue: arXiv
Score: 0.492 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Alex Foo; Wynne Hsu; Mong Li Lee&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.492 (consider)&lt;/p&gt;
&lt;p&gt;Discovering object-centric representations from images can significantly enhance the robustness, sample efficiency and generalizability of vision models. Works on images with multi-part objects typically follow an implicit object representation approach, which fail to recognize these learned objects in occluded or out-of-distribution contexts. This is due to the assumption that object part-whole relations are implicitly encoded into the representations through indirect training objectives. We address this limitation by proposing a novel method that leverages on explicit graph representations for parts and present a co-part object discovery algorithm. We then introduce three benchmarks to evaluate the robustness of object-centric methods in recognizing multi-part objects within occluded and out-of-distribution settings. Experimental results on simulated, realistic, and real-world images show marked improvements in the quality of discovered objects compared to state-of-the-art methods, as well as the accurate recognition of multi-part objects in occluded and out-of-distribution contexts. We also show that the discovered object-centric representations can more accurately predict key object properties in a downstream task, highlighting the potential of our method to advance the field of object-centric representations.&lt;/p&gt;</content:encoded></item><item><title>Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models</title><link>https://arxiv.org/abs/2512.20557v1</link><guid>http://arxiv.org/abs/2512.20557v1</guid><pubDate>Tue, 23 Dec 2025 17:56:36 +0000</pubDate><dc:creator>Shengchao Zhou</dc:creator><dc:creator>Yuxin Chen</dc:creator><dc:creator>Yuying Ge</dc:creator><dc:creator>Wei Huang</dc:creator><dc:creator>Jiehong Lin</dc:creator><dc:creator>Ying Shan</dc:creator><dc:creator>Xiaojuan Qi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.
Published: 2025-12-23T17:56:36+00:00
Venue: arXiv
Score: 0.492 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shengchao Zhou; Yuxin Chen; Yuying Ge; Wei Huang; Jiehong Lin; Ying Shan; Xiaojuan Qi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.492 (consider)&lt;/p&gt;
&lt;p&gt;Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.&lt;/p&gt;</content:encoded></item><item><title>Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models</title><link>https://arxiv.org/abs/2512.19686v1</link><guid>http://arxiv.org/abs/2512.19686v1</guid><pubDate>Mon, 22 Dec 2025 18:59:03 +0000</pubDate><dc:creator>Zixuan Ye</dc:creator><dc:creator>Quande Liu</dc:creator><dc:creator>Cong Wei</dc:creator><dc:creator>Yuanxing Zhang</dc:creator><dc:creator>Xintao Wang</dc:creator><dc:creator>Pengfei Wan</dc:creator><dc:creator>Kun Gai</dc:creator><dc:creator>Wenhan Luo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recently, the introduction of Chain-of-Thought (CoT) has largely improved the generation ability of unified models. However, it is observed that the current thinking process during generation mainly focuses on the text consistency with the text prompt, ignoring the \textbf{visual context consistency} with the visual reference images during the multi-modal generation, e.g., multi-reference generation. The lack of such consistency results in the failure in maintaining key visual features (like human ID, object attribute, style). To this end, we integrate the visual context consistency into the reasoning of unified models, explicitly motivating the model to sustain such consistency by 1) Adaptive Visual Planning: generating structured visual check list to figure out the visual element of needed consistency keeping, and 2) Iterative Visual Correction: performing self-reflection with the guidance of check lists and refining the generated result in an iterative manner. To achieve this, we use supervised finetuning to teach the model how to plan the visual checking, conduct self-reflection and self-refinement, and use flow-GRPO to further enhance the visual consistency through a customized visual checking reward. The experiments show that our method outperforms both zero-shot unified models and those with text CoTs in multi-modal generation, demonstrating higher visual context consistency.
Published: 2025-12-22T18:59:03+00:00
Venue: arXiv
Score: 0.491 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zixuan Ye; Quande Liu; Cong Wei; Yuanxing Zhang; Xintao Wang; Pengfei Wan; Kun Gai; Wenhan Luo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.491 (consider)&lt;/p&gt;
&lt;p&gt;Recently, the introduction of Chain-of-Thought (CoT) has largely improved the generation ability of unified models. However, it is observed that the current thinking process during generation mainly focuses on the text consistency with the text prompt, ignoring the \textbf{visual context consistency} with the visual reference images during the multi-modal generation, e.g., multi-reference generation. The lack of such consistency results in the failure in maintaining key visual features (like human ID, object attribute, style). To this end, we integrate the visual context consistency into the reasoning of unified models, explicitly motivating the model to sustain such consistency by 1) Adaptive Visual Planning: generating structured visual check list to figure out the visual element of needed consistency keeping, and 2) Iterative Visual Correction: performing self-reflection with the guidance of check lists and refining the generated result in an iterative manner. To achieve this, we use supervised finetuning to teach the model how to plan the visual checking, conduct self-reflection and self-refinement, and use flow-GRPO to further enhance the visual consistency through a customized visual checking reward. The experiments show that our method outperforms both zero-shot unified models and those with text CoTs in multi-modal generation, demonstrating higher visual context consistency.&lt;/p&gt;</content:encoded></item><item><title>Thinking Beyond Labels: Vocabulary-Free Fine-Grained Recognition using Reasoning-Augmented LMMs</title><link>https://arxiv.org/abs/2512.18897v1</link><guid>http://arxiv.org/abs/2512.18897v1</guid><pubDate>Sun, 21 Dec 2025 22:01:29 +0000</pubDate><dc:creator>Dmitry Demidov</dc:creator><dc:creator>Zaigham Zaheer</dc:creator><dc:creator>Zongyan Han</dc:creator><dc:creator>Omkar Thawakar</dc:creator><dc:creator>Rao Anwer</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vocabulary-free fine-grained image recognition aims to distinguish visually similar categories within a meta-class without a fixed, human-defined label set. Existing solutions for this problem are limited by either the usage of a large and rigid list of vocabularies or by the dependency on complex pipelines with fragile heuristics where errors propagate across stages. Meanwhile, the ability of recent large multi-modal models (LMMs) equipped with explicit or implicit reasoning to comprehend visual-language data, decompose problems, retrieve latent knowledge, and self-correct suggests a more principled and effective alternative. Building on these capabilities, we propose FiNDR (Fine-grained Name Discovery via Reasoning), the first reasoning-augmented LMM-based framework for vocabulary-free fine-grained recognition. The system operates in three automated steps: (i) a reasoning-enabled LMM generates descriptive candidate labels for each image; (ii) a vision-language model filters and ranks these candidates to form a coherent class set; and (iii) the verified names instantiate a lightweight multi-modal classifier used at inference time. Extensive experiments on popular fine-grained classification benchmarks demonstrate state-of-the-art performance under the vocabulary-free setting, with a significant relative margin of up to 18.8% over previous approaches. Remarkably, the proposed method surpasses zero-shot baselines that exploit pre-defined ground-truth names, challenging the assumption that human-curated vocabularies define an upper bound. Additionally, we show that carefully curated prompts enable open-source LMMs to match proprietary counterparts. These findings establish reasoning-augmented LMMs as an effective foundation for scalable, fully automated, open-world fine-grained visual recognition. The source code is available on github.com/demidovd98/FiNDR.
Published: 2025-12-21T22:01:29+00:00
Venue: arXiv
Score: 0.490 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dmitry Demidov; Zaigham Zaheer; Zongyan Han; Omkar Thawakar; Rao Anwer&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.490 (consider)&lt;/p&gt;
&lt;p&gt;Vocabulary-free fine-grained image recognition aims to distinguish visually similar categories within a meta-class without a fixed, human-defined label set. Existing solutions for this problem are limited by either the usage of a large and rigid list of vocabularies or by the dependency on complex pipelines with fragile heuristics where errors propagate across stages. Meanwhile, the ability of recent large multi-modal models (LMMs) equipped with explicit or implicit reasoning to comprehend visual-language data, decompose problems, retrieve latent knowledge, and self-correct suggests a more principled and effective alternative. Building on these capabilities, we propose FiNDR (Fine-grained Name Discovery via Reasoning), the first reasoning-augmented LMM-based framework for vocabulary-free fine-grained recognition. The system operates in three automated steps: (i) a reasoning-enabled LMM generates descriptive candidate labels for each image; (ii) a vision-language model filters and ranks these candidates to form a coherent class set; and (iii) the verified names instantiate a lightweight multi-modal classifier used at inference time. Extensive experiments on popular fine-grained classification benchmarks demonstrate state-of-the-art performance under the vocabulary-free setting, with a significant relative margin of up to 18.8% over previous approaches. Remarkably, the proposed method surpasses zero-shot baselines that exploit pre-defined ground-truth names, challenging the assumption that human-curated vocabularies define an upper bound. Additionally, we show that carefully curated prompts enable open-source LMMs to match proprietary counterparts. These findings establish reasoning-augmented LMMs as an effective foundation for scalable, fully automated, open-world fine-grained visual recognition. The source code is available on github.com/demidovd98/FiNDR.&lt;/p&gt;</content:encoded></item><item><title>Watch Closely: Mitigating Object Hallucinations in Large Vision-Language Models with Disentangled Decoding</title><link>https://arxiv.org/abs/2512.19070v1</link><guid>http://arxiv.org/abs/2512.19070v1</guid><pubDate>Mon, 22 Dec 2025 06:20:53 +0000</pubDate><dc:creator>Ruiqi Ma</dc:creator><dc:creator>Yu Yan</dc:creator><dc:creator>Chunhong Zhang</dc:creator><dc:creator>Minghao Yin</dc:creator><dc:creator>XinChao Liu</dc:creator><dc:creator>Zhihong Jin</dc:creator><dc:creator>Zheng Hu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Vision-Language Models (LVLMs) bridge the gap between visual and linguistic modalities, demonstrating strong potential across a variety of domains. However, despite significant progress, LVLMs still suffer from severe hallucination issues in object recognition tasks. These models often fail to accurately identify certain objects, leading to text generation that appears fluent but does not correspond to the visual content, which can have serious consequences in real-world applications. Recently, several methods have been proposed to alleviate LVLM hallucinations, but most focus solely on reducing hallucinations in the language modality. To mitigate hallucinations in both the language and visual modalities, we introduce Hallucination Disentangled Decoding (HDD) method that requires no training. HDD enhances the original image by segmenting it and selecting images that augment the original, while also utilizing a blank image to eliminate language prior hallucinations in both the original and segmented images. This design not only reduces the model's dependence on language priors but also enhances its visual performance. (Code: https://github.com/rickeyhhh/Hallucination-Disentangled-Decoding)
Published: 2025-12-22T06:20:53+00:00
Venue: arXiv
Score: 0.487 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruiqi Ma; Yu Yan; Chunhong Zhang; Minghao Yin; XinChao Liu; Zhihong Jin; Zheng Hu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.487 (consider)&lt;/p&gt;
&lt;p&gt;Large Vision-Language Models (LVLMs) bridge the gap between visual and linguistic modalities, demonstrating strong potential across a variety of domains. However, despite significant progress, LVLMs still suffer from severe hallucination issues in object recognition tasks. These models often fail to accurately identify certain objects, leading to text generation that appears fluent but does not correspond to the visual content, which can have serious consequences in real-world applications. Recently, several methods have been proposed to alleviate LVLM hallucinations, but most focus solely on reducing hallucinations in the language modality. To mitigate hallucinations in both the language and visual modalities, we introduce Hallucination Disentangled Decoding (HDD) method that requires no training. HDD enhances the original image by segmenting it and selecting images that augment the original, while also utilizing a blank image to eliminate language prior hallucinations in both the original and segmented images. This design not only reduces the model&amp;#x27;s dependence on language priors but also enhances its visual performance. (Code: https://github.com/rickeyhhh/Hallucination-Disentangled-Decoding)&lt;/p&gt;</content:encoded></item><item><title>Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark</title><link>https://arxiv.org/abs/2512.20174v1</link><guid>http://arxiv.org/abs/2512.20174v1</guid><pubDate>Tue, 23 Dec 2025 09:14:16 +0000</pubDate><dc:creator>Hao Guo</dc:creator><dc:creator>Xugong Qin</dc:creator><dc:creator>Jun Jie Ou Yang</dc:creator><dc:creator>Peng Zhang</dc:creator><dc:creator>Gangyan Zeng</dc:creator><dc:creator>Yubo Li</dc:creator><dc:creator>Hailun Lin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Document image retrieval (DIR) aims to retrieve document images from a gallery according to a given query. Existing DIR methods are primarily based on image queries that retrieve documents within the same coarse semantic category, e.g., newspapers or receipts. However, these methods struggle to effectively retrieve document images in real-world scenarios where textual queries with fine-grained semantics are usually provided. To bridge this gap, we introduce a new Natural Language-based Document Image Retrieval (NL-DIR) benchmark with corresponding evaluation metrics. In this work, natural language descriptions serve as semantically rich queries for the DIR task. The NL-DIR dataset contains 41K authentic document images, each paired with five high-quality, fine-grained semantic queries generated and evaluated through large language models in conjunction with manual verification. We perform zero-shot and fine-tuning evaluations of existing mainstream contrastive vision-language models and OCR-free visual document understanding (VDU) models. A two-stage retrieval method is further investigated for performance improvement while achieving both time and space efficiency. We hope the proposed NL-DIR benchmark can bring new opportunities and facilitate research for the VDU community. Datasets and codes will be publicly available at huggingface.co/datasets/nianbing/NL-DIR.
Published: 2025-12-23T09:14:16+00:00
Venue: arXiv
Score: 0.486 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Guo; Xugong Qin; Jun Jie Ou Yang; Peng Zhang; Gangyan Zeng; Yubo Li; Hailun Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.486 (consider)&lt;/p&gt;
&lt;p&gt;Document image retrieval (DIR) aims to retrieve document images from a gallery according to a given query. Existing DIR methods are primarily based on image queries that retrieve documents within the same coarse semantic category, e.g., newspapers or receipts. However, these methods struggle to effectively retrieve document images in real-world scenarios where textual queries with fine-grained semantics are usually provided. To bridge this gap, we introduce a new Natural Language-based Document Image Retrieval (NL-DIR) benchmark with corresponding evaluation metrics. In this work, natural language descriptions serve as semantically rich queries for the DIR task. The NL-DIR dataset contains 41K authentic document images, each paired with five high-quality, fine-grained semantic queries generated and evaluated through large language models in conjunction with manual verification. We perform zero-shot and fine-tuning evaluations of existing mainstream contrastive vision-language models and OCR-free visual document understanding (VDU) models. A two-stage retrieval method is further investigated for performance improvement while achieving both time and space efficiency. We hope the proposed NL-DIR benchmark can bring new opportunities and facilitate research for the VDU community. Datasets and codes will be publicly available at huggingface.co/datasets/nianbing/NL-DIR.&lt;/p&gt;</content:encoded></item><item><title>DC-SAM: In-Context Segment Anything in Images and Videos via Dual Consistency</title><link>https://doi.org/10.1109/tpami.2025.3646919</link><guid>10.1109/tpami.2025.3646919</guid><pubDate>Mon, 22 Dec 2025 18:41:22 +0000</pubDate><dc:creator>Mengshi Qi</dc:creator><dc:creator>Pengfei Zhu</dc:creator><dc:creator>Xiangtai Li</dc:creator><dc:creator>Xiaoyang Bi</dc:creator><dc:creator>Lu Qi</dc:creator><dc:creator>Huadong Ma</dc:creator><dc:creator>Ming-Hsuan Yang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3646919</prism:doi><description>Given a single labeled examples, in-context segmentation aims to segment corresponding objects. This setting, known as one-shot segmentation in few-shot learning, explores the segmentation model's generalization ability and has been applied to various vision tasks, including scene understanding and image/video editing. While recent Segment Anything Models (SAMs) have achieved state-of-the-art results in interactive segmentation, these approaches are not directly applicable to in-context segmentation. In this work, we propose the Dual Consistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2 for in-context segmentation of both images and videos. Our key insights are to enhance the features of the SAM's prompt encoder in segmentation by providing high-quality visual prompts. When generating a mask prior from support images, we fuse the SAM features to better align the prompt encoder rather than relying solely on a pre-trained backbone. Then, we design a cycle-consistent cross-attention on fused features and initial visual prompts. This design leverages coarse masks from the SAM mask decoder to ensure consistency between features and visual prompts. Next, a dual-branch design is provided by using the discriminative positive and negative prompts in the prompt encoder. Furthermore, we design a simple mask-tube training strategy to adopt our proposed dual consistency method into the mask tube. Although the proposed DC-SAM is primarily designed for images, it can be seamlessly extended to the video domain with the support of SAM2. Given the absence of in-context segmentation in the video domain, we manually curate and construct the first benchmark from existing video segmentation datasets, named In-Context Video Object Segmentation (IC-VOS), to better assess the in-context capability of the model. Extensive experiments demonstrate that our method achieves 55.5 (+1.4) mIoU on COCO-20 i ^{i} , 73.0 (+1.1) mIoU on PASCAL-5 i ^{i} , and a J &amp; F \mathcal {J\&amp;F} score...
Published: 2025-12-22T18:41:22+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.486 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mengshi Qi; Pengfei Zhu; Xiangtai Li; Xiaoyang Bi; Lu Qi; Huadong Ma; Ming-Hsuan Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3646919"&gt;10.1109/tpami.2025.3646919&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.486 (consider)&lt;/p&gt;
&lt;p&gt;Given a single labeled examples, in-context segmentation aims to segment corresponding objects. This setting, known as one-shot segmentation in few-shot learning, explores the segmentation model&amp;#x27;s generalization ability and has been applied to various vision tasks, including scene understanding and image/video editing. While recent Segment Anything Models (SAMs) have achieved state-of-the-art results in interactive segmentation, these approaches are not directly applicable to in-context segmentation. In this work, we propose the Dual Consistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2 for in-context segmentation of both images and videos. Our key insights are to enhance the features of the SAM&amp;#x27;s prompt encoder in segmentation by providing high-quality visual prompts. When generating a mask prior from support images, we fuse the SAM features to better align the prompt encoder rather than relying solely on a pre-trained backbone. Then, we design a cycle-consistent cross-attention on fused features and initial visual prompts. This design leverages coarse masks from the SAM mask decoder to ensure consistency between features and visual prompts. Next, a dual-branch design is provided by using the discriminative positive and negative prompts in the prompt encoder. Furthermore, we design a simple mask-tube training strategy to adopt our proposed dual consistency method into the mask tube. Although the proposed DC-SAM is primarily designed for images, it can be seamlessly extended to the video domain with the support of SAM2. Given the absence of in-context segmentation in the video domain, we manually curate and construct the first benchmark from existing video segmentation datasets, named In-Context Video Object Segmentation (IC-VOS), to better assess the in-context capability of the model. Extensive experiments demonstrate that our method achieves 55.5 (+1.4) mIoU on COCO-20 i ^{i} , 73.0 (+1.1) mIoU on PASCAL-5 i ^{i} , and a J &amp;amp; F \mathcal {J\&amp;amp;F} score...&lt;/p&gt;</content:encoded></item><item><title>Local Saliency-Guided Dynamic Matching for Cross-Modal Remote Sensing Image-Text Retrieval</title><link>https://doi.org/10.1109/tgrs.2025.3646809</link><guid>10.1109/tgrs.2025.3646809</guid><pubDate>Mon, 22 Dec 2025 18:41:29 +0000</pubDate><dc:creator>Jie Shao</dc:creator><dc:creator>Yiran Xie</dc:creator><dc:creator>Pengda Wang</dc:creator><dc:creator>Guohao Feng</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3646809</prism:doi><description>Remote sensing cross-modal text-image retrieval (RSCTIR) has emerged as a fundamental task in remote sensing analysis, aiming to bridge the semantic gap between visual and textual modalities under complex scenarios. Despite recent progress, salient information representation and accurate cross-modal alignments remain significant challenges. In this work, we demonstrate that the correlation between visual and textual features is a valuable cue that can be leveraged to enhance saliency analysis and metric learning in cross-modal retrieval. To this end, we propose a Local Saliency Mining module that uses textual attention to guide the extraction of visual salient features. Additionally, we introduce a Multi-granularity Similarity Contrastive Loss and a Dynamic Similarity Matching Loss to enhance semantic alignment. Finally, a graph-based diffusion reranking algorithm is developed to optimize retrieval ranking by leveraging the intrinsic manifold structure of multimodal data, thereby mitigating local optima during inference. Extensive experiments on the RSICD, RSITMD and UCM-Captions datasets confirm the effectiveness and superiority of our method over state-of-the-art approaches.
Published: 2025-12-22T18:41:29+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.485 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jie Shao; Yiran Xie; Pengda Wang; Guohao Feng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3646809"&gt;10.1109/tgrs.2025.3646809&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.485 (consider)&lt;/p&gt;
&lt;p&gt;Remote sensing cross-modal text-image retrieval (RSCTIR) has emerged as a fundamental task in remote sensing analysis, aiming to bridge the semantic gap between visual and textual modalities under complex scenarios. Despite recent progress, salient information representation and accurate cross-modal alignments remain significant challenges. In this work, we demonstrate that the correlation between visual and textual features is a valuable cue that can be leveraged to enhance saliency analysis and metric learning in cross-modal retrieval. To this end, we propose a Local Saliency Mining module that uses textual attention to guide the extraction of visual salient features. Additionally, we introduce a Multi-granularity Similarity Contrastive Loss and a Dynamic Similarity Matching Loss to enhance semantic alignment. Finally, a graph-based diffusion reranking algorithm is developed to optimize retrieval ranking by leveraging the intrinsic manifold structure of multimodal data, thereby mitigating local optima during inference. Extensive experiments on the RSICD, RSITMD and UCM-Captions datasets confirm the effectiveness and superiority of our method over state-of-the-art approaches.&lt;/p&gt;</content:encoded></item><item><title>Semantic representation of cross-modal events based on social multi-view graph attention network</title><link>https://doi.org/10.1016/j.ins.2025.123025</link><guid>10.1016/j.ins.2025.123025</guid><pubDate>Tue, 23 Dec 2025 07:38:00 +0000</pubDate><dc:creator>Wanqiu Cui</dc:creator><dc:creator>Dawei Wang</dc:creator><dc:creator>Wengang Feng</dc:creator><dc:creator>Jingjing Lu</dc:creator><prism:publicationName>Information Sciences</prism:publicationName><prism:doi>10.1016/j.ins.2025.123025</prism:doi><description>Semantic representation learning is a crucial technology for analyzing and detecting public opinion events propagated through social networks. However, the randomness and sparsity of news present significant challenges to effective semantic learning. In addition to text and image, temporal information is another vital element for accurately reflecting events. Therefore, we propose a Multi-View Graph Attention Network guided by Time disruption information (T-MVGAN). This model enhances event semantics by employing neighbor aggregation and multi-view fusion within a heterogeneous cross-modal event graph. Specifically, we construct a cross-modal heterogeneous graph by incorporating hashtags to link isolated messages and provide a comprehensive description of events. Then, we learn view-specific representations of events via graph convolutional networks, considering the perspectives of text, image, and temporal distribution, respectively. Finally, we design a time-based multi-view graph attention mechanism to capture the intrinsic interactions across different views and integrate their feature representations. This approach learns the deep semantics of social network events by transforming diverse observations into a unified semantic space. Extensive experiments on public Twitter datasets reveal that T-MVGAN performs favorably compared to many state-of-the-art semantic learning algorithms. These results also show that more meaningful signals, such as publication time and hashtags, can contribute to enhancing the performance of semantic learning.
Published: 2025-12-23T07:38:00+00:00
Venue: Information Sciences
Score: 0.485 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wanqiu Cui; Dawei Wang; Wengang Feng; Jingjing Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Sciences&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.ins.2025.123025"&gt;10.1016/j.ins.2025.123025&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.485 (consider)&lt;/p&gt;
&lt;p&gt;Semantic representation learning is a crucial technology for analyzing and detecting public opinion events propagated through social networks. However, the randomness and sparsity of news present significant challenges to effective semantic learning. In addition to text and image, temporal information is another vital element for accurately reflecting events. Therefore, we propose a Multi-View Graph Attention Network guided by Time disruption information (T-MVGAN). This model enhances event semantics by employing neighbor aggregation and multi-view fusion within a heterogeneous cross-modal event graph. Specifically, we construct a cross-modal heterogeneous graph by incorporating hashtags to link isolated messages and provide a comprehensive description of events. Then, we learn view-specific representations of events via graph convolutional networks, considering the perspectives of text, image, and temporal distribution, respectively. Finally, we design a time-based multi-view graph attention mechanism to capture the intrinsic interactions across different views and integrate their feature representations. This approach learns the deep semantics of social network events by transforming diverse observations into a unified semantic space. Extensive experiments on public Twitter datasets reveal that T-MVGAN performs favorably compared to many state-of-the-art semantic learning algorithms. These results also show that more meaningful signals, such as publication time and hashtags, can contribute to enhancing the performance of semantic learning.&lt;/p&gt;</content:encoded></item><item><title>$M^3-Verse$: A "Spot the Difference" Challenge for Large Multimodal Models</title><link>https://arxiv.org/abs/2512.18735v1</link><guid>http://arxiv.org/abs/2512.18735v1</guid><pubDate>Sun, 21 Dec 2025 13:50:26 +0000</pubDate><dc:creator>Kewei Wei</dc:creator><dc:creator>Bocheng Hu</dc:creator><dc:creator>Jie Cao</dc:creator><dc:creator>Xiaohan Chen</dc:creator><dc:creator>Zhengxi Lu</dc:creator><dc:creator>Wubing Xia</dc:creator><dc:creator>Weili Xu</dc:creator><dc:creator>Jiaao Wu</dc:creator><dc:creator>Junchen He</dc:creator><dc:creator>Mingyu Jia</dc:creator><dc:creator>Ciyun Zhao</dc:creator><dc:creator>Ye Sun</dc:creator><dc:creator>Yizhi Li</dc:creator><dc:creator>Zhonghan Zhao</dc:creator><dc:creator>Jian Zhang</dc:creator><dc:creator>Gaoang Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Modern Large Multimodal Models (LMMs) have demonstrated extraordinary ability in static image and single-state spatial-temporal understanding. However, their capacity to comprehend the dynamic changes of objects within a shared spatial context between two distinct video observations, remains largely unexplored. This ability to reason about transformations within a consistent environment is particularly crucial for advancements in the field of spatial intelligence. In this paper, we introduce $M^3-Verse$, a Multi-Modal, Multi-State, Multi-Dimensional benchmark, to formally evaluate this capability. It is built upon paired videos that provide multi-perspective observations of an indoor scene before and after a state change. The benchmark contains a total of 270 scenes and 2,932 questions, which are categorized into over 50 subtasks that probe 4 core capabilities. We evaluate 16 state-of-the-art LMMs and observe their limitations in tracking state transitions. To address these challenges, we further propose a simple yet effective baseline that achieves significant performance improvements in multi-state perception. $M^3-Verse$ thus provides a challenging new testbed to catalyze the development of next-generation models with a more holistic understanding of our dynamic visual world. You can get the construction pipeline from https://github.com/Wal-K-aWay/M3-Verse_pipeline and full benchmark data from https://www.modelscope.cn/datasets/WalKaWay/M3-Verse.
Published: 2025-12-21T13:50:26+00:00
Venue: arXiv
Score: 0.483 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kewei Wei; Bocheng Hu; Jie Cao; Xiaohan Chen; Zhengxi Lu; Wubing Xia; Weili Xu; Jiaao Wu; Junchen He; Mingyu Jia; Ciyun Zhao; Ye Sun; Yizhi Li; Zhonghan Zhao; Jian Zhang; Gaoang Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.483 (consider)&lt;/p&gt;
&lt;p&gt;Modern Large Multimodal Models (LMMs) have demonstrated extraordinary ability in static image and single-state spatial-temporal understanding. However, their capacity to comprehend the dynamic changes of objects within a shared spatial context between two distinct video observations, remains largely unexplored. This ability to reason about transformations within a consistent environment is particularly crucial for advancements in the field of spatial intelligence. In this paper, we introduce $M^3-Verse$, a Multi-Modal, Multi-State, Multi-Dimensional benchmark, to formally evaluate this capability. It is built upon paired videos that provide multi-perspective observations of an indoor scene before and after a state change. The benchmark contains a total of 270 scenes and 2,932 questions, which are categorized into over 50 subtasks that probe 4 core capabilities. We evaluate 16 state-of-the-art LMMs and observe their limitations in tracking state transitions. To address these challenges, we further propose a simple yet effective baseline that achieves significant performance improvements in multi-state perception. $M^3-Verse$ thus provides a challenging new testbed to catalyze the development of next-generation models with a more holistic understanding of our dynamic visual world. You can get the construction pipeline from https://github.com/Wal-K-aWay/M3-Verse_pipeline and full benchmark data from https://www.modelscope.cn/datasets/WalKaWay/M3-Verse.&lt;/p&gt;</content:encoded></item><item><title>SG-RIFE: Semantic-Guided Real-Time Intermediate Flow Estimation with Diffusion-Competitive Perceptual Quality</title><link>https://arxiv.org/abs/2512.18241v1</link><guid>http://arxiv.org/abs/2512.18241v1</guid><pubDate>Sat, 20 Dec 2025 06:50:55 +0000</pubDate><dc:creator>Pan Ben Wong</dc:creator><dc:creator>Chengli Wu</dc:creator><dc:creator>Hanyue Lu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Real-time Video Frame Interpolation (VFI) has long been dominated by flow-based methods like RIFE, which offer high throughput but often fail in complicated scenarios involving large motion and occlusion. Conversely, recent diffusion-based approaches (e.g., Consec. BB) achieve state-of-the-art perceptual quality but suffer from prohibitive latency, rendering them impractical for real-time applications. To bridge this gap, we propose Semantic-Guided RIFE (SG-RIFE). Instead of training from scratch, we introduce a parameter-efficient fine-tuning strategy that augments a pre-trained RIFE backbone with semantic priors from a frozen DINOv3 Vision Transformer. We propose a Split-Fidelity Aware Projection Module (Split-FAPM) to compress and refine high-dimensional features, and a Deformable Semantic Fusion (DSF) module to align these semantic priors with pixel-level motion fields. Experiments on SNU-FILM demonstrate that semantic injection provides a decisive boost in perceptual fidelity. SG-RIFE outperforms diffusion-based LDMVFI in FID/LPIPS and achieves quality comparable to Consec. BB on complex benchmarks while running significantly faster, proving that semantic consistency enables flow-based methods to achieve diffusion-competitive perceptual quality in near real-time.
Published: 2025-12-20T06:50:55+00:00
Venue: arXiv
Score: 0.482 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pan Ben Wong; Chengli Wu; Hanyue Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.482 (consider)&lt;/p&gt;
&lt;p&gt;Real-time Video Frame Interpolation (VFI) has long been dominated by flow-based methods like RIFE, which offer high throughput but often fail in complicated scenarios involving large motion and occlusion. Conversely, recent diffusion-based approaches (e.g., Consec. BB) achieve state-of-the-art perceptual quality but suffer from prohibitive latency, rendering them impractical for real-time applications. To bridge this gap, we propose Semantic-Guided RIFE (SG-RIFE). Instead of training from scratch, we introduce a parameter-efficient fine-tuning strategy that augments a pre-trained RIFE backbone with semantic priors from a frozen DINOv3 Vision Transformer. We propose a Split-Fidelity Aware Projection Module (Split-FAPM) to compress and refine high-dimensional features, and a Deformable Semantic Fusion (DSF) module to align these semantic priors with pixel-level motion fields. Experiments on SNU-FILM demonstrate that semantic injection provides a decisive boost in perceptual fidelity. SG-RIFE outperforms diffusion-based LDMVFI in FID/LPIPS and achieves quality comparable to Consec. BB on complex benchmarks while running significantly faster, proving that semantic consistency enables flow-based methods to achieve diffusion-competitive perceptual quality in near real-time.&lt;/p&gt;</content:encoded></item><item><title>Generating the Past, Present and Future from a Motion-Blurred Image</title><link>https://arxiv.org/abs/2512.19817v1</link><guid>http://arxiv.org/abs/2512.19817v1</guid><pubDate>Mon, 22 Dec 2025 19:12:33 +0000</pubDate><dc:creator>SaiKiran Tedla</dc:creator><dc:creator>Kelly Zhu</dc:creator><dc:creator>Trevor Canham</dc:creator><dc:creator>Felix Taubner</dc:creator><dc:creator>Michael S. Brown</dc:creator><dc:creator>Kiriakos N. Kutulakos</dc:creator><dc:creator>David B. Lindell</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1145/3763306</prism:doi><description>We seek to answer the question: what can a motion-blurred image reveal about a scene's past, present, and future? Although motion blur obscures image details and degrades visual quality, it also encodes information about scene and camera motion during an exposure. Previous techniques leverage this information to estimate a sharp image from an input blurry one, or to predict a sequence of video frames showing what might have occurred at the moment of image capture. However, they rely on handcrafted priors or network architectures to resolve ambiguities in this inverse problem, and do not incorporate image and video priors on large-scale datasets. As such, existing methods struggle to reproduce complex scene dynamics and do not attempt to recover what occurred before or after an image was taken. Here, we introduce a new technique that repurposes a pre-trained video diffusion model trained on internet-scale datasets to recover videos revealing complex scene dynamics during the moment of capture and what might have occurred immediately into the past or future. Our approach is robust and versatile; it outperforms previous methods for this task, generalizes to challenging in-the-wild images, and supports downstream tasks such as recovering camera trajectories, object motion, and dynamic 3D scene structure. Code and data are available at https://blur2vid.github.io
Published: 2025-12-22T19:12:33+00:00
Venue: arXiv
Score: 0.479 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; SaiKiran Tedla; Kelly Zhu; Trevor Canham; Felix Taubner; Michael S. Brown; Kiriakos N. Kutulakos; David B. Lindell&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1145/3763306"&gt;10.1145/3763306&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.479 (consider)&lt;/p&gt;
&lt;p&gt;We seek to answer the question: what can a motion-blurred image reveal about a scene&amp;#x27;s past, present, and future? Although motion blur obscures image details and degrades visual quality, it also encodes information about scene and camera motion during an exposure. Previous techniques leverage this information to estimate a sharp image from an input blurry one, or to predict a sequence of video frames showing what might have occurred at the moment of image capture. However, they rely on handcrafted priors or network architectures to resolve ambiguities in this inverse problem, and do not incorporate image and video priors on large-scale datasets. As such, existing methods struggle to reproduce complex scene dynamics and do not attempt to recover what occurred before or after an image was taken. Here, we introduce a new technique that repurposes a pre-trained video diffusion model trained on internet-scale datasets to recover videos revealing complex scene dynamics during the moment of capture and what might have occurred immediately into the past or future. Our approach is robust and versatile; it outperforms previous methods for this task, generalizes to challenging in-the-wild images, and supports downstream tasks such as recovering camera trajectories, object motion, and dynamic 3D scene structure. Code and data are available at https://blur2vid.github.io&lt;/p&gt;</content:encoded></item><item><title>OpenView: Empowering MLLMs with Out-of-view VQA</title><link>https://arxiv.org/abs/2512.18563v1</link><guid>http://arxiv.org/abs/2512.18563v1</guid><pubDate>Sun, 21 Dec 2025 02:11:40 +0000</pubDate><dc:creator>Qixiang Chen</dc:creator><dc:creator>Cheng Zhang</dc:creator><dc:creator>Chi-Wing Fu</dc:creator><dc:creator>Jingwen Ye</dc:creator><dc:creator>Jianfei Cai</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent multimodal large language models (MLLMs) show great potential in natural image understanding. Yet, they perform well, mainly on reasoning in-view contents within the image frame. This paper presents the first study on out-of-view (OOV) understanding, i.e., the ability to reason objects, activities, and scenes beyond the visible frame of a perspective view. Our technical contributions are threefold. First, we design OpenView, a four-stage pipeline to massively generate multi-choice VQA by leveraging panoramic imagery to enable context-rich and spatial-grounded VQA synthesis with free-view framing. Second, we curate OpenView-Dataset, a high-quality synthetic dataset from diverse real-world panoramas to empower MLLMs upon supervised fine-tuning. Third, we build OpenView-Bench, a benchmark that jointly measures choice and rationale accuracy for interpretable and diagnosable evaluation. Experimental results show that despite having a large gap from human performance in OOV VQA answer selection, upon empowered by OpenView, multiple MLLMs can consistently boost their performance, uplifted from 48.6% to 64.1% on average. Code, benchmark, and data will be available at https://github.com/q1xiangchen/OpenView.
Published: 2025-12-21T02:11:40+00:00
Venue: arXiv
Score: 0.478 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qixiang Chen; Cheng Zhang; Chi-Wing Fu; Jingwen Ye; Jianfei Cai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.478 (consider)&lt;/p&gt;
&lt;p&gt;Recent multimodal large language models (MLLMs) show great potential in natural image understanding. Yet, they perform well, mainly on reasoning in-view contents within the image frame. This paper presents the first study on out-of-view (OOV) understanding, i.e., the ability to reason objects, activities, and scenes beyond the visible frame of a perspective view. Our technical contributions are threefold. First, we design OpenView, a four-stage pipeline to massively generate multi-choice VQA by leveraging panoramic imagery to enable context-rich and spatial-grounded VQA synthesis with free-view framing. Second, we curate OpenView-Dataset, a high-quality synthetic dataset from diverse real-world panoramas to empower MLLMs upon supervised fine-tuning. Third, we build OpenView-Bench, a benchmark that jointly measures choice and rationale accuracy for interpretable and diagnosable evaluation. Experimental results show that despite having a large gap from human performance in OOV VQA answer selection, upon empowered by OpenView, multiple MLLMs can consistently boost their performance, uplifted from 48.6% to 64.1% on average. Code, benchmark, and data will be available at https://github.com/q1xiangchen/OpenView.&lt;/p&gt;</content:encoded></item><item><title>A Study of Finetuning Video Transformers for Multi-view Geometry Tasks</title><link>https://arxiv.org/abs/2512.18684v1</link><guid>http://arxiv.org/abs/2512.18684v1</guid><pubDate>Sun, 21 Dec 2025 10:41:11 +0000</pubDate><dc:creator>Huimin Wu</dc:creator><dc:creator>Kwang-Ting Cheng</dc:creator><dc:creator>Stephen Lin</dc:creator><dc:creator>Zhirong Wu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>This paper presents an investigation of vision transformer learning for multi-view geometry tasks, such as optical flow estimation, by fine-tuning video foundation models. Unlike previous methods that involve custom architectural designs and task-specific pretraining, our research finds that general-purpose models pretrained on videos can be readily transferred to multi-view problems with minimal adaptation. The core insight is that general-purpose attention between patches learns temporal and spatial information for geometric reasoning. We demonstrate that appending a linear decoder to the Transformer backbone produces satisfactory results, and iterative refinement can further elevate performance to stateof-the-art levels. This conceptually simple approach achieves top cross-dataset generalization results for optical flow estimation with end-point error (EPE) of 0.69, 1.78, and 3.15 on the Sintel clean, Sintel final, and KITTI datasets, respectively. Our method additionally establishes a new record on the online test benchmark with EPE values of 0.79, 1.88, and F1 value of 3.79. Applications to 3D depth estimation and stereo matching also show strong performance, illustrating the versatility of video-pretrained models in addressing geometric vision tasks.
Published: 2025-12-21T10:41:11+00:00
Venue: arXiv
Score: 0.477 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Huimin Wu; Kwang-Ting Cheng; Stephen Lin; Zhirong Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.477 (consider)&lt;/p&gt;
&lt;p&gt;This paper presents an investigation of vision transformer learning for multi-view geometry tasks, such as optical flow estimation, by fine-tuning video foundation models. Unlike previous methods that involve custom architectural designs and task-specific pretraining, our research finds that general-purpose models pretrained on videos can be readily transferred to multi-view problems with minimal adaptation. The core insight is that general-purpose attention between patches learns temporal and spatial information for geometric reasoning. We demonstrate that appending a linear decoder to the Transformer backbone produces satisfactory results, and iterative refinement can further elevate performance to stateof-the-art levels. This conceptually simple approach achieves top cross-dataset generalization results for optical flow estimation with end-point error (EPE) of 0.69, 1.78, and 3.15 on the Sintel clean, Sintel final, and KITTI datasets, respectively. Our method additionally establishes a new record on the online test benchmark with EPE values of 0.79, 1.88, and F1 value of 3.79. Applications to 3D depth estimation and stereo matching also show strong performance, illustrating the versatility of video-pretrained models in addressing geometric vision tasks.&lt;/p&gt;</content:encoded></item><item><title>Revealing Perception and Generation Dynamics in LVLMs: Mitigating Hallucinations via Validated Dominance Correction</title><link>https://arxiv.org/abs/2512.18813v1</link><guid>http://arxiv.org/abs/2512.18813v1</guid><pubDate>Sun, 21 Dec 2025 17:05:42 +0000</pubDate><dc:creator>Guangtao Lyu</dc:creator><dc:creator>Xinyi Cheng</dc:creator><dc:creator>Chenghao Xu</dc:creator><dc:creator>Qi Liu</dc:creator><dc:creator>Muli Yang</dc:creator><dc:creator>Fen Fang</dc:creator><dc:creator>Huilin Chen</dc:creator><dc:creator>Jiexi Yan</dc:creator><dc:creator>Xu Yang</dc:creator><dc:creator>Cheng Deng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Vision-Language Models (LVLMs) have shown remarkable capabilities, yet hallucinations remain a persistent challenge. This work presents a systematic analysis of the internal evolution of visual perception and token generation in LVLMs, revealing two key patterns. First, perception follows a three-stage GATE process: early layers perform a Global scan, intermediate layers Approach and Tighten on core content, and later layers Explore supplementary regions. Second, generation exhibits an SAD (Subdominant Accumulation to Dominant) pattern, where hallucinated tokens arise from the repeated accumulation of subdominant tokens lacking support from attention (visual perception) or feed-forward network (internal knowledge). Guided by these findings, we devise the VDC (Validated Dominance Correction) strategy, which detects unsupported tokens and replaces them with validated dominant ones to improve output reliability. Extensive experiments across multiple models and benchmarks confirm that VDC substantially mitigates hallucinations.
Published: 2025-12-21T17:05:42+00:00
Venue: arXiv
Score: 0.477 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guangtao Lyu; Xinyi Cheng; Chenghao Xu; Qi Liu; Muli Yang; Fen Fang; Huilin Chen; Jiexi Yan; Xu Yang; Cheng Deng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.477 (consider)&lt;/p&gt;
&lt;p&gt;Large Vision-Language Models (LVLMs) have shown remarkable capabilities, yet hallucinations remain a persistent challenge. This work presents a systematic analysis of the internal evolution of visual perception and token generation in LVLMs, revealing two key patterns. First, perception follows a three-stage GATE process: early layers perform a Global scan, intermediate layers Approach and Tighten on core content, and later layers Explore supplementary regions. Second, generation exhibits an SAD (Subdominant Accumulation to Dominant) pattern, where hallucinated tokens arise from the repeated accumulation of subdominant tokens lacking support from attention (visual perception) or feed-forward network (internal knowledge). Guided by these findings, we devise the VDC (Validated Dominance Correction) strategy, which detects unsupported tokens and replaces them with validated dominant ones to improve output reliability. Extensive experiments across multiple models and benchmarks confirm that VDC substantially mitigates hallucinations.&lt;/p&gt;</content:encoded></item><item><title>Generative Giants, Retrieval Weaklings: Why do Multimodal Large Language Models Fail at Multimodal Retrieval?</title><link>https://arxiv.org/abs/2512.19115v1</link><guid>http://arxiv.org/abs/2512.19115v1</guid><pubDate>Mon, 22 Dec 2025 07:36:20 +0000</pubDate><dc:creator>Hengyi Feng</dc:creator><dc:creator>Zeang Sheng</dc:creator><dc:creator>Meiyi Qiang</dc:creator><dc:creator>Wentao Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Despite the remarkable success of multimodal large language models (MLLMs) in generative tasks, we observe that they exhibit a counterintuitive deficiency in the zero-shot multimodal retrieval task. In this work, we investigate the underlying mechanisms that hinder MLLMs from serving as effective retrievers. With the help of sparse autoencoders (SAEs), we decompose MLLM output representations into interpretable semantic concepts to probe their intrinsic behavior. Our analysis reveals that the representation space of MLLMs is overwhelmingly dominated by textual semantics; the visual information essential for multimodal retrieval only constitutes a small portion. This imbalance is compounded by the heavy focus of MLLMs on bridging image-text modalities, which facilitates generation but homogenizes embeddings and finally diminishes the discriminative power required for multimodal retrieval. We further discover that the specific feature components that contribute most to the similarity computations for MLLMs are in fact distractors that actively degrade retrieval performance. Overall, our work provides the first in-depth interpretability analysis of MLLM representations in the context of multimodal retrieval and offers possible directions for enhancing the multimodal retrieval capabilities of MLLMs.
Published: 2025-12-22T07:36:20+00:00
Venue: arXiv
Score: 0.476 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hengyi Feng; Zeang Sheng; Meiyi Qiang; Wentao Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.476 (consider)&lt;/p&gt;
&lt;p&gt;Despite the remarkable success of multimodal large language models (MLLMs) in generative tasks, we observe that they exhibit a counterintuitive deficiency in the zero-shot multimodal retrieval task. In this work, we investigate the underlying mechanisms that hinder MLLMs from serving as effective retrievers. With the help of sparse autoencoders (SAEs), we decompose MLLM output representations into interpretable semantic concepts to probe their intrinsic behavior. Our analysis reveals that the representation space of MLLMs is overwhelmingly dominated by textual semantics; the visual information essential for multimodal retrieval only constitutes a small portion. This imbalance is compounded by the heavy focus of MLLMs on bridging image-text modalities, which facilitates generation but homogenizes embeddings and finally diminishes the discriminative power required for multimodal retrieval. We further discover that the specific feature components that contribute most to the similarity computations for MLLMs are in fact distractors that actively degrade retrieval performance. Overall, our work provides the first in-depth interpretability analysis of MLLM representations in the context of multimodal retrieval and offers possible directions for enhancing the multimodal retrieval capabilities of MLLMs.&lt;/p&gt;</content:encoded></item><item><title>MtvTrack: Robust Visual Tracking via Modeling Time-Variant State of the Target</title><link>https://doi.org/10.1109/tnnls.2025.3642941</link><guid>10.1109/tnnls.2025.3642941</guid><pubDate>Tue, 23 Dec 2025 18:31:32 +0000</pubDate><dc:creator>Jiaxu Zhao</dc:creator><dc:creator>Mingxing Jia</dc:creator><dc:creator>Lei Guo</dc:creator><dc:creator>Xingyu Han</dc:creator><dc:creator>Dapeng Niu</dc:creator><prism:publicationName>IEEE Transactions on Neural Networks and Learning Systems</prism:publicationName><prism:doi>10.1109/tnnls.2025.3642941</prism:doi><description>Current single-object tracking algorithms depend on the information supplied by the template to identify and locate the object within the search area. However, environmental complexities and unknown factors can alter the object’s state, causing mismatches in template information. The existing works using the template update mechanism (TUM) and multiple template feature fusion have the following problems: 1) TUM is affected by input superposition, making it hard to eliminate noise; 2) they suffer a temporal lag in their responsiveness to changes that occur in the object during the tracking process; 3) it is insufficient to rely solely on visual features within the search area of the current frame to improve the template; and 4) the prior knowledge regarding the input is not fully leveraged to learn the time-variant state of the object. We observe that in complex tracking scenarios, humans subconsciously analyze the evolutionary patterns of the object and its surroundings and integrate this information with the object’s initial impression, thereby maintaining an awareness of the object’s temporal state. Motivated by this, we propose a novel solution to the above problem, named MtvTrack, which can model the time-variant state of the object through the dynamic evolution pattern and static initial impression. Simultaneously, we propose a method for predicting the evolution pattern of scenes by utilizing past, present, and future (PPF) states. This approach effectively eliminates the information redundancy between consecutive frames and addresses the issue of delayed predictions of the target state in relation to changes within the search area. We establish a joint probability generative model and fully utilize prior knowledge to learn the time-variant state of the object. In addition, we develop a vector quantized-PPF (VQPPF) module for predicting the object’s time-variant state. Experimental results on public benchmarks confirm the superior performance of our method. So...
Published: 2025-12-23T18:31:32+00:00
Venue: IEEE Transactions on Neural Networks and Learning Systems
Score: 0.476 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaxu Zhao; Mingxing Jia; Lei Guo; Xingyu Han; Dapeng Niu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Neural Networks and Learning Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tnnls.2025.3642941"&gt;10.1109/tnnls.2025.3642941&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.476 (consider)&lt;/p&gt;
&lt;p&gt;Current single-object tracking algorithms depend on the information supplied by the template to identify and locate the object within the search area. However, environmental complexities and unknown factors can alter the object’s state, causing mismatches in template information. The existing works using the template update mechanism (TUM) and multiple template feature fusion have the following problems: 1) TUM is affected by input superposition, making it hard to eliminate noise; 2) they suffer a temporal lag in their responsiveness to changes that occur in the object during the tracking process; 3) it is insufficient to rely solely on visual features within the search area of the current frame to improve the template; and 4) the prior knowledge regarding the input is not fully leveraged to learn the time-variant state of the object. We observe that in complex tracking scenarios, humans subconsciously analyze the evolutionary patterns of the object and its surroundings and integrate this information with the object’s initial impression, thereby maintaining an awareness of the object’s temporal state. Motivated by this, we propose a novel solution to the above problem, named MtvTrack, which can model the time-variant state of the object through the dynamic evolution pattern and static initial impression. Simultaneously, we propose a method for predicting the evolution pattern of scenes by utilizing past, present, and future (PPF) states. This approach effectively eliminates the information redundancy between consecutive frames and addresses the issue of delayed predictions of the target state in relation to changes within the search area. We establish a joint probability generative model and fully utilize prior knowledge to learn the time-variant state of the object. In addition, we develop a vector quantized-PPF (VQPPF) module for predicting the object’s time-variant state. Experimental results on public benchmarks confirm the superior performance of our method. So...&lt;/p&gt;</content:encoded></item><item><title>Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios</title><link>https://arxiv.org/abs/2512.20556v1</link><guid>http://arxiv.org/abs/2512.20556v1</guid><pubDate>Tue, 23 Dec 2025 17:55:35 +0000</pubDate><dc:creator>Mingwei Tang</dc:creator><dc:creator>Jiahao Nie</dc:creator><dc:creator>Guang Yang</dc:creator><dc:creator>Ziqing Cui</dc:creator><dc:creator>Jie Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Image fusion aims to synthesize a single high-quality image from a pair of inputs captured under challenging conditions, such as differing exposure levels or focal depths. A core challenge lies in effectively handling disparities in dynamic range and focus depth between the inputs. With the advent of vision-language models, recent methods incorporate textual descriptions as auxiliary guidance to enhance fusion quality. However, simply incorporating coarse-grained descriptions hampers the understanding of fine-grained details and poses challenges for precise cross-modal alignment. To address these limitations, we propose Multi-grained Text-guided Image Fusion (MTIF), a novel fusion paradigm with three key designs. First, it introduces multi-grained textual descriptions that separately capture fine details, structural cues, and semantic content, guiding image fusion through a hierarchical cross-modal modulation module. Second, it involves supervision signals at each granularity to facilitate alignment between visual and textual features and enhance the utility of auxiliary text. Third, it adopts a saliency-driven enrichment module to augment training data with dense semantic content, further strengthening the cross-modal modulation and alignment. Extensive experiments show that MTIF consistently outperforms previous methods on both multi-exposure and multi-focus image fusion tasks.
Published: 2025-12-23T17:55:35+00:00
Venue: arXiv
Score: 0.475 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingwei Tang; Jiahao Nie; Guang Yang; Ziqing Cui; Jie Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.475 (consider)&lt;/p&gt;
&lt;p&gt;Image fusion aims to synthesize a single high-quality image from a pair of inputs captured under challenging conditions, such as differing exposure levels or focal depths. A core challenge lies in effectively handling disparities in dynamic range and focus depth between the inputs. With the advent of vision-language models, recent methods incorporate textual descriptions as auxiliary guidance to enhance fusion quality. However, simply incorporating coarse-grained descriptions hampers the understanding of fine-grained details and poses challenges for precise cross-modal alignment. To address these limitations, we propose Multi-grained Text-guided Image Fusion (MTIF), a novel fusion paradigm with three key designs. First, it introduces multi-grained textual descriptions that separately capture fine details, structural cues, and semantic content, guiding image fusion through a hierarchical cross-modal modulation module. Second, it involves supervision signals at each granularity to facilitate alignment between visual and textual features and enhance the utility of auxiliary text. Third, it adopts a saliency-driven enrichment module to augment training data with dense semantic content, further strengthening the cross-modal modulation and alignment. Extensive experiments show that MTIF consistently outperforms previous methods on both multi-exposure and multi-focus image fusion tasks.&lt;/p&gt;</content:encoded></item><item><title>Self-Attention with State-Object Weighted Combination for Compositional Zero Shot Learning</title><link>https://arxiv.org/abs/2512.18969v1</link><guid>http://arxiv.org/abs/2512.18969v1</guid><pubDate>Mon, 22 Dec 2025 02:30:19 +0000</pubDate><dc:creator>Cheng-Hong Chang</dc:creator><dc:creator>Pei-Hsuan Tsai</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Object recognition has become prevalent across various industries. However, most existing applications are limited to identifying objects alone, without considering their associated states. The ability to recognize both the state and object simultaneously remains less common. One approach to address this is by treating state and object as a single category during training. However, this approach poses challenges in data collection and training since it requires comprehensive data for all possible combinations. Compositional Zero-shot Learning (CZSL) emerges as a viable solution by treating the state and object as distinct categories during training. CZSL facilitates the identification of novel compositions even in the absence of data for every conceivable combination. The current state-of-the-art method, KG-SP, addresses this issue by training distinct classifiers for states and objects, while leveraging a semantic model to evaluate the plausibility of composed compositions. However, KG-SP's accuracy in state and object recognition can be further improved, and it fails to consider the weighting of states and objects during composition. In this study, we propose SASOW, an enhancement of KG-SP that considers the weighting of states and objects while improving composition recognition accuracy. First, we introduce self-attention mechanisms into the classifiers for states and objects, leading to enhanced accuracy in recognizing both. Additionally, we incorporate the weighting of states and objects during composition to generate more reasonable and accurate compositions. Our validation process involves testing SASOW on three established benchmark datasets. Experimental outcomes affirm when compared against OW-CZSL approach, KG-SP, SASOW showcases improvements of 2.1%, 1.7%, and 0.4% in terms of accuracy for unseen compositions across the MIT-States, UT Zappos, and C-GQA datasets, respectively.
Published: 2025-12-22T02:30:19+00:00
Venue: arXiv
Score: 0.474 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Cheng-Hong Chang; Pei-Hsuan Tsai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.474 (consider)&lt;/p&gt;
&lt;p&gt;Object recognition has become prevalent across various industries. However, most existing applications are limited to identifying objects alone, without considering their associated states. The ability to recognize both the state and object simultaneously remains less common. One approach to address this is by treating state and object as a single category during training. However, this approach poses challenges in data collection and training since it requires comprehensive data for all possible combinations. Compositional Zero-shot Learning (CZSL) emerges as a viable solution by treating the state and object as distinct categories during training. CZSL facilitates the identification of novel compositions even in the absence of data for every conceivable combination. The current state-of-the-art method, KG-SP, addresses this issue by training distinct classifiers for states and objects, while leveraging a semantic model to evaluate the plausibility of composed compositions. However, KG-SP&amp;#x27;s accuracy in state and object recognition can be further improved, and it fails to consider the weighting of states and objects during composition. In this study, we propose SASOW, an enhancement of KG-SP that considers the weighting of states and objects while improving composition recognition accuracy. First, we introduce self-attention mechanisms into the classifiers for states and objects, leading to enhanced accuracy in recognizing both. Additionally, we incorporate the weighting of states and objects during composition to generate more reasonable and accurate compositions. Our validation process involves testing SASOW on three established benchmark datasets. Experimental outcomes affirm when compared against OW-CZSL approach, KG-SP, SASOW showcases improvements of 2.1%, 1.7%, and 0.4% in terms of accuracy for unseen compositions across the MIT-States, UT Zappos, and C-GQA datasets, respectively.&lt;/p&gt;</content:encoded></item><item><title>CLIP2RS: Leveraging Pretrained Vision-Language Model for Semantic Segmentation of Remote Sensing Images</title><link>https://doi.org/10.1109/tgrs.2025.3647015</link><guid>10.1109/tgrs.2025.3647015</guid><pubDate>Mon, 22 Dec 2025 18:41:29 +0000</pubDate><dc:creator>Yinghui Xing</dc:creator><dc:creator>Dexuan Kong</dc:creator><dc:creator>Shizhou Zhang</dc:creator><dc:creator>Ziyi Li</dc:creator><dc:creator>Qingyi Li</dc:creator><dc:creator>Yanning Zhang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3647015</prism:doi><description>Semantic segmentation of Remote Sensing (RS) images is a very challenging task due to the complicated characteristics such as diversity, complexity and massiveness. Current research endeavors are predominantly centered on utilizing visual context information exclusively through meticulous architecture design, often overlooking significant semantic details. This oversight limits the efficacy in tackling the challenge of intra-class variations. While in this paper, we propose CLIP2RS which is devised to leverage the pretrained Vision-Language Model (VLM) for semantic segmentation of RS images via the guidance of prior knowledge stored in the pretrained foundation model. Specifically, CLIP2RS utilizes a two-stage training strategy to overcome the domain gap challenge between natural images and remote sensing images. A dual-granularity alignment framework that simultaneously aligns pixel-level local features and image-level global features is designed to alleviate severe class sample imbalance problem. Additionally, a novel prompting mechanism is effectively explored to to fully harness the potential of CLIP textual descriptions. We conduct comprehensive experiments on the iSAID, Potsdam, and Vaihingen datasets, and the experimental results show that our proposed method achieves state-of-the-art performances, demonstrating its superiority.
Published: 2025-12-22T18:41:29+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.474 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yinghui Xing; Dexuan Kong; Shizhou Zhang; Ziyi Li; Qingyi Li; Yanning Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3647015"&gt;10.1109/tgrs.2025.3647015&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.474 (consider)&lt;/p&gt;
&lt;p&gt;Semantic segmentation of Remote Sensing (RS) images is a very challenging task due to the complicated characteristics such as diversity, complexity and massiveness. Current research endeavors are predominantly centered on utilizing visual context information exclusively through meticulous architecture design, often overlooking significant semantic details. This oversight limits the efficacy in tackling the challenge of intra-class variations. While in this paper, we propose CLIP2RS which is devised to leverage the pretrained Vision-Language Model (VLM) for semantic segmentation of RS images via the guidance of prior knowledge stored in the pretrained foundation model. Specifically, CLIP2RS utilizes a two-stage training strategy to overcome the domain gap challenge between natural images and remote sensing images. A dual-granularity alignment framework that simultaneously aligns pixel-level local features and image-level global features is designed to alleviate severe class sample imbalance problem. Additionally, a novel prompting mechanism is effectively explored to to fully harness the potential of CLIP textual descriptions. We conduct comprehensive experiments on the iSAID, Potsdam, and Vaihingen datasets, and the experimental results show that our proposed method achieves state-of-the-art performances, demonstrating its superiority.&lt;/p&gt;</content:encoded></item><item><title>InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search</title><link>https://arxiv.org/abs/2512.18745v1</link><guid>http://arxiv.org/abs/2512.18745v1</guid><pubDate>Sun, 21 Dec 2025 14:23:07 +0000</pubDate><dc:creator>Kaican Li</dc:creator><dc:creator>Lewei Yao</dc:creator><dc:creator>Jiannan Wu</dc:creator><dc:creator>Tiezheng Yu</dc:creator><dc:creator>Jierun Chen</dc:creator><dc:creator>Haoli Bai</dc:creator><dc:creator>Lu Hou</dc:creator><dc:creator>Lanqing Hong</dc:creator><dc:creator>Wei Zhang</dc:creator><dc:creator>Nevin L. Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The ability for AI agents to "think with images" requires a sophisticated blend of reasoning and perception. However, current open multimodal agents still largely fall short on the reasoning aspect crucial for real-world tasks like analyzing documents with dense charts/diagrams and navigating maps. To address this gap, we introduce O3-Bench, a new benchmark designed to evaluate multimodal reasoning with interleaved attention to visual details. O3-Bench features challenging problems that require agents to piece together subtle visual information from distinct image areas through multi-step reasoning. The problems are highly challenging even for frontier systems like OpenAI o3, which only obtains 40.8% accuracy on O3-Bench. To make progress, we propose InSight-o3, a multi-agent framework consisting of a visual reasoning agent (vReasoner) and a visual search agent (vSearcher) for which we introduce the task of generalized visual search -- locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images. We then present a multimodal LLM purpose-trained for this task via reinforcement learning. As a plug-and-play agent, our vSearcher empowers frontier multimodal models (as vReasoners), significantly improving their performance on a wide range of benchmarks. This marks a concrete step towards powerful o3-like open systems. Our code and dataset can be found at https://github.com/m-Just/InSight-o3 .
Published: 2025-12-21T14:23:07+00:00
Venue: arXiv
Score: 0.473 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kaican Li; Lewei Yao; Jiannan Wu; Tiezheng Yu; Jierun Chen; Haoli Bai; Lu Hou; Lanqing Hong; Wei Zhang; Nevin L. Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.473 (consider)&lt;/p&gt;
&lt;p&gt;The ability for AI agents to &amp;quot;think with images&amp;quot; requires a sophisticated blend of reasoning and perception. However, current open multimodal agents still largely fall short on the reasoning aspect crucial for real-world tasks like analyzing documents with dense charts/diagrams and navigating maps. To address this gap, we introduce O3-Bench, a new benchmark designed to evaluate multimodal reasoning with interleaved attention to visual details. O3-Bench features challenging problems that require agents to piece together subtle visual information from distinct image areas through multi-step reasoning. The problems are highly challenging even for frontier systems like OpenAI o3, which only obtains 40.8% accuracy on O3-Bench. To make progress, we propose InSight-o3, a multi-agent framework consisting of a visual reasoning agent (vReasoner) and a visual search agent (vSearcher) for which we introduce the task of generalized visual search -- locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images. We then present a multimodal LLM purpose-trained for this task via reinforcement learning. As a plug-and-play agent, our vSearcher empowers frontier multimodal models (as vReasoners), significantly improving their performance on a wide range of benchmarks. This marks a concrete step towards powerful o3-like open systems. Our code and dataset can be found at https://github.com/m-Just/InSight-o3 .&lt;/p&gt;</content:encoded></item></channel></rss>