<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sat, 24 Jan 2026 02:47:45 +0000</lastBuildDate><item><title>DGL-RSIS: Decoupling global spatial context and local class semantics for training-free remote sensing image segmentation</title><link>https://doi.org/10.1016/j.jag.2026.105113</link><guid>10.1016/j.jag.2026.105113</guid><pubDate>Fri, 23 Jan 2026 20:33:45 +0000</pubDate><dc:creator>Boyi Li</dc:creator><dc:creator>Ce Zhang</dc:creator><dc:creator>Richard M. Timmerman</dc:creator><dc:creator>Wenxuan Bao</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2026.105113</prism:doi><description>The emergence of vision language models (VLMs) bridges the gap between vision and language, enabling multimodal understanding beyond traditional visual-only deep learning models. However, transferring VLMs from the natural image domain to remote sensing (RS) segmentation remains challenging due to the large domain gap and the diversity of RS inputs across tasks, particularly in open-vocabulary semantic segmentation (OVSS) and referring expression segmentation (RES). Here, we propose a training-free unified framework, termed DGL-RSIS, which decouples visual and textual representations and performs visual-language alignment at both local semantic and global contextual levels. Specifically, a Global–Local Decoupling (GLD) module decomposes textual inputs into local semantic tokens and global contextual tokens, while image inputs are partitioned into class-agnostic mask proposals. Then, a Local Visual–Textual Alignment (LVTA) module adaptively extracts context-aware visual features from the mask proposals and enriches textual features through knowledge-guided prompt engineering, achieving OVSS from a local perspective. Furthermore, a Global Visual–Textual Alignment (GVTA) module employs a global-enhanced Grad-CAM mechanism to capture contextual cues for referring expressions, followed by a mask selection module that integrates pixel-level activations into mask-level segmentation outputs, thereby achieving RES from a global perspective. Experiments on the iSAID (OVSS) and RRSIS-D (RES) benchmarks demonstrate that DGL-RSIS outperforms existing training-free approaches. Ablation studies further validate the effectiveness of each module. To the best of our knowledge, this is the first unified training-free framework for RS image segmentation, which effectively transfers the semantic capability of VLMs trained on natural images to the RS domain without additional training.
Published: 2026-01-23T20:33:45+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.600 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Boyi Li; Ce Zhang; Richard M. Timmerman; Wenxuan Bao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2026.105113"&gt;10.1016/j.jag.2026.105113&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.600 (must_read)&lt;/p&gt;
&lt;p&gt;The emergence of vision language models (VLMs) bridges the gap between vision and language, enabling multimodal understanding beyond traditional visual-only deep learning models. However, transferring VLMs from the natural image domain to remote sensing (RS) segmentation remains challenging due to the large domain gap and the diversity of RS inputs across tasks, particularly in open-vocabulary semantic segmentation (OVSS) and referring expression segmentation (RES). Here, we propose a training-free unified framework, termed DGL-RSIS, which decouples visual and textual representations and performs visual-language alignment at both local semantic and global contextual levels. Specifically, a Global–Local Decoupling (GLD) module decomposes textual inputs into local semantic tokens and global contextual tokens, while image inputs are partitioned into class-agnostic mask proposals. Then, a Local Visual–Textual Alignment (LVTA) module adaptively extracts context-aware visual features from the mask proposals and enriches textual features through knowledge-guided prompt engineering, achieving OVSS from a local perspective. Furthermore, a Global Visual–Textual Alignment (GVTA) module employs a global-enhanced Grad-CAM mechanism to capture contextual cues for referring expressions, followed by a mask selection module that integrates pixel-level activations into mask-level segmentation outputs, thereby achieving RES from a global perspective. Experiments on the iSAID (OVSS) and RRSIS-D (RES) benchmarks demonstrate that DGL-RSIS outperforms existing training-free approaches. Ablation studies further validate the effectiveness of each module. To the best of our knowledge, this is the first unified training-free framework for RS image segmentation, which effectively transfers the semantic capability of VLMs trained on natural images to the RS domain without additional training.&lt;/p&gt;</content:encoded></item><item><title>RAM-VQA: Restoration Assisted Multi-modality Video Quality Assessment</title><link>https://doi.org/10.1109/tip.2026.3655117</link><guid>10.1109/tip.2026.3655117</guid><pubDate>Fri, 23 Jan 2026 21:01:47 +0000</pubDate><dc:creator>Pengfei Chen</dc:creator><dc:creator>Jiebin Yan</dc:creator><dc:creator>Rajiv Soundararajan</dc:creator><dc:creator>Giuseppe Valenzise</dc:creator><dc:creator>Cai Li</dc:creator><dc:creator>Leida Li</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3655117</prism:doi><description>Video Quality Assessment (VQA) strives to computationally emulate human perceptual judgments and has garnered significant attention given its widespread applicability. However, existing methodologies face two primary impediments: (1) limited proficiency in evaluating samples at quality extremes (e.g., severely degraded or near-perfect videos), and (2) insufficient sensitivity to nuanced quality variations arising from a misalignment with human perceptual mechanisms. Although vision-language models offer promising semantic understanding, their reliance on visual encoders pre-trained for high-level tasks often compromises their sensitivity to low-level distortions. To surmount these challenges, we propose the Restoration-Assisted Multi-modality VQA (RAM-VQA) framework. Uniquely, our approach leverages video restoration as a proxy to explicitly model distortion-sensitive features. The framework operates through two synergistic stages: a prompt learning stage that constructs a quality-aware textual space using triple-level references (degraded, restored, and pristine) derived from the restoration process, and a dual-branch evaluation stage that integrates semantic cues with technical quality indicators via spatio-temporal differential analysis. Extensive experiments demonstrate that RAM-VQA achieves state-of-the-art performance across diverse benchmarks, exhibiting superior capability in handling extreme-quality content while ensuring robust generalization.
Published: 2026-01-23T21:01:47+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.583 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pengfei Chen; Jiebin Yan; Rajiv Soundararajan; Giuseppe Valenzise; Cai Li; Leida Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3655117"&gt;10.1109/tip.2026.3655117&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.583 (consider)&lt;/p&gt;
&lt;p&gt;Video Quality Assessment (VQA) strives to computationally emulate human perceptual judgments and has garnered significant attention given its widespread applicability. However, existing methodologies face two primary impediments: (1) limited proficiency in evaluating samples at quality extremes (e.g., severely degraded or near-perfect videos), and (2) insufficient sensitivity to nuanced quality variations arising from a misalignment with human perceptual mechanisms. Although vision-language models offer promising semantic understanding, their reliance on visual encoders pre-trained for high-level tasks often compromises their sensitivity to low-level distortions. To surmount these challenges, we propose the Restoration-Assisted Multi-modality VQA (RAM-VQA) framework. Uniquely, our approach leverages video restoration as a proxy to explicitly model distortion-sensitive features. The framework operates through two synergistic stages: a prompt learning stage that constructs a quality-aware textual space using triple-level references (degraded, restored, and pristine) derived from the restoration process, and a dual-branch evaluation stage that integrates semantic cues with technical quality indicators via spatio-temporal differential analysis. Extensive experiments demonstrate that RAM-VQA achieves state-of-the-art performance across diverse benchmarks, exhibiting superior capability in handling extreme-quality content while ensuring robust generalization.&lt;/p&gt;</content:encoded></item><item><title>GEOMR: Integrating Image Geographic Features and Human Reasoning Knowledge for Image Geolocalization</title><link>https://doi.org/10.1016/j.knosys.2026.115391</link><guid>10.1016/j.knosys.2026.115391</guid><pubDate>Fri, 23 Jan 2026 16:51:03 +0000</pubDate><dc:creator>Jian Fang</dc:creator><dc:creator>Siyi Qian</dc:creator><dc:creator>Shaohui Liu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115391</prism:doi><description>Worldwide image geolocalization aims to accurately predict the geographic location where a given image was captured. Due to the vast scale of the Earth and the uneven distribution of geographic features, this task remains highly challenging. Traditional methods exhibit clear limitations when handling global-scale data. To address these challenges, we propose GEOMR, an effective and adaptive framework that integrates image geographic features and human reasoning knowledge to enhance global geolocalization accuracy. GEOMR consists of two modules. The first module extracts geographic features from images by jointly learning multimodal features. The second module involves training a multimodal large language model in a two-phase process to enhance its geolocalization reasoning capabilities. The first phase learns human geolocalization reasoning knowledge, enabling the model to utilize geographic cues present in images effectively. The second phase focuses on learning how to use reference information to infer the correct geographic coordinates. Extensive experiments conducted on the IM2GPS3K, YFCC4K, and YFCC26K datasets demonstrate that GEOMR significantly outperforms state-of-the-art methods.
Published: 2026-01-23T16:51:03+00:00
Venue: Knowledge-Based Systems
Score: 0.580 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jian Fang; Siyi Qian; Shaohui Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115391"&gt;10.1016/j.knosys.2026.115391&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.580 (consider)&lt;/p&gt;
&lt;p&gt;Worldwide image geolocalization aims to accurately predict the geographic location where a given image was captured. Due to the vast scale of the Earth and the uneven distribution of geographic features, this task remains highly challenging. Traditional methods exhibit clear limitations when handling global-scale data. To address these challenges, we propose GEOMR, an effective and adaptive framework that integrates image geographic features and human reasoning knowledge to enhance global geolocalization accuracy. GEOMR consists of two modules. The first module extracts geographic features from images by jointly learning multimodal features. The second module involves training a multimodal large language model in a two-phase process to enhance its geolocalization reasoning capabilities. The first phase learns human geolocalization reasoning knowledge, enabling the model to utilize geographic cues present in images effectively. The second phase focuses on learning how to use reference information to infer the correct geographic coordinates. Extensive experiments conducted on the IM2GPS3K, YFCC4K, and YFCC26K datasets demonstrate that GEOMR significantly outperforms state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Graph Recognition via Subgraph Prediction</title><link>https://arxiv.org/abs/2601.15133v1</link><guid>http://arxiv.org/abs/2601.15133v1</guid><pubDate>Wed, 21 Jan 2026 16:07:17 +0000</pubDate><dc:creator>André Eberhard</dc:creator><dc:creator>Gerhard Neumann</dc:creator><dc:creator>Pascal Friederich</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Despite tremendous improvements in tasks such as image classification, object detection, and segmentation, the recognition of visual relationships, commonly modeled as the extraction of a graph from an image, remains a challenging task. We believe that this mainly stems from the fact that there is no canonical way to approach the visual graph recognition task. Most existing solutions are specific to a problem and cannot be transferred between different contexts out-of-the box, even though the conceptual problem remains the same. With broad applicability and simplicity in mind, in this paper we develop a method, \textbf{Gra}ph Recognition via \textbf{S}ubgraph \textbf{P}rediction (\textbf{GraSP}), for recognizing graphs in images. We show across several synthetic benchmarks and one real-world application that our method works with a set of diverse types of graphs and their drawings, and can be transferred between tasks without task-specific modifications, paving the way to a more unified framework for visual graph recognition.
Published: 2026-01-21T16:07:17+00:00
Venue: arXiv
Score: 0.565 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; André Eberhard; Gerhard Neumann; Pascal Friederich&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.565 (consider)&lt;/p&gt;
&lt;p&gt;Despite tremendous improvements in tasks such as image classification, object detection, and segmentation, the recognition of visual relationships, commonly modeled as the extraction of a graph from an image, remains a challenging task. We believe that this mainly stems from the fact that there is no canonical way to approach the visual graph recognition task. Most existing solutions are specific to a problem and cannot be transferred between different contexts out-of-the box, even though the conceptual problem remains the same. With broad applicability and simplicity in mind, in this paper we develop a method, \textbf{Gra}ph Recognition via \textbf{S}ubgraph \textbf{P}rediction (\textbf{GraSP}), for recognizing graphs in images. We show across several synthetic benchmarks and one real-world application that our method works with a set of diverse types of graphs and their drawings, and can be transferred between tasks without task-specific modifications, paving the way to a more unified framework for visual graph recognition.&lt;/p&gt;</content:encoded></item><item><title>Fine-Grained Zero-Shot Composed Image Retrieval with Complementary Visual-Semantic Integration</title><link>https://arxiv.org/abs/2601.14060v1</link><guid>http://arxiv.org/abs/2601.14060v1</guid><pubDate>Tue, 20 Jan 2026 15:17:14 +0000</pubDate><dc:creator>Yongcong Ye</dc:creator><dc:creator>Kai Zhang</dc:creator><dc:creator>Yanghai Zhang</dc:creator><dc:creator>Enhong Chen</dc:creator><dc:creator>Longfei Li</dc:creator><dc:creator>Jun Zhou</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Zero-shot composed image retrieval (ZS-CIR) is a rapidly growing area with significant practical applications, allowing users to retrieve a target image by providing a reference image and a relative caption describing the desired modifications. Existing ZS-CIR methods often struggle to capture fine-grained changes and integrate visual and semantic information effectively. They primarily rely on either transforming the multimodal query into a single text using image-to-text models or employing large language models for target image description generation, approaches that often fail to capture complementary visual information and complete semantic context. To address these limitations, we propose a novel Fine-Grained Zero-Shot Composed Image Retrieval method with Complementary Visual-Semantic Integration (CVSI). Specifically, CVSI leverages three key components: (1) Visual Information Extraction, which not only extracts global image features but also uses a pre-trained mapping network to convert the image into a pseudo token, combining it with the modification text and the objects most likely to be added. (2) Semantic Information Extraction, which involves using a pre-trained captioning model to generate multiple captions for the reference image, followed by leveraging an LLM to generate the modified captions and the objects most likely to be added. (3) Complementary Information Retrieval, which integrates information extracted from both the query and database images to retrieve the target image, enabling the system to efficiently handle retrieval queries in a variety of situations. Extensive experiments on three public datasets (e.g., CIRR, CIRCO, and FashionIQ) demonstrate that CVSI significantly outperforms existing state-of-the-art methods. Our code is available at https://github.com/yyc6631/CVSI.
Published: 2026-01-20T15:17:14+00:00
Venue: arXiv
Score: 0.559 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yongcong Ye; Kai Zhang; Yanghai Zhang; Enhong Chen; Longfei Li; Jun Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.559 (consider)&lt;/p&gt;
&lt;p&gt;Zero-shot composed image retrieval (ZS-CIR) is a rapidly growing area with significant practical applications, allowing users to retrieve a target image by providing a reference image and a relative caption describing the desired modifications. Existing ZS-CIR methods often struggle to capture fine-grained changes and integrate visual and semantic information effectively. They primarily rely on either transforming the multimodal query into a single text using image-to-text models or employing large language models for target image description generation, approaches that often fail to capture complementary visual information and complete semantic context. To address these limitations, we propose a novel Fine-Grained Zero-Shot Composed Image Retrieval method with Complementary Visual-Semantic Integration (CVSI). Specifically, CVSI leverages three key components: (1) Visual Information Extraction, which not only extracts global image features but also uses a pre-trained mapping network to convert the image into a pseudo token, combining it with the modification text and the objects most likely to be added. (2) Semantic Information Extraction, which involves using a pre-trained captioning model to generate multiple captions for the reference image, followed by leveraging an LLM to generate the modified captions and the objects most likely to be added. (3) Complementary Information Retrieval, which integrates information extracted from both the query and database images to retrieve the target image, enabling the system to efficiently handle retrieval queries in a variety of situations. Extensive experiments on three public datasets (e.g., CIRR, CIRCO, and FashionIQ) demonstrate that CVSI significantly outperforms existing state-of-the-art methods. Our code is available at https://github.com/yyc6631/CVSI.&lt;/p&gt;</content:encoded></item><item><title>Weak supervision makes strong details: fine-grained object recognition in remote sensing images via regional diffusion with VLM</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.024</link><guid>10.1016/j.isprsjprs.2026.01.024</guid><pubDate>Fri, 23 Jan 2026 14:36:28 +0000</pubDate><dc:creator>Liuqian Wang</dc:creator><dc:creator>Jing Zhang</dc:creator><dc:creator>Guangming Mi</dc:creator><dc:creator>Li Zhuo</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.024</prism:doi><description>Fine-grained object recognition (FGOR) is gaining increasing attention in automated remote sensing analysis and interpretation (RSAI). However, the full potential of FGOR in remote sensing images (RSIs) is still constrained by several key issues: the reliance on high-quality labeled data, the difficulty of reconstructing fine details in low-resolution images, and the limited robustness of FGOR model for distinguishing similar object categories. In response, we propose an automatic fine-grained object recognition network (AutoFGOR) that follows a hierarchical dual-pipeline architecture for object analysis at global and regional levels. Specifically, Pipeline I: region detection network, which leverages geometric invariance module for weakly-supervised learning to improve the detection accuracy of sparsely labeled RSIs and extract category-free regions; and on top of that, Pipeline II: regional diffusion with vision language model (RD-VLM), which pioneers the combination of stable diffusion XL (SDXL) and large language and vision assistant (LLaVA) through a specially designed adaptive resolution adaptor (ARA) for object region super-resolution reconstruction, fundamentally solving the difficulties of feature extraction from low-quality regions and fine-grained feature mining. In addition, we introduce a winner-takes-all (WTA) strategy that utilizes a voting mechanism to enhance the reliability of fine-grained classification in complex scenes. Experimental results on FAIR1M-v2.0, VEDAI, and HRSC2016 datasets demonstrate our AutoFGOR achieving 31.72%, 80.25%, and 88.05% mAP, respectively, with highly competitive performance. In addition, the × 4 reconstruction results achieve scores of 0.5275 and 0.8173 on the MANIQA and CLIP-IQA indicators, respectively. The code will be available on GitHub: https://github.com/BJUT-AIVBD/AutoFGOR .
Published: 2026-01-23T14:36:28+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.558 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Liuqian Wang; Jing Zhang; Guangming Mi; Li Zhuo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.024"&gt;10.1016/j.isprsjprs.2026.01.024&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.558 (consider)&lt;/p&gt;
&lt;p&gt;Fine-grained object recognition (FGOR) is gaining increasing attention in automated remote sensing analysis and interpretation (RSAI). However, the full potential of FGOR in remote sensing images (RSIs) is still constrained by several key issues: the reliance on high-quality labeled data, the difficulty of reconstructing fine details in low-resolution images, and the limited robustness of FGOR model for distinguishing similar object categories. In response, we propose an automatic fine-grained object recognition network (AutoFGOR) that follows a hierarchical dual-pipeline architecture for object analysis at global and regional levels. Specifically, Pipeline I: region detection network, which leverages geometric invariance module for weakly-supervised learning to improve the detection accuracy of sparsely labeled RSIs and extract category-free regions; and on top of that, Pipeline II: regional diffusion with vision language model (RD-VLM), which pioneers the combination of stable diffusion XL (SDXL) and large language and vision assistant (LLaVA) through a specially designed adaptive resolution adaptor (ARA) for object region super-resolution reconstruction, fundamentally solving the difficulties of feature extraction from low-quality regions and fine-grained feature mining. In addition, we introduce a winner-takes-all (WTA) strategy that utilizes a voting mechanism to enhance the reliability of fine-grained classification in complex scenes. Experimental results on FAIR1M-v2.0, VEDAI, and HRSC2016 datasets demonstrate our AutoFGOR achieving 31.72%, 80.25%, and 88.05% mAP, respectively, with highly competitive performance. In addition, the × 4 reconstruction results achieve scores of 0.5275 and 0.8173 on the MANIQA and CLIP-IQA indicators, respectively. The code will be available on GitHub: https://github.com/BJUT-AIVBD/AutoFGOR .&lt;/p&gt;</content:encoded></item><item><title>MRFNet: Multi-Reference Fusion for Image Deblurring</title><link>https://doi.org/10.1016/j.inffus.2026.104169</link><guid>10.1016/j.inffus.2026.104169</guid><pubDate>Thu, 22 Jan 2026 17:08:55 +0000</pubDate><dc:creator>Tingrui Guo</dc:creator><dc:creator>Chi Xu</dc:creator><dc:creator>Kaifeng Tang</dc:creator><dc:creator>Hao Qian</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104169</prism:doi><description>Motion blur is a persistent challenge in visual data processing. While single-image deblurring methods have made significant progress, using multiple reference images from the same scene for deblurring remains an overlooked problem. Existing methods struggle to integrate information from multiple reference images with differences in lighting, color, and perspective. Herein, we propose a novel framework MRFNet which leverages any number of discontinuous reference images for deblurring. The framework consists of two key components: (1) the Offset Fusion Module (OFM) guided by dense matching, which aggregates features from discontinuous reference images through high-frequency detail enhancement and permutation-invariant units; and (2) the Deformable Enrichment Module (DEM), which refines misaligned features using deformable convolutions for precise detail recovery. Quantitative and qualitative evaluations on synthetic and real-world datasets show that the proposed method outperforms state-of-the-art deblurring approaches. Additionally, a new real-world dataset is provided to fill the gap in evaluating discontinuous reference problems.
Published: 2026-01-22T17:08:55+00:00
Venue: Information Fusion
Score: 0.557 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tingrui Guo; Chi Xu; Kaifeng Tang; Hao Qian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104169"&gt;10.1016/j.inffus.2026.104169&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.557 (consider)&lt;/p&gt;
&lt;p&gt;Motion blur is a persistent challenge in visual data processing. While single-image deblurring methods have made significant progress, using multiple reference images from the same scene for deblurring remains an overlooked problem. Existing methods struggle to integrate information from multiple reference images with differences in lighting, color, and perspective. Herein, we propose a novel framework MRFNet which leverages any number of discontinuous reference images for deblurring. The framework consists of two key components: (1) the Offset Fusion Module (OFM) guided by dense matching, which aggregates features from discontinuous reference images through high-frequency detail enhancement and permutation-invariant units; and (2) the Deformable Enrichment Module (DEM), which refines misaligned features using deformable convolutions for precise detail recovery. Quantitative and qualitative evaluations on synthetic and real-world datasets show that the proposed method outperforms state-of-the-art deblurring approaches. Additionally, a new real-world dataset is provided to fill the gap in evaluating discontinuous reference problems.&lt;/p&gt;</content:encoded></item><item><title>PREGEN: Uncovering Latent Thoughts in Composed Video Retrieval</title><link>https://arxiv.org/abs/2601.13797v1</link><guid>http://arxiv.org/abs/2601.13797v1</guid><pubDate>Tue, 20 Jan 2026 09:57:04 +0000</pubDate><dc:creator>Gabriele Serussi</dc:creator><dc:creator>David Vainshtein</dc:creator><dc:creator>Jonathan Kouchly</dc:creator><dc:creator>Dotan Di Castro</dc:creator><dc:creator>Chaim Baskin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Composed Video Retrieval (CoVR) aims to retrieve a video based on a query video and a modifying text. Current CoVR methods fail to fully exploit modern Vision-Language Models (VLMs), either using outdated architectures or requiring computationally expensive fine-tuning and slow caption generation. We introduce PREGEN (PRE GENeration extraction), an efficient and powerful CoVR framework that overcomes these limitations. Our approach uniquely pairs a frozen, pre-trained VLM with a lightweight encoding model, eliminating the need for any VLM fine-tuning. We feed the query video and modifying text into the VLM and extract the hidden state of the final token from each layer. A simple encoder is then trained on these pooled representations, creating a semantically rich and compact embedding for retrieval. PREGEN significantly advances the state of the art, surpassing all prior methods on standard CoVR benchmarks with substantial gains in Recall@1 of +27.23 and +69.59. Our method demonstrates robustness across different VLM backbones and exhibits strong zero-shot generalization to more complex textual modifications, highlighting its effectiveness and semantic capabilities.
Published: 2026-01-20T09:57:04+00:00
Venue: arXiv
Score: 0.556 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gabriele Serussi; David Vainshtein; Jonathan Kouchly; Dotan Di Castro; Chaim Baskin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.556 (consider)&lt;/p&gt;
&lt;p&gt;Composed Video Retrieval (CoVR) aims to retrieve a video based on a query video and a modifying text. Current CoVR methods fail to fully exploit modern Vision-Language Models (VLMs), either using outdated architectures or requiring computationally expensive fine-tuning and slow caption generation. We introduce PREGEN (PRE GENeration extraction), an efficient and powerful CoVR framework that overcomes these limitations. Our approach uniquely pairs a frozen, pre-trained VLM with a lightweight encoding model, eliminating the need for any VLM fine-tuning. We feed the query video and modifying text into the VLM and extract the hidden state of the final token from each layer. A simple encoder is then trained on these pooled representations, creating a semantically rich and compact embedding for retrieval. PREGEN significantly advances the state of the art, surpassing all prior methods on standard CoVR benchmarks with substantial gains in Recall@1 of +27.23 and +69.59. Our method demonstrates robustness across different VLM backbones and exhibits strong zero-shot generalization to more complex textual modifications, highlighting its effectiveness and semantic capabilities.&lt;/p&gt;</content:encoded></item><item><title>Knowledge distillation with spatial semantic enhancement for remote sensing object detection</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.017</link><guid>10.1016/j.isprsjprs.2026.01.017</guid><pubDate>Thu, 22 Jan 2026 17:36:36 +0000</pubDate><dc:creator>Kai Hu</dc:creator><dc:creator>Jiaxin Li</dc:creator><dc:creator>Nan Ji</dc:creator><dc:creator>Xueshang Xiang</dc:creator><dc:creator>Kai Jiang</dc:creator><dc:creator>Xieping Gao</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.017</prism:doi><description>Knowledge distillation is extensively utilized in remote sensing object detection within resource-constrained environments. Among knowledge distillation methods, prediction imitation has garnered significant attention due to its ease of deployment. However, prevailing prediction imitation paradigms, which rely on an isolated, point-wise alignment of prediction scores, neglect the crucial spatial semantic information. This oversight is particularly detrimental in remote sensing images due to the abundance of objects with weak feature responses. To this end, we propose a novel Spatial Semantic Enhanced Knowledge Distillation framework, called S 2 " role="presentation"&gt; S 2 S 2 EKD , for remote sensing object detection. Through two complementary modules, S 2 " role="presentation"&gt; S 2 S 2 EKD shifts the focus of prediction imitation from matching isolated values to learning structured spatial semantic information. First, for classification distillation, we introduce a Weak-feature Response Enhancement Module, which models the structured spatial relationships between objects and their background to establish an initial perception of objects with weak feature responses. Second, to further capture more refined spatial information, we propose a Teacher Boundary Refinement Module for localization distillation. It provides robust boundary guidance by constructing a regression target enriched with more comprehensive spatial information. Furthermore, we introduce a Feature Mapping mechanism to ensure this spatial semantic knowledge is effectively utilized. Through extensive experiments on the DIOR and DOTA-v1.0 datasets, our method’s superiority is consistently demonstrated across diverse architectures, including both single-stage and two-stage detectors. The results show that our S 2 " role="presentation"&gt; S 2 S 2 EKD achieves state-of-the-art results and, in some cases, even surpasses the performance of its teacher model. The code will be available soon.
Published: 2026-01-22T17:36:36+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.555 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kai Hu; Jiaxin Li; Nan Ji; Xueshang Xiang; Kai Jiang; Xieping Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.017"&gt;10.1016/j.isprsjprs.2026.01.017&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.555 (consider)&lt;/p&gt;
&lt;p&gt;Knowledge distillation is extensively utilized in remote sensing object detection within resource-constrained environments. Among knowledge distillation methods, prediction imitation has garnered significant attention due to its ease of deployment. However, prevailing prediction imitation paradigms, which rely on an isolated, point-wise alignment of prediction scores, neglect the crucial spatial semantic information. This oversight is particularly detrimental in remote sensing images due to the abundance of objects with weak feature responses. To this end, we propose a novel Spatial Semantic Enhanced Knowledge Distillation framework, called S 2 &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; S 2 S 2 EKD , for remote sensing object detection. Through two complementary modules, S 2 &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; S 2 S 2 EKD shifts the focus of prediction imitation from matching isolated values to learning structured spatial semantic information. First, for classification distillation, we introduce a Weak-feature Response Enhancement Module, which models the structured spatial relationships between objects and their background to establish an initial perception of objects with weak feature responses. Second, to further capture more refined spatial information, we propose a Teacher Boundary Refinement Module for localization distillation. It provides robust boundary guidance by constructing a regression target enriched with more comprehensive spatial information. Furthermore, we introduce a Feature Mapping mechanism to ensure this spatial semantic knowledge is effectively utilized. Through extensive experiments on the DIOR and DOTA-v1.0 datasets, our method’s superiority is consistently demonstrated across diverse architectures, including both single-stage and two-stage detectors. The results show that our S 2 &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; S 2 S 2 EKD achieves state-of-the-art results and, in some cases, even surpasses the performance of its teacher model. The code will be available soon.&lt;/p&gt;</content:encoded></item><item><title>Leveraging Domain Characteristics to Refine Deep-Learning-Based Semantic Segmentation of Outdoor Point Clouds</title><link>https://doi.org/10.1109/tgrs.2026.3657418</link><guid>10.1109/tgrs.2026.3657418</guid><pubDate>Fri, 23 Jan 2026 20:58:37 +0000</pubDate><dc:creator>Kevin Qiu</dc:creator><dc:creator>Qipeng Mei</dc:creator><dc:creator>Dimitri Bulatov</dc:creator><dc:creator>Dorota Iwaszczuk</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3657418</prism:doi><description>High-resolution point clouds generated by modern LiDAR sensors on drones enable point clouds of much higher resolution and detail, presenting new challenges for semantic segmentation, such as efficiency, limited receptive fields, and implausible class prediction results. To address these, we integrate relative elevation and differential morphological profiles – both of which are domain-specific features in remote sensing – into RandLA-Net for 3D point cloud segmentation. This enables enhanced feature representation without increasing network complexity or requiring input downsampling. On the Hessigheim dataset, including relative elevation and morphological profiles improves the mF1 score by +5.52%. Additionally, we utilize Conditional Random Fields with an inter-class reliability matrix to refine predictions and enforce realistic class neighborhoods, further increasing the mF1 score to exceed 78%. Overall, this approach ensures accurate and efficient segmentation, leveraging domain-specific pre-processing characteristics and domain knowledge about class neighborhoods. A comparison with competing methods, mostly favoring our approach, indicates that all deep learning networks operating on remote sensing point clouds could benefit from explicit incorporation of these domain characteristics.
Published: 2026-01-23T20:58:37+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.555 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kevin Qiu; Qipeng Mei; Dimitri Bulatov; Dorota Iwaszczuk&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3657418"&gt;10.1109/tgrs.2026.3657418&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.555 (consider)&lt;/p&gt;
&lt;p&gt;High-resolution point clouds generated by modern LiDAR sensors on drones enable point clouds of much higher resolution and detail, presenting new challenges for semantic segmentation, such as efficiency, limited receptive fields, and implausible class prediction results. To address these, we integrate relative elevation and differential morphological profiles – both of which are domain-specific features in remote sensing – into RandLA-Net for 3D point cloud segmentation. This enables enhanced feature representation without increasing network complexity or requiring input downsampling. On the Hessigheim dataset, including relative elevation and morphological profiles improves the mF1 score by +5.52%. Additionally, we utilize Conditional Random Fields with an inter-class reliability matrix to refine predictions and enforce realistic class neighborhoods, further increasing the mF1 score to exceed 78%. Overall, this approach ensures accurate and efficient segmentation, leveraging domain-specific pre-processing characteristics and domain knowledge about class neighborhoods. A comparison with competing methods, mostly favoring our approach, indicates that all deep learning networks operating on remote sensing point clouds could benefit from explicit incorporation of these domain characteristics.&lt;/p&gt;</content:encoded></item><item><title>UniPerception: Towards Unification of Perception Using Multi-Stage Training Pipeline in Adverse Weather Conditions</title><link>https://doi.org/10.1109/tiv.2026.3656901</link><guid>10.1109/tiv.2026.3656901</guid><pubDate>Thu, 22 Jan 2026 21:03:41 +0000</pubDate><dc:creator>Jianping Li</dc:creator><dc:creator>Qifan Tan</dc:creator><dc:creator>Songchao Tan</dc:creator><dc:creator>Xiao Ke</dc:creator><dc:creator>Zhiwei Li</dc:creator><dc:creator>Tianyu Shen</dc:creator><dc:creator>Guozhen Tan</dc:creator><prism:publicationName>IEEE Transactions on Intelligent Vehicles</prism:publicationName><prism:doi>10.1109/tiv.2026.3656901</prism:doi><description>Panoptic perception forms the foundation for decision-making in autonomous vehicles. This comprehensive perception includes essential functions such as lane line detection, drivable area recognition and vehicle detection. However, existing methods mainly focus on normal weather conditions, resulting in a significant degradation of Panoptic perception performance under inclement weather conditions including snow rain and haze. To improve the accuracy and robustness of panoramic perception under inclement Weather conditions, a multi-task network is proposed termed UniPerception, which uses a hybrid architecture of Transformer and CNN and a multi-stage learning strategy for parameter updating. Due to the lack of a dataset for severe weather, we developed the BDD100 K dataset using image enhancement techniques. Experimental results indicate that the UniPerception model consistently outperforms advanced multitasking and single-tasking networks in a variety of tasks under inclement weather conditions.
Published: 2026-01-22T21:03:41+00:00
Venue: IEEE Transactions on Intelligent Vehicles
Score: 0.551 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianping Li; Qifan Tan; Songchao Tan; Xiao Ke; Zhiwei Li; Tianyu Shen; Guozhen Tan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Intelligent Vehicles&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tiv.2026.3656901"&gt;10.1109/tiv.2026.3656901&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.551 (consider)&lt;/p&gt;
&lt;p&gt;Panoptic perception forms the foundation for decision-making in autonomous vehicles. This comprehensive perception includes essential functions such as lane line detection, drivable area recognition and vehicle detection. However, existing methods mainly focus on normal weather conditions, resulting in a significant degradation of Panoptic perception performance under inclement weather conditions including snow rain and haze. To improve the accuracy and robustness of panoramic perception under inclement Weather conditions, a multi-task network is proposed termed UniPerception, which uses a hybrid architecture of Transformer and CNN and a multi-stage learning strategy for parameter updating. Due to the lack of a dataset for severe weather, we developed the BDD100 K dataset using image enhancement techniques. Experimental results indicate that the UniPerception model consistently outperforms advanced multitasking and single-tasking networks in a variety of tasks under inclement weather conditions.&lt;/p&gt;</content:encoded></item><item><title>PromptMix: LLM-Aided Prompt Learning for Generalizing Vision-Language Models</title><link>https://doi.org/10.1016/j.inffus.2026.104186</link><guid>10.1016/j.inffus.2026.104186</guid><pubDate>Fri, 23 Jan 2026 16:17:45 +0000</pubDate><dc:creator>Yongcai Chen</dc:creator><dc:creator>Qinghua Zhang</dc:creator><dc:creator>Xinfa Shi</dc:creator><dc:creator>Lei Zhang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104186</prism:doi><description>Intelligent engineering tasks step into real application with the development of deep learning techniques. However, performance in real conditions often falls into decline caused by scarce data, or subtle, easily confused patterns. Although vision-language models with prompt learning provide a new way for learning without retraining the backbone, these approaches still suffer from problems of overfitting under low-data regimes or poor expressive ability of prompts. To address these challenges, we propose a novel framework PromptMix that jointly considers semantic prompt learning, multimodal information fusion, and the alignment between pre-trained and domain-specific data. Specifically, PromptMix integrates three key components: (1) a Modality-Agnostic Shared Representation module to construct a shared latent space that mitigates the distribution discrepancies between pre-trained and target data, (2) a LLM-Aided Prompt Evolution mechanism to semantically enrich and iteratively refine learnable context prompts, and (3) a Cross-Attentive Adapter to enhance multimodal information fusion and robustness under low-sample conditions. Experiments on seven datasets, including six public benchmarks and one custom industrial dataset, demonstrate that PromptMix effectively enhances vision-language model adaptability, improves semantic representations, and achieves robust generalization under both base-to-novel and few-shot learning scenarios, delivering superior performance in engineering applications with limited labeled data.
Published: 2026-01-23T16:17:45+00:00
Venue: Information Fusion
Score: 0.551 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yongcai Chen; Qinghua Zhang; Xinfa Shi; Lei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104186"&gt;10.1016/j.inffus.2026.104186&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.551 (consider)&lt;/p&gt;
&lt;p&gt;Intelligent engineering tasks step into real application with the development of deep learning techniques. However, performance in real conditions often falls into decline caused by scarce data, or subtle, easily confused patterns. Although vision-language models with prompt learning provide a new way for learning without retraining the backbone, these approaches still suffer from problems of overfitting under low-data regimes or poor expressive ability of prompts. To address these challenges, we propose a novel framework PromptMix that jointly considers semantic prompt learning, multimodal information fusion, and the alignment between pre-trained and domain-specific data. Specifically, PromptMix integrates three key components: (1) a Modality-Agnostic Shared Representation module to construct a shared latent space that mitigates the distribution discrepancies between pre-trained and target data, (2) a LLM-Aided Prompt Evolution mechanism to semantically enrich and iteratively refine learnable context prompts, and (3) a Cross-Attentive Adapter to enhance multimodal information fusion and robustness under low-sample conditions. Experiments on seven datasets, including six public benchmarks and one custom industrial dataset, demonstrate that PromptMix effectively enhances vision-language model adaptability, improves semantic representations, and achieves robust generalization under both base-to-novel and few-shot learning scenarios, delivering superior performance in engineering applications with limited labeled data.&lt;/p&gt;</content:encoded></item><item><title>Semantic-Enhanced LiDAR-Inertial SLAM with Robust Loop Closure and Global Consistency</title><link>https://doi.org/10.1109/tcsvt.2026.3656945</link><guid>10.1109/tcsvt.2026.3656945</guid><pubDate>Thu, 22 Jan 2026 21:03:59 +0000</pubDate><dc:creator>Zihao Pan</dc:creator><dc:creator>Junyi Hou</dc:creator><dc:creator>Lei Yu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3656945</prism:doi><description>Reliable localization and mapping in large-scale outdoor environments remain a critical requirement for autonomous driving and intelligent robotics. While LiDAR-Inertial SLAM systems provide robust performance in many cases, their reliance on purely geometric features often leads to drift or loop closure failure in semantically repetitive or feature-degraded scenes. To address these limitations, we propose a LiDAR-IMU-semantic fusion SLAM framework that tightly integrates semantic perception with geometric and inertial constraints. At the core of our system is a Semantic-enhanced Spatial Triangular Descriptor (S-STD), which jointly models geometric structures, semantic categories, and label confidence to achieve discriminative and robust representation. This descriptor is embedded into a semantic-aware ICP registration model coupled with IMU pre-integration for accurate and stable odometry, and a semantic factor graph optimization framework with a two-stage loop closure detection strategy that combines global semantic vector retrieval and S-STD matching. Extensive evaluations on four public datasets, including KITTI, NCLT, SemanticPoss, and MCD-ViRAL, demonstrate that our approach significantly improves registration accuracy, loop closure recall, and trajectory estimation compared with state-of-the-art methods, while maintaining real-time performance. These results highlight the potential of the proposed framework for robust perception and consistent map construction in autonomous driving and long-term robotic navigation.
Published: 2026-01-22T21:03:59+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.550 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zihao Pan; Junyi Hou; Lei Yu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3656945"&gt;10.1109/tcsvt.2026.3656945&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.550 (consider)&lt;/p&gt;
&lt;p&gt;Reliable localization and mapping in large-scale outdoor environments remain a critical requirement for autonomous driving and intelligent robotics. While LiDAR-Inertial SLAM systems provide robust performance in many cases, their reliance on purely geometric features often leads to drift or loop closure failure in semantically repetitive or feature-degraded scenes. To address these limitations, we propose a LiDAR-IMU-semantic fusion SLAM framework that tightly integrates semantic perception with geometric and inertial constraints. At the core of our system is a Semantic-enhanced Spatial Triangular Descriptor (S-STD), which jointly models geometric structures, semantic categories, and label confidence to achieve discriminative and robust representation. This descriptor is embedded into a semantic-aware ICP registration model coupled with IMU pre-integration for accurate and stable odometry, and a semantic factor graph optimization framework with a two-stage loop closure detection strategy that combines global semantic vector retrieval and S-STD matching. Extensive evaluations on four public datasets, including KITTI, NCLT, SemanticPoss, and MCD-ViRAL, demonstrate that our approach significantly improves registration accuracy, loop closure recall, and trajectory estimation compared with state-of-the-art methods, while maintaining real-time performance. These results highlight the potential of the proposed framework for robust perception and consistent map construction in autonomous driving and long-term robotic navigation.&lt;/p&gt;</content:encoded></item><item><title>Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning</title><link>https://arxiv.org/abs/2601.13942v1</link><guid>http://arxiv.org/abs/2601.13942v1</guid><pubDate>Tue, 20 Jan 2026 13:18:18 +0000</pubDate><dc:creator>Hongbo Bai</dc:creator><dc:creator>Yujin Zhou</dc:creator><dc:creator>Yile Wu</dc:creator><dc:creator>Chi-Min Chan</dc:creator><dc:creator>Pengcheng Wen</dc:creator><dc:creator>Kunhao Pan</dc:creator><dc:creator>Sirui Han</dc:creator><dc:creator>Yike Guo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Multimodal Models (LMMs) have achieved remarkable success in visual understanding, yet they struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Recent search-augmented approaches attempt to address this limitation, but existing methods rely on indiscriminate whole-image retrieval that introduces substantial visual redundancy and noise, and lack deep iterative reflection, limiting their effectiveness on complex visual queries. To overcome these challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. We design a dual-stage training strategy: Reflective GoG Behavior Alignment via supervised fine-tuning instills the fundamental GoG paradigm, while Complexity-Adaptive Reinforcement Learning further enhances the model's capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance. Ablation studies confirm that both Selective Gaze and complexity-adaptive RL are essential for effective visual search. We will release our data and models for further exploration soon.
Published: 2026-01-20T13:18:18+00:00
Venue: arXiv
Score: 0.547 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongbo Bai; Yujin Zhou; Yile Wu; Chi-Min Chan; Pengcheng Wen; Kunhao Pan; Sirui Han; Yike Guo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.547 (consider)&lt;/p&gt;
&lt;p&gt;Large Multimodal Models (LMMs) have achieved remarkable success in visual understanding, yet they struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Recent search-augmented approaches attempt to address this limitation, but existing methods rely on indiscriminate whole-image retrieval that introduces substantial visual redundancy and noise, and lack deep iterative reflection, limiting their effectiveness on complex visual queries. To overcome these challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. We design a dual-stage training strategy: Reflective GoG Behavior Alignment via supervised fine-tuning instills the fundamental GoG paradigm, while Complexity-Adaptive Reinforcement Learning further enhances the model&amp;#x27;s capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance. Ablation studies confirm that both Selective Gaze and complexity-adaptive RL are essential for effective visual search. We will release our data and models for further exploration soon.&lt;/p&gt;</content:encoded></item><item><title>SpatialMem: Unified 3D Memory with Metric Anchoring and Fast Retrieval</title><link>https://arxiv.org/abs/2601.14895v1</link><guid>http://arxiv.org/abs/2601.14895v1</guid><pubDate>Wed, 21 Jan 2026 11:32:24 +0000</pubDate><dc:creator>Xinyi Zheng</dc:creator><dc:creator>Yunze Liu</dc:creator><dc:creator>Chi-Hao Wu</dc:creator><dc:creator>Fan Zhang</dc:creator><dc:creator>Hao Zheng</dc:creator><dc:creator>Wenqi Zhou</dc:creator><dc:creator>Walterio W. Mayol-Cuevas</dc:creator><dc:creator>Junxiao Shen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present SpatialMem, a memory-centric system that unifies 3D geometry, semantics, and language into a single, queryable representation. Starting from casually captured egocentric RGB video, SpatialMem reconstructs metrically scaled indoor environments, detects structural 3D anchors (walls, doors, windows) as the first-layer scaffold, and populates a hierarchical memory with open-vocabulary object nodes -- linking evidence patches, visual embeddings, and two-layer textual descriptions to 3D coordinates -- for compact storage and fast retrieval. This design enables interpretable reasoning over spatial relations (e.g., distance, direction, visibility) and supports downstream tasks such as language-guided navigation and object retrieval without specialized sensors. Experiments across three real-life indoor scenes demonstrate that SpatialMem maintains strong anchor-description-level navigation completion and hierarchical retrieval accuracy under increasing clutter and occlusion, offering an efficient and extensible framework for embodied spatial intelligence.
Published: 2026-01-21T11:32:24+00:00
Venue: arXiv
Score: 0.547 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinyi Zheng; Yunze Liu; Chi-Hao Wu; Fan Zhang; Hao Zheng; Wenqi Zhou; Walterio W. Mayol-Cuevas; Junxiao Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.547 (consider)&lt;/p&gt;
&lt;p&gt;We present SpatialMem, a memory-centric system that unifies 3D geometry, semantics, and language into a single, queryable representation. Starting from casually captured egocentric RGB video, SpatialMem reconstructs metrically scaled indoor environments, detects structural 3D anchors (walls, doors, windows) as the first-layer scaffold, and populates a hierarchical memory with open-vocabulary object nodes -- linking evidence patches, visual embeddings, and two-layer textual descriptions to 3D coordinates -- for compact storage and fast retrieval. This design enables interpretable reasoning over spatial relations (e.g., distance, direction, visibility) and supports downstream tasks such as language-guided navigation and object retrieval without specialized sensors. Experiments across three real-life indoor scenes demonstrate that SpatialMem maintains strong anchor-description-level navigation completion and hierarchical retrieval accuracy under increasing clutter and occlusion, offering an efficient and extensible framework for embodied spatial intelligence.&lt;/p&gt;</content:encoded></item><item><title>Vision-Based Natural Language Scene Understanding for Autonomous Driving: An Extended Dataset and a New Model for Traffic Scene Description Generation</title><link>https://arxiv.org/abs/2601.14438v1</link><guid>http://arxiv.org/abs/2601.14438v1</guid><pubDate>Tue, 20 Jan 2026 19:50:42 +0000</pubDate><dc:creator>Danial Sadrian Zadeh</dc:creator><dc:creator>Otman A. Basir</dc:creator><dc:creator>Behzad Moshiri</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Traffic scene understanding is essential for enabling autonomous vehicles to accurately perceive and interpret their environment, thereby ensuring safe navigation. This paper presents a novel framework that transforms a single frontal-view camera image into a concise natural language description, effectively capturing spatial layouts, semantic relationships, and driving-relevant cues. The proposed model leverages a hybrid attention mechanism to enhance spatial and semantic feature extraction and integrates these features to generate contextually rich and detailed scene descriptions. To address the limited availability of specialized datasets in this domain, a new dataset derived from the BDD100K dataset has been developed, with comprehensive guidelines provided for its construction. Furthermore, the study offers an in-depth discussion of relevant evaluation metrics, identifying the most appropriate measures for this task. Extensive quantitative evaluations using metrics such as CIDEr and SPICE, complemented by human judgment assessments, demonstrate that the proposed model achieves strong performance and effectively fulfills its intended objectives on the newly developed dataset.
Published: 2026-01-20T19:50:42+00:00
Venue: arXiv
Score: 0.544 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Danial Sadrian Zadeh; Otman A. Basir; Behzad Moshiri&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.544 (consider)&lt;/p&gt;
&lt;p&gt;Traffic scene understanding is essential for enabling autonomous vehicles to accurately perceive and interpret their environment, thereby ensuring safe navigation. This paper presents a novel framework that transforms a single frontal-view camera image into a concise natural language description, effectively capturing spatial layouts, semantic relationships, and driving-relevant cues. The proposed model leverages a hybrid attention mechanism to enhance spatial and semantic feature extraction and integrates these features to generate contextually rich and detailed scene descriptions. To address the limited availability of specialized datasets in this domain, a new dataset derived from the BDD100K dataset has been developed, with comprehensive guidelines provided for its construction. Furthermore, the study offers an in-depth discussion of relevant evaluation metrics, identifying the most appropriate measures for this task. Extensive quantitative evaluations using metrics such as CIDEr and SPICE, complemented by human judgment assessments, demonstrate that the proposed model achieves strong performance and effectively fulfills its intended objectives on the newly developed dataset.&lt;/p&gt;</content:encoded></item><item><title>HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval</title><link>https://arxiv.org/abs/2601.16155v1</link><guid>http://arxiv.org/abs/2601.16155v1</guid><pubDate>Thu, 22 Jan 2026 17:57:42 +0000</pubDate><dc:creator>Zequn Xie</dc:creator><dc:creator>Xin Liu</dc:creator><dc:creator>Boyun Zhang</dc:creator><dc:creator>Yuxiao Lin</dc:creator><dc:creator>Sihang Cai</dc:creator><dc:creator>Tao Jin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from "blind" feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.
Published: 2026-01-22T17:57:42+00:00
Venue: arXiv
Score: 0.543 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zequn Xie; Xin Liu; Boyun Zhang; Yuxiao Lin; Sihang Cai; Tao Jin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.543 (consider)&lt;/p&gt;
&lt;p&gt;The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from &amp;quot;blind&amp;quot; feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.&lt;/p&gt;</content:encoded></item><item><title>HR-SemNet: A High-Resolution Network for Enhanced Small Object Detection With Local Contextual Semantics</title><link>https://doi.org/10.1109/tip.2026.3654770</link><guid>10.1109/tip.2026.3654770</guid><pubDate>Fri, 23 Jan 2026 21:01:47 +0000</pubDate><dc:creator>Can Peng</dc:creator><dc:creator>Manxin Chao</dc:creator><dc:creator>Ruoyu Li</dc:creator><dc:creator>Zaiqing Chen</dc:creator><dc:creator>Lijun Yun</dc:creator><dc:creator>Yuelong Xia</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3654770</prism:doi><description>Using higher-resolution feature maps in the network is an effective approach for detecting small objects. However, high-resolution feature maps face the challenge of lacking semantic information. This has led previous methods to rely on downsampling feature maps, applying large-kernel convolution layers, and then upsampling the feature maps to obtain semantic information. However, these methods have certain limitations: first, large kernel convolutions in deeper layers typically provide significant global semantic information, but our experiments reveal that such prominent semantic information introduces background smear, which in turn leads to overfitting. Second, deep features often contain substantial redundant information, and the features of small objects are either minimal or have disappeared, which causes a degradation in detection performance when directly relying on deep features. To address these issues, we propose a high-resolution network based on local contextual semantics (HR-SemNet). The network is built on the proposed high-resolution backbone (HRB), which replaces the traditional backbone-FPN architecture by focusing all computational resources of large kernel convolutions on highresolution feature layers to capture clearer features of small objects. Additionally, a local context semantic module (LCSM) is employed to extract semantic information from the background, confining the semantic extraction to a local window to avoid interference from large-scale backgrounds and objects. HRSemNet decouples small object semantics from contextual semantics, with HRB and LCSM independently extracting these features. Extensive experiments and comprehensive evaluations on the VisDrone, AI-TOD, and TinyPerson datasets validate the effectiveness of the method. On the VisDrone dataset, which contains a large number of small objects, HR-SemNet improves the mean average precision (mAP) by 4.6%, reduces the computational cost (GFLOPs) by 49.9%, and decreases the param...
Published: 2026-01-23T21:01:47+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.542 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Can Peng; Manxin Chao; Ruoyu Li; Zaiqing Chen; Lijun Yun; Yuelong Xia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3654770"&gt;10.1109/tip.2026.3654770&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.542 (consider)&lt;/p&gt;
&lt;p&gt;Using higher-resolution feature maps in the network is an effective approach for detecting small objects. However, high-resolution feature maps face the challenge of lacking semantic information. This has led previous methods to rely on downsampling feature maps, applying large-kernel convolution layers, and then upsampling the feature maps to obtain semantic information. However, these methods have certain limitations: first, large kernel convolutions in deeper layers typically provide significant global semantic information, but our experiments reveal that such prominent semantic information introduces background smear, which in turn leads to overfitting. Second, deep features often contain substantial redundant information, and the features of small objects are either minimal or have disappeared, which causes a degradation in detection performance when directly relying on deep features. To address these issues, we propose a high-resolution network based on local contextual semantics (HR-SemNet). The network is built on the proposed high-resolution backbone (HRB), which replaces the traditional backbone-FPN architecture by focusing all computational resources of large kernel convolutions on highresolution feature layers to capture clearer features of small objects. Additionally, a local context semantic module (LCSM) is employed to extract semantic information from the background, confining the semantic extraction to a local window to avoid interference from large-scale backgrounds and objects. HRSemNet decouples small object semantics from contextual semantics, with HRB and LCSM independently extracting these features. Extensive experiments and comprehensive evaluations on the VisDrone, AI-TOD, and TinyPerson datasets validate the effectiveness of the method. On the VisDrone dataset, which contains a large number of small objects, HR-SemNet improves the mean average precision (mAP) by 4.6%, reduces the computational cost (GFLOPs) by 49.9%, and decreases the param...&lt;/p&gt;</content:encoded></item><item><title>DIS2: Disentanglement Meets Distillation with Classwise Attention for Robust Remote Sensing Segmentation under Missing Modalities</title><link>https://arxiv.org/abs/2601.13502v1</link><guid>http://arxiv.org/abs/2601.13502v1</guid><pubDate>Tue, 20 Jan 2026 01:33:54 +0000</pubDate><dc:creator>Nhi Kieu</dc:creator><dc:creator>Kien Nguyen</dc:creator><dc:creator>Arnold Wiliem</dc:creator><dc:creator>Clinton Fookes</dc:creator><dc:creator>Sridha Sridharan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The efficacy of multimodal learning in remote sensing (RS) is severely undermined by missing modalities. The challenge is exacerbated by the RS highly heterogeneous data and huge scale variation. Consequently, paradigms proven effective in other domains often fail when confronted with these unique data characteristics. Conventional disentanglement learning, which relies on significant feature overlap between modalities (modality-invariant), is insufficient for this heterogeneity. Similarly, knowledge distillation becomes an ill-posed mimicry task where a student fails to focus on the necessary compensatory knowledge, leaving the semantic gap unaddressed. Our work is therefore built upon three pillars uniquely designed for RS: (1) principled missing information compensation, (2) class-specific modality contribution, and (3) multi-resolution feature importance. We propose a novel method DIS2, a new paradigm shifting from modality-shared feature dependence and untargeted imitation to active, guided missing features compensation. Its core novelty lies in a reformulated synergy between disentanglement learning and knowledge distillation, termed DLKD. Compensatory features are explicitly captured which, when fused with the features of the available modality, approximate the ideal fused representation of the full-modality case. To address the class-specific challenge, our Classwise Feature Learning Module (CFLM) adaptively learn discriminative evidence for each target depending on signal availability. Both DLKD and CFLM are supported by a hierarchical hybrid fusion (HF) structure using features across resolutions to strengthen prediction. Extensive experiments validate that our proposed approach significantly outperforms state-of-the-art methods across benchmarks.
Published: 2026-01-20T01:33:54+00:00
Venue: arXiv
Score: 0.536 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nhi Kieu; Kien Nguyen; Arnold Wiliem; Clinton Fookes; Sridha Sridharan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.536 (consider)&lt;/p&gt;
&lt;p&gt;The efficacy of multimodal learning in remote sensing (RS) is severely undermined by missing modalities. The challenge is exacerbated by the RS highly heterogeneous data and huge scale variation. Consequently, paradigms proven effective in other domains often fail when confronted with these unique data characteristics. Conventional disentanglement learning, which relies on significant feature overlap between modalities (modality-invariant), is insufficient for this heterogeneity. Similarly, knowledge distillation becomes an ill-posed mimicry task where a student fails to focus on the necessary compensatory knowledge, leaving the semantic gap unaddressed. Our work is therefore built upon three pillars uniquely designed for RS: (1) principled missing information compensation, (2) class-specific modality contribution, and (3) multi-resolution feature importance. We propose a novel method DIS2, a new paradigm shifting from modality-shared feature dependence and untargeted imitation to active, guided missing features compensation. Its core novelty lies in a reformulated synergy between disentanglement learning and knowledge distillation, termed DLKD. Compensatory features are explicitly captured which, when fused with the features of the available modality, approximate the ideal fused representation of the full-modality case. To address the class-specific challenge, our Classwise Feature Learning Module (CFLM) adaptively learn discriminative evidence for each target depending on signal availability. Both DLKD and CFLM are supported by a hierarchical hybrid fusion (HF) structure using features across resolutions to strengthen prediction. Extensive experiments validate that our proposed approach significantly outperforms state-of-the-art methods across benchmarks.&lt;/p&gt;</content:encoded></item><item><title>Natural Language-Driven Global Mapping of Martian Landforms</title><link>https://arxiv.org/abs/2601.15949v1</link><guid>http://arxiv.org/abs/2601.15949v1</guid><pubDate>Thu, 22 Jan 2026 13:38:13 +0000</pubDate><dc:creator>Yiran Wang</dc:creator><dc:creator>Shuoyuan Wang</dc:creator><dc:creator>Zhaoran Wei</dc:creator><dc:creator>Jiannan Zhao</dc:creator><dc:creator>Zhonghua Yao</dc:creator><dc:creator>Zejian Xie</dc:creator><dc:creator>Songxin Zhang</dc:creator><dc:creator>Jun Huang</dc:creator><dc:creator>Bingyi Jing</dc:creator><dc:creator>Hongxin Wei</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.
Published: 2026-01-22T13:38:13+00:00
Venue: arXiv
Score: 0.533 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yiran Wang; Shuoyuan Wang; Zhaoran Wei; Jiannan Zhao; Zhonghua Yao; Zejian Xie; Songxin Zhang; Jun Huang; Bingyi Jing; Hongxin Wei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.533 (consider)&lt;/p&gt;
&lt;p&gt;Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.&lt;/p&gt;</content:encoded></item><item><title>ReinPath: A Multimodal Reinforcement Learning Approach for Pathology</title><link>https://arxiv.org/abs/2601.14757v1</link><guid>http://arxiv.org/abs/2601.14757v1</guid><pubDate>Wed, 21 Jan 2026 08:21:35 +0000</pubDate><dc:creator>Kangcheng Zhou</dc:creator><dc:creator>Jun Jiang</dc:creator><dc:creator>Qing Zhang</dc:creator><dc:creator>Shuang Zheng</dc:creator><dc:creator>Qingli Li</dc:creator><dc:creator>Shugong Xu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Interpretability is significant in computational pathology, leading to the development of multimodal information integration from histopathological image and corresponding text data.However, existing multimodal methods have limited interpretability due to the lack of high-quality dataset that support explicit reasoning and inference and simple reasoning process.To address the above problems, we introduce a novel multimodal pathology large language model with strong reasoning capabilities.To improve the generation of accurate and contextually relevant textual descriptions, we design a semantic reward strategy integrated with group relative policy optimization.We construct a high-quality pathology visual question answering (VQA) dataset, specifically designed to support complex reasoning tasks.Comprehensive experiments conducted on this dataset demonstrate that our method outperforms state-of-the-art methods, even when trained with only 20% of the data.Our method also achieves comparable performance on downstream zero-shot image classification task compared with CLIP.
Published: 2026-01-21T08:21:35+00:00
Venue: arXiv
Score: 0.532 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kangcheng Zhou; Jun Jiang; Qing Zhang; Shuang Zheng; Qingli Li; Shugong Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.532 (consider)&lt;/p&gt;
&lt;p&gt;Interpretability is significant in computational pathology, leading to the development of multimodal information integration from histopathological image and corresponding text data.However, existing multimodal methods have limited interpretability due to the lack of high-quality dataset that support explicit reasoning and inference and simple reasoning process.To address the above problems, we introduce a novel multimodal pathology large language model with strong reasoning capabilities.To improve the generation of accurate and contextually relevant textual descriptions, we design a semantic reward strategy integrated with group relative policy optimization.We construct a high-quality pathology visual question answering (VQA) dataset, specifically designed to support complex reasoning tasks.Comprehensive experiments conducted on this dataset demonstrate that our method outperforms state-of-the-art methods, even when trained with only 20% of the data.Our method also achieves comparable performance on downstream zero-shot image classification task compared with CLIP.&lt;/p&gt;</content:encoded></item><item><title>DEM super-resolution guided by high-resolution remote sensing images using multitask learning</title><link>https://doi.org/10.1016/j.jag.2026.105099</link><guid>10.1016/j.jag.2026.105099</guid><pubDate>Fri, 23 Jan 2026 12:54:20 +0000</pubDate><dc:creator>Wei Liu</dc:creator><dc:creator>Yuhang Zhong</dc:creator><dc:creator>Shida Zhao</dc:creator><dc:creator>Songling Luo</dc:creator><dc:creator>Yongtao Yu</dc:creator><dc:creator>Xiaomei Zhong</dc:creator><dc:creator>Weikai Tan</dc:creator><dc:creator>Haiyan Guan</dc:creator><dc:creator>Hongjie He</dc:creator><dc:creator>Jonathan Li</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2026.105099</prism:doi><description>High-resolution digital elevation models (DEMs) are critical for applications such as environmental monitoring and urban planning, motivating the development of advanced DEM super-resolution (SR) techniques. While recent methods have shown promising results, effectively exploiting high-resolution remote sensing images (HRSIs) to guide DEM SR remains challenging, and progress has been hindered by the lack of large-scale, open-source benchmark datasets. We propose GSRMTL, a novel and parameter-efficient multi-task learning framework for HRSI-guided DEM SR. Given a low-resolution DEM and a paired HRSI, GSRMTL jointly performs DEM SR and semantic segmentation of the optical imagery, where segmentation acts as an auxiliary task to provide semantic priors for elevation reconstruction. To address the dataset bottleneck, we introduce GDEMSR, the first large-scale benchmark dataset specifically designed for HRSI-guided DEM SR. Extensive experiments on GDEMSR and the RGB-guided depth SR benchmark NYU-v2 demonstrate that GSRMTL consistently outperforms state-of-the-art methods while using significantly fewer parameters, highlighting its effectiveness and practical deployment potential.
Published: 2026-01-23T12:54:20+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.532 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wei Liu; Yuhang Zhong; Shida Zhao; Songling Luo; Yongtao Yu; Xiaomei Zhong; Weikai Tan; Haiyan Guan; Hongjie He; Jonathan Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2026.105099"&gt;10.1016/j.jag.2026.105099&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.532 (consider)&lt;/p&gt;
&lt;p&gt;High-resolution digital elevation models (DEMs) are critical for applications such as environmental monitoring and urban planning, motivating the development of advanced DEM super-resolution (SR) techniques. While recent methods have shown promising results, effectively exploiting high-resolution remote sensing images (HRSIs) to guide DEM SR remains challenging, and progress has been hindered by the lack of large-scale, open-source benchmark datasets. We propose GSRMTL, a novel and parameter-efficient multi-task learning framework for HRSI-guided DEM SR. Given a low-resolution DEM and a paired HRSI, GSRMTL jointly performs DEM SR and semantic segmentation of the optical imagery, where segmentation acts as an auxiliary task to provide semantic priors for elevation reconstruction. To address the dataset bottleneck, we introduce GDEMSR, the first large-scale benchmark dataset specifically designed for HRSI-guided DEM SR. Extensive experiments on GDEMSR and the RGB-guided depth SR benchmark NYU-v2 demonstrate that GSRMTL consistently outperforms state-of-the-art methods while using significantly fewer parameters, highlighting its effectiveness and practical deployment potential.&lt;/p&gt;</content:encoded></item><item><title>DVLTA-VQA: Decoupled Vision-Language Modeling with Text-Guided Adaptation for Blind Video Quality Assessment</title><link>https://doi.org/10.1109/tcsvt.2026.3657415</link><guid>10.1109/tcsvt.2026.3657415</guid><pubDate>Fri, 23 Jan 2026 21:01:10 +0000</pubDate><dc:creator>Li Yu</dc:creator><dc:creator>Situo Wang</dc:creator><dc:creator>Wei Zhou</dc:creator><dc:creator>Moncef Gabbouj</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3657415</prism:doi><description>Inspired by the dual-stream (dorsal and ventral streams) theory of the human visual system (HVS), recent Video Quality Assessment (VQA) methods have integrated Contrastive Language-Image Pretraining (CLIP) to enhance semantic understanding. However, as CLIP is originally designed for images, it lacks the ability to adequately capture the temporal dynamics and motion perception (dorsal stream) inherent in videos. To address this limitation, we propose DVLTA-VQA (Decoupled Vision-Language Modeling with Text-Guided Adaptation), which decouples CLIP’s visual and textual components to better align with the NR-VQA pipeline. Specifically, we introduce a Video-Based Temporal CLIP module and a Temporal Context Module to explicitly model motion dynamics, effectively enhancing the dorsal stream representation. Complementing this, a Basic Visual Feature Extraction Module is employed to strengthen spatial detail analysis in the ventral stream. Furthermore, we propose a text-guided adaptive fusion strategy that leverages textual semantics to dynamically weight visual features, facilitating effective spatiotemporal integration. Extensive experiments on multiple public datasets demonstrate that the proposed method achieves state-of-the-art performance, significantly improving prediction accuracy and generalization capability.
Published: 2026-01-23T21:01:10+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.532 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Li Yu; Situo Wang; Wei Zhou; Moncef Gabbouj&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3657415"&gt;10.1109/tcsvt.2026.3657415&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.532 (consider)&lt;/p&gt;
&lt;p&gt;Inspired by the dual-stream (dorsal and ventral streams) theory of the human visual system (HVS), recent Video Quality Assessment (VQA) methods have integrated Contrastive Language-Image Pretraining (CLIP) to enhance semantic understanding. However, as CLIP is originally designed for images, it lacks the ability to adequately capture the temporal dynamics and motion perception (dorsal stream) inherent in videos. To address this limitation, we propose DVLTA-VQA (Decoupled Vision-Language Modeling with Text-Guided Adaptation), which decouples CLIP’s visual and textual components to better align with the NR-VQA pipeline. Specifically, we introduce a Video-Based Temporal CLIP module and a Temporal Context Module to explicitly model motion dynamics, effectively enhancing the dorsal stream representation. Complementing this, a Basic Visual Feature Extraction Module is employed to strengthen spatial detail analysis in the ventral stream. Furthermore, we propose a text-guided adaptive fusion strategy that leverages textual semantics to dynamically weight visual features, facilitating effective spatiotemporal integration. Extensive experiments on multiple public datasets demonstrate that the proposed method achieves state-of-the-art performance, significantly improving prediction accuracy and generalization capability.&lt;/p&gt;</content:encoded></item><item><title>GMG: A Video Prediction Method Based on Global Focus and Motion Guided</title><link>https://doi.org/10.1109/tcsvt.2026.3657055</link><guid>10.1109/tcsvt.2026.3657055</guid><pubDate>Thu, 22 Jan 2026 21:03:59 +0000</pubDate><dc:creator>Yuhao Du</dc:creator><dc:creator>Hui Liu</dc:creator><dc:creator>Haoxiang Peng</dc:creator><dc:creator>Xinyuan Cheng</dc:creator><dc:creator>Chengrong Wu</dc:creator><dc:creator>Jiankai Zhang</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3657055</prism:doi><description>Recent years, video prediction has gained significant attention particularly in weather forecasting. However, accurately predicting weather remains a challenge due to the rapid variability of meteorological data and potential teleconnections. Current spatiotemporal forecasting models primarily rely on convolution operations or sliding windows for feature extraction. These methods are limited by the size of the convolutional kernel or sliding window, making it difficult to capture and identify potential teleconnection features in meteorological data. Additionally, weather data often involve non-rigid bodies, whose motion processes are accompanied by unpredictable deformations, further complicating the forecasting task. In this paper, we propose the GMG model to address these two core challenges. The Global Focus Module, a key component of our model, enhances the global receptive field, while the Motion Guided Module adapts to the growth or dissipation processes of non-rigid bodies. Through extensive evaluations, our method demonstrates competitive performance across various complex tasks, providing a novel approach to improving the predictive accuracy of complex spatiotemporal data.
Published: 2026-01-22T21:03:59+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.531 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuhao Du; Hui Liu; Haoxiang Peng; Xinyuan Cheng; Chengrong Wu; Jiankai Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3657055"&gt;10.1109/tcsvt.2026.3657055&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.531 (consider)&lt;/p&gt;
&lt;p&gt;Recent years, video prediction has gained significant attention particularly in weather forecasting. However, accurately predicting weather remains a challenge due to the rapid variability of meteorological data and potential teleconnections. Current spatiotemporal forecasting models primarily rely on convolution operations or sliding windows for feature extraction. These methods are limited by the size of the convolutional kernel or sliding window, making it difficult to capture and identify potential teleconnection features in meteorological data. Additionally, weather data often involve non-rigid bodies, whose motion processes are accompanied by unpredictable deformations, further complicating the forecasting task. In this paper, we propose the GMG model to address these two core challenges. The Global Focus Module, a key component of our model, enhances the global receptive field, while the Motion Guided Module adapts to the growth or dissipation processes of non-rigid bodies. Through extensive evaluations, our method demonstrates competitive performance across various complex tasks, providing a novel approach to improving the predictive accuracy of complex spatiotemporal data.&lt;/p&gt;</content:encoded></item><item><title>Insight: Interpretable Semantic Hierarchies in Vision-Language Encoders</title><link>https://arxiv.org/abs/2601.13798v1</link><guid>http://arxiv.org/abs/2601.13798v1</guid><pubDate>Tue, 20 Jan 2026 09:57:26 +0000</pubDate><dc:creator>Kai Wittenmayer</dc:creator><dc:creator>Sukrut Rao</dc:creator><dc:creator>Amin Parchami-Araghi</dc:creator><dc:creator>Bernt Schiele</dc:creator><dc:creator>Jonas Fischer</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Language-aligned vision foundation models perform strongly across diverse downstream tasks. Yet, their learned representations remain opaque, making interpreting their decision-making hard. Recent works decompose these representations into human-interpretable concepts, but provide poor spatial grounding and are limited to image classification tasks. In this work, we propose Insight, a language-aligned concept foundation model that provides fine-grained concepts, which are human-interpretable and spatially grounded in the input image. We leverage a hierarchical sparse autoencoder and a foundation model with strong semantic representations to automatically extract concepts at various granularities. Examining local co-occurrence dependencies of concepts allows us to define concept relationships. Through these relations we further improve concept naming and obtain richer explanations. On benchmark data, we show that Insight provides performance on classification and segmentation that is competitive with opaque foundation models while providing fine-grained, high quality concept-based explanations. Code is available at https://github.com/kawi19/Insight.
Published: 2026-01-20T09:57:26+00:00
Venue: arXiv
Score: 0.531 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kai Wittenmayer; Sukrut Rao; Amin Parchami-Araghi; Bernt Schiele; Jonas Fischer&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.531 (consider)&lt;/p&gt;
&lt;p&gt;Language-aligned vision foundation models perform strongly across diverse downstream tasks. Yet, their learned representations remain opaque, making interpreting their decision-making hard. Recent works decompose these representations into human-interpretable concepts, but provide poor spatial grounding and are limited to image classification tasks. In this work, we propose Insight, a language-aligned concept foundation model that provides fine-grained concepts, which are human-interpretable and spatially grounded in the input image. We leverage a hierarchical sparse autoencoder and a foundation model with strong semantic representations to automatically extract concepts at various granularities. Examining local co-occurrence dependencies of concepts allows us to define concept relationships. Through these relations we further improve concept naming and obtain richer explanations. On benchmark data, we show that Insight provides performance on classification and segmentation that is competitive with opaque foundation models while providing fine-grained, high quality concept-based explanations. Code is available at https://github.com/kawi19/Insight.&lt;/p&gt;</content:encoded></item><item><title>CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models</title><link>https://arxiv.org/abs/2601.13622v1</link><guid>http://arxiv.org/abs/2601.13622v1</guid><pubDate>Tue, 20 Jan 2026 05:44:33 +0000</pubDate><dc:creator>Donghee Lee</dc:creator><dc:creator>Rui Cai</dc:creator><dc:creator>Zhe Zhao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advancements in Large Vision-Language Models (LVLMs) have pushed them closer to becoming general-purpose assistants. Despite their strong performance, LVLMs still struggle with vision-centric tasks such as image classification, underperforming compared to their base vision encoders, which are often CLIP-based models. To address this limitation, we propose Context-Aware Image Representation Prioritization via Ensemble (CARPE), a novel, model-agnostic framework which introduces vision-integration layers and a context-aware ensemble strategy to identify when to prioritize image representations or rely on the reasoning capabilities of the language model. This design enhances the model's ability to adaptively weight visual and textual modalities and enables the model to capture various aspects of image representations, leading to consistent improvements in generalization across classification and vision-language benchmarks. Extensive experiments demonstrate that CARPE not only improves performance on image classification benchmarks but also enhances results across various vision-language benchmarks. Finally, CARPE is designed to be effectively integrated with most open-source LVLMs that consist of a vision encoder and a language model, ensuring its adaptability across diverse architectures.
Published: 2026-01-20T05:44:33+00:00
Venue: arXiv
Score: 0.529 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Donghee Lee; Rui Cai; Zhe Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.529 (consider)&lt;/p&gt;
&lt;p&gt;Recent advancements in Large Vision-Language Models (LVLMs) have pushed them closer to becoming general-purpose assistants. Despite their strong performance, LVLMs still struggle with vision-centric tasks such as image classification, underperforming compared to their base vision encoders, which are often CLIP-based models. To address this limitation, we propose Context-Aware Image Representation Prioritization via Ensemble (CARPE), a novel, model-agnostic framework which introduces vision-integration layers and a context-aware ensemble strategy to identify when to prioritize image representations or rely on the reasoning capabilities of the language model. This design enhances the model&amp;#x27;s ability to adaptively weight visual and textual modalities and enables the model to capture various aspects of image representations, leading to consistent improvements in generalization across classification and vision-language benchmarks. Extensive experiments demonstrate that CARPE not only improves performance on image classification benchmarks but also enhances results across various vision-language benchmarks. Finally, CARPE is designed to be effectively integrated with most open-source LVLMs that consist of a vision encoder and a language model, ensuring its adaptability across diverse architectures.&lt;/p&gt;</content:encoded></item><item><title>Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video</title><link>https://arxiv.org/abs/2601.15780v1</link><guid>http://arxiv.org/abs/2601.15780v1</guid><pubDate>Thu, 22 Jan 2026 09:14:11 +0000</pubDate><dc:creator>Pascal Benschop</dc:creator><dc:creator>Justin Dauwels</dc:creator><dc:creator>Jan van Gemert</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.
Published: 2026-01-22T09:14:11+00:00
Venue: arXiv
Score: 0.528 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pascal Benschop; Justin Dauwels; Jan van Gemert&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.528 (consider)&lt;/p&gt;
&lt;p&gt;Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.&lt;/p&gt;</content:encoded></item><item><title>Channel-hierarchical graph convolutional network with semantic alignment for long-tailed multi-label image recognition</title><link>https://doi.org/10.1016/j.neucom.2026.132832</link><guid>10.1016/j.neucom.2026.132832</guid><pubDate>Fri, 23 Jan 2026 00:23:49 +0000</pubDate><dc:creator>Liuyi Fan</dc:creator><dc:creator>Xinbo Ai</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132832</prism:doi><description>Multi-label image recognition often suffers from long-tailed distributions. Despite recent progress, existing methods still face challenges in imbalanced class representations and complex label relationships. In this paper, we propose a Channel-Hierarchical Graph Convolutional Network with Semantic Alignment (CGSA), which leverages full-channel and sub-channel heads to extract information from multiple perspectives and ensures the alignment between visual and label semantics. To model the complex relationships between labels and image regions and enhance feature representations, we treat the divided image patches and label features as graph nodes and employ a multi-head graph convolutional network to propagate information among nodes. In addition, CGSA initializes label embeddings using the frozen CLIP text encoder to capture implicit semantic correlations among classes. To mitigate visual-semantic domain bias and encourage consistent responses between the image and label branches, we design a semantic loss that jointly considers visual-semantic consistency and the semantic deviation between initial and final label nodes. Extensive experiments demonstrate the effectiveness of the proposed method, achieving competitive results on the VOC-LT and COCO-LT benchmarks.
Published: 2026-01-23T00:23:49+00:00
Venue: Neurocomputing
Score: 0.527 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Liuyi Fan; Xinbo Ai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132832"&gt;10.1016/j.neucom.2026.132832&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.527 (consider)&lt;/p&gt;
&lt;p&gt;Multi-label image recognition often suffers from long-tailed distributions. Despite recent progress, existing methods still face challenges in imbalanced class representations and complex label relationships. In this paper, we propose a Channel-Hierarchical Graph Convolutional Network with Semantic Alignment (CGSA), which leverages full-channel and sub-channel heads to extract information from multiple perspectives and ensures the alignment between visual and label semantics. To model the complex relationships between labels and image regions and enhance feature representations, we treat the divided image patches and label features as graph nodes and employ a multi-head graph convolutional network to propagate information among nodes. In addition, CGSA initializes label embeddings using the frozen CLIP text encoder to capture implicit semantic correlations among classes. To mitigate visual-semantic domain bias and encourage consistent responses between the image and label branches, we design a semantic loss that jointly considers visual-semantic consistency and the semantic deviation between initial and final label nodes. Extensive experiments demonstrate the effectiveness of the proposed method, achieving competitive results on the VOC-LT and COCO-LT benchmarks.&lt;/p&gt;</content:encoded></item><item><title>M2I2HA: A Multi-modal Object Detection Method Based on Intra- and Inter-Modal Hypergraph Attention</title><link>https://arxiv.org/abs/2601.14776v1</link><guid>http://arxiv.org/abs/2601.14776v1</guid><pubDate>Wed, 21 Jan 2026 08:55:07 +0000</pubDate><dc:creator>Xiaofan Yang</dc:creator><dc:creator>Yubin Liu</dc:creator><dc:creator>Wei Pan</dc:creator><dc:creator>Guoqing Chu</dc:creator><dc:creator>Junming Zhang</dc:creator><dc:creator>Jie Zhao</dc:creator><dc:creator>Zhuoqi Man</dc:creator><dc:creator>Xuanming Cao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advances in multi-modal detection have significantly improved detection accuracy in challenging environments (e.g., low light, overexposure). By integrating RGB with modalities such as thermal and depth, multi-modal fusion increases data redundancy and system robustness. However, significant challenges remain in effectively extracting task-relevant information both within and across modalities, as well as in achieving precise cross-modal alignment. While CNNs excel at feature extraction, they are limited by constrained receptive fields, strong inductive biases, and difficulty in capturing long-range dependencies. Transformer-based models offer global context but suffer from quadratic computational complexity and are confined to pairwise correlation modeling. Mamba and other State Space Models (SSMs), on the other hand, are hindered by their sequential scanning mechanism, which flattens 2D spatial structures into 1D sequences, disrupting topological relationships and limiting the modeling of complex higher-order dependencies. To address these issues, we propose a multi-modal perception network based on hypergraph theory called M2I2HA. Our architecture includes an Intra-Hypergraph Enhancement module to capture global many-to-many high-order relationships within each modality, and an Inter-Hypergraph Fusion module to align, enhance, and fuse cross-modal features by bridging configuration and spatial gaps between data sources. We further introduce a M2-FullPAD module to enable adaptive multi-level fusion of multi-modal enhanced features within the network, meanwhile enhancing data distribution and flow across the architecture. Extensive object detection experiments on multiple public datasets against baselines demonstrate that M2I2HA achieves state-of-the-art performance in multi-modal object detection tasks.
Published: 2026-01-21T08:55:07+00:00
Venue: arXiv
Score: 0.527 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaofan Yang; Yubin Liu; Wei Pan; Guoqing Chu; Junming Zhang; Jie Zhao; Zhuoqi Man; Xuanming Cao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.527 (consider)&lt;/p&gt;
&lt;p&gt;Recent advances in multi-modal detection have significantly improved detection accuracy in challenging environments (e.g., low light, overexposure). By integrating RGB with modalities such as thermal and depth, multi-modal fusion increases data redundancy and system robustness. However, significant challenges remain in effectively extracting task-relevant information both within and across modalities, as well as in achieving precise cross-modal alignment. While CNNs excel at feature extraction, they are limited by constrained receptive fields, strong inductive biases, and difficulty in capturing long-range dependencies. Transformer-based models offer global context but suffer from quadratic computational complexity and are confined to pairwise correlation modeling. Mamba and other State Space Models (SSMs), on the other hand, are hindered by their sequential scanning mechanism, which flattens 2D spatial structures into 1D sequences, disrupting topological relationships and limiting the modeling of complex higher-order dependencies. To address these issues, we propose a multi-modal perception network based on hypergraph theory called M2I2HA. Our architecture includes an Intra-Hypergraph Enhancement module to capture global many-to-many high-order relationships within each modality, and an Inter-Hypergraph Fusion module to align, enhance, and fuse cross-modal features by bridging configuration and spatial gaps between data sources. We further introduce a M2-FullPAD module to enable adaptive multi-level fusion of multi-modal enhanced features within the network, meanwhile enhancing data distribution and flow across the architecture. Extensive object detection experiments on multiple public datasets against baselines demonstrate that M2I2HA achieves state-of-the-art performance in multi-modal object detection tasks.&lt;/p&gt;</content:encoded></item><item><title>Reliable Reasoning Path: Distilling Effective Guidance for LLM Reasoning With Knowledge Graphs</title><link>https://doi.org/10.1109/tkde.2026.3656900</link><guid>10.1109/tkde.2026.3656900</guid><pubDate>Thu, 22 Jan 2026 21:03:31 +0000</pubDate><dc:creator>Yilin Xiao</dc:creator><dc:creator>Chuang Zhou</dc:creator><dc:creator>Qinggang Zhang</dc:creator><dc:creator>Bo Li</dc:creator><dc:creator>Qing Li</dc:creator><dc:creator>Xiao Huang</dc:creator><prism:publicationName>IEEE Transactions on Knowledge and Data Engineering</prism:publicationName><prism:doi>10.1109/tkde.2026.3656900</prism:doi><description>Large language models (LLMs) often struggle with knowledge-intensive tasks due to a lack of background knowledge and a tendency to hallucinate. To address these limitations, integrating knowledge graphs (KGs) with LLMs has been intensively studied. Existing KG-enhanced LLMs focus on supplementary factual knowledge, but still struggle with solving complex questions. We argue that refining the relationships among facts and organizing them into a logically consistent reasoning path is equally important as factual knowledge itself. Despite their potential, extracting reliable reasoning paths from KGs poses the following challenges: the complexity of graph structures and the existence of multiple generated paths, making it difficult to distinguish between useful and redundant ones. To tackle these challenges, we propose the RRP framework to mine the knowledge graph, which combines the semantic strengths of LLMs with structural information obtained through relation embedding and bidirectional distribution learning. Additionally, we introduce a rethinking module that evaluates and refines reasoning paths according to their significance. Experimental results on two public datasets show that RRP achieves state-of-the-art performance compared to existing baseline methods. Moreover, RRP can be easily integrated into various LLMs to enhance their reasoning abilities in a plug-and-play manner. By generating high-quality reasoning paths tailored to specific questions, RRP distills effective guidance for LLM reasoning.
Published: 2026-01-22T21:03:31+00:00
Venue: IEEE Transactions on Knowledge and Data Engineering
Score: 0.525 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yilin Xiao; Chuang Zhou; Qinggang Zhang; Bo Li; Qing Li; Xiao Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Knowledge and Data Engineering&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tkde.2026.3656900"&gt;10.1109/tkde.2026.3656900&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.525 (consider)&lt;/p&gt;
&lt;p&gt;Large language models (LLMs) often struggle with knowledge-intensive tasks due to a lack of background knowledge and a tendency to hallucinate. To address these limitations, integrating knowledge graphs (KGs) with LLMs has been intensively studied. Existing KG-enhanced LLMs focus on supplementary factual knowledge, but still struggle with solving complex questions. We argue that refining the relationships among facts and organizing them into a logically consistent reasoning path is equally important as factual knowledge itself. Despite their potential, extracting reliable reasoning paths from KGs poses the following challenges: the complexity of graph structures and the existence of multiple generated paths, making it difficult to distinguish between useful and redundant ones. To tackle these challenges, we propose the RRP framework to mine the knowledge graph, which combines the semantic strengths of LLMs with structural information obtained through relation embedding and bidirectional distribution learning. Additionally, we introduce a rethinking module that evaluates and refines reasoning paths according to their significance. Experimental results on two public datasets show that RRP achieves state-of-the-art performance compared to existing baseline methods. Moreover, RRP can be easily integrated into various LLMs to enhance their reasoning abilities in a plug-and-play manner. By generating high-quality reasoning paths tailored to specific questions, RRP distills effective guidance for LLM reasoning.&lt;/p&gt;</content:encoded></item></channel></rss>