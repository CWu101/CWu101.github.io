<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sat, 17 Jan 2026 02:38:57 +0000</lastBuildDate><item><title>Reasoning Text-to-Image Retrieval with Large Language Models and Digital Twin Representations</title><link>https://doi.org/10.1016/j.knosys.2026.115313</link><guid>10.1016/j.knosys.2026.115313</guid><pubDate>Thu, 15 Jan 2026 23:39:59 +0000</pubDate><dc:creator>Zexu Lin</dc:creator><dc:creator>Dell Zhang</dc:creator><dc:creator>Yiqing Shen</dc:creator><dc:creator>Xuelong Li</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115313</prism:doi><description>Text-to-image retrieval aims to identify images from large-scale collections that are semantically relevant to given textual queries. Existing embedding similarity methods rely on global feature matching that captures surface-level similarities, limiting their ability to handle implicit queries that require reasoning about object attributes, spatial relationships, and scene semantics. While some recent approaches employ multi-stage processing pipelines to enhance cross-modal understanding, they still struggle with complex implicit reasoning tasks. Additionally, these methods typically return entire images without localizing specific objects that satisfy query constraints, making them inadequate for applications demanding fine-grained retrieval. To address these limitations, we define a new task called reasoning text-to-image retrieval , a novel task that goes beyond simple similarity matching. The goal is to retrieve not only the relevant images but also the specific objects within them that satisfy implicit, reasoning-based queries. To tackle this task, we propose a two-phase framework called DTIR ( D igital T win I mage R etrieval). It bridges visual and text modalities for LLM reasoning by introducing intermediate digital twin (DT) representations. Specifically, DTIR first converts images into DT representations, which are textual descriptions that encode object semantics, attributes, and spatial relationships while preserving fine-grained visual context. Subsequently, an LLM-based agent performs reasoning and hierarchical retrieval to determine the target images as well as the objects within the images from DT representations. To evaluate reasoning-based retrieval capabilities, we construct a novel benchmark dataset RT2I, comprising 1,260 query-image pairs that require reasoning. On RT2I, DTIR achieves a Recall@1 of 37.38%, a 61% relative improvement over the strongest baseline, and establishes new state-of-the-art results on 4 conventional benchmarks. Code and dataset are available at https://github.com/oneoflzx/DTIR .
Published: 2026-01-15T23:39:59+00:00
Venue: Knowledge-Based Systems
Score: 0.590 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zexu Lin; Dell Zhang; Yiqing Shen; Xuelong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115313"&gt;10.1016/j.knosys.2026.115313&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.590 (consider)&lt;/p&gt;
&lt;p&gt;Text-to-image retrieval aims to identify images from large-scale collections that are semantically relevant to given textual queries. Existing embedding similarity methods rely on global feature matching that captures surface-level similarities, limiting their ability to handle implicit queries that require reasoning about object attributes, spatial relationships, and scene semantics. While some recent approaches employ multi-stage processing pipelines to enhance cross-modal understanding, they still struggle with complex implicit reasoning tasks. Additionally, these methods typically return entire images without localizing specific objects that satisfy query constraints, making them inadequate for applications demanding fine-grained retrieval. To address these limitations, we define a new task called reasoning text-to-image retrieval , a novel task that goes beyond simple similarity matching. The goal is to retrieve not only the relevant images but also the specific objects within them that satisfy implicit, reasoning-based queries. To tackle this task, we propose a two-phase framework called DTIR ( D igital T win I mage R etrieval). It bridges visual and text modalities for LLM reasoning by introducing intermediate digital twin (DT) representations. Specifically, DTIR first converts images into DT representations, which are textual descriptions that encode object semantics, attributes, and spatial relationships while preserving fine-grained visual context. Subsequently, an LLM-based agent performs reasoning and hierarchical retrieval to determine the target images as well as the objects within the images from DT representations. To evaluate reasoning-based retrieval capabilities, we construct a novel benchmark dataset RT2I, comprising 1,260 query-image pairs that require reasoning. On RT2I, DTIR achieves a Recall@1 of 37.38%, a 61% relative improvement over the strongest baseline, and establishes new state-of-the-art results on 4 conventional benchmarks. Code and dataset are available at https://github.com/oneoflzx/DTIR .&lt;/p&gt;</content:encoded></item><item><title>Salience-SGG: Enhancing Unbiased Scene Graph Generation with Iterative Salience Estimation</title><link>https://arxiv.org/abs/2601.08728v1</link><guid>http://arxiv.org/abs/2601.08728v1</guid><pubDate>Tue, 13 Jan 2026 16:57:09 +0000</pubDate><dc:creator>Runfeng Qu</dc:creator><dc:creator>Ole Hall</dc:creator><dc:creator>Pia K Bideau</dc:creator><dc:creator>Julie Ouerfelli-Ethier</dc:creator><dc:creator>Martin Rolfs</dc:creator><dc:creator>Klaus Obermayer</dc:creator><dc:creator>Olaf Hellwich</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Scene Graph Generation (SGG) suffers from a long-tailed distribution, where a few predicate classes dominate while many others are underrepresented, leading to biased models that underperform on rare relations. Unbiased-SGG methods address this issue by implementing debiasing strategies, but often at the cost of spatial understanding, resulting in an over-reliance on semantic priors. We introduce Salience-SGG, a novel framework featuring an Iterative Salience Decoder (ISD) that emphasizes triplets with salient spatial structures. To support this, we propose semantic-agnostic salience labels guiding ISD. Evaluations on Visual Genome, Open Images V6, and GQA-200 show that Salience-SGG achieves state-of-the-art performance and improves existing Unbiased-SGG methods in their spatial understanding as demonstrated by the Pairwise Localization Average Precision
Published: 2026-01-13T16:57:09+00:00
Venue: arXiv
Score: 0.580 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Runfeng Qu; Ole Hall; Pia K Bideau; Julie Ouerfelli-Ethier; Martin Rolfs; Klaus Obermayer; Olaf Hellwich&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.580 (consider)&lt;/p&gt;
&lt;p&gt;Scene Graph Generation (SGG) suffers from a long-tailed distribution, where a few predicate classes dominate while many others are underrepresented, leading to biased models that underperform on rare relations. Unbiased-SGG methods address this issue by implementing debiasing strategies, but often at the cost of spatial understanding, resulting in an over-reliance on semantic priors. We introduce Salience-SGG, a novel framework featuring an Iterative Salience Decoder (ISD) that emphasizes triplets with salient spatial structures. To support this, we propose semantic-agnostic salience labels guiding ISD. Evaluations on Visual Genome, Open Images V6, and GQA-200 show that Salience-SGG achieves state-of-the-art performance and improves existing Unbiased-SGG methods in their spatial understanding as demonstrated by the Pairwise Localization Average Precision&lt;/p&gt;</content:encoded></item><item><title>DR$^2$Seg: Decomposed Two-Stage Rollouts for Efficient Reasoning Segmentation in Multimodal Large Language Models</title><link>https://arxiv.org/abs/2601.09981v1</link><guid>http://arxiv.org/abs/2601.09981v1</guid><pubDate>Thu, 15 Jan 2026 01:48:45 +0000</pubDate><dc:creator>Yulin He</dc:creator><dc:creator>Wei Chen</dc:creator><dc:creator>Zhikang Jian</dc:creator><dc:creator>Tianhang Guo</dc:creator><dc:creator>Wenjuan Zhou</dc:creator><dc:creator>Minglong Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reasoning segmentation is an emerging vision-language task that requires reasoning over intricate text queries to precisely segment objects. However, existing methods typically suffer from overthinking, generating verbose reasoning chains that interfere with object localization in multimodal large language models (MLLMs). To address this issue, we propose DR$^2$Seg, a self-rewarding framework that improves both reasoning efficiency and segmentation accuracy without requiring extra thinking supervision. DR$^2$Seg employs a two-stage rollout strategy that decomposes reasoning segmentation into multimodal reasoning and referring segmentation. In the first stage, the model generates a self-contained description that explicitly specifies the target object. In the second stage, this description replaces the original complex query to verify its self-containment. Based on this design, two self-rewards are introduced to strengthen goal-oriented reasoning and suppress redundant thinking. Extensive experiments across MLLMs of varying scales and segmentation models demonstrate that DR$^2$Seg consistently improves reasoning efficiency and overall segmentation performance.
Published: 2026-01-15T01:48:45+00:00
Venue: arXiv
Score: 0.571 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yulin He; Wei Chen; Zhikang Jian; Tianhang Guo; Wenjuan Zhou; Minglong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.571 (consider)&lt;/p&gt;
&lt;p&gt;Reasoning segmentation is an emerging vision-language task that requires reasoning over intricate text queries to precisely segment objects. However, existing methods typically suffer from overthinking, generating verbose reasoning chains that interfere with object localization in multimodal large language models (MLLMs). To address this issue, we propose DR$^2$Seg, a self-rewarding framework that improves both reasoning efficiency and segmentation accuracy without requiring extra thinking supervision. DR$^2$Seg employs a two-stage rollout strategy that decomposes reasoning segmentation into multimodal reasoning and referring segmentation. In the first stage, the model generates a self-contained description that explicitly specifies the target object. In the second stage, this description replaces the original complex query to verify its self-containment. Based on this design, two self-rewards are introduced to strengthen goal-oriented reasoning and suppress redundant thinking. Extensive experiments across MLLMs of varying scales and segmentation models demonstrate that DR$^2$Seg consistently improves reasoning efficiency and overall segmentation performance.&lt;/p&gt;</content:encoded></item><item><title>AnchorReF: A novel anchor-based visual re-localization framework aided by multi-sensor data fusion</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.019</link><guid>10.1016/j.isprsjprs.2026.01.019</guid><pubDate>Fri, 16 Jan 2026 12:27:16 +0000</pubDate><dc:creator>Hao Wu</dc:creator><dc:creator>Yu Ran</dc:creator><dc:creator>Xiaoxiang Zhang</dc:creator><dc:creator>Xinying Luo</dc:creator><dc:creator>Li Wang</dc:creator><dc:creator>Teng Zhao</dc:creator><dc:creator>Yongcheng Song</dc:creator><dc:creator>Zhijun Zhang</dc:creator><dc:creator>Huisong Zhang</dc:creator><dc:creator>Jin Liu</dc:creator><dc:creator>Jian Li</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.019</prism:doi><description>Visual relocalization estimates the precise pose of a query image within a pre-built visual map, serving as a fundamental component for robot navigation, autonomous driving, surveying and mapping, etc. In the past few decades, significant research efforts have been devoted to achieving high relocalization accuracy. However, challenges remain when the query images exhibit significant changes compared to the reference scene. This paper primarily addresses the problem of pose verification and correction of inaccurate pose estimations from the relocalization. We propose a novel anchor-based visual relocalization framework that achieves robust pose estimations through multi-view co-visibility verification. Our approach further utilizes a tightly-coupled multi-sensor data fusion for pose refinement. Comprehensive evaluations on large-scale, real-world urban driving datasets (containing frequent dynamic objects, severe occlusions, and long-term environmental changes) demonstrate that our framework achieves state-of-the-art performance. Specifically, compared to traditional SFM-based and Transformer-based methods under these challenging conditions, our approach reduces the translation error by 46.2% and the rotation error by 8.55%.
Published: 2026-01-16T12:27:16+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.564 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Wu; Yu Ran; Xiaoxiang Zhang; Xinying Luo; Li Wang; Teng Zhao; Yongcheng Song; Zhijun Zhang; Huisong Zhang; Jin Liu; Jian Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.019"&gt;10.1016/j.isprsjprs.2026.01.019&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.564 (consider)&lt;/p&gt;
&lt;p&gt;Visual relocalization estimates the precise pose of a query image within a pre-built visual map, serving as a fundamental component for robot navigation, autonomous driving, surveying and mapping, etc. In the past few decades, significant research efforts have been devoted to achieving high relocalization accuracy. However, challenges remain when the query images exhibit significant changes compared to the reference scene. This paper primarily addresses the problem of pose verification and correction of inaccurate pose estimations from the relocalization. We propose a novel anchor-based visual relocalization framework that achieves robust pose estimations through multi-view co-visibility verification. Our approach further utilizes a tightly-coupled multi-sensor data fusion for pose refinement. Comprehensive evaluations on large-scale, real-world urban driving datasets (containing frequent dynamic objects, severe occlusions, and long-term environmental changes) demonstrate that our framework achieves state-of-the-art performance. Specifically, compared to traditional SFM-based and Transformer-based methods under these challenging conditions, our approach reduces the translation error by 46.2% and the rotation error by 8.55%.&lt;/p&gt;</content:encoded></item><item><title>MulMoSenT: Multimodal Sentiment Analysis for a Low-Resource Language Using Textual-Visual Cross-Attention and Fusion</title><link>https://doi.org/10.1016/j.inffus.2026.104129</link><guid>10.1016/j.inffus.2026.104129</guid><pubDate>Thu, 15 Jan 2026 23:44:22 +0000</pubDate><dc:creator>Sadia Afroze</dc:creator><dc:creator>Md Rajib Hossain</dc:creator><dc:creator>Mohammed Moshiul Hoque</dc:creator><dc:creator>Nazmul Siddique</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104129</prism:doi><description>The widespread availability of the Internet and the growing use of smart devices have fueled the rapid expansion of multimodal (image-text) sentiment analysis (MSA), a burgeoning research field. This growth is driven by the massive volume of image-text data generated by these technologies. However, MSA faces significant challenges, notably the misalignment between images and text, where an image may carry multiple interpretations or contradict its paired text. In addition, short textual content often lacks sufficient context, complicating sentiment prediction. These issues are particularly acute in low-resource languages, where annotated image-text corpora are scarce, and Vision-Language Models (VLMs) and Large Language Models (LLMs) exhibit limited performance. This research introduces MulMoSenT , a multimodal image-text sentiment analysis system tailored to tackle these challenges for low-resource languages. The development of MulMoSenT unfolds across four key phases: corpus development, baseline model evaluation and selection, hyperparameter adaptation, and model fine-tuning and inference. The proposed MulMoSenT model achieves a peak accuracy of 84.90%, surpassing all baseline models. Delivers a 37. 83% improvement over VLMs, a 35.28% gain over image-only models, and a 0.71% enhancement over text-only models. Both the dataset and the solution are publicly accessible at: https://github.com/sadia-afroze/MulMoSenT .
Published: 2026-01-15T23:44:22+00:00
Venue: Information Fusion
Score: 0.563 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sadia Afroze; Md Rajib Hossain; Mohammed Moshiul Hoque; Nazmul Siddique&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104129"&gt;10.1016/j.inffus.2026.104129&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.563 (consider)&lt;/p&gt;
&lt;p&gt;The widespread availability of the Internet and the growing use of smart devices have fueled the rapid expansion of multimodal (image-text) sentiment analysis (MSA), a burgeoning research field. This growth is driven by the massive volume of image-text data generated by these technologies. However, MSA faces significant challenges, notably the misalignment between images and text, where an image may carry multiple interpretations or contradict its paired text. In addition, short textual content often lacks sufficient context, complicating sentiment prediction. These issues are particularly acute in low-resource languages, where annotated image-text corpora are scarce, and Vision-Language Models (VLMs) and Large Language Models (LLMs) exhibit limited performance. This research introduces MulMoSenT , a multimodal image-text sentiment analysis system tailored to tackle these challenges for low-resource languages. The development of MulMoSenT unfolds across four key phases: corpus development, baseline model evaluation and selection, hyperparameter adaptation, and model fine-tuning and inference. The proposed MulMoSenT model achieves a peak accuracy of 84.90%, surpassing all baseline models. Delivers a 37. 83% improvement over VLMs, a 35.28% gain over image-only models, and a 0.71% enhancement over text-only models. Both the dataset and the solution are publicly accessible at: https://github.com/sadia-afroze/MulMoSenT .&lt;/p&gt;</content:encoded></item><item><title>Urban Socio-Semantic Segmentation with Vision-Language Reasoning</title><link>https://arxiv.org/abs/2601.10477v1</link><guid>http://arxiv.org/abs/2601.10477v1</guid><pubDate>Thu, 15 Jan 2026 15:00:36 +0000</pubDate><dc:creator>Yu Wang</dc:creator><dc:creator>Yi Wang</dc:creator><dc:creator>Rui Dai</dc:creator><dc:creator>Yujie Wang</dc:creator><dc:creator>Kaikui Liu</dc:creator><dc:creator>Xiangxiang Chu</dc:creator><dc:creator>Yansheng Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.
Published: 2026-01-15T15:00:36+00:00
Venue: arXiv
Score: 0.562 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yu Wang; Yi Wang; Rui Dai; Yujie Wang; Kaikui Liu; Xiangxiang Chu; Yansheng Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.562 (consider)&lt;/p&gt;
&lt;p&gt;As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach&amp;#x27;s gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.&lt;/p&gt;</content:encoded></item><item><title>The Spatial Blindspot of Vision-Language Models</title><link>https://arxiv.org/abs/2601.09954v1</link><guid>http://arxiv.org/abs/2601.09954v1</guid><pubDate>Thu, 15 Jan 2026 00:30:34 +0000</pubDate><dc:creator>Nahid Alam</dc:creator><dc:creator>Leema Krishna Murali</dc:creator><dc:creator>Siddhant Bharadwaj</dc:creator><dc:creator>Patrick Liu</dc:creator><dc:creator>Timothy Chung</dc:creator><dc:creator>Drishti Sharma</dc:creator><dc:creator>Akshata A</dc:creator><dc:creator>Kranthi Kiran</dc:creator><dc:creator>Wesley Tam</dc:creator><dc:creator>Bala Krishna S Vegesna</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.
Published: 2026-01-15T00:30:34+00:00
Venue: arXiv
Score: 0.561 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nahid Alam; Leema Krishna Murali; Siddhant Bharadwaj; Patrick Liu; Timothy Chung; Drishti Sharma; Akshata A; Kranthi Kiran; Wesley Tam; Bala Krishna S Vegesna&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.561 (consider)&lt;/p&gt;
&lt;p&gt;Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.&lt;/p&gt;</content:encoded></item><item><title>Boosting Segment Anything Model to Generalize Visually Non-Salient Scenarios</title><link>https://doi.org/10.1109/tip.2026.3651951</link><guid>10.1109/tip.2026.3651951</guid><pubDate>Thu, 15 Jan 2026 20:52:29 +0000</pubDate><dc:creator>Guangqian Guo</dc:creator><dc:creator>Pengfei Chen</dc:creator><dc:creator>Yong Guo</dc:creator><dc:creator>Huafeng Chen</dc:creator><dc:creator>Boqiang Zhang</dc:creator><dc:creator>Shan Gao</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3651951</prism:doi><description>Segment Anything Model (SAM), known for its remarkable zero-shot segmentation capabilities, has garnered significant attention in the community. Nevertheless, its performance is challenged when dealing with what we refer to as visually non-salient scenarios, where there is low contrast between the foreground and background. In these cases, existing methods often cannot capture accurate contours and fail to produce promising segmentation results. In this paper, we propose Visually Non-Salient SAM (VNS-SAM), aiming to enhance SAM’s perception of visually non-salient scenarios while preserving its original zero-shot generalizability. We achieve this by effectively exploiting SAM’s low-level features through two designs: Mask-Edge Token Interactive decoder and Non-Salient Feature Mining module. These designs help the SAM decoder gain a deeper understanding of non-salient characteristics with only marginal parameter increments and computational requirements. The additional parameters of VNS-SAM can be optimized within 4 hours, demonstrating its feasibility and practicality. In terms of data, we established VNS-SEG, a unified dataset for various VNS scenarios, with more than 35K images, in contrast to previous single-task adaptations. It is designed to make the model learn more robust VNS features and comprehensively benchmark the model’s segmentation performance and generalizability on VNS scenarios. Extensive experiments across various VNS segmentation tasks demonstrate the superior performance of VNS-SAM, particularly under zero-shot settings, highlighting its potential for broad real-world applications. Codes and datasets will be publicly available at https://guangqian-guo.github.io/VNS-SAM/.
Published: 2026-01-15T20:52:29+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.557 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guangqian Guo; Pengfei Chen; Yong Guo; Huafeng Chen; Boqiang Zhang; Shan Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3651951"&gt;10.1109/tip.2026.3651951&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.557 (consider)&lt;/p&gt;
&lt;p&gt;Segment Anything Model (SAM), known for its remarkable zero-shot segmentation capabilities, has garnered significant attention in the community. Nevertheless, its performance is challenged when dealing with what we refer to as visually non-salient scenarios, where there is low contrast between the foreground and background. In these cases, existing methods often cannot capture accurate contours and fail to produce promising segmentation results. In this paper, we propose Visually Non-Salient SAM (VNS-SAM), aiming to enhance SAM’s perception of visually non-salient scenarios while preserving its original zero-shot generalizability. We achieve this by effectively exploiting SAM’s low-level features through two designs: Mask-Edge Token Interactive decoder and Non-Salient Feature Mining module. These designs help the SAM decoder gain a deeper understanding of non-salient characteristics with only marginal parameter increments and computational requirements. The additional parameters of VNS-SAM can be optimized within 4 hours, demonstrating its feasibility and practicality. In terms of data, we established VNS-SEG, a unified dataset for various VNS scenarios, with more than 35K images, in contrast to previous single-task adaptations. It is designed to make the model learn more robust VNS features and comprehensively benchmark the model’s segmentation performance and generalizability on VNS scenarios. Extensive experiments across various VNS segmentation tasks demonstrate the superior performance of VNS-SAM, particularly under zero-shot settings, highlighting its potential for broad real-world applications. Codes and datasets will be publicly available at https://guangqian-guo.github.io/VNS-SAM/.&lt;/p&gt;</content:encoded></item><item><title>RAG-3DSG: Enhancing 3D Scene Graphs with Re-Shot Guided Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2601.10168v1</link><guid>http://arxiv.org/abs/2601.10168v1</guid><pubDate>Thu, 15 Jan 2026 08:15:01 +0000</pubDate><dc:creator>Yue Chang</dc:creator><dc:creator>Rufeng Chen</dc:creator><dc:creator>Zhaofan Zhang</dc:creator><dc:creator>Yi Chen</dc:creator><dc:creator>Sihong Xie</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Open-vocabulary 3D Scene Graph (3DSG) generation can enhance various downstream tasks in robotics, such as manipulation and navigation, by leveraging structured semantic representations. A 3DSG is constructed from multiple images of a scene, where objects are represented as nodes and relationships as edges. However, existing works for open-vocabulary 3DSG generation suffer from both low object-level recognition accuracy and speed, mainly due to constrained viewpoints, occlusions, and redundant surface density. To address these challenges, we propose RAG-3DSG to mitigate aggregation noise through re-shot guided uncertainty estimation and support object-level Retrieval-Augmented Generation (RAG) via reliable low-uncertainty objects. Furthermore, we propose a dynamic downsample-mapping strategy to accelerate cross-image object aggregation with adaptive granularity. Experiments on Replica dataset demonstrate that RAG-3DSG significantly improves node captioning accuracy in 3DSG generation while reducing the mapping time by two-thirds compared to the vanilla version.
Published: 2026-01-15T08:15:01+00:00
Venue: arXiv
Score: 0.556 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yue Chang; Rufeng Chen; Zhaofan Zhang; Yi Chen; Sihong Xie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.556 (consider)&lt;/p&gt;
&lt;p&gt;Open-vocabulary 3D Scene Graph (3DSG) generation can enhance various downstream tasks in robotics, such as manipulation and navigation, by leveraging structured semantic representations. A 3DSG is constructed from multiple images of a scene, where objects are represented as nodes and relationships as edges. However, existing works for open-vocabulary 3DSG generation suffer from both low object-level recognition accuracy and speed, mainly due to constrained viewpoints, occlusions, and redundant surface density. To address these challenges, we propose RAG-3DSG to mitigate aggregation noise through re-shot guided uncertainty estimation and support object-level Retrieval-Augmented Generation (RAG) via reliable low-uncertainty objects. Furthermore, we propose a dynamic downsample-mapping strategy to accelerate cross-image object aggregation with adaptive granularity. Experiments on Replica dataset demonstrate that RAG-3DSG significantly improves node captioning accuracy in 3DSG generation while reducing the mapping time by two-thirds compared to the vanilla version.&lt;/p&gt;</content:encoded></item><item><title>Hippocampal Memory-Like Separation-Completion Collaborative Network for Unbiased Scene Graph Generation</title><link>https://doi.org/10.1109/tip.2025.3650668</link><guid>10.1109/tip.2025.3650668</guid><pubDate>Thu, 15 Jan 2026 20:52:29 +0000</pubDate><dc:creator>Ruonan Zhang</dc:creator><dc:creator>Gaoyun An</dc:creator><dc:creator>Yiqing Hao</dc:creator><dc:creator>Dapeng Oliver Wu</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3650668</prism:doi><description>Scene Graph Generation (SGG) is a challenging cross-modal task, which aims to identify entities and relationships in a scene simultaneously. Due to highly skewed long-tailed distribution, the generated scene graphs are dominated by relation categories of head samples. Current works address this problem by designing re-balancing strategies at the data level or refining relation representations at the feature level. Different from them, we attribute this impact to catastrophic interference, that is, the subsequent learning of dominant relations tends to overwrite the earlier learning of rare relations. To address it at the modeling level, a Hippocampal Memory-Like Separation-Completion Collaborative Network (HMSC2) is proposed here, which imitates the hippocampal encoding and retrieval process. Inspired by the pattern separation of dentate gyrus during memory encoding, a Gradient Separation Classifier and a Prototype Separation Learning module are proposed to relieve the catastrophic interference of tail categories by modeling the separated classifier and prototypes. In addition, inspired by the pattern completion of area CA3 of hippocampus during memory retrieval, a Prototype Completion Module is designed to supplement the incomplete information of prototypes by introducing relation representations as cues. Finally, the completed prototype and relation representations are connected within a hypersphere space by a Contrastive Connected Module. Experimental results on Visual Genome and GQA datasets show our HMSC2 achieves state-of-the-art performance on the unbiased SGG task, effectively relieving the long-tailed problem. The source codes are released on GitHub: https://github.com/Nora-Zhang98/HMSC2.
Published: 2026-01-15T20:52:29+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.552 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruonan Zhang; Gaoyun An; Yiqing Hao; Dapeng Oliver Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3650668"&gt;10.1109/tip.2025.3650668&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.552 (consider)&lt;/p&gt;
&lt;p&gt;Scene Graph Generation (SGG) is a challenging cross-modal task, which aims to identify entities and relationships in a scene simultaneously. Due to highly skewed long-tailed distribution, the generated scene graphs are dominated by relation categories of head samples. Current works address this problem by designing re-balancing strategies at the data level or refining relation representations at the feature level. Different from them, we attribute this impact to catastrophic interference, that is, the subsequent learning of dominant relations tends to overwrite the earlier learning of rare relations. To address it at the modeling level, a Hippocampal Memory-Like Separation-Completion Collaborative Network (HMSC2) is proposed here, which imitates the hippocampal encoding and retrieval process. Inspired by the pattern separation of dentate gyrus during memory encoding, a Gradient Separation Classifier and a Prototype Separation Learning module are proposed to relieve the catastrophic interference of tail categories by modeling the separated classifier and prototypes. In addition, inspired by the pattern completion of area CA3 of hippocampus during memory retrieval, a Prototype Completion Module is designed to supplement the incomplete information of prototypes by introducing relation representations as cues. Finally, the completed prototype and relation representations are connected within a hypersphere space by a Contrastive Connected Module. Experimental results on Visual Genome and GQA datasets show our HMSC2 achieves state-of-the-art performance on the unbiased SGG task, effectively relieving the long-tailed problem. The source codes are released on GitHub: https://github.com/Nora-Zhang98/HMSC2.&lt;/p&gt;</content:encoded></item><item><title>LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning</title><link>https://arxiv.org/abs/2601.10129v1</link><guid>http://arxiv.org/abs/2601.10129v1</guid><pubDate>Thu, 15 Jan 2026 07:14:24 +0000</pubDate><dc:creator>Linquan Wu</dc:creator><dc:creator>Tianxiang Jiang</dc:creator><dc:creator>Yifei Dong</dc:creator><dc:creator>Haoyu Yang</dc:creator><dc:creator>Fengji Zhang</dc:creator><dc:creator>Shichaang Meng</dc:creator><dc:creator>Ai Xuan</dc:creator><dc:creator>Linqi Song</dc:creator><dc:creator>Jacky Keung</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Current multimodal latent reasoning often relies on external supervision (e.g., auxiliary images), ignoring intrinsic visual attention dynamics. In this work, we identify a critical Perception Gap in distillation: student models frequently mimic a teacher's textual output while attending to fundamentally divergent visual regions, effectively relying on language priors rather than grounded perception. To bridge this, we propose LaViT, a framework that aligns latent visual thoughts rather than static embeddings. LaViT compels the student to autoregressively reconstruct the teacher's visual semantics and attention trajectories prior to text generation, employing a curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show that LaViT significantly enhances visual grounding, achieving up to +16.9% gains on complex reasoning tasks and enabling a compact 3B model to outperform larger open-source variants and proprietary models like GPT-4o.
Published: 2026-01-15T07:14:24+00:00
Venue: arXiv
Score: 0.552 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Linquan Wu; Tianxiang Jiang; Yifei Dong; Haoyu Yang; Fengji Zhang; Shichaang Meng; Ai Xuan; Linqi Song; Jacky Keung&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.552 (consider)&lt;/p&gt;
&lt;p&gt;Current multimodal latent reasoning often relies on external supervision (e.g., auxiliary images), ignoring intrinsic visual attention dynamics. In this work, we identify a critical Perception Gap in distillation: student models frequently mimic a teacher&amp;#x27;s textual output while attending to fundamentally divergent visual regions, effectively relying on language priors rather than grounded perception. To bridge this, we propose LaViT, a framework that aligns latent visual thoughts rather than static embeddings. LaViT compels the student to autoregressively reconstruct the teacher&amp;#x27;s visual semantics and attention trajectories prior to text generation, employing a curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show that LaViT significantly enhances visual grounding, achieving up to +16.9% gains on complex reasoning tasks and enabling a compact 3B model to outperform larger open-source variants and proprietary models like GPT-4o.&lt;/p&gt;</content:encoded></item><item><title>Fast Track Anything with Sparse Spatio-Temporal Propagation for Unified Video Segmentation</title><link>https://doi.org/10.1109/tip.2025.3649365</link><guid>10.1109/tip.2025.3649365</guid><pubDate>Thu, 15 Jan 2026 20:52:29 +0000</pubDate><dc:creator>Jisheng Dang</dc:creator><dc:creator>Huicheng Zheng</dc:creator><dc:creator>Zhixuan Chen</dc:creator><dc:creator>Zhang Li</dc:creator><dc:creator>Yulan Guo</dc:creator><dc:creator>Tat-Seng Chua</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3649365</prism:doi><description>advances in "track-anything" models have significantly improved fine-grained video understanding by simultaneously handling multiple video segmentation and tracking tasks. However, existing models often struggle with robust and efficient temporal propagation. To address these challenges, we propose the Sparse Spatio-Temporal Propagation (SSTP) method, which achieves robust and efficient unified video segmentation by selectively leveraging key spatio-temporal features in videos. Specifically, we design a dynamic 3D spatio-temporal convolution to aggregate global multi-frame spatio-temporal information into memory frames during memory construction. Additionally, we introduce a spatio-temporal aggregation reading strategy to efficiently aggregate the relevant spatio-temporal features from multiple memory frames during memory retrieval. By combining SSTP with an image segmentation foundation model, such as the segment anything model, our method effectively addresses multiple data-scarce video segmentation tasks. Our experimental results demonstrate state-of-the-art performance on five video segmentation tasks across eleven datasets, outperforming both task-specific and unified methods. Notably, SSTP exhibits strong robustness in handling sparse, low-frame-rate videos, making it well-suited for real-world applications.
Published: 2026-01-15T20:52:29+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.550 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jisheng Dang; Huicheng Zheng; Zhixuan Chen; Zhang Li; Yulan Guo; Tat-Seng Chua&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3649365"&gt;10.1109/tip.2025.3649365&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.550 (consider)&lt;/p&gt;
&lt;p&gt;advances in &amp;quot;track-anything&amp;quot; models have significantly improved fine-grained video understanding by simultaneously handling multiple video segmentation and tracking tasks. However, existing models often struggle with robust and efficient temporal propagation. To address these challenges, we propose the Sparse Spatio-Temporal Propagation (SSTP) method, which achieves robust and efficient unified video segmentation by selectively leveraging key spatio-temporal features in videos. Specifically, we design a dynamic 3D spatio-temporal convolution to aggregate global multi-frame spatio-temporal information into memory frames during memory construction. Additionally, we introduce a spatio-temporal aggregation reading strategy to efficiently aggregate the relevant spatio-temporal features from multiple memory frames during memory retrieval. By combining SSTP with an image segmentation foundation model, such as the segment anything model, our method effectively addresses multiple data-scarce video segmentation tasks. Our experimental results demonstrate state-of-the-art performance on five video segmentation tasks across eleven datasets, outperforming both task-specific and unified methods. Notably, SSTP exhibits strong robustness in handling sparse, low-frame-rate videos, making it well-suited for real-world applications.&lt;/p&gt;</content:encoded></item><item><title>Enhancing Visual In-Context Learning by Multi-Faceted Fusion</title><link>https://arxiv.org/abs/2601.10107v1</link><guid>http://arxiv.org/abs/2601.10107v1</guid><pubDate>Thu, 15 Jan 2026 06:25:09 +0000</pubDate><dc:creator>Wenwen Liao</dc:creator><dc:creator>Jianbo Yu</dc:creator><dc:creator>Yuansong Wang</dc:creator><dc:creator>Qingchao Jiang</dc:creator><dc:creator>Xiaofeng Yang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual In-Context Learning (VICL) has emerged as a powerful paradigm, enabling models to perform novel visual tasks by learning from in-context examples. The dominant "retrieve-then-prompt" approach typically relies on selecting the single best visual prompt, a practice that often discards valuable contextual information from other suitable candidates. While recent work has explored fusing the top-K prompts into a single, enhanced representation, this still simply collapses multiple rich signals into one, limiting the model's reasoning capability. We argue that a more multi-faceted, collaborative fusion is required to unlock the full potential of these diverse contexts. To address this limitation, we introduce a novel framework that moves beyond single-prompt fusion towards an multi-combination collaborative fusion. Instead of collapsing multiple prompts into one, our method generates three contextual representation branches, each formed by integrating information from different combinations of top-quality prompts. These complementary guidance signals are then fed into proposed MULTI-VQGAN architecture, which is designed to jointly interpret and utilize collaborative information from multiple sources. Extensive experiments on diverse tasks, including foreground segmentation, single-object detection, and image colorization, highlight its strong cross-task generalization, effective contextual fusion, and ability to produce more robust and accurate predictions than existing methods.
Published: 2026-01-15T06:25:09+00:00
Venue: arXiv
Score: 0.550 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenwen Liao; Jianbo Yu; Yuansong Wang; Qingchao Jiang; Xiaofeng Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.550 (consider)&lt;/p&gt;
&lt;p&gt;Visual In-Context Learning (VICL) has emerged as a powerful paradigm, enabling models to perform novel visual tasks by learning from in-context examples. The dominant &amp;quot;retrieve-then-prompt&amp;quot; approach typically relies on selecting the single best visual prompt, a practice that often discards valuable contextual information from other suitable candidates. While recent work has explored fusing the top-K prompts into a single, enhanced representation, this still simply collapses multiple rich signals into one, limiting the model&amp;#x27;s reasoning capability. We argue that a more multi-faceted, collaborative fusion is required to unlock the full potential of these diverse contexts. To address this limitation, we introduce a novel framework that moves beyond single-prompt fusion towards an multi-combination collaborative fusion. Instead of collapsing multiple prompts into one, our method generates three contextual representation branches, each formed by integrating information from different combinations of top-quality prompts. These complementary guidance signals are then fed into proposed MULTI-VQGAN architecture, which is designed to jointly interpret and utilize collaborative information from multiple sources. Extensive experiments on diverse tasks, including foreground segmentation, single-object detection, and image colorization, highlight its strong cross-task generalization, effective contextual fusion, and ability to produce more robust and accurate predictions than existing methods.&lt;/p&gt;</content:encoded></item><item><title>EFIN: A Novel Enhanced Feature Interaction Network for Temporal Sentence Grounding in Videos</title><link>https://doi.org/10.1109/tmm.2026.3654403</link><guid>10.1109/tmm.2026.3654403</guid><pubDate>Thu, 15 Jan 2026 20:50:42 +0000</pubDate><dc:creator>Chongxu Hu</dc:creator><dc:creator>Xianbin Wen</dc:creator><dc:creator>Yibo Zhao</dc:creator><dc:creator>Chunjie Ma</dc:creator><dc:creator>Weili Guan</dc:creator><dc:creator>Riwei Wang</dc:creator><dc:creator>Zan Gao</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3654403</prism:doi><description>Temporal sentence grounding in videos (TSGV) is a challenging task that aims to match text queries with semantically relevant segments in untrimmed videos. However, existing methods face limitations in modeling modality features, which constrains the expressive power of candidate moment features. To address this challenge, we propose a novel Enhanced Feature Interaction Network (EFIN) that effectively captures semantic information within each modality and aligns relationships between modalities. Additionally, EFIN enhances the fusion of information between candidate moments and modality features. Specifically, our model begins by extracting modality features to generate candidate moments as priors. Building upon these modality features, we introduce an enhanced feature encoder to extract semantic information within each modality, thereby improving intra-modality feature representation. Simultaneously, the encoder captures alignment relationships between modalities to optimize cross-modality feature representation, enhancing the overall modeling capacity of modality features. Moreover, we design an information fusion module to enrich the comprehension of modality information for candidate moments. Extensive experiments on four benchmark datasets demonstrate the superiority of our proposed EFIN model. Notably, EFIN achieves a maximum performance improvement of approximately 1.67% and 1.91% across different evaluation metrics on TACoS dataset.
Published: 2026-01-15T20:50:42+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.548 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chongxu Hu; Xianbin Wen; Yibo Zhao; Chunjie Ma; Weili Guan; Riwei Wang; Zan Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3654403"&gt;10.1109/tmm.2026.3654403&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.548 (consider)&lt;/p&gt;
&lt;p&gt;Temporal sentence grounding in videos (TSGV) is a challenging task that aims to match text queries with semantically relevant segments in untrimmed videos. However, existing methods face limitations in modeling modality features, which constrains the expressive power of candidate moment features. To address this challenge, we propose a novel Enhanced Feature Interaction Network (EFIN) that effectively captures semantic information within each modality and aligns relationships between modalities. Additionally, EFIN enhances the fusion of information between candidate moments and modality features. Specifically, our model begins by extracting modality features to generate candidate moments as priors. Building upon these modality features, we introduce an enhanced feature encoder to extract semantic information within each modality, thereby improving intra-modality feature representation. Simultaneously, the encoder captures alignment relationships between modalities to optimize cross-modality feature representation, enhancing the overall modeling capacity of modality features. Moreover, we design an information fusion module to enrich the comprehension of modality information for candidate moments. Extensive experiments on four benchmark datasets demonstrate the superiority of our proposed EFIN model. Notably, EFIN achieves a maximum performance improvement of approximately 1.67% and 1.91% across different evaluation metrics on TACoS dataset.&lt;/p&gt;</content:encoded></item><item><title>MAP-GR: Medical Aware Prompt and Graph-guided Reasoning for Enhanced Medical Visual Question Answering</title><link>https://doi.org/10.1016/j.neucom.2026.132645</link><guid>10.1016/j.neucom.2026.132645</guid><pubDate>Fri, 16 Jan 2026 07:39:13 +0000</pubDate><dc:creator>Yuhai Yu</dc:creator><dc:creator>Xinghao Li</dc:creator><dc:creator>Jiana Meng</dc:creator><dc:creator>Xinyue Wang</dc:creator><dc:creator>Xinran Yan</dc:creator><dc:creator>Lin Lu</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132645</prism:doi><description>Medical Visual Question Answering (Med-VQA) refers to the task of automatically generating accurate answers based on medical images and corresponding questions. It plays a critical role in computer-aided diagnosis.However, it still poses significant challenges in interpreting subtle pathological visual cues and complex clinical semantics.To address these challenges, this paper proposes the MAP-GR model, which integrates a Medical-Aware Prompt (MAP) and Graph-guided Reasoning (GR). Compared with the cross-modal prompts of UNIDCP [22], MAP initializes prompt vectors through entities extracted by PubMed NER, enhancing the localization of fine-grained lesions. In contrast to the latent prompt enhancement approach of LaPA [23], GR directly empowers the model with the ability to reason about complex organ-disease relationships and nuanced semantics embedded in clinical questions via a Graph Attention Network (GAT). On the VQA-RAD, SLAKE, and VQA-2019 datasets, the overall accuracy of MAP-GR reaches 79.83%±0.15%, 84.97%±0.12%, and 83.23%±0.10% respectively, outperforming existing state-of-the-art methods in both accuracy and robustness.
Published: 2026-01-16T07:39:13+00:00
Venue: Neurocomputing
Score: 0.545 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuhai Yu; Xinghao Li; Jiana Meng; Xinyue Wang; Xinran Yan; Lin Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132645"&gt;10.1016/j.neucom.2026.132645&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.545 (consider)&lt;/p&gt;
&lt;p&gt;Medical Visual Question Answering (Med-VQA) refers to the task of automatically generating accurate answers based on medical images and corresponding questions. It plays a critical role in computer-aided diagnosis.However, it still poses significant challenges in interpreting subtle pathological visual cues and complex clinical semantics.To address these challenges, this paper proposes the MAP-GR model, which integrates a Medical-Aware Prompt (MAP) and Graph-guided Reasoning (GR). Compared with the cross-modal prompts of UNIDCP [22], MAP initializes prompt vectors through entities extracted by PubMed NER, enhancing the localization of fine-grained lesions. In contrast to the latent prompt enhancement approach of LaPA [23], GR directly empowers the model with the ability to reason about complex organ-disease relationships and nuanced semantics embedded in clinical questions via a Graph Attention Network (GAT). On the VQA-RAD, SLAKE, and VQA-2019 datasets, the overall accuracy of MAP-GR reaches 79.83%±0.15%, 84.97%±0.12%, and 83.23%±0.10% respectively, outperforming existing state-of-the-art methods in both accuracy and robustness.&lt;/p&gt;</content:encoded></item><item><title>Visual-Language Active Search for Wide-Area Remote Sensing Imagery</title><link>https://doi.org/10.1016/j.patcog.2026.113106</link><guid>10.1016/j.patcog.2026.113106</guid><pubDate>Thu, 15 Jan 2026 16:09:14 +0000</pubDate><dc:creator>Nuo Xu</dc:creator><dc:creator>Kelu Yao</dc:creator><dc:creator>Rong Yang</dc:creator><dc:creator>Chao Li</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113106</prism:doi><description>Visual-Language Active Search (VLAS) represents a task adept at leveraging wide-area visual observations alongside language instructions to guide agent exploration and exploitation. Its paramount objective is to swiftly identify sub-regions harboring specified objects within expansive geographical realms. While contemporary Visual Active Search (VAS) models typically employ neural networks to progressively map wide-area images into sub-regional selection vectors. This methodology primarily thrives in uncomplicated scenarios characterized, for example, by single object category. However, the efficacy of VAS models is often curtailed in more intricate settings. To address this limitation, we have elevated the VAS paradigm to encompass visual-language interactions, thus establishing the novel framework VLAS. Building upon this advancement, a hierarchical controller, denoted as Progressive guidAnce by Graph-based Enhancement (PAGE), is proposed to enhance the capacity of VLAS for high-level strategic thinking. By incorporating language instructions and graph modeling, our approach empowers intelligent agents to address active search challenges under intricate real-world geographical scenarios. Rigorous experiments demonstrate that VLAS outperforms state-of-the-art benchmarks across multiple open-source datasets. Code available: https://github.com/nuoxu/VLAS .
Published: 2026-01-15T16:09:14+00:00
Venue: Pattern Recognition
Score: 0.543 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nuo Xu; Kelu Yao; Rong Yang; Chao Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113106"&gt;10.1016/j.patcog.2026.113106&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.543 (consider)&lt;/p&gt;
&lt;p&gt;Visual-Language Active Search (VLAS) represents a task adept at leveraging wide-area visual observations alongside language instructions to guide agent exploration and exploitation. Its paramount objective is to swiftly identify sub-regions harboring specified objects within expansive geographical realms. While contemporary Visual Active Search (VAS) models typically employ neural networks to progressively map wide-area images into sub-regional selection vectors. This methodology primarily thrives in uncomplicated scenarios characterized, for example, by single object category. However, the efficacy of VAS models is often curtailed in more intricate settings. To address this limitation, we have elevated the VAS paradigm to encompass visual-language interactions, thus establishing the novel framework VLAS. Building upon this advancement, a hierarchical controller, denoted as Progressive guidAnce by Graph-based Enhancement (PAGE), is proposed to enhance the capacity of VLAS for high-level strategic thinking. By incorporating language instructions and graph modeling, our approach empowers intelligent agents to address active search challenges under intricate real-world geographical scenarios. Rigorous experiments demonstrate that VLAS outperforms state-of-the-art benchmarks across multiple open-source datasets. Code available: https://github.com/nuoxu/VLAS .&lt;/p&gt;</content:encoded></item><item><title>MMToT: Multi-Modal Token-of-Thought Reasoning for Large Models</title><link>https://doi.org/10.1109/tmm.2026.3654463</link><guid>10.1109/tmm.2026.3654463</guid><pubDate>Thu, 15 Jan 2026 20:50:42 +0000</pubDate><dc:creator>Ning Xu</dc:creator><dc:creator>Zimu Lu</dc:creator><dc:creator>Hongshuo Tian</dc:creator><dc:creator>Bolun Zheng</dc:creator><dc:creator>Jinbo Cao</dc:creator><dc:creator>An-An Liu</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3654463</prism:doi><description>With the development of Large models (LMs), recent methods tend to leverage them to complete various downstream tasks, like VQA and image caption. A typical method is Chain-of-Thought (CoT) prompting, which improves the reasoning abilities of LMs by providing intermediate steps. However, existing CoT prompts have two main drawbacks: 1) They typically represent CoT through multiple sentences, which often introduce irrelevant textual or visual context that may confuse LMs. 2) The current CoT methods fail to consider how the contributions of different tokens vary for answer inference. For this, we propose the Multi-Modal Token-of-Thought (MMToT), a novel token-level prompt method to improve LMs' multi-modal reasoning capabilities. Furthermore, MMToT stress on two strengths against CoT: 1) To prevent LMs from being affected by irrelevant contexts, we propose to extract explicit multi-modal tokens rather than sentences to construct MMToT, enhancing the reliability of generated answers. 2) To ensure that LMs prioritize tokens with high contribution scores during answer generation, we propose a confident decision-making module to evaluate and integrate each token's contribution in MMToT. Compared to existing methods, the proposed MMToT demonstrates superior performance on Science-QA, MATH, OKVQA, and VQA-introspect datasets. Furthermore, ablation studies and visualization results validate the effectiveness and interpretability of MMToT.
Published: 2026-01-15T20:50:42+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.543 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ning Xu; Zimu Lu; Hongshuo Tian; Bolun Zheng; Jinbo Cao; An-An Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3654463"&gt;10.1109/tmm.2026.3654463&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.543 (consider)&lt;/p&gt;
&lt;p&gt;With the development of Large models (LMs), recent methods tend to leverage them to complete various downstream tasks, like VQA and image caption. A typical method is Chain-of-Thought (CoT) prompting, which improves the reasoning abilities of LMs by providing intermediate steps. However, existing CoT prompts have two main drawbacks: 1) They typically represent CoT through multiple sentences, which often introduce irrelevant textual or visual context that may confuse LMs. 2) The current CoT methods fail to consider how the contributions of different tokens vary for answer inference. For this, we propose the Multi-Modal Token-of-Thought (MMToT), a novel token-level prompt method to improve LMs&amp;#x27; multi-modal reasoning capabilities. Furthermore, MMToT stress on two strengths against CoT: 1) To prevent LMs from being affected by irrelevant contexts, we propose to extract explicit multi-modal tokens rather than sentences to construct MMToT, enhancing the reliability of generated answers. 2) To ensure that LMs prioritize tokens with high contribution scores during answer generation, we propose a confident decision-making module to evaluate and integrate each token&amp;#x27;s contribution in MMToT. Compared to existing methods, the proposed MMToT demonstrates superior performance on Science-QA, MATH, OKVQA, and VQA-introspect datasets. Furthermore, ablation studies and visualization results validate the effectiveness and interpretability of MMToT.&lt;/p&gt;</content:encoded></item><item><title>Unleashing the Capabilities of Large Vision-Language Models for Intelligent Perception of Roadside Infrastructure</title><link>https://arxiv.org/abs/2601.10551v1</link><guid>http://arxiv.org/abs/2601.10551v1</guid><pubDate>Thu, 15 Jan 2026 16:16:34 +0000</pubDate><dc:creator>Luxuan Fu</dc:creator><dc:creator>Chong Liu</dc:creator><dc:creator>Bisheng Yang</dc:creator><dc:creator>Zhen Dong</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Automated perception of urban roadside infrastructure is crucial for smart city management, yet general-purpose models often struggle to capture the necessary fine-grained attributes and domain rules. While Large Vision Language Models (VLMs) excel at open-world recognition, they often struggle to accurately interpret complex facility states in compliance with engineering standards, leading to unreliable performance in real-world applications. To address this, we propose a domain-adapted framework that transforms VLMs into specialized agents for intelligent infrastructure analysis. Our approach integrates a data-efficient fine-tuning strategy with a knowledge-grounded reasoning mechanism. Specifically, we leverage open-vocabulary fine-tuning on Grounding DINO to robustly localize diverse assets with minimal supervision, followed by LoRA-based adaptation on Qwen-VL for deep semantic attribute reasoning. To mitigate hallucinations and enforce professional compliance, we introduce a dual-modality Retrieval-Augmented Generation (RAG) module that dynamically retrieves authoritative industry standards and visual exemplars during inference. Evaluated on a comprehensive new dataset of urban roadside scenes, our framework achieves a detection performance of 58.9 mAP and an attribute recognition accuracy of 95.5%, demonstrating a robust solution for intelligent infrastructure monitoring.
Published: 2026-01-15T16:16:34+00:00
Venue: arXiv
Score: 0.542 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Luxuan Fu; Chong Liu; Bisheng Yang; Zhen Dong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.542 (consider)&lt;/p&gt;
&lt;p&gt;Automated perception of urban roadside infrastructure is crucial for smart city management, yet general-purpose models often struggle to capture the necessary fine-grained attributes and domain rules. While Large Vision Language Models (VLMs) excel at open-world recognition, they often struggle to accurately interpret complex facility states in compliance with engineering standards, leading to unreliable performance in real-world applications. To address this, we propose a domain-adapted framework that transforms VLMs into specialized agents for intelligent infrastructure analysis. Our approach integrates a data-efficient fine-tuning strategy with a knowledge-grounded reasoning mechanism. Specifically, we leverage open-vocabulary fine-tuning on Grounding DINO to robustly localize diverse assets with minimal supervision, followed by LoRA-based adaptation on Qwen-VL for deep semantic attribute reasoning. To mitigate hallucinations and enforce professional compliance, we introduce a dual-modality Retrieval-Augmented Generation (RAG) module that dynamically retrieves authoritative industry standards and visual exemplars during inference. Evaluated on a comprehensive new dataset of urban roadside scenes, our framework achieves a detection performance of 58.9 mAP and an attribute recognition accuracy of 95.5%, demonstrating a robust solution for intelligent infrastructure monitoring.&lt;/p&gt;</content:encoded></item><item><title>CogniMap3D: Cognitive 3D Mapping and Rapid Retrieval</title><link>https://arxiv.org/abs/2601.08175v1</link><guid>http://arxiv.org/abs/2601.08175v1</guid><pubDate>Tue, 13 Jan 2026 03:09:35 +0000</pubDate><dc:creator>Feiran Wang</dc:creator><dc:creator>Junyi Wu</dc:creator><dc:creator>Dawen Cai</dc:creator><dc:creator>Yuan Hong</dc:creator><dc:creator>Yan Yan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present CogniMap3D, a bioinspired framework for dynamic 3D scene understanding and reconstruction that emulates human cognitive processes. Our approach maintains a persistent memory bank of static scenes, enabling efficient spatial knowledge storage and rapid retrieval. CogniMap3D integrates three core capabilities: a multi-stage motion cue framework for identifying dynamic objects, a cognitive mapping system for storing, recalling, and updating static scenes across multiple visits, and a factor graph optimization strategy for refining camera poses. Given an image stream, our model identifies dynamic regions through motion cues with depth and camera pose priors, then matches static elements against its memory bank. When revisiting familiar locations, CogniMap3D retrieves stored scenes, relocates cameras, and updates memory with new observations. Evaluations on video depth estimation, camera pose reconstruction, and 3D mapping tasks demonstrate its state-of-the-art performance, while effectively supporting continuous scene understanding across extended sequences and multiple visits.
Published: 2026-01-13T03:09:35+00:00
Venue: arXiv
Score: 0.539 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Feiran Wang; Junyi Wu; Dawen Cai; Yuan Hong; Yan Yan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.539 (consider)&lt;/p&gt;
&lt;p&gt;We present CogniMap3D, a bioinspired framework for dynamic 3D scene understanding and reconstruction that emulates human cognitive processes. Our approach maintains a persistent memory bank of static scenes, enabling efficient spatial knowledge storage and rapid retrieval. CogniMap3D integrates three core capabilities: a multi-stage motion cue framework for identifying dynamic objects, a cognitive mapping system for storing, recalling, and updating static scenes across multiple visits, and a factor graph optimization strategy for refining camera poses. Given an image stream, our model identifies dynamic regions through motion cues with depth and camera pose priors, then matches static elements against its memory bank. When revisiting familiar locations, CogniMap3D retrieves stored scenes, relocates cameras, and updates memory with new observations. Evaluations on video depth estimation, camera pose reconstruction, and 3D mapping tasks demonstrate its state-of-the-art performance, while effectively supporting continuous scene understanding across extended sequences and multiple visits.&lt;/p&gt;</content:encoded></item><item><title>RTPSeg: A multi-modality dataset for LiDAR point cloud semantic segmentation assisted with RGB-thermal images in autonomous driving</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.008</link><guid>10.1016/j.isprsjprs.2026.01.008</guid><pubDate>Fri, 16 Jan 2026 14:12:44 +0000</pubDate><dc:creator>Yifan Sun</dc:creator><dc:creator>Chenguang Dai</dc:creator><dc:creator>Wenke Li</dc:creator><dc:creator>Xinpu Liu</dc:creator><dc:creator>Yongqi Sun</dc:creator><dc:creator>Ye Zhang</dc:creator><dc:creator>Weijun Guan</dc:creator><dc:creator>Yongsheng Zhang</dc:creator><dc:creator>Yulan Guo</dc:creator><dc:creator>Hanyun Wang</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.008</prism:doi><description>LiDAR point cloud semantic segmentation is crucial for scene understanding in autonomous driving, yet the sparse and textureless characteristics of point clouds cause huge challenges for this task. To address this, numerous studies have explored to leverage the dense color and fine-grained texture from RGB images for multi-modality 3D semantic segmentation. Nevertheless, these methods still encounter certain limitations when facing complex scenarios, as RGB images degrade under poor lighting conditions. In contrast, thermal infrared (TIR) images can provide thermal radiation information of road objects and are robust to illumination change, offering complementary advantages to RGB images. Therefore, in this work we introduce RTPSeg, the first and only multi-modality dataset to simultaneously provide RGB and TIR images for point cloud semantic segmentation. RTPSeg includes over 3000 synchronized frames collected by RGB camera, infrared camera, and LiDAR, providing over 248M pointwise annotations for 18 semantic categories in autonomous driving, involving urban and village scenes during both daytime and nighttime. Based on RTPSeg, we also propose RTPSegNet, a baseline model for point cloud semantic segmentation jointly assisted with RGB and TIR images. Extensive experiments demonstrate that the RTPSeg dataset presents considerable challenges and opportunities to existing point cloud semantic segmentation approaches, and our RTPSegNet exhibits promising effectiveness in jointly leveraging the complementary information between point clouds, RGB images, and TIR images. More importantly, the experimental results also confirm that 3D semantic segmentation can be effectively enhanced by introducing additional TIR image modality, revealing the promising potential of this innovative research and application. We anticipate that the RTPSeg will catalyze in-depth research in this field. Both RTPSeg and RTPSegNet will be released at https://github.com/sssssyf/RTPSeg
Published: 2026-01-16T14:12:44+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.539 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yifan Sun; Chenguang Dai; Wenke Li; Xinpu Liu; Yongqi Sun; Ye Zhang; Weijun Guan; Yongsheng Zhang; Yulan Guo; Hanyun Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.008"&gt;10.1016/j.isprsjprs.2026.01.008&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.539 (consider)&lt;/p&gt;
&lt;p&gt;LiDAR point cloud semantic segmentation is crucial for scene understanding in autonomous driving, yet the sparse and textureless characteristics of point clouds cause huge challenges for this task. To address this, numerous studies have explored to leverage the dense color and fine-grained texture from RGB images for multi-modality 3D semantic segmentation. Nevertheless, these methods still encounter certain limitations when facing complex scenarios, as RGB images degrade under poor lighting conditions. In contrast, thermal infrared (TIR) images can provide thermal radiation information of road objects and are robust to illumination change, offering complementary advantages to RGB images. Therefore, in this work we introduce RTPSeg, the first and only multi-modality dataset to simultaneously provide RGB and TIR images for point cloud semantic segmentation. RTPSeg includes over 3000 synchronized frames collected by RGB camera, infrared camera, and LiDAR, providing over 248M pointwise annotations for 18 semantic categories in autonomous driving, involving urban and village scenes during both daytime and nighttime. Based on RTPSeg, we also propose RTPSegNet, a baseline model for point cloud semantic segmentation jointly assisted with RGB and TIR images. Extensive experiments demonstrate that the RTPSeg dataset presents considerable challenges and opportunities to existing point cloud semantic segmentation approaches, and our RTPSegNet exhibits promising effectiveness in jointly leveraging the complementary information between point clouds, RGB images, and TIR images. More importantly, the experimental results also confirm that 3D semantic segmentation can be effectively enhanced by introducing additional TIR image modality, revealing the promising potential of this innovative research and application. We anticipate that the RTPSeg will catalyze in-depth research in this field. Both RTPSeg and RTPSegNet will be released at https://github.com/sssssyf/RTPSeg&lt;/p&gt;</content:encoded></item><item><title>Semantic Misalignment in Vision-Language Models under Perceptual Degradation</title><link>https://arxiv.org/abs/2601.08355v2</link><guid>http://arxiv.org/abs/2601.08355v2</guid><pubDate>Tue, 13 Jan 2026 09:13:05 +0000</pubDate><dc:creator>Guo Cheng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Models (VLMs) are increasingly deployed in autonomous driving and embodied AI systems, where reliable perception is critical for safe semantic reasoning and decision-making. While recent VLMs demonstrate strong performance on multimodal benchmarks, their robustness to realistic perception degradation remains poorly understood. In this work, we systematically study semantic misalignment in VLMs under controlled degradation of upstream visual perception, using semantic segmentation on the Cityscapes dataset as a representative perception module. We introduce perception-realistic corruptions that induce only moderate drops in conventional segmentation metrics, yet observe severe failures in downstream VLM behavior, including hallucinated object mentions, omission of safety-critical entities, and inconsistent safety judgments. To quantify these effects, we propose a set of language-level misalignment metrics that capture hallucination, critical omission, and safety misinterpretation, and analyze their relationship with segmentation quality across multiple contrastive and generative VLMs. Our results reveal a clear disconnect between pixel-level robustness and multimodal semantic reliability, highlighting a critical limitation of current VLM-based systems and motivating the need for evaluation frameworks that explicitly account for perception uncertainty in safety-critical applications.
Published: 2026-01-13T09:13:05+00:00
Venue: arXiv
Score: 0.537 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guo Cheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.537 (consider)&lt;/p&gt;
&lt;p&gt;Vision-Language Models (VLMs) are increasingly deployed in autonomous driving and embodied AI systems, where reliable perception is critical for safe semantic reasoning and decision-making. While recent VLMs demonstrate strong performance on multimodal benchmarks, their robustness to realistic perception degradation remains poorly understood. In this work, we systematically study semantic misalignment in VLMs under controlled degradation of upstream visual perception, using semantic segmentation on the Cityscapes dataset as a representative perception module. We introduce perception-realistic corruptions that induce only moderate drops in conventional segmentation metrics, yet observe severe failures in downstream VLM behavior, including hallucinated object mentions, omission of safety-critical entities, and inconsistent safety judgments. To quantify these effects, we propose a set of language-level misalignment metrics that capture hallucination, critical omission, and safety misinterpretation, and analyze their relationship with segmentation quality across multiple contrastive and generative VLMs. Our results reveal a clear disconnect between pixel-level robustness and multimodal semantic reliability, highlighting a critical limitation of current VLM-based systems and motivating the need for evaluation frameworks that explicitly account for perception uncertainty in safety-critical applications.&lt;/p&gt;</content:encoded></item><item><title>See More, Store Less: Memory-Efficient Resolution for Video Moment Retrieval</title><link>https://arxiv.org/abs/2601.09350v1</link><guid>http://arxiv.org/abs/2601.09350v1</guid><pubDate>Wed, 14 Jan 2026 10:28:11 +0000</pubDate><dc:creator>Mingyu Jeon</dc:creator><dc:creator>Sungjin Han</dc:creator><dc:creator>Jinkwon Hwang</dc:creator><dc:creator>Minchol Kwon</dc:creator><dc:creator>Jonghee Kim</dc:creator><dc:creator>Junyeong Kim</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advances in Multimodal Large Language Models (MLLMs) have improved image recognition and reasoning, but video-related tasks remain challenging due to memory constraints from dense frame processing. Existing Video Moment Retrieval (VMR) methodologies rely on sparse frame sampling, risking potential information loss, especially in lengthy videos. We propose SMORE (See MORE, store less), a framework that enhances memory efficiency while maintaining high information resolution. SMORE (1) uses query-guided captions to encode semantics aligned with user intent, (2) applies query-aware importance modulation to highlight relevant segments, and (3) adaptively compresses frames to preserve key content while reducing redundancy. This enables efficient video understanding without exceeding memory budgets. Experimental validation reveals that SMORE achieves state-of-the-art performance on QVHighlights, Charades-STA, and ActivityNet-Captions benchmarks.
Published: 2026-01-14T10:28:11+00:00
Venue: arXiv
Score: 0.537 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingyu Jeon; Sungjin Han; Jinkwon Hwang; Minchol Kwon; Jonghee Kim; Junyeong Kim&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.537 (consider)&lt;/p&gt;
&lt;p&gt;Recent advances in Multimodal Large Language Models (MLLMs) have improved image recognition and reasoning, but video-related tasks remain challenging due to memory constraints from dense frame processing. Existing Video Moment Retrieval (VMR) methodologies rely on sparse frame sampling, risking potential information loss, especially in lengthy videos. We propose SMORE (See MORE, store less), a framework that enhances memory efficiency while maintaining high information resolution. SMORE (1) uses query-guided captions to encode semantics aligned with user intent, (2) applies query-aware importance modulation to highlight relevant segments, and (3) adaptively compresses frames to preserve key content while reducing redundancy. This enables efficient video understanding without exceeding memory budgets. Experimental validation reveals that SMORE achieves state-of-the-art performance on QVHighlights, Charades-STA, and ActivityNet-Captions benchmarks.&lt;/p&gt;</content:encoded></item><item><title>Gradient-Prior-Guided Dual-Branch Network for Preserving Fine Structures in Remote Sensing Image Super-Resolution</title><link>https://doi.org/10.1109/tgrs.2026.3654769</link><guid>10.1109/tgrs.2026.3654769</guid><pubDate>Thu, 15 Jan 2026 20:49:54 +0000</pubDate><dc:creator>Ruyi Feng</dc:creator><dc:creator>Zhijie Zhang</dc:creator><dc:creator>Lizhe Wang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3654769</prism:doi><description>In the task of remote sensing image super-resolution reconstruction, both CNN and Transformer have demonstrated impressive capabilities. However, many existing dual-branch models only perform feature fusion through simple operations such as concatenation or weighting, which fail to fully exploit the complementary advantages of CNN and Transformer in feature representation. To address this limitation, this paper proposes a novel Gradient-Prior-Guided Dual-Branch Network (GPG-DBN) for remote sensing image super-resolution. Specifically, the model incorporates an enhanced Transformer structure to capture long-range dependencies and a CNN branch to extract local structural details. To further enhance detail recovery, a Gradient-aware Enhancement Module (GEM) is introduced to guide the network using explicit gradient priors, effectively highlighting high-frequency information such as edges and contours. In addition, we design a Feature Fusion Module (FFM) to deeply integrate the global and local features from both branches, achieving more effective information interaction and complementary learning. Extensive experiments conducted on the UC-Merced and WHU-RS19 datasets demonstrate that the proposed GPG-DBN achieves superior performance in both quantitative metrics and visual quality compared to existing state-of-the-art methods, validating its effectiveness and robustness.
Published: 2026-01-15T20:49:54+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.531 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruyi Feng; Zhijie Zhang; Lizhe Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3654769"&gt;10.1109/tgrs.2026.3654769&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.531 (consider)&lt;/p&gt;
&lt;p&gt;In the task of remote sensing image super-resolution reconstruction, both CNN and Transformer have demonstrated impressive capabilities. However, many existing dual-branch models only perform feature fusion through simple operations such as concatenation or weighting, which fail to fully exploit the complementary advantages of CNN and Transformer in feature representation. To address this limitation, this paper proposes a novel Gradient-Prior-Guided Dual-Branch Network (GPG-DBN) for remote sensing image super-resolution. Specifically, the model incorporates an enhanced Transformer structure to capture long-range dependencies and a CNN branch to extract local structural details. To further enhance detail recovery, a Gradient-aware Enhancement Module (GEM) is introduced to guide the network using explicit gradient priors, effectively highlighting high-frequency information such as edges and contours. In addition, we design a Feature Fusion Module (FFM) to deeply integrate the global and local features from both branches, achieving more effective information interaction and complementary learning. Extensive experiments conducted on the UC-Merced and WHU-RS19 datasets demonstrate that the proposed GPG-DBN achieves superior performance in both quantitative metrics and visual quality compared to existing state-of-the-art methods, validating its effectiveness and robustness.&lt;/p&gt;</content:encoded></item><item><title>MMLGNet: Cross-Modal Alignment of Remote Sensing Data using CLIP</title><link>https://arxiv.org/abs/2601.08420v1</link><guid>http://arxiv.org/abs/2601.08420v1</guid><pubDate>Tue, 13 Jan 2026 10:44:37 +0000</pubDate><dc:creator>Aditya Chaudhary</dc:creator><dc:creator>Sneha Barman</dc:creator><dc:creator>Mainak Singha</dc:creator><dc:creator>Ankit Jha</dc:creator><dc:creator>Girish Mishra</dc:creator><dc:creator>Biplab Banerjee</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>In this paper, we propose a novel multimodal framework, Multimodal Language-Guided Network (MMLGNet), to align heterogeneous remote sensing modalities like Hyperspectral Imaging (HSI) and LiDAR with natural language semantics using vision-language models such as CLIP. With the increasing availability of multimodal Earth observation data, there is a growing need for methods that effectively fuse spectral, spatial, and geometric information while enabling semantic-level understanding. MMLGNet employs modality-specific encoders and aligns visual features with handcrafted textual embeddings in a shared latent space via bi-directional contrastive learning. Inspired by CLIP's training paradigm, our approach bridges the gap between high-dimensional remote sensing data and language-guided interpretation. Notably, MMLGNet achieves strong performance with simple CNN-based encoders, outperforming several established multimodal visual-only methods on two benchmark datasets, demonstrating the significant benefit of language supervision. Codes are available at https://github.com/AdityaChaudhary2913/CLIP_HSI.
Published: 2026-01-13T10:44:37+00:00
Venue: arXiv
Score: 0.531 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Aditya Chaudhary; Sneha Barman; Mainak Singha; Ankit Jha; Girish Mishra; Biplab Banerjee&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.531 (consider)&lt;/p&gt;
&lt;p&gt;In this paper, we propose a novel multimodal framework, Multimodal Language-Guided Network (MMLGNet), to align heterogeneous remote sensing modalities like Hyperspectral Imaging (HSI) and LiDAR with natural language semantics using vision-language models such as CLIP. With the increasing availability of multimodal Earth observation data, there is a growing need for methods that effectively fuse spectral, spatial, and geometric information while enabling semantic-level understanding. MMLGNet employs modality-specific encoders and aligns visual features with handcrafted textual embeddings in a shared latent space via bi-directional contrastive learning. Inspired by CLIP&amp;#x27;s training paradigm, our approach bridges the gap between high-dimensional remote sensing data and language-guided interpretation. Notably, MMLGNet achieves strong performance with simple CNN-based encoders, outperforming several established multimodal visual-only methods on two benchmark datasets, demonstrating the significant benefit of language supervision. Codes are available at https://github.com/AdityaChaudhary2913/CLIP_HSI.&lt;/p&gt;</content:encoded></item><item><title>V-Zero: Self-Improving Multimodal Reasoning with Zero Annotation</title><link>https://arxiv.org/abs/2601.10094v1</link><guid>http://arxiv.org/abs/2601.10094v1</guid><pubDate>Thu, 15 Jan 2026 05:47:43 +0000</pubDate><dc:creator>Han Wang</dc:creator><dc:creator>Yi Yang</dc:creator><dc:creator>Jingyuan Hu</dc:creator><dc:creator>Minfeng Zhu</dc:creator><dc:creator>Wei Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advances in multimodal learning have significantly enhanced the reasoning capabilities of vision-language models (VLMs). However, state-of-the-art approaches rely heavily on large-scale human-annotated datasets, which are costly and time-consuming to acquire. To overcome this limitation, we introduce V-Zero, a general post-training framework that facilitates self-improvement using exclusively unlabeled images. V-Zero establishes a co-evolutionary loop by instantiating two distinct roles: a Questioner and a Solver. The Questioner learns to synthesize high-quality, challenging questions by leveraging a dual-track reasoning reward that contrasts intuitive guesses with reasoned results. The Solver is optimized using pseudo-labels derived from majority voting over its own sampled responses. Both roles are trained iteratively via Group Relative Policy Optimization (GRPO), driving a cycle of mutual enhancement. Remarkably, without a single human annotation, V-Zero achieves consistent performance gains on Qwen2.5-VL-7B-Instruct, improving visual mathematical reasoning by +1.7 and general vision-centric by +2.6, demonstrating the potential of self-improvement in multimodal systems. Code is available at https://github.com/SatonoDia/V-Zero
Published: 2026-01-15T05:47:43+00:00
Venue: arXiv
Score: 0.531 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Han Wang; Yi Yang; Jingyuan Hu; Minfeng Zhu; Wei Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.531 (consider)&lt;/p&gt;
&lt;p&gt;Recent advances in multimodal learning have significantly enhanced the reasoning capabilities of vision-language models (VLMs). However, state-of-the-art approaches rely heavily on large-scale human-annotated datasets, which are costly and time-consuming to acquire. To overcome this limitation, we introduce V-Zero, a general post-training framework that facilitates self-improvement using exclusively unlabeled images. V-Zero establishes a co-evolutionary loop by instantiating two distinct roles: a Questioner and a Solver. The Questioner learns to synthesize high-quality, challenging questions by leveraging a dual-track reasoning reward that contrasts intuitive guesses with reasoned results. The Solver is optimized using pseudo-labels derived from majority voting over its own sampled responses. Both roles are trained iteratively via Group Relative Policy Optimization (GRPO), driving a cycle of mutual enhancement. Remarkably, without a single human annotation, V-Zero achieves consistent performance gains on Qwen2.5-VL-7B-Instruct, improving visual mathematical reasoning by +1.7 and general vision-centric by +2.6, demonstrating the potential of self-improvement in multimodal systems. Code is available at https://github.com/SatonoDia/V-Zero&lt;/p&gt;</content:encoded></item><item><title>OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding</title><link>https://arxiv.org/abs/2601.09575v1</link><guid>http://arxiv.org/abs/2601.09575v1</guid><pubDate>Wed, 14 Jan 2026 15:45:57 +0000</pubDate><dc:creator>Sheng-Yu Huang</dc:creator><dc:creator>Jaesung Choe</dc:creator><dc:creator>Yu-Chiang Frank Wang</dc:creator><dc:creator>Cheng Sun</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We propose OpenVoxel, a training-free algorithm for grouping and captioning sparse voxels for the open-vocabulary 3D scene understanding tasks. Given the sparse voxel rasterization (SVR) model obtained from multi-view images of a 3D scene, our OpenVoxel is able to produce meaningful groups that describe different objects in the scene. Also, by leveraging powerful Vision Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), our OpenVoxel successfully build an informative scene map by captioning each group, enabling further 3D scene understanding tasks such as open-vocabulary segmentation (OVS) or referring expression segmentation (RES). Unlike previous methods, our method is training-free and does not introduce embeddings from a CLIP/BERT text encoder. Instead, we directly proceed with text-to-text search using MLLMs. Through extensive experiments, our method demonstrates superior performance compared to recent studies, particularly in complex referring expression segmentation (RES) tasks. The code will be open.
Published: 2026-01-14T15:45:57+00:00
Venue: arXiv
Score: 0.530 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sheng-Yu Huang; Jaesung Choe; Yu-Chiang Frank Wang; Cheng Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.530 (consider)&lt;/p&gt;
&lt;p&gt;We propose OpenVoxel, a training-free algorithm for grouping and captioning sparse voxels for the open-vocabulary 3D scene understanding tasks. Given the sparse voxel rasterization (SVR) model obtained from multi-view images of a 3D scene, our OpenVoxel is able to produce meaningful groups that describe different objects in the scene. Also, by leveraging powerful Vision Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), our OpenVoxel successfully build an informative scene map by captioning each group, enabling further 3D scene understanding tasks such as open-vocabulary segmentation (OVS) or referring expression segmentation (RES). Unlike previous methods, our method is training-free and does not introduce embeddings from a CLIP/BERT text encoder. Instead, we directly proceed with text-to-text search using MLLMs. Through extensive experiments, our method demonstrates superior performance compared to recent studies, particularly in complex referring expression segmentation (RES) tasks. The code will be open.&lt;/p&gt;</content:encoded></item><item><title>Gain from Give Up: Intuitive Data Augmentation Framework for Image Retrieval</title><link>https://doi.org/10.1109/tmm.2026.3654462</link><guid>10.1109/tmm.2026.3654462</guid><pubDate>Thu, 15 Jan 2026 20:50:42 +0000</pubDate><dc:creator>Yi Lu</dc:creator><dc:creator>Yurong Qian</dc:creator><dc:creator>Shu Li</dc:creator><dc:creator>Jiaying Chen</dc:creator><dc:creator>Guangqi Yang</dc:creator><dc:creator>Yuning Huang</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3654462</prism:doi><description>Modern deep hashing methods rely on data augmentation to fully realize their potential in the face of large-scale retrieval galleries and over-parameterized visual models. However, this work observes that mainstream label-preserving augmentation methods are unreliable for information retrieval because they lead to incomplete alignment between data and labels. This misalignment impairs metric losses in distinguishing original/augmented data during same-class clustering, compromising nearest-neighbor search efficacy. To address these issues, we propose an innovative plug-and-play data augmentation framework tailored for retrieval tasks, based on the concept of Gaining robust features by randomly Giving up parts of the image (GG). Inspired by the ease with which visual changes induced by discard transformations can be estimated, we design two intuitive augmentation methods along with corresponding semantic shift estimators to measure the semantic changes introduced by each operation. Additionally, we optimize the metric loss based on the semantic retention scores, guiding the metric objective to properly allocate gradients for generated samples. This adjustment mitigates the adverse effects caused by incomplete alignment, optimizing the intra-class distance of both original and augmented data in the Hamming space, while ensuring the relevance and accuracy of the retrieval results. Extensive experiments conducted on six datasets demonstrate the effectiveness and robustness of our proposed framework. Code is available at https://github.com/wuhulahu/GG.
Published: 2026-01-15T20:50:42+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.525 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yi Lu; Yurong Qian; Shu Li; Jiaying Chen; Guangqi Yang; Yuning Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3654462"&gt;10.1109/tmm.2026.3654462&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.525 (consider)&lt;/p&gt;
&lt;p&gt;Modern deep hashing methods rely on data augmentation to fully realize their potential in the face of large-scale retrieval galleries and over-parameterized visual models. However, this work observes that mainstream label-preserving augmentation methods are unreliable for information retrieval because they lead to incomplete alignment between data and labels. This misalignment impairs metric losses in distinguishing original/augmented data during same-class clustering, compromising nearest-neighbor search efficacy. To address these issues, we propose an innovative plug-and-play data augmentation framework tailored for retrieval tasks, based on the concept of Gaining robust features by randomly Giving up parts of the image (GG). Inspired by the ease with which visual changes induced by discard transformations can be estimated, we design two intuitive augmentation methods along with corresponding semantic shift estimators to measure the semantic changes introduced by each operation. Additionally, we optimize the metric loss based on the semantic retention scores, guiding the metric objective to properly allocate gradients for generated samples. This adjustment mitigates the adverse effects caused by incomplete alignment, optimizing the intra-class distance of both original and augmented data in the Hamming space, while ensuring the relevance and accuracy of the retrieval results. Extensive experiments conducted on six datasets demonstrate the effectiveness and robustness of our proposed framework. Code is available at https://github.com/wuhulahu/GG.&lt;/p&gt;</content:encoded></item><item><title>Video-MSR: Benchmarking Multi-hop Spatial Reasoning Capabilities of MLLMs</title><link>https://arxiv.org/abs/2601.09430v1</link><guid>http://arxiv.org/abs/2601.09430v1</guid><pubDate>Wed, 14 Jan 2026 12:24:47 +0000</pubDate><dc:creator>Rui Zhu</dc:creator><dc:creator>Xin Shen</dc:creator><dc:creator>Shuchen Wu</dc:creator><dc:creator>Chenxi Miao</dc:creator><dc:creator>Xin Yu</dc:creator><dc:creator>Yang Li</dc:creator><dc:creator>Weikang Li</dc:creator><dc:creator>Deguo Xia</dc:creator><dc:creator>Jizhou Huang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Spatial reasoning has emerged as a critical capability for Multimodal Large Language Models (MLLMs), drawing increasing attention and rapid advancement. However, existing benchmarks primarily focus on single-step perception-to-judgment tasks, leaving scenarios requiring complex visual-spatial logical chains significantly underexplored. To bridge this gap, we introduce Video-MSR, the first benchmark specifically designed to evaluate Multi-hop Spatial Reasoning (MSR) in dynamic video scenarios. Video-MSR systematically probes MSR capabilities through four distinct tasks: Constrained Localization, Chain-based Reference Retrieval, Route Planning, and Counterfactual Physical Deduction. Our benchmark comprises 3,052 high-quality video instances with 4,993 question-answer pairs, constructed via a scalable, visually-grounded pipeline combining advanced model generation with rigorous human verification. Through a comprehensive evaluation of 20 state-of-the-art MLLMs, we uncover significant limitations, revealing that while models demonstrate proficiency in surface-level perception, they exhibit distinct performance drops in MSR tasks, frequently suffering from spatial disorientation and hallucination during multi-step deductions. To mitigate these shortcomings and empower models with stronger MSR capabilities, we further curate MSR-9K, a specialized instruction-tuning dataset, and fine-tune Qwen-VL, achieving a +7.82% absolute improvement on Video-MSR. Our results underscore the efficacy of multi-hop spatial instruction data and establish Video-MSR as a vital foundation for future research. The code and data will be available at https://github.com/ruiz-nju/Video-MSR.
Published: 2026-01-14T12:24:47+00:00
Venue: arXiv
Score: 0.523 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Rui Zhu; Xin Shen; Shuchen Wu; Chenxi Miao; Xin Yu; Yang Li; Weikang Li; Deguo Xia; Jizhou Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.523 (consider)&lt;/p&gt;
&lt;p&gt;Spatial reasoning has emerged as a critical capability for Multimodal Large Language Models (MLLMs), drawing increasing attention and rapid advancement. However, existing benchmarks primarily focus on single-step perception-to-judgment tasks, leaving scenarios requiring complex visual-spatial logical chains significantly underexplored. To bridge this gap, we introduce Video-MSR, the first benchmark specifically designed to evaluate Multi-hop Spatial Reasoning (MSR) in dynamic video scenarios. Video-MSR systematically probes MSR capabilities through four distinct tasks: Constrained Localization, Chain-based Reference Retrieval, Route Planning, and Counterfactual Physical Deduction. Our benchmark comprises 3,052 high-quality video instances with 4,993 question-answer pairs, constructed via a scalable, visually-grounded pipeline combining advanced model generation with rigorous human verification. Through a comprehensive evaluation of 20 state-of-the-art MLLMs, we uncover significant limitations, revealing that while models demonstrate proficiency in surface-level perception, they exhibit distinct performance drops in MSR tasks, frequently suffering from spatial disorientation and hallucination during multi-step deductions. To mitigate these shortcomings and empower models with stronger MSR capabilities, we further curate MSR-9K, a specialized instruction-tuning dataset, and fine-tune Qwen-VL, achieving a +7.82% absolute improvement on Video-MSR. Our results underscore the efficacy of multi-hop spatial instruction data and establish Video-MSR as a vital foundation for future research. The code and data will be available at https://github.com/ruiz-nju/Video-MSR.&lt;/p&gt;</content:encoded></item><item><title>Hybrid guided variational autoencoder for visual place recognition</title><link>https://arxiv.org/abs/2601.09248v1</link><guid>http://arxiv.org/abs/2601.09248v1</guid><pubDate>Wed, 14 Jan 2026 07:33:53 +0000</pubDate><dc:creator>Ni Wang</dc:creator><dc:creator>Zihan You</dc:creator><dc:creator>Emre Neftci</dc:creator><dc:creator>Thorben Schoepe</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Autonomous agents such as cars, robots and drones need to precisely localize themselves in diverse environments, including in GPS-denied indoor environments. One approach for precise localization is visual place recognition (VPR), which estimates the place of an image based on previously seen places. State-of-the-art VPR models require high amounts of memory, making them unwieldy for mobile deployment, while more compact models lack robustness and generalization capabilities. This work overcomes these limitations for robotics using a combination of event-based vision sensors and an event-based novel guided variational autoencoder (VAE). The encoder part of our model is based on a spiking neural network model which is compatible with power-efficient low latency neuromorphic hardware. The VAE successfully disentangles the visual features of 16 distinct places in our new indoor VPR dataset with a classification performance comparable to other state-of-the-art approaches while, showing robust performance also under various illumination conditions. When tested with novel visual inputs from unknown scenes, our model can distinguish between these places, which demonstrates a high generalization capability by learning the essential features of location. Our compact and robust guided VAE with generalization capabilities poses a promising model for visual place recognition that can significantly enhance mobile robot navigation in known and unknown indoor environments.
Published: 2026-01-14T07:33:53+00:00
Venue: arXiv
Score: 0.521 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ni Wang; Zihan You; Emre Neftci; Thorben Schoepe&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.521 (consider)&lt;/p&gt;
&lt;p&gt;Autonomous agents such as cars, robots and drones need to precisely localize themselves in diverse environments, including in GPS-denied indoor environments. One approach for precise localization is visual place recognition (VPR), which estimates the place of an image based on previously seen places. State-of-the-art VPR models require high amounts of memory, making them unwieldy for mobile deployment, while more compact models lack robustness and generalization capabilities. This work overcomes these limitations for robotics using a combination of event-based vision sensors and an event-based novel guided variational autoencoder (VAE). The encoder part of our model is based on a spiking neural network model which is compatible with power-efficient low latency neuromorphic hardware. The VAE successfully disentangles the visual features of 16 distinct places in our new indoor VPR dataset with a classification performance comparable to other state-of-the-art approaches while, showing robust performance also under various illumination conditions. When tested with novel visual inputs from unknown scenes, our model can distinguish between these places, which demonstrates a high generalization capability by learning the essential features of location. Our compact and robust guided VAE with generalization capabilities poses a promising model for visual place recognition that can significantly enhance mobile robot navigation in known and unknown indoor environments.&lt;/p&gt;</content:encoded></item><item><title>How Do Optical Flow and Textual Prompts Collaborate to Assist in Audio-Visual Semantic Segmentation?</title><link>https://arxiv.org/abs/2601.08133v1</link><guid>http://arxiv.org/abs/2601.08133v1</guid><pubDate>Tue, 13 Jan 2026 01:53:20 +0000</pubDate><dc:creator>Peng Gao</dc:creator><dc:creator>Yujian Lee</dc:creator><dc:creator>Yongqi Xu</dc:creator><dc:creator>Wentao Fan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Audio-visual semantic segmentation (AVSS) represents an extension of the audio-visual segmentation (AVS) task, necessitating a semantic understanding of audio-visual scenes beyond merely identifying sound-emitting objects at the visual pixel level. Contrary to a previous methodology, by decomposing the AVSS task into two discrete subtasks by initially providing a prompted segmentation mask to facilitate subsequent semantic analysis, our approach innovates on this foundational strategy. We introduce a novel collaborative framework, \textit{S}tepping \textit{S}tone \textit{P}lus (SSP), which integrates optical flow and textual prompts to assist the segmentation process. In scenarios where sound sources frequently coexist with moving objects, our pre-mask technique leverages optical flow to capture motion dynamics, providing essential temporal context for precise segmentation. To address the challenge posed by stationary sound-emitting objects, such as alarm clocks, SSP incorporates two specific textual prompts: one identifies the category of the sound-emitting object, and the other provides a broader description of the scene. Additionally, we implement a visual-textual alignment module (VTA) to facilitate cross-modal integration, delivering more coherent and contextually relevant semantic interpretations. Our training regimen involves a post-mask technique aimed at compelling the model to learn the diagram of the optical flow. Experimental results demonstrate that SSP outperforms existing AVS methods, delivering efficient and precise segmentation results.
Published: 2026-01-13T01:53:20+00:00
Venue: arXiv
Score: 0.521 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Peng Gao; Yujian Lee; Yongqi Xu; Wentao Fan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.521 (consider)&lt;/p&gt;
&lt;p&gt;Audio-visual semantic segmentation (AVSS) represents an extension of the audio-visual segmentation (AVS) task, necessitating a semantic understanding of audio-visual scenes beyond merely identifying sound-emitting objects at the visual pixel level. Contrary to a previous methodology, by decomposing the AVSS task into two discrete subtasks by initially providing a prompted segmentation mask to facilitate subsequent semantic analysis, our approach innovates on this foundational strategy. We introduce a novel collaborative framework, \textit{S}tepping \textit{S}tone \textit{P}lus (SSP), which integrates optical flow and textual prompts to assist the segmentation process. In scenarios where sound sources frequently coexist with moving objects, our pre-mask technique leverages optical flow to capture motion dynamics, providing essential temporal context for precise segmentation. To address the challenge posed by stationary sound-emitting objects, such as alarm clocks, SSP incorporates two specific textual prompts: one identifies the category of the sound-emitting object, and the other provides a broader description of the scene. Additionally, we implement a visual-textual alignment module (VTA) to facilitate cross-modal integration, delivering more coherent and contextually relevant semantic interpretations. Our training regimen involves a post-mask technique aimed at compelling the model to learn the diagram of the optical flow. Experimental results demonstrate that SSP outperforms existing AVS methods, delivering efficient and precise segmentation results.&lt;/p&gt;</content:encoded></item></channel></rss>