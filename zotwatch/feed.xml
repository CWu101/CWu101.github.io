<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Wed, 28 Jan 2026 02:52:43 +0000</lastBuildDate><item><title>TextMonkey: an OCR-Free Large Multimodal Model for Understanding Document</title><link>https://doi.org/10.1109/tpami.2026.3653415</link><guid>10.1109/tpami.2026.3653415</guid><pubDate>Tue, 27 Jan 2026 05:48:07 +0000</pubDate><dc:creator>Yuliang Liu</dc:creator><dc:creator>Biao Yang</dc:creator><dc:creator>Qiang Liu</dc:creator><dc:creator>Zhang Li</dc:creator><dc:creator>Zhiyin Ma</dc:creator><dc:creator>Shuo Zhang</dc:creator><dc:creator>Xiang Bai</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3653415</prism:doi><description>We present TextMonkey, a large multimodal model (LMM) tailored for text-centric tasks. Our approach introduces enhancement across several dimensions: By adopting Shifted Window Attention layer, we achieve cross-window connectivity at higher input resolutions and stabilize early training; We hypothesize that images may contain redundant tokens, and by using similarity to filter out significant tokens, we can not only streamline the token length but also enhance the model's performance. Moreover, by expanding our model's capabilities to encompass text spotting and grounding, and incorporating positional information into responses, we enhance interpretability. Evaluation on 12 benchmarks shows notable improvements: 5.2% in Scene Text-Centric tasks (including STVQA, TextVQA, and OCRVQA), 6.9% in Document-Oriented tasks (such as DocVQA, InfoVQA, ChartVQA, DeepForm, Kleister Charity, and WikiTableQuestions), and 2.8% in Key Information Extraction tasks (comprising FUNSD, SROIE, and POIE). It outperforms in scene text spotting with a 10.9% increase and sets a new standard on OCRBench, a comprehensive benchmark consisting of 29 OCR-related assessments, with a score of 561, surpassing previous open-sourced large multimodal models for document understanding. Code is released at https://github.com/Yuliang-Liu/Monkey.
Published: 2026-01-27T05:48:07+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.573 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuliang Liu; Biao Yang; Qiang Liu; Zhang Li; Zhiyin Ma; Shuo Zhang; Xiang Bai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3653415"&gt;10.1109/tpami.2026.3653415&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.573 (consider)&lt;/p&gt;
&lt;p&gt;We present TextMonkey, a large multimodal model (LMM) tailored for text-centric tasks. Our approach introduces enhancement across several dimensions: By adopting Shifted Window Attention layer, we achieve cross-window connectivity at higher input resolutions and stabilize early training; We hypothesize that images may contain redundant tokens, and by using similarity to filter out significant tokens, we can not only streamline the token length but also enhance the model&amp;#x27;s performance. Moreover, by expanding our model&amp;#x27;s capabilities to encompass text spotting and grounding, and incorporating positional information into responses, we enhance interpretability. Evaluation on 12 benchmarks shows notable improvements: 5.2% in Scene Text-Centric tasks (including STVQA, TextVQA, and OCRVQA), 6.9% in Document-Oriented tasks (such as DocVQA, InfoVQA, ChartVQA, DeepForm, Kleister Charity, and WikiTableQuestions), and 2.8% in Key Information Extraction tasks (comprising FUNSD, SROIE, and POIE). It outperforms in scene text spotting with a 10.9% increase and sets a new standard on OCRBench, a comprehensive benchmark consisting of 29 OCR-related assessments, with a score of 561, surpassing previous open-sourced large multimodal models for document understanding. Code is released at https://github.com/Yuliang-Liu/Monkey.&lt;/p&gt;</content:encoded></item><item><title>Pixel-Grounded Retrieval for Knowledgeable Large Multimodal Models</title><link>https://arxiv.org/abs/2601.19060v1</link><guid>http://arxiv.org/abs/2601.19060v1</guid><pubDate>Tue, 27 Jan 2026 00:46:08 +0000</pubDate><dc:creator>Jeonghwan Kim</dc:creator><dc:creator>Renjie Tao</dc:creator><dc:creator>Sanat Sharma</dc:creator><dc:creator>Jiaqi Wang</dc:creator><dc:creator>Kai Sun</dc:creator><dc:creator>Zhaojiang Lin</dc:creator><dc:creator>Seungwhan Moon</dc:creator><dc:creator>Lambert Mathias</dc:creator><dc:creator>Anuj Kumar</dc:creator><dc:creator>Heng Ji</dc:creator><dc:creator>Xin Luna Dong</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual Question Answering (VQA) often requires coupling fine-grained perception with factual knowledge beyond the input image. Prior multimodal Retrieval-Augmented Generation (MM-RAG) systems improve factual grounding but lack an internal policy for when and how to retrieve. We propose PixSearch, the first end-to-end Segmenting Large Multimodal Model (LMM) that unifies region-level perception and retrieval-augmented reasoning. During encoding, PixSearch emits &lt;search&gt; tokens to trigger retrieval, selects query modalities (text, image, or region), and generates pixel-level masks that directly serve as visual queries, eliminating the reliance on modular pipelines (detectors, segmenters, captioners, etc.). A two-stage supervised fine-tuning regimen with search-interleaved supervision teaches retrieval timing and query selection while preserving segmentation ability. On egocentric and entity-centric VQA benchmarks, PixSearch substantially improves factual consistency and generalization, yielding a 19.7% relative gain in accuracy on CRAG-MM compared to whole image retrieval, while retaining competitive reasoning performance on various VQA and text-only QA tasks.
Published: 2026-01-27T00:46:08+00:00
Venue: arXiv
Score: 0.570 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jeonghwan Kim; Renjie Tao; Sanat Sharma; Jiaqi Wang; Kai Sun; Zhaojiang Lin; Seungwhan Moon; Lambert Mathias; Anuj Kumar; Heng Ji; Xin Luna Dong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.570 (consider)&lt;/p&gt;
&lt;p&gt;Visual Question Answering (VQA) often requires coupling fine-grained perception with factual knowledge beyond the input image. Prior multimodal Retrieval-Augmented Generation (MM-RAG) systems improve factual grounding but lack an internal policy for when and how to retrieve. We propose PixSearch, the first end-to-end Segmenting Large Multimodal Model (LMM) that unifies region-level perception and retrieval-augmented reasoning. During encoding, PixSearch emits &amp;lt;search&amp;gt; tokens to trigger retrieval, selects query modalities (text, image, or region), and generates pixel-level masks that directly serve as visual queries, eliminating the reliance on modular pipelines (detectors, segmenters, captioners, etc.). A two-stage supervised fine-tuning regimen with search-interleaved supervision teaches retrieval timing and query selection while preserving segmentation ability. On egocentric and entity-centric VQA benchmarks, PixSearch substantially improves factual consistency and generalization, yielding a 19.7% relative gain in accuracy on CRAG-MM compared to whole image retrieval, while retaining competitive reasoning performance on various VQA and text-only QA tasks.&lt;/p&gt;</content:encoded></item><item><title>QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding</title><link>https://arxiv.org/abs/2601.18195v1</link><guid>http://arxiv.org/abs/2601.18195v1</guid><pubDate>Mon, 26 Jan 2026 06:27:03 +0000</pubDate><dc:creator>Linhan Cao</dc:creator><dc:creator>Wei Sun</dc:creator><dc:creator>Weixia Zhang</dc:creator><dc:creator>Xiangyang Zhu</dc:creator><dc:creator>Kaiwei Zhang</dc:creator><dc:creator>Jun Jia</dc:creator><dc:creator>Dandan Zhu</dc:creator><dc:creator>Guangtao Zhai</dc:creator><dc:creator>Xiongkuo Min</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \textit{fine-grained spatiotemporal perception} and \textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \textbf{QualiRAG}, a \textit{training-free} \textbf{R}etrieval-\textbf{A}ugmented \textbf{G}eneration \textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \textit{visual metadata}, \textit{subject localization}, \textit{global quality summaries}, and \textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at https://github.com/clh124/QualiRAG.
Published: 2026-01-26T06:27:03+00:00
Venue: arXiv
Score: 0.568 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Linhan Cao; Wei Sun; Weixia Zhang; Xiangyang Zhu; Kaiwei Zhang; Jun Jia; Dandan Zhu; Guangtao Zhai; Xiongkuo Min&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.568 (consider)&lt;/p&gt;
&lt;p&gt;Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \textit{fine-grained spatiotemporal perception} and \textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \textbf{QualiRAG}, a \textit{training-free} \textbf{R}etrieval-\textbf{A}ugmented \textbf{G}eneration \textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \textit{visual metadata}, \textit{subject localization}, \textit{global quality summaries}, and \textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at https://github.com/clh124/QualiRAG.&lt;/p&gt;</content:encoded></item><item><title>CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction</title><link>https://arxiv.org/abs/2601.17420v1</link><guid>http://arxiv.org/abs/2601.17420v1</guid><pubDate>Sat, 24 Jan 2026 11:41:54 +0000</pubDate><dc:creator>Shiu-hong Kao</dc:creator><dc:creator>Chak Ho Huang</dc:creator><dc:creator>Huaiqian Liu</dc:creator><dc:creator>Yu-Wing Tai</dc:creator><dc:creator>Chi-Keung Tang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg's ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation.
Published: 2026-01-24T11:41:54+00:00
Venue: arXiv
Score: 0.559 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shiu-hong Kao; Chak Ho Huang; Huaiqian Liu; Yu-Wing Tai; Chi-Keung Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.559 (consider)&lt;/p&gt;
&lt;p&gt;Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg&amp;#x27;s ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation.&lt;/p&gt;</content:encoded></item><item><title>Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing</title><link>https://arxiv.org/abs/2601.17673v1</link><guid>http://arxiv.org/abs/2601.17673v1</guid><pubDate>Sun, 25 Jan 2026 03:22:26 +0000</pubDate><dc:creator>Weiyu Zhang</dc:creator><dc:creator>Yuan Hu</dc:creator><dc:creator>Yong Li</dc:creator><dc:creator>Yu Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks.
Published: 2026-01-25T03:22:26+00:00
Venue: arXiv
Score: 0.556 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weiyu Zhang; Yuan Hu; Yong Li; Yu Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.556 (consider)&lt;/p&gt;
&lt;p&gt;Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks.&lt;/p&gt;</content:encoded></item><item><title>Multi-Perspective Subimage CLIP with Keyword Guidance for Remote Sensing Image-Text Retrieval</title><link>https://arxiv.org/abs/2601.18190v1</link><guid>http://arxiv.org/abs/2601.18190v1</guid><pubDate>Mon, 26 Jan 2026 06:16:53 +0000</pubDate><dc:creator>Yifan Li</dc:creator><dc:creator>Shiying Wang</dc:creator><dc:creator>Jianqiang Huang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Pre-training (VLP) models like CLIP have significantly advanced Remote Sensing Image-Text Retrieval (RSITR). However, existing methods predominantly rely on coarse-grained global alignment, which often overlooks the dense, multi-scale semantics inherent in overhead imagery. Moreover, adapting these heavy models via full fine-tuning incurs prohibitive computational costs and risks catastrophic forgetting. To address these challenges, we propose MPS-CLIP, a parameter-efficient framework designed to shift the retrieval paradigm from global matching to keyword-guided fine-grained alignment. Specifically, we leverage a Large Language Model (LLM) to extract core semantic keywords, guiding the Segment Anything Model (SamGeo) to generate semantically relevant sub-perspectives. To efficiently adapt the frozen backbone, we introduce a Gated Global Attention (G^2A) adapter, which captures global context and long-range dependencies with minimal overhead. Furthermore, a Multi-Perspective Representation (MPR) module aggregates these local cues into robust multi-perspective embeddings. The framework is optimized via a hybrid objective combining multi-perspective contrastive and weighted triplet losses, which dynamically selects maximum-response perspectives to suppress noise and enforce precise semantic matching. Extensive experiments on the RSICD and RSITMD benchmarks demonstrate that MPS-CLIP achieves state-of-the-art performance with 35.18% and 48.40% mean Recall (mR), respectively, significantly outperforming full fine-tuning baselines and recent competitive methods. Code is available at https://github.com/Lcrucial1f/MPS-CLIP.
Published: 2026-01-26T06:16:53+00:00
Venue: arXiv
Score: 0.552 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yifan Li; Shiying Wang; Jianqiang Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.552 (consider)&lt;/p&gt;
&lt;p&gt;Vision-Language Pre-training (VLP) models like CLIP have significantly advanced Remote Sensing Image-Text Retrieval (RSITR). However, existing methods predominantly rely on coarse-grained global alignment, which often overlooks the dense, multi-scale semantics inherent in overhead imagery. Moreover, adapting these heavy models via full fine-tuning incurs prohibitive computational costs and risks catastrophic forgetting. To address these challenges, we propose MPS-CLIP, a parameter-efficient framework designed to shift the retrieval paradigm from global matching to keyword-guided fine-grained alignment. Specifically, we leverage a Large Language Model (LLM) to extract core semantic keywords, guiding the Segment Anything Model (SamGeo) to generate semantically relevant sub-perspectives. To efficiently adapt the frozen backbone, we introduce a Gated Global Attention (G^2A) adapter, which captures global context and long-range dependencies with minimal overhead. Furthermore, a Multi-Perspective Representation (MPR) module aggregates these local cues into robust multi-perspective embeddings. The framework is optimized via a hybrid objective combining multi-perspective contrastive and weighted triplet losses, which dynamically selects maximum-response perspectives to suppress noise and enforce precise semantic matching. Extensive experiments on the RSICD and RSITMD benchmarks demonstrate that MPS-CLIP achieves state-of-the-art performance with 35.18% and 48.40% mean Recall (mR), respectively, significantly outperforming full fine-tuning baselines and recent competitive methods. Code is available at https://github.com/Lcrucial1f/MPS-CLIP.&lt;/p&gt;</content:encoded></item><item><title>Spatial-Conditioned Reasoning in Long-Egocentric Videos</title><link>https://arxiv.org/abs/2601.18100v1</link><guid>http://arxiv.org/abs/2601.18100v1</guid><pubDate>Mon, 26 Jan 2026 03:21:35 +0000</pubDate><dc:creator>James Tribble</dc:creator><dc:creator>Hao Wang</dc:creator><dc:creator>Si-En Hong</dc:creator><dc:creator>Chaoyi Zhou</dc:creator><dc:creator>Ashish Bastola</dc:creator><dc:creator>Siyu Huang</dc:creator><dc:creator>Abolfazl Razi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.
Published: 2026-01-26T03:21:35+00:00
Venue: arXiv
Score: 0.547 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; James Tribble; Hao Wang; Si-En Hong; Chaoyi Zhou; Ashish Bastola; Siyu Huang; Abolfazl Razi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.547 (consider)&lt;/p&gt;
&lt;p&gt;Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.&lt;/p&gt;</content:encoded></item><item><title>SpatialMath: Spatial Comprehension-Infused Symbolic Reasoning for Mathematical Problem-Solving</title><link>https://arxiv.org/abs/2601.17489v1</link><guid>http://arxiv.org/abs/2601.17489v1</guid><pubDate>Sat, 24 Jan 2026 15:31:20 +0000</pubDate><dc:creator>Ashutosh Bajpai</dc:creator><dc:creator>Akshat Bhandari</dc:creator><dc:creator>Akshay Nambi</dc:creator><dc:creator>Tanmoy Chakraborty</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal Small-to-Medium sized Language Models (MSLMs) have demonstrated strong capabilities in integrating visual and textual information but still face significant limitations in visual comprehension and mathematical reasoning, particularly in geometric problems with diverse levels of visual infusion. Current models struggle to accurately decompose intricate visual inputs and connect perception with structured reasoning, leading to suboptimal performance. To address these challenges, we propose SpatialMath, a novel Spatial Comprehension-Infused Symbolic Reasoning Framework designed to integrate spatial representations into structured symbolic reasoning chains. SpatialMath employs a specialized perception module to extract spatially-grounded representations from visual diagrams, capturing critical geometric structures and spatial relationships. These representations are then methodically infused into symbolic reasoning chains, facilitating visual comprehension-aware structured reasoning. To this end, we introduce MATHVERSE-PLUS, a novel dataset containing structured visual interpretations and step-by-step reasoning paths for vision-intensive mathematical problems. SpatialMath significantly outperforms strong multimodal baselines, achieving up to 10 percentage points improvement over supervised fine-tuning with data augmentation in vision-intensive settings. Robustness analysis reveals that enhanced spatial representations directly improve reasoning accuracy, reinforcing the need for structured perception-to-reasoning pipelines in MSLMs.
Published: 2026-01-24T15:31:20+00:00
Venue: arXiv
Score: 0.546 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ashutosh Bajpai; Akshat Bhandari; Akshay Nambi; Tanmoy Chakraborty&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.546 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal Small-to-Medium sized Language Models (MSLMs) have demonstrated strong capabilities in integrating visual and textual information but still face significant limitations in visual comprehension and mathematical reasoning, particularly in geometric problems with diverse levels of visual infusion. Current models struggle to accurately decompose intricate visual inputs and connect perception with structured reasoning, leading to suboptimal performance. To address these challenges, we propose SpatialMath, a novel Spatial Comprehension-Infused Symbolic Reasoning Framework designed to integrate spatial representations into structured symbolic reasoning chains. SpatialMath employs a specialized perception module to extract spatially-grounded representations from visual diagrams, capturing critical geometric structures and spatial relationships. These representations are then methodically infused into symbolic reasoning chains, facilitating visual comprehension-aware structured reasoning. To this end, we introduce MATHVERSE-PLUS, a novel dataset containing structured visual interpretations and step-by-step reasoning paths for vision-intensive mathematical problems. SpatialMath significantly outperforms strong multimodal baselines, achieving up to 10 percentage points improvement over supervised fine-tuning with data augmentation in vision-intensive settings. Robustness analysis reveals that enhanced spatial representations directly improve reasoning accuracy, reinforcing the need for structured perception-to-reasoning pipelines in MSLMs.&lt;/p&gt;</content:encoded></item><item><title>m2sv: A Scalable Benchmark for Map-to-Street-View Spatial Reasoning</title><link>https://arxiv.org/abs/2601.19099v1</link><guid>http://arxiv.org/abs/2601.19099v1</guid><pubDate>Tue, 27 Jan 2026 02:01:56 +0000</pubDate><dc:creator>Yosub Shin</dc:creator><dc:creator>Michael Buriek</dc:creator><dc:creator>Igor Molybog</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision--language models (VLMs) achieve strong performance on many multimodal benchmarks but remain brittle on spatial reasoning tasks that require aligning abstract overhead representations with egocentric views. We introduce m2sv, a scalable benchmark for map-to-street-view spatial reasoning that asks models to infer camera viewing direction by aligning a north-up overhead map with a Street View image captured at the same real-world intersection. We release m2sv-20k, a geographically diverse benchmark with controlled ambiguity, along with m2sv-sft-11k, a curated set of structured reasoning traces for supervised fine-tuning.
  Despite strong performance on existing multimodal benchmarks, the best evaluated VLM achieves only 65.2% accuracy on m2sv, far below the human baseline of 95%. While supervised fine-tuning and reinforcement learning yield consistent gains, cross-benchmark evaluations reveal limited transfer. Beyond aggregate accuracy, we systematically analyze difficulty in map-to-street-view reasoning using both structural signals and human effort, and conduct an extensive failure analysis of adapted open models. Our findings highlight persistent gaps in geometric alignment, evidence aggregation, and reasoning consistency, motivating future work on grounded spatial reasoning across viewpoints.
Published: 2026-01-27T02:01:56+00:00
Venue: arXiv
Score: 0.545 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yosub Shin; Michael Buriek; Igor Molybog&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.545 (consider)&lt;/p&gt;
&lt;p&gt;Vision--language models (VLMs) achieve strong performance on many multimodal benchmarks but remain brittle on spatial reasoning tasks that require aligning abstract overhead representations with egocentric views. We introduce m2sv, a scalable benchmark for map-to-street-view spatial reasoning that asks models to infer camera viewing direction by aligning a north-up overhead map with a Street View image captured at the same real-world intersection. We release m2sv-20k, a geographically diverse benchmark with controlled ambiguity, along with m2sv-sft-11k, a curated set of structured reasoning traces for supervised fine-tuning.
  Despite strong performance on existing multimodal benchmarks, the best evaluated VLM achieves only 65.2% accuracy on m2sv, far below the human baseline of 95%. While supervised fine-tuning and reinforcement learning yield consistent gains, cross-benchmark evaluations reveal limited transfer. Beyond aggregate accuracy, we systematically analyze difficulty in map-to-street-view reasoning using both structural signals and human effort, and conduct an extensive failure analysis of adapted open models. Our findings highlight persistent gaps in geometric alignment, evidence aggregation, and reasoning consistency, motivating future work on grounded spatial reasoning across viewpoints.&lt;/p&gt;</content:encoded></item><item><title>RMNet: Dual-Dimensional Difference Recalibration-Guided CNN-VMamba Synergistic Network for Remote Sensing Image Change Captioning</title><link>https://doi.org/10.1109/tgrs.2026.3658213</link><guid>10.1109/tgrs.2026.3658213</guid><pubDate>Tue, 27 Jan 2026 05:48:56 +0000</pubDate><dc:creator>Xintong Cao</dc:creator><dc:creator>Wenqian Dong</dc:creator><dc:creator>Jiahui Qu</dc:creator><dc:creator>Yunsong Li</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3658213</prism:doi><description>Global surface changes are increasingly monitored using multi-temporal remote sensing technologies. As an emerging technology, change captioning can organically integrate the location information of changed regions with the semantic analysis of regional attributes to generate natural language descriptions of changes, providing critical support for the intuitive interpretation of remote sensing monitoring results. However, existing methods have two key limitations: first, single-stream CNNs fail to extract spatiotemporal information sufficiently which leads to the easy omission of subtle changes, while single-stream Transformers suffer from relatively high parameter counts; second, using pixel-level difference information directly causes the model to focus on pseudo-changes induced by illumination or noise, reducing the accuracy of real change characterization and subsequent description. To address these issues, this paper proposes RMNet which is a dual-dimensional difference recalibration-guided CNN-VMamba synergistic network, with two core innovations: 1) a dual-stream architecture adopted that combines the strong local feature extraction capability of CNNs with the powerful global context modeling ability of VMamba, further enhanced by a channel-wise spatial window attention mechanism; 2) a dual-dimensional difference recalibration mechanism that optimizes features by highlighting core changes and suppressing pseudo-changes through dimension-specific enhancement strategies. Extensive experiments on the LEVIR-CC dataset demonstrate significant performance improvements across all evaluation metrics, validating the effectiveness of RMNet in overcoming current limitations in remote sensing change captioning tasks. The code is available at https://github.com/Jiahuiqu/RMNet.
Published: 2026-01-27T05:48:56+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.538 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xintong Cao; Wenqian Dong; Jiahui Qu; Yunsong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3658213"&gt;10.1109/tgrs.2026.3658213&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.538 (consider)&lt;/p&gt;
&lt;p&gt;Global surface changes are increasingly monitored using multi-temporal remote sensing technologies. As an emerging technology, change captioning can organically integrate the location information of changed regions with the semantic analysis of regional attributes to generate natural language descriptions of changes, providing critical support for the intuitive interpretation of remote sensing monitoring results. However, existing methods have two key limitations: first, single-stream CNNs fail to extract spatiotemporal information sufficiently which leads to the easy omission of subtle changes, while single-stream Transformers suffer from relatively high parameter counts; second, using pixel-level difference information directly causes the model to focus on pseudo-changes induced by illumination or noise, reducing the accuracy of real change characterization and subsequent description. To address these issues, this paper proposes RMNet which is a dual-dimensional difference recalibration-guided CNN-VMamba synergistic network, with two core innovations: 1) a dual-stream architecture adopted that combines the strong local feature extraction capability of CNNs with the powerful global context modeling ability of VMamba, further enhanced by a channel-wise spatial window attention mechanism; 2) a dual-dimensional difference recalibration mechanism that optimizes features by highlighting core changes and suppressing pseudo-changes through dimension-specific enhancement strategies. Extensive experiments on the LEVIR-CC dataset demonstrate significant performance improvements across all evaluation metrics, validating the effectiveness of RMNet in overcoming current limitations in remote sensing change captioning tasks. The code is available at https://github.com/Jiahuiqu/RMNet.&lt;/p&gt;</content:encoded></item><item><title>Agentic Very Long Video Understanding</title><link>https://arxiv.org/abs/2601.18157v1</link><guid>http://arxiv.org/abs/2601.18157v1</guid><pubDate>Mon, 26 Jan 2026 05:20:47 +0000</pubDate><dc:creator>Aniket Rege</dc:creator><dc:creator>Arka Sadhu</dc:creator><dc:creator>Yuliang Li</dc:creator><dc:creator>Kejie Li</dc:creator><dc:creator>Ramya Korlakai Vinayak</dc:creator><dc:creator>Yuning Chai</dc:creator><dc:creator>Yong Jae Lee</dc:creator><dc:creator>Hyo Jin Kim</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.
Published: 2026-01-26T05:20:47+00:00
Venue: arXiv
Score: 0.533 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Aniket Rege; Arka Sadhu; Yuliang Li; Kejie Li; Ramya Korlakai Vinayak; Yuning Chai; Yong Jae Lee; Hyo Jin Kim&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.533 (consider)&lt;/p&gt;
&lt;p&gt;The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.&lt;/p&gt;</content:encoded></item><item><title>LocationAgent: A Hierarchical Agent for Image Geolocation via Decoupling Strategy and Evidence from Parametric Knowledge</title><link>https://arxiv.org/abs/2601.19155v1</link><guid>http://arxiv.org/abs/2601.19155v1</guid><pubDate>Tue, 27 Jan 2026 03:40:03 +0000</pubDate><dc:creator>Qiujun Li</dc:creator><dc:creator>Zijin Xiao</dc:creator><dc:creator>Xulin Wang</dc:creator><dc:creator>Zhidan Ma</dc:creator><dc:creator>Cheng Yang</dc:creator><dc:creator>Haifeng Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Image geolocation aims to infer capture locations based on visual content. Fundamentally, this constitutes a reasoning process composed of \textit{hypothesis-verification cycles}, requiring models to possess both geospatial reasoning capabilities and the ability to verify evidence against geographic facts. Existing methods typically internalize location knowledge and reasoning patterns into static memory via supervised training or trajectory-based reinforcement fine-tuning. Consequently, these methods are prone to factual hallucinations and generalization bottlenecks in open-world settings or scenarios requiring dynamic knowledge. To address these challenges, we propose a Hierarchical Localization Agent, called LocationAgent. Our core philosophy is to retain hierarchical reasoning logic within the model while offloading the verification of geographic evidence to external tools. To implement hierarchical reasoning, we design the RER architecture (Reasoner-Executor-Recorder), which employs role separation and context compression to prevent the drifting problem in multi-step reasoning. For evidence verification, we construct a suite of clue exploration tools that provide diverse evidence to support location reasoning. Furthermore, to address data leakage and the scarcity of Chinese data in existing datasets, we introduce CCL-Bench (China City Location Bench), an image geolocation benchmark encompassing various scene granularities and difficulty levels. Extensive experiments demonstrate that LocationAgent significantly outperforms existing methods by at least 30\% in zero-shot settings.
Published: 2026-01-27T03:40:03+00:00
Venue: arXiv
Score: 0.530 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qiujun Li; Zijin Xiao; Xulin Wang; Zhidan Ma; Cheng Yang; Haifeng Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.530 (consider)&lt;/p&gt;
&lt;p&gt;Image geolocation aims to infer capture locations based on visual content. Fundamentally, this constitutes a reasoning process composed of \textit{hypothesis-verification cycles}, requiring models to possess both geospatial reasoning capabilities and the ability to verify evidence against geographic facts. Existing methods typically internalize location knowledge and reasoning patterns into static memory via supervised training or trajectory-based reinforcement fine-tuning. Consequently, these methods are prone to factual hallucinations and generalization bottlenecks in open-world settings or scenarios requiring dynamic knowledge. To address these challenges, we propose a Hierarchical Localization Agent, called LocationAgent. Our core philosophy is to retain hierarchical reasoning logic within the model while offloading the verification of geographic evidence to external tools. To implement hierarchical reasoning, we design the RER architecture (Reasoner-Executor-Recorder), which employs role separation and context compression to prevent the drifting problem in multi-step reasoning. For evidence verification, we construct a suite of clue exploration tools that provide diverse evidence to support location reasoning. Furthermore, to address data leakage and the scarcity of Chinese data in existing datasets, we introduce CCL-Bench (China City Location Bench), an image geolocation benchmark encompassing various scene granularities and difficulty levels. Extensive experiments demonstrate that LocationAgent significantly outperforms existing methods by at least 30\% in zero-shot settings.&lt;/p&gt;</content:encoded></item><item><title>See then tell: Enhancing key information extraction with vision grounding</title><link>https://doi.org/10.1016/j.neucom.2026.132858</link><guid>10.1016/j.neucom.2026.132858</guid><pubDate>Tue, 27 Jan 2026 07:44:13 +0000</pubDate><dc:creator>Shuhang Liu</dc:creator><dc:creator>Zhenrong Zhang</dc:creator><dc:creator>Pengfei Hu</dc:creator><dc:creator>Jiefeng Ma</dc:creator><dc:creator>Jun Du</dc:creator><dc:creator>Qing Wang</dc:creator><dc:creator>Jianshu Zhang</dc:creator><dc:creator>Chenyu Liu</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132858</prism:doi><description>In the digital era, understanding visually rich documents that combine text, complex layouts, and imagery is crucial. Traditional Key Information Extraction (KIE) approaches heavily rely on Optical Character Recognition (OCR) tools, making them vulnerable to cascading recognition errors, which can severely degrade overall performance. OCR-free models address these issues but often lack vision grounding. Recent methods incorporate explicit coordinate outputs, yet depend on downstream coordinate annotations that are not always available in real-world settings. In this paper, we introduce STNet ( " role="presentation"&gt; ee then " role="presentation"&gt; ell Net), an end-to-end model that jointly produces textual answers and their corresponding vision grounding. The core innovation in STNet is a novel " role="presentation"&gt; token, prepended to each response, which implicitly encodes the physical coordinates. During generation, " role="presentation"&gt; directs the model to first " role="presentation"&gt; — attending to image regions relevant to the question — and then " role="presentation"&gt; , emitting the textual answer. To enhance the model’s " role="presentation"&gt; capabilities, we collect extensive structured table recognition datasets. Based on these datasets, we leverage GPT-4 to develop TVG ( " role="presentation"&gt; ableQA with " role="presentation"&gt; ision " role="presentation"&gt; rounding), a dataset of Question Answering (QA) pairs annotated with vision grounding. Using comparable backbones, our approach achieves state-of-the-art performance on public KIE benchmarks, including CORD, SROIE, and DocVQA, and generalizes well without access to downstream coordinate annotations during fine-tuning. Moreover, the proposed vision grounding mechanism can be integrated into Multimodal Large Language Models (MLLMs) like Qwen2-VL, improving zero-shot KIE. The code and dataset will be made publicly available.
Published: 2026-01-27T07:44:13+00:00
Venue: Neurocomputing
Score: 0.530 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuhang Liu; Zhenrong Zhang; Pengfei Hu; Jiefeng Ma; Jun Du; Qing Wang; Jianshu Zhang; Chenyu Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132858"&gt;10.1016/j.neucom.2026.132858&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.530 (consider)&lt;/p&gt;
&lt;p&gt;In the digital era, understanding visually rich documents that combine text, complex layouts, and imagery is crucial. Traditional Key Information Extraction (KIE) approaches heavily rely on Optical Character Recognition (OCR) tools, making them vulnerable to cascading recognition errors, which can severely degrade overall performance. OCR-free models address these issues but often lack vision grounding. Recent methods incorporate explicit coordinate outputs, yet depend on downstream coordinate annotations that are not always available in real-world settings. In this paper, we introduce STNet ( &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; ee then &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; ell Net), an end-to-end model that jointly produces textual answers and their corresponding vision grounding. The core innovation in STNet is a novel &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; token, prepended to each response, which implicitly encodes the physical coordinates. During generation, &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; directs the model to first &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; — attending to image regions relevant to the question — and then &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; , emitting the textual answer. To enhance the model’s &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; capabilities, we collect extensive structured table recognition datasets. Based on these datasets, we leverage GPT-4 to develop TVG ( &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; ableQA with &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; ision &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; rounding), a dataset of Question Answering (QA) pairs annotated with vision grounding. Using comparable backbones, our approach achieves state-of-the-art performance on public KIE benchmarks, including CORD, SROIE, and DocVQA, and generalizes well without access to downstream coordinate annotations during fine-tuning. Moreover, the proposed vision grounding mechanism can be integrated into Multimodal Large Language Models (MLLMs) like Qwen2-VL, improving zero-shot KIE. The code and dataset will be made publicly available.&lt;/p&gt;</content:encoded></item><item><title>SU-RMT: Toward Bridging Semantic Representation and Structural Detail Modeling for Medical Image Segmentation</title><link>https://doi.org/10.1016/j.inffus.2026.104182</link><guid>10.1016/j.inffus.2026.104182</guid><pubDate>Mon, 26 Jan 2026 06:56:57 +0000</pubDate><dc:creator>Peibo Song</dc:creator><dc:creator>Zihao Wang</dc:creator><dc:creator>Jinshuo Zhang</dc:creator><dc:creator>Shujun Fu</dc:creator><dc:creator>Yunfeng Zhang</dc:creator><dc:creator>Wei Wu</dc:creator><dc:creator>Fangxun Bao</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104182</prism:doi><description>Accurate medical image segmentation requires models that capture high-level semantics while preserving fine-grained structural details, due to anatomical heterogeneity and subtle textures in clinical scenarios. However, existing U-shaped networks usually lack a unified perspective to reconcile semantic representation with structural detail. To this end, we present SU-RMT , a U-shaped network that embodies this unified perspective by redesigning the encoder, bottleneck, and skip connection. The encoder employs the Dy namic S patial A ttention (DySA) mechanism to capture global context with spatial priors. The bottleneck introduces a H ybrid S pectral A daptive (HSA) module to transform abstract semantics into structure-aware features. The first skip connection incorporates a F requency- F used (F 2 ) block to enhance boundary details without amplifying noise. Across several medical image segmentation tasks, SU-RMT demonstrates strong performance. The code is at the link .
Published: 2026-01-26T06:56:57+00:00
Venue: Information Fusion
Score: 0.529 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Peibo Song; Zihao Wang; Jinshuo Zhang; Shujun Fu; Yunfeng Zhang; Wei Wu; Fangxun Bao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104182"&gt;10.1016/j.inffus.2026.104182&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.529 (consider)&lt;/p&gt;
&lt;p&gt;Accurate medical image segmentation requires models that capture high-level semantics while preserving fine-grained structural details, due to anatomical heterogeneity and subtle textures in clinical scenarios. However, existing U-shaped networks usually lack a unified perspective to reconcile semantic representation with structural detail. To this end, we present SU-RMT , a U-shaped network that embodies this unified perspective by redesigning the encoder, bottleneck, and skip connection. The encoder employs the Dy namic S patial A ttention (DySA) mechanism to capture global context with spatial priors. The bottleneck introduces a H ybrid S pectral A daptive (HSA) module to transform abstract semantics into structure-aware features. The first skip connection incorporates a F requency- F used (F 2 ) block to enhance boundary details without amplifying noise. Across several medical image segmentation tasks, SU-RMT demonstrates strong performance. The code is at the link .&lt;/p&gt;</content:encoded></item><item><title>RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming</title><link>https://arxiv.org/abs/2601.19433v1</link><guid>http://arxiv.org/abs/2601.19433v1</guid><pubDate>Tue, 27 Jan 2026 10:10:55 +0000</pubDate><dc:creator>Jisheng Chu</dc:creator><dc:creator>Wenrui Li</dc:creator><dc:creator>Rui Zhao</dc:creator><dc:creator>Wangmeng Zuo</dc:creator><dc:creator>Shifeng Chen</dc:creator><dc:creator>Xiaopeng Fan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at https://github.com/JS-CHU/RoamScene3D.
Published: 2026-01-27T10:10:55+00:00
Venue: arXiv
Score: 0.528 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jisheng Chu; Wenrui Li; Rui Zhao; Wangmeng Zuo; Shifeng Chen; Xiaopeng Fan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.528 (consider)&lt;/p&gt;
&lt;p&gt;Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at https://github.com/JS-CHU/RoamScene3D.&lt;/p&gt;</content:encoded></item><item><title>ViTCoP: Accelerating Large Vision-Language Models via Visual and Textual Semantic Collaborative Pruning</title><link>https://arxiv.org/abs/2601.17818v1</link><guid>http://arxiv.org/abs/2601.17818v1</guid><pubDate>Sun, 25 Jan 2026 12:47:30 +0000</pubDate><dc:creator>Wen Luo</dc:creator><dc:creator>Peng Chen</dc:creator><dc:creator>Xiaotao Huang</dc:creator><dc:creator>LiQun Huang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Vision-Language Models (LVLMs) incur high computational costs due to significant redundancy in their visual tokens. To effectively reduce this cost, researchers have proposed various visual token pruning methods. However, existing methods are generally limited, either losing critical visual information prematurely due to pruning in the vision encoder, or leading to information redundancy among the selected tokens due to pruning in the Large Language Models (LLMs). To address these challenges, we propose a Visual and Textual Semantic Collaborative Pruning framework (ViTCoP) that combines redundancy filtering in the vision encoder with step-wise co-pruning within the LLM based on its hierarchical characteristics, to efficiently preserve critical and informationally diverse visual tokens. Meanwhile, to ensure compatibility with acceleration techniques like FlashAttention, we introduce the L2 norm of K-vectors as the token saliency metric in the LLM. Extensive experiments on various Large Vision-Language Models demonstrate that ViTCoP not only achieves state-of-the-art performance surpassing existing methods on both image and video understanding tasks, but also significantly reduces model inference latency and GPU memory consumption. Notably, its performance advantage over other methods becomes even more pronounced under extreme pruning rates.
Published: 2026-01-25T12:47:30+00:00
Venue: arXiv
Score: 0.527 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wen Luo; Peng Chen; Xiaotao Huang; LiQun Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.527 (consider)&lt;/p&gt;
&lt;p&gt;Large Vision-Language Models (LVLMs) incur high computational costs due to significant redundancy in their visual tokens. To effectively reduce this cost, researchers have proposed various visual token pruning methods. However, existing methods are generally limited, either losing critical visual information prematurely due to pruning in the vision encoder, or leading to information redundancy among the selected tokens due to pruning in the Large Language Models (LLMs). To address these challenges, we propose a Visual and Textual Semantic Collaborative Pruning framework (ViTCoP) that combines redundancy filtering in the vision encoder with step-wise co-pruning within the LLM based on its hierarchical characteristics, to efficiently preserve critical and informationally diverse visual tokens. Meanwhile, to ensure compatibility with acceleration techniques like FlashAttention, we introduce the L2 norm of K-vectors as the token saliency metric in the LLM. Extensive experiments on various Large Vision-Language Models demonstrate that ViTCoP not only achieves state-of-the-art performance surpassing existing methods on both image and video understanding tasks, but also significantly reduces model inference latency and GPU memory consumption. Notably, its performance advantage over other methods becomes even more pronounced under extreme pruning rates.&lt;/p&gt;</content:encoded></item><item><title>Making medical vision-language models think causally across modalities with retrieval-augmented cross-modal reasoning</title><link>https://arxiv.org/abs/2601.18356v1</link><guid>http://arxiv.org/abs/2601.18356v1</guid><pubDate>Mon, 26 Jan 2026 11:03:00 +0000</pubDate><dc:creator>Weiqin Yang</dc:creator><dc:creator>Haowen Xue</dc:creator><dc:creator>Qingyi Peng</dc:creator><dc:creator>Hexuan Hu</dc:creator><dc:creator>Qian Huang</dc:creator><dc:creator>Tingbo Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Medical vision-language models (VLMs) achieve strong performance in diagnostic reporting and image-text alignment, yet their underlying reasoning mechanisms remain fundamentally correlational, exhibiting reliance on superficial statistical associations that fail to capture the causal pathophysiological mechanisms central to clinical decision-making. This limitation makes them fragile, prone to hallucinations, and sensitive to dataset biases. Retrieval-augmented generation (RAG) offers a partial remedy by grounding predictions in external knowledge. However, conventional RAG depends on semantic similarity, introducing new spurious correlations. We propose Multimodal Causal Retrieval-Augmented Generation, a framework that integrates causal inference principles with multimodal retrieval. It retrieves clinically relevant exemplars and causal graphs from external sources, conditioning model reasoning on counterfactual and interventional evidence rather than correlations alone. Applied to radiology report generation, diagnosis prediction, and visual question answering, it improves factual accuracy, robustness to distribution shifts, and interpretability. Our results highlight causal retrieval as a scalable path toward medical VLMs that think beyond pattern matching, enabling trustworthy multimodal reasoning in high-stakes clinical settings.
Published: 2026-01-26T11:03:00+00:00
Venue: arXiv
Score: 0.526 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weiqin Yang; Haowen Xue; Qingyi Peng; Hexuan Hu; Qian Huang; Tingbo Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.526 (consider)&lt;/p&gt;
&lt;p&gt;Medical vision-language models (VLMs) achieve strong performance in diagnostic reporting and image-text alignment, yet their underlying reasoning mechanisms remain fundamentally correlational, exhibiting reliance on superficial statistical associations that fail to capture the causal pathophysiological mechanisms central to clinical decision-making. This limitation makes them fragile, prone to hallucinations, and sensitive to dataset biases. Retrieval-augmented generation (RAG) offers a partial remedy by grounding predictions in external knowledge. However, conventional RAG depends on semantic similarity, introducing new spurious correlations. We propose Multimodal Causal Retrieval-Augmented Generation, a framework that integrates causal inference principles with multimodal retrieval. It retrieves clinically relevant exemplars and causal graphs from external sources, conditioning model reasoning on counterfactual and interventional evidence rather than correlations alone. Applied to radiology report generation, diagnosis prediction, and visual question answering, it improves factual accuracy, robustness to distribution shifts, and interpretability. Our results highlight causal retrieval as a scalable path toward medical VLMs that think beyond pattern matching, enabling trustworthy multimodal reasoning in high-stakes clinical settings.&lt;/p&gt;</content:encoded></item><item><title>STARS: Shared-specific Translation and Alignment for missing-modality Remote Sensing Semantic Segmentation</title><link>https://arxiv.org/abs/2601.17342v1</link><guid>http://arxiv.org/abs/2601.17342v1</guid><pubDate>Sat, 24 Jan 2026 07:07:16 +0000</pubDate><dc:creator>Tong Wang</dc:creator><dc:creator>Xiaodong Zhang</dc:creator><dc:creator>Guanzhou Chen</dc:creator><dc:creator>Jiaqi Wang</dc:creator><dc:creator>Chenxi Liu</dc:creator><dc:creator>Xiaoliang Tan</dc:creator><dc:creator>Wenchao Guo</dc:creator><dc:creator>Xuyang Li</dc:creator><dc:creator>Xuanrui Wang</dc:creator><dc:creator>Zifan Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal remote sensing technology significantly enhances the understanding of surface semantics by integrating heterogeneous data such as optical images, Synthetic Aperture Radar (SAR), and Digital Surface Models (DSM). However, in practical applications, the missing of modality data (e.g., optical or DSM) is a common and severe challenge, which leads to performance decline in traditional multimodal fusion models. Existing methods for addressing missing modalities still face limitations, including feature collapse and overly generalized recovered features. To address these issues, we propose \textbf{STARS} (\textbf{S}hared-specific \textbf{T}ranslation and \textbf{A}lignment for missing-modality \textbf{R}emote \textbf{S}ensing), a robust semantic segmentation framework for incomplete multimodal inputs. STARS is built on two key designs. First, we introduce an asymmetric alignment mechanism with bidirectional translation and stop-gradient, which effectively prevents feature collapse and reduces sensitivity to hyperparameters. Second, we propose a Pixel-level Semantic sampling Alignment (PSA) strategy that combines class-balanced pixel sampling with cross-modality semantic alignment loss, to mitigate alignment failures caused by severe class imbalance and improve minority-class recognition.
Published: 2026-01-24T07:07:16+00:00
Venue: arXiv
Score: 0.525 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tong Wang; Xiaodong Zhang; Guanzhou Chen; Jiaqi Wang; Chenxi Liu; Xiaoliang Tan; Wenchao Guo; Xuyang Li; Xuanrui Wang; Zifan Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.525 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal remote sensing technology significantly enhances the understanding of surface semantics by integrating heterogeneous data such as optical images, Synthetic Aperture Radar (SAR), and Digital Surface Models (DSM). However, in practical applications, the missing of modality data (e.g., optical or DSM) is a common and severe challenge, which leads to performance decline in traditional multimodal fusion models. Existing methods for addressing missing modalities still face limitations, including feature collapse and overly generalized recovered features. To address these issues, we propose \textbf{STARS} (\textbf{S}hared-specific \textbf{T}ranslation and \textbf{A}lignment for missing-modality \textbf{R}emote \textbf{S}ensing), a robust semantic segmentation framework for incomplete multimodal inputs. STARS is built on two key designs. First, we introduce an asymmetric alignment mechanism with bidirectional translation and stop-gradient, which effectively prevents feature collapse and reduces sensitivity to hyperparameters. Second, we propose a Pixel-level Semantic sampling Alignment (PSA) strategy that combines class-balanced pixel sampling with cross-modality semantic alignment loss, to mitigate alignment failures caused by severe class imbalance and improve minority-class recognition.&lt;/p&gt;</content:encoded></item><item><title>MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance</title><link>https://arxiv.org/abs/2601.17866v1</link><guid>http://arxiv.org/abs/2601.17866v1</guid><pubDate>Sun, 25 Jan 2026 15:00:37 +0000</pubDate><dc:creator>Yoonwoo Jeong</dc:creator><dc:creator>Cheng Sun</dc:creator><dc:creator>Yu-Chiang Frank Wang</dc:creator><dc:creator>Minsu Cho</dc:creator><dc:creator>Jaesung Choe</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps -- 3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel-point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.
Published: 2026-01-25T15:00:37+00:00
Venue: arXiv
Score: 0.524 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yoonwoo Jeong; Cheng Sun; Yu-Chiang Frank Wang; Minsu Cho; Jaesung Choe&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.524 (consider)&lt;/p&gt;
&lt;p&gt;Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps -- 3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel-point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.&lt;/p&gt;</content:encoded></item><item><title>Towards Governance-Oriented Low-Altitude Intelligence: A Management-Centric Multi-Modal Benchmark With Implicitly Coordinated Vision-Language Reasoning Framework</title><link>https://arxiv.org/abs/2601.19640v1</link><guid>http://arxiv.org/abs/2601.19640v1</guid><pubDate>Tue, 27 Jan 2026 14:17:04 +0000</pubDate><dc:creator>Hao Chang</dc:creator><dc:creator>Zhihui Wang</dc:creator><dc:creator>Lingxiang Wu</dc:creator><dc:creator>Peijin Wang</dc:creator><dc:creator>Wenhui Diao</dc:creator><dc:creator>Jinqiao Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Low-altitude vision systems are becoming a critical infrastructure for smart city governance. However, existing object-centric perception paradigms and loosely coupled vision-language pipelines are still difficult to support management-oriented anomaly understanding required in real-world urban governance. To bridge this gap, we introduce GovLA-10K, the first management-oriented multi-modal benchmark for low-altitude intelligence, along with GovLA-Reasoner, a unified vision-language reasoning framework tailored for governance-aware aerial perception. Unlike existing studies that aim to exhaustively annotate all visible objects, GovLA-10K is deliberately designed around functionally salient targets that directly correspond to practical management needs, and further provides actionable management suggestions grounded in these observations. To effectively coordinate the fine-grained visual grounding with high-level contextual language reasoning, GovLA-Reasoner introduces an efficient feature adapter that implicitly coordinates discriminative representation sharing between the visual detector and the large language model (LLM). Extensive experiments show that our method significantly improves performance while avoiding the need of fine-tuning for any task-specific individual components. We believe our work offers a new perspective and foundation for future studies on management-aware low-altitude vision-language systems.
Published: 2026-01-27T14:17:04+00:00
Venue: arXiv
Score: 0.523 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Chang; Zhihui Wang; Lingxiang Wu; Peijin Wang; Wenhui Diao; Jinqiao Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.523 (consider)&lt;/p&gt;
&lt;p&gt;Low-altitude vision systems are becoming a critical infrastructure for smart city governance. However, existing object-centric perception paradigms and loosely coupled vision-language pipelines are still difficult to support management-oriented anomaly understanding required in real-world urban governance. To bridge this gap, we introduce GovLA-10K, the first management-oriented multi-modal benchmark for low-altitude intelligence, along with GovLA-Reasoner, a unified vision-language reasoning framework tailored for governance-aware aerial perception. Unlike existing studies that aim to exhaustively annotate all visible objects, GovLA-10K is deliberately designed around functionally salient targets that directly correspond to practical management needs, and further provides actionable management suggestions grounded in these observations. To effectively coordinate the fine-grained visual grounding with high-level contextual language reasoning, GovLA-Reasoner introduces an efficient feature adapter that implicitly coordinates discriminative representation sharing between the visual detector and the large language model (LLM). Extensive experiments show that our method significantly improves performance while avoiding the need of fine-tuning for any task-specific individual components. We believe our work offers a new perspective and foundation for future studies on management-aware low-altitude vision-language systems.&lt;/p&gt;</content:encoded></item><item><title>Towards Pixel-Level VLM Perception via Simple Points Prediction</title><link>https://arxiv.org/abs/2601.19228v1</link><guid>http://arxiv.org/abs/2601.19228v1</guid><pubDate>Tue, 27 Jan 2026 05:50:40 +0000</pubDate><dc:creator>Tianhui Song</dc:creator><dc:creator>Haoyu Lu</dc:creator><dc:creator>Hao Yang</dc:creator><dc:creator>Lin Sui</dc:creator><dc:creator>Haoning Wu</dc:creator><dc:creator>Zaida Zhou</dc:creator><dc:creator>Zhiqi Huang</dc:creator><dc:creator>Yiping Bao</dc:creator><dc:creator>Y. Charles</dc:creator><dc:creator>Xinyu Zhou</dc:creator><dc:creator>Limin Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present SimpleSeg, a strikingly simple yet highly effective approach to endow Multimodal Large Language Models (MLLMs) with native pixel-level perception. Our method reframes segmentation as a simple sequence generation problem: the model directly predicts sequences of points (textual coordinates) delineating object boundaries, entirely within its language space. To achieve high fidelity, we introduce a two-stage SF$\to$RL training pipeline, where Reinforcement Learning with an IoU-based reward refines the point sequences to accurately match ground-truth contours. We find that the standard MLLM architecture possesses a strong, inherent capacity for low-level perception that can be unlocked without any specialized architecture. On segmentation benchmarks, SimpleSeg achieves performance that is comparable to, and often surpasses, methods relying on complex, task-specific designs. This work lays out that precise spatial understanding can emerge from simple point prediction, challenging the prevailing need for auxiliary components and paving the way for more unified and capable VLMs. Homepage: https://simpleseg.github.io/
Published: 2026-01-27T05:50:40+00:00
Venue: arXiv
Score: 0.522 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianhui Song; Haoyu Lu; Hao Yang; Lin Sui; Haoning Wu; Zaida Zhou; Zhiqi Huang; Yiping Bao; Y. Charles; Xinyu Zhou; Limin Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.522 (consider)&lt;/p&gt;
&lt;p&gt;We present SimpleSeg, a strikingly simple yet highly effective approach to endow Multimodal Large Language Models (MLLMs) with native pixel-level perception. Our method reframes segmentation as a simple sequence generation problem: the model directly predicts sequences of points (textual coordinates) delineating object boundaries, entirely within its language space. To achieve high fidelity, we introduce a two-stage SF$\to$RL training pipeline, where Reinforcement Learning with an IoU-based reward refines the point sequences to accurately match ground-truth contours. We find that the standard MLLM architecture possesses a strong, inherent capacity for low-level perception that can be unlocked without any specialized architecture. On segmentation benchmarks, SimpleSeg achieves performance that is comparable to, and often surpasses, methods relying on complex, task-specific designs. This work lays out that precise spatial understanding can emerge from simple point prediction, challenging the prevailing need for auxiliary components and paving the way for more unified and capable VLMs. Homepage: https://simpleseg.github.io/&lt;/p&gt;</content:encoded></item><item><title>DBFNM: Dual-Branch Fusion Network with Mamba Decoder for Indoor Depth Completion</title><link>https://doi.org/10.1109/tcsvt.2026.3657718</link><guid>10.1109/tcsvt.2026.3657718</guid><pubDate>Tue, 27 Jan 2026 06:05:44 +0000</pubDate><dc:creator>Yujie Diao</dc:creator><dc:creator>Zhisheng Wang</dc:creator><dc:creator>Jiayu Fan</dc:creator><dc:creator>Yuhua Cong</dc:creator><dc:creator>Quan Ouyang</dc:creator><dc:creator>Xin Tang</dc:creator><dc:creator>Rufei Zhang</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3657718</prism:doi><description>Accurate depth maps are essential for indoor navigation and modeling by robots, but raw depth maps often have missing areas due to sensor limitations, environmental factors, and distance constraints. Existing methods that fuse RGB images with depth maps usually cannot utilize spatial structural information and exhibit poor accuracy at object edges. To bridge this gap, a dual-branch fusion network with Mamba decoder, called DBFNM, is proposed for depth completion in this work. It consists of two complementary branches: one branch utilizes semantic and texture information from RGB images as visual guidance, while the other extracts spatial geometric structures from normal maps as structural guidance. In particular, a geometric gated encoder is utilized to fully leverage spatial information. In the dual-branch decoding stage, a dual-branch feature interaction alignment module is designed, which is composed of three components, including dual-branch edge feature alignment, dual-branch interaction, and global alignment. Then, the decoded dual-branch features are processed by a dual-modal fusion network based on a spatial propagation network to obtain dense depth map predictions. Extensive experimental results on the NYU-Depth V2 and SUN RGB-D datasets demonstrate that DBF achieves superior depth completion performance compared to existing methods in indoor scenes, particularly in handling large-scale missing depth regions and preserving edge details.
Published: 2026-01-27T06:05:44+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.522 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yujie Diao; Zhisheng Wang; Jiayu Fan; Yuhua Cong; Quan Ouyang; Xin Tang; Rufei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3657718"&gt;10.1109/tcsvt.2026.3657718&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.522 (consider)&lt;/p&gt;
&lt;p&gt;Accurate depth maps are essential for indoor navigation and modeling by robots, but raw depth maps often have missing areas due to sensor limitations, environmental factors, and distance constraints. Existing methods that fuse RGB images with depth maps usually cannot utilize spatial structural information and exhibit poor accuracy at object edges. To bridge this gap, a dual-branch fusion network with Mamba decoder, called DBFNM, is proposed for depth completion in this work. It consists of two complementary branches: one branch utilizes semantic and texture information from RGB images as visual guidance, while the other extracts spatial geometric structures from normal maps as structural guidance. In particular, a geometric gated encoder is utilized to fully leverage spatial information. In the dual-branch decoding stage, a dual-branch feature interaction alignment module is designed, which is composed of three components, including dual-branch edge feature alignment, dual-branch interaction, and global alignment. Then, the decoded dual-branch features are processed by a dual-modal fusion network based on a spatial propagation network to obtain dense depth map predictions. Extensive experimental results on the NYU-Depth V2 and SUN RGB-D datasets demonstrate that DBF achieves superior depth completion performance compared to existing methods in indoor scenes, particularly in handling large-scale missing depth regions and preserving edge details.&lt;/p&gt;</content:encoded></item><item><title>Video-KTR: Reinforcing Video Reasoning via Key Token Attribution</title><link>https://arxiv.org/abs/2601.19686v1</link><guid>http://arxiv.org/abs/2601.19686v1</guid><pubDate>Tue, 27 Jan 2026 15:02:23 +0000</pubDate><dc:creator>Ziyue Wang</dc:creator><dc:creator>Sheng Jin</dc:creator><dc:creator>Zhongrong Zuo</dc:creator><dc:creator>Jiawei Wu</dc:creator><dc:creator>Han Qiu</dc:creator><dc:creator>Qi She</dc:creator><dc:creator>Hao Zhang</dc:creator><dc:creator>Xudong Jiang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at https://github.com/zywang0104/Video-KTR.
Published: 2026-01-27T15:02:23+00:00
Venue: arXiv
Score: 0.519 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ziyue Wang; Sheng Jin; Zhongrong Zuo; Jiawei Wu; Han Qiu; Qi She; Hao Zhang; Xudong Jiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.519 (consider)&lt;/p&gt;
&lt;p&gt;Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at https://github.com/zywang0104/Video-KTR.&lt;/p&gt;</content:encoded></item><item><title>SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation</title><link>https://arxiv.org/abs/2601.17657v1</link><guid>http://arxiv.org/abs/2601.17657v1</guid><pubDate>Sun, 25 Jan 2026 02:32:01 +0000</pubDate><dc:creator>Taewan Cho</dc:creator><dc:creator>Taeryang Kim</dc:creator><dc:creator>Andrew Jaeyong Choi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for semantic understanding but inherently struggles to perceive geometric structure. Existing methods attempt to bridge this gap by querying CLIP with textual prompts, a process that is often indirect and inefficient. This paper introduces a fundamentally different approach using a dual-pathway decoder. We present SPACE-CLIP, an architecture that unlocks and interprets latent geometric knowledge directly from a frozen CLIP vision encoder, completely bypassing the text encoder and its associated textual prompts. A semantic pathway interprets high-level features, dynamically conditioned on global context using feature-wise linear modulation (FiLM). In addition, a structural pathway extracts fine-grained spatial details from early layers. These complementary streams are hierarchically fused, enabling a robust synthesis of semantic context and precise geometry. Extensive experiments on the KITTI benchmark show that SPACE-CLIP dramatically outperforms previous CLIP-based methods. Our ablation studies validate that the synergistic fusion of our dual pathways is critical to this success. SPACE-CLIP offers a new, efficient, and architecturally elegant blueprint for repurposing large-scale vision models. The proposed method is not just a standalone depth estimator, but a readily integrable spatial perception module for the next generation of embodied AI systems, such as vision-language-action (VLA) models. Our model is available at https://github.com/taewan2002/space-clip
Published: 2026-01-25T02:32:01+00:00
Venue: arXiv
Score: 0.519 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Taewan Cho; Taeryang Kim; Andrew Jaeyong Choi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.519 (consider)&lt;/p&gt;
&lt;p&gt;Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for semantic understanding but inherently struggles to perceive geometric structure. Existing methods attempt to bridge this gap by querying CLIP with textual prompts, a process that is often indirect and inefficient. This paper introduces a fundamentally different approach using a dual-pathway decoder. We present SPACE-CLIP, an architecture that unlocks and interprets latent geometric knowledge directly from a frozen CLIP vision encoder, completely bypassing the text encoder and its associated textual prompts. A semantic pathway interprets high-level features, dynamically conditioned on global context using feature-wise linear modulation (FiLM). In addition, a structural pathway extracts fine-grained spatial details from early layers. These complementary streams are hierarchically fused, enabling a robust synthesis of semantic context and precise geometry. Extensive experiments on the KITTI benchmark show that SPACE-CLIP dramatically outperforms previous CLIP-based methods. Our ablation studies validate that the synergistic fusion of our dual pathways is critical to this success. SPACE-CLIP offers a new, efficient, and architecturally elegant blueprint for repurposing large-scale vision models. The proposed method is not just a standalone depth estimator, but a readily integrable spatial perception module for the next generation of embodied AI systems, such as vision-language-action (VLA) models. Our model is available at https://github.com/taewan2002/space-clip&lt;/p&gt;</content:encoded></item><item><title>SoEdit: Improving Instruction-Driven Object Editing by Focusing on a Single Object within a Cropped Region</title><link>https://doi.org/10.1109/tcsvt.2026.3657945</link><guid>10.1109/tcsvt.2026.3657945</guid><pubDate>Tue, 27 Jan 2026 06:05:44 +0000</pubDate><dc:creator>Wuyang Luo</dc:creator><dc:creator>Su Yang</dc:creator><dc:creator>Hao Niu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3657945</prism:doi><description>Recently, instruction-driven image editing methods have demonstrated promising capabilities, requiring only a brief text to guide image modifications. However, most of them often yield suboptimal results for object editing in complex scenes, due to two major defects: (1) Over-editing, where unintended regions of the image are inadvertently altered; (2) Inability to precisely adhere to instructions, particularly in scenes with numerous elements. To resolve these issues, we propose a Single object Editing scheme, termed SoEdit, which distills complex editing tasks into single-object editing within cropped regions through a pipeline that integrates task parsing, object localization, editing, and context blending. This approach minimizes interference from irrelevant areas, ensures proper object size and placement, and ultimately enhances model performance. Furthermore, we introduce a lightweight Spatially-Adaptive Mixture of Experts (SAMOE) to better model spatial heterogeneity, enabling tokenwise adaptive processing and further enhancing the overall editing capability with minimal additional parameters. Moreover, we introduce a large-scale object-centric dataset to further optimize the model. Extensive experiments demonstrate that SoEdit outperforms existing methods, especially in precise responses to fine-grained editing requirements, such as multi-action and quantity-sensitive object editing.
Published: 2026-01-27T06:05:44+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.519 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wuyang Luo; Su Yang; Hao Niu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3657945"&gt;10.1109/tcsvt.2026.3657945&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.519 (consider)&lt;/p&gt;
&lt;p&gt;Recently, instruction-driven image editing methods have demonstrated promising capabilities, requiring only a brief text to guide image modifications. However, most of them often yield suboptimal results for object editing in complex scenes, due to two major defects: (1) Over-editing, where unintended regions of the image are inadvertently altered; (2) Inability to precisely adhere to instructions, particularly in scenes with numerous elements. To resolve these issues, we propose a Single object Editing scheme, termed SoEdit, which distills complex editing tasks into single-object editing within cropped regions through a pipeline that integrates task parsing, object localization, editing, and context blending. This approach minimizes interference from irrelevant areas, ensures proper object size and placement, and ultimately enhances model performance. Furthermore, we introduce a lightweight Spatially-Adaptive Mixture of Experts (SAMOE) to better model spatial heterogeneity, enabling tokenwise adaptive processing and further enhancing the overall editing capability with minimal additional parameters. Moreover, we introduce a large-scale object-centric dataset to further optimize the model. Extensive experiments demonstrate that SoEdit outperforms existing methods, especially in precise responses to fine-grained editing requirements, such as multi-action and quantity-sensitive object editing.&lt;/p&gt;</content:encoded></item><item><title>AGSP-DSA: An Adaptive Graph Signal Processing Framework for Robust Multimodal Fusion with Dynamic Semantic Alignment</title><link>https://arxiv.org/abs/2601.18589v1</link><guid>http://arxiv.org/abs/2601.18589v1</guid><pubDate>Mon, 26 Jan 2026 15:35:03 +0000</pubDate><dc:creator>KV Karthikeya</dc:creator><dc:creator>Ashok Kumar Das</dc:creator><dc:creator>Shantanu Pal</dc:creator><dc:creator>Vivekananda Bhat K</dc:creator><dc:creator>Arun Sekar Rajasekaran</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>In this paper, we introduce an Adaptive Graph Signal Processing with Dynamic Semantic Alignment (AGSP DSA) framework to perform robust multimodal data fusion over heterogeneous sources, including text, audio, and images. The requested approach uses a dual-graph construction to learn both intra-modal and inter-modal relations, spectral graph filtering to boost the informative signals, and effective node embedding with Multi-scale Graph Convolutional Networks (GCNs). Semantic aware attention mechanism: each modality may dynamically contribute to the context with respect to contextual relevance. The experimental outcomes on three benchmark datasets, including CMU-MOSEI, AVE, and MM-IMDB, show that AGSP-DSA performs as the state of the art. More precisely, it achieves 95.3% accuracy, 0.936 F1-score, and 0.924 mAP on CMU-MOSEI, improving MM-GNN by 2.6 percent in accuracy. It gets 93.4% accuracy and 0.911 F1-score on AVE and 91.8% accuracy and 0.886 F1-score on MM-IMDB, which demonstrate good generalization and robustness in the missing modality setting. These findings verify the efficiency of AGSP-DSA in promoting multimodal learning in sentiment analysis, event recognition and multimedia classification.
Published: 2026-01-26T15:35:03+00:00
Venue: arXiv
Score: 0.519 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; KV Karthikeya; Ashok Kumar Das; Shantanu Pal; Vivekananda Bhat K; Arun Sekar Rajasekaran&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.519 (consider)&lt;/p&gt;
&lt;p&gt;In this paper, we introduce an Adaptive Graph Signal Processing with Dynamic Semantic Alignment (AGSP DSA) framework to perform robust multimodal data fusion over heterogeneous sources, including text, audio, and images. The requested approach uses a dual-graph construction to learn both intra-modal and inter-modal relations, spectral graph filtering to boost the informative signals, and effective node embedding with Multi-scale Graph Convolutional Networks (GCNs). Semantic aware attention mechanism: each modality may dynamically contribute to the context with respect to contextual relevance. The experimental outcomes on three benchmark datasets, including CMU-MOSEI, AVE, and MM-IMDB, show that AGSP-DSA performs as the state of the art. More precisely, it achieves 95.3% accuracy, 0.936 F1-score, and 0.924 mAP on CMU-MOSEI, improving MM-GNN by 2.6 percent in accuracy. It gets 93.4% accuracy and 0.911 F1-score on AVE and 91.8% accuracy and 0.886 F1-score on MM-IMDB, which demonstrate good generalization and robustness in the missing modality setting. These findings verify the efficiency of AGSP-DSA in promoting multimodal learning in sentiment analysis, event recognition and multimedia classification.&lt;/p&gt;</content:encoded></item><item><title>S²TA-Fuse: Semantic-Superpixel Tokenized Attention for Spatial Spectral Fusion</title><link>https://doi.org/10.1109/tgrs.2026.3657766</link><guid>10.1109/tgrs.2026.3657766</guid><pubDate>Tue, 27 Jan 2026 05:48:56 +0000</pubDate><dc:creator>Jiawei Jiang</dc:creator><dc:creator>Wei Li</dc:creator><dc:creator>Jieyuan Pei</dc:creator><dc:creator>Junwei Zhu</dc:creator><dc:creator>Honghui Xu</dc:creator><dc:creator>Yuchao Feng</dc:creator><dc:creator>Jianwei Zheng</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3657766</prism:doi><description>Superpixel paradigms have long been regarded as a well-established approach of mitigating spatial redundancy in hyperspectral images, yet their reliance on non-differentiable and irreversible segmentation renders them unsuitable for end-to-end spatial–spectral fusion (SSF). To tackle this limitation, this study introduces S²TA-Fuse, a transformer-based solver named Semantic-Superpixel Tokenized Attention for Fusion, which preserves the efficiency of superpixels while removing the need for explicit segmentation. The central design lies in a semantic attention mechanism that adaptively organizes pixels into deformable and content-aware semantic groups. Pixels sharing similar latent states are softly aggregated and encoded as compact tokens, upon which attention is computed to capture intricate long-range dependencies. This formulation endows the model with an inherent ability to accommodate scale variations while maintaining linear computational complexity with respect to the number of pixels. On top of the semantic backbone, two complementary components are devised. The Local Spectral Pyramid enhances the representation of multi-scale spectral cues in the spatial domain, whereas FreqNet supplements global information by modeling frequency-dependent variations through amplitude and phase decomposition. Comprehensive experiments on widely used benchmarks for spatial–spectral fusion demonstrate that S²TA-Fuse consistently surpasses the state of the art both in quantitative accuracy and visual fidelity.
Published: 2026-01-27T05:48:56+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.516 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiawei Jiang; Wei Li; Jieyuan Pei; Junwei Zhu; Honghui Xu; Yuchao Feng; Jianwei Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3657766"&gt;10.1109/tgrs.2026.3657766&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.516 (consider)&lt;/p&gt;
&lt;p&gt;Superpixel paradigms have long been regarded as a well-established approach of mitigating spatial redundancy in hyperspectral images, yet their reliance on non-differentiable and irreversible segmentation renders them unsuitable for end-to-end spatial–spectral fusion (SSF). To tackle this limitation, this study introduces S²TA-Fuse, a transformer-based solver named Semantic-Superpixel Tokenized Attention for Fusion, which preserves the efficiency of superpixels while removing the need for explicit segmentation. The central design lies in a semantic attention mechanism that adaptively organizes pixels into deformable and content-aware semantic groups. Pixels sharing similar latent states are softly aggregated and encoded as compact tokens, upon which attention is computed to capture intricate long-range dependencies. This formulation endows the model with an inherent ability to accommodate scale variations while maintaining linear computational complexity with respect to the number of pixels. On top of the semantic backbone, two complementary components are devised. The Local Spectral Pyramid enhances the representation of multi-scale spectral cues in the spatial domain, whereas FreqNet supplements global information by modeling frequency-dependent variations through amplitude and phase decomposition. Comprehensive experiments on widely used benchmarks for spatial–spectral fusion demonstrate that S²TA-Fuse consistently surpasses the state of the art both in quantitative accuracy and visual fidelity.&lt;/p&gt;</content:encoded></item><item><title>Revisiting Aerial Scene Classification on the AID Benchmark</title><link>https://arxiv.org/abs/2601.18263v1</link><guid>http://arxiv.org/abs/2601.18263v1</guid><pubDate>Mon, 26 Jan 2026 08:39:02 +0000</pubDate><dc:creator>Subhajeet Das</dc:creator><dc:creator>Susmita Ghosh</dc:creator><dc:creator>Abhiroop Chatterjee</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Aerial images play a vital role in urban planning and environmental preservation, as they consist of various structures, representing different types of buildings, forests, mountains, and unoccupied lands. Due to its heterogeneous nature, developing robust models for scene classification remains a challenge. In this study, we conduct a literature review of various machine learning methods for aerial image classification. Our survey covers a range of approaches from handcrafted features (e.g., SIFT, LBP) to traditional CNNs (e.g., VGG, GoogLeNet), and advanced deep hybrid networks. In this connection, we have also designed Aerial-Y-Net, a spatial attention-enhanced CNN with multi-scale feature fusion mechanism, which acts as an attention-based model and helps us to better understand the complexities of aerial images. Evaluated on the AID dataset, our model achieves 91.72% accuracy, outperforming several baseline architectures.
Published: 2026-01-26T08:39:02+00:00
Venue: arXiv
Score: 0.516 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Subhajeet Das; Susmita Ghosh; Abhiroop Chatterjee&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.516 (consider)&lt;/p&gt;
&lt;p&gt;Aerial images play a vital role in urban planning and environmental preservation, as they consist of various structures, representing different types of buildings, forests, mountains, and unoccupied lands. Due to its heterogeneous nature, developing robust models for scene classification remains a challenge. In this study, we conduct a literature review of various machine learning methods for aerial image classification. Our survey covers a range of approaches from handcrafted features (e.g., SIFT, LBP) to traditional CNNs (e.g., VGG, GoogLeNet), and advanced deep hybrid networks. In this connection, we have also designed Aerial-Y-Net, a spatial attention-enhanced CNN with multi-scale feature fusion mechanism, which acts as an attention-based model and helps us to better understand the complexities of aerial images. Evaluated on the AID dataset, our model achieves 91.72% accuracy, outperforming several baseline architectures.&lt;/p&gt;</content:encoded></item><item><title>Co-PLNet: A Collaborative Point-Line Network for Prompt-Guided Wireframe Parsing</title><link>https://arxiv.org/abs/2601.18252v1</link><guid>http://arxiv.org/abs/2601.18252v1</guid><pubDate>Mon, 26 Jan 2026 08:16:02 +0000</pubDate><dc:creator>Chao Wang</dc:creator><dc:creator>Xuanying Li</dc:creator><dc:creator>Cheng Dai</dc:creator><dc:creator>Jinglei Feng</dc:creator><dc:creator>Yuxiang Luo</dc:creator><dc:creator>Yuqi Ouyang</dc:creator><dc:creator>Hao Qin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Wireframe parsing aims to recover line segments and their junctions to form a structured geometric representation useful for downstream tasks such as Simultaneous Localization and Mapping (SLAM). Existing methods predict lines and junctions separately and reconcile them post-hoc, causing mismatches and reduced robustness. We present Co-PLNet, a point-line collaborative framework that exchanges spatial cues between the two tasks, where early detections are converted into spatial prompts via a Point-Line Prompt Encoder (PLP-Encoder), which encodes geometric attributes into compact and spatially aligned maps. A Cross-Guidance Line Decoder (CGL-Decoder) then refines predictions with sparse attention conditioned on complementary prompts, enforcing point-line consistency and efficiency. Experiments on Wireframe and YorkUrban show consistent improvements in accuracy and robustness, together with favorable real-time efficiency, demonstrating our effectiveness for structured geometry perception.
Published: 2026-01-26T08:16:02+00:00
Venue: arXiv
Score: 0.515 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chao Wang; Xuanying Li; Cheng Dai; Jinglei Feng; Yuxiang Luo; Yuqi Ouyang; Hao Qin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.515 (consider)&lt;/p&gt;
&lt;p&gt;Wireframe parsing aims to recover line segments and their junctions to form a structured geometric representation useful for downstream tasks such as Simultaneous Localization and Mapping (SLAM). Existing methods predict lines and junctions separately and reconcile them post-hoc, causing mismatches and reduced robustness. We present Co-PLNet, a point-line collaborative framework that exchanges spatial cues between the two tasks, where early detections are converted into spatial prompts via a Point-Line Prompt Encoder (PLP-Encoder), which encodes geometric attributes into compact and spatially aligned maps. A Cross-Guidance Line Decoder (CGL-Decoder) then refines predictions with sparse attention conditioned on complementary prompts, enforcing point-line consistency and efficiency. Experiments on Wireframe and YorkUrban show consistent improvements in accuracy and robustness, together with favorable real-time efficiency, demonstrating our effectiveness for structured geometry perception.&lt;/p&gt;</content:encoded></item><item><title>ScenePilot-Bench: A Large-Scale Dataset and Benchmark for Evaluation of Vision-Language Models in Autonomous Driving</title><link>https://arxiv.org/abs/2601.19582v1</link><guid>http://arxiv.org/abs/2601.19582v1</guid><pubDate>Tue, 27 Jan 2026 13:17:50 +0000</pubDate><dc:creator>Yujin Wang</dc:creator><dc:creator>Yutong Zheng</dc:creator><dc:creator>Wenxian Fan</dc:creator><dc:creator>Tianyi Wang</dc:creator><dc:creator>Hongqing Chu</dc:creator><dc:creator>Daxin Tian</dc:creator><dc:creator>Bingzhao Gao</dc:creator><dc:creator>Jianqiang Wang</dc:creator><dc:creator>Hong Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>In this paper, we introduce ScenePilot-Bench, a large-scale first-person driving benchmark designed to evaluate vision-language models (VLMs) in autonomous driving scenarios. ScenePilot-Bench is built upon ScenePilot-4K, a diverse dataset comprising 3,847 hours of driving videos, annotated with multi-granularity information including scene descriptions, risk assessments, key participant identification, ego trajectories, and camera parameters. The benchmark features a four-axis evaluation suite that assesses VLM capabilities in scene understanding, spatial perception, motion planning, and GPT-Score, with safety-aware metrics and cross-region generalization settings. We benchmark representative VLMs on ScenePilot-Bench, providing empirical analyses that clarify current performance boundaries and identify gaps for driving-oriented reasoning. ScenePilot-Bench offers a comprehensive framework for evaluating and advancing VLMs in safety-critical autonomous driving contexts.
Published: 2026-01-27T13:17:50+00:00
Venue: arXiv
Score: 0.514 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yujin Wang; Yutong Zheng; Wenxian Fan; Tianyi Wang; Hongqing Chu; Daxin Tian; Bingzhao Gao; Jianqiang Wang; Hong Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.514 (consider)&lt;/p&gt;
&lt;p&gt;In this paper, we introduce ScenePilot-Bench, a large-scale first-person driving benchmark designed to evaluate vision-language models (VLMs) in autonomous driving scenarios. ScenePilot-Bench is built upon ScenePilot-4K, a diverse dataset comprising 3,847 hours of driving videos, annotated with multi-granularity information including scene descriptions, risk assessments, key participant identification, ego trajectories, and camera parameters. The benchmark features a four-axis evaluation suite that assesses VLM capabilities in scene understanding, spatial perception, motion planning, and GPT-Score, with safety-aware metrics and cross-region generalization settings. We benchmark representative VLMs on ScenePilot-Bench, providing empirical analyses that clarify current performance boundaries and identify gaps for driving-oriented reasoning. ScenePilot-Bench offers a comprehensive framework for evaluating and advancing VLMs in safety-critical autonomous driving contexts.&lt;/p&gt;</content:encoded></item></channel></rss>