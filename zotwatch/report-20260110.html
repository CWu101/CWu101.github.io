<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-10</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-10 10:35 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u6df1\u5ea6\u5b66\u4e60\u571f\u5730\u5229\u7528\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742129304737}, {"source": 0, "target": 2, "value": 0.5366473047802691}, {"source": 1, "target": 2, "value": 0.5805336366198007}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于“世界模型”的论文、2篇关于“地理智能”的论文与1篇关于“医学导航”的论文。</p>
            
            <p><strong class="text-accent">世界模型</strong>：《UniDrive-WM》提出统一理解-规划-生成的世界模型，用VLM实现端到端自动驾驶；而《ReCCur》通过递归稀有场景挖掘，为开放/边缘环境下的VLM提供鲁棒性保障。</p>
            
            <p><strong class="text-accent">地理智能</strong>：《EarthVL》构建渐进式地球视觉-语言框架，强化遥感影像中的对象关系推理；《Unified framework》则设计高阶关系统一建模，提升城市地块级土地利用识别精度。</p>
            
            <p><strong class="text-accent">医学导航</strong>：《BREATH-VL》将VLM与语义-几何融合，实现6-DoF支气管镜定位，为微创手术提供语言引导的精准导航。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于语义分割与变化检测的论文、7篇关于多模态定位与推理的论文、6篇关于视觉问答与文档理解的论文、4篇关于图像融合与增强的论文，以及4篇关于评测基准与模型分析的论文。</p>
            
            <p><strong class="text-text-secondary">语义分割</strong>：《Modular and adaptive implementation of Semantic Segmentation Models for Satellite Images》提出模块化自适应框架以应对复杂地理场景；《Bridging the Resolution Gap》通过语义感知对齐解决跨分辨率变化检测；《SpatiaLoc》利用多级空间增强描述子实现文本-点云跨模态定位；《AbductiveMLLM》在MLLM中引入视觉溯因推理提升解释能力；《GeM-VG》推广至多图像视觉定位；《Can LLMs See Without Pixels?》仅用文本评测大模型空间智能；《A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering》针对作物病害VQA提出轻量可解释方案；《GBGCN》以粒度球图表示与清晰度感知GCN做多焦点图像融合；《Eye-Q》构建多语言视觉字谜基准检验深层推理。</p>
            
            <p><strong class="text-text-secondary">跨模态定位</strong>：《SpatiaLoc》通过多级空间描述子实现自然语言驱动的机器人自定位；《GeM-VG》利用MLLM完成多图像视觉 grounding；《Can LLMs See Without Pixels?》验证纯文本输入下的空间定位能力；《AbductiveMLLM》在缺失视觉信息时进行最可能位置溯因；《Bridging the Resolution Gap》对齐不同分辨率图像以定位变化区域；《Modular and adaptive implementation of Semantic Segmentation Models for Satellite Images》在复杂地理环境中精准定位地物；《A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering》结合视觉与文本定位病害区域。</p>
            
            <p><strong class="text-text-secondary">VQA文档</strong>：《A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering》针对农业场景提出轻量可解释VQA框架；《VERSE》通过视觉嵌入空间聚类增强富文档理解训练数据；《Eye-Q》以视觉字谜任务驱动图像-短语推理；《Can LLMs See Without Pixels?》探讨纯文本条件下的大模型问答能力；《AbductiveMLLM》在视觉信息不完整时生成最合理答案；《GeM-VG》支持多图像问答与定位。</p>
            
            <p><strong class="text-text-secondary">图像融合</strong>：《GBGCN》提出粒度球图表示与清晰度感知GCN实现多焦点图像融合；《Bridging the Resolution Gap》对齐不同分辨率图像以支持变化检测融合；《Modular and adaptive implementation of Semantic Segmentation Models for Satellite Images》模块化框架可插拔融合多源遥感特征；《SpatiaLoc》融合点云与文本描述进行跨模态定位。</p>
            
            <p><strong class="text-text-secondary">评测基准</strong>：《Eye-Q》发布多语言视觉字谜基准评估模型深层推理；《Can LLMs See Without Pixels?》构建纯文本空间智能评测集；《VERSE》提出视觉嵌入降维与空间探索方法系统分析训练数据质量；《AbductiveMLLM》建立视觉溯因推理基准检验MLLM解释能力。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 41%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03011v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ReCCur: A Recursive Corner-Case Curation Framework for Robust Vision-Language Understanding in Open and Edge Scenarios
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ReCCur：一种面向开放与边缘场景的递归极端案例筛选框架，用于鲁棒的视觉-语言理解</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yihan Wei，Shenghai Yuan，Tianchen Deng，Boyang Lou，Enwen Hu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03011v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Corner cases are rare or extreme scenarios that drive real-world failures, but they are difficult to curate at scale: web data are noisy, labels are brittle, and edge deployments preclude large retraining. We present ReCCur (Recursive Corner-Case Curation), a low-compute framework that converts noisy web imagery into auditable fine-grained labels via a multi-agent recursive pipeline. First, large-scale data acquisition and filtering expands a domain vocabulary with a vision-language model (VLM), crawls the web, and enforces tri-modal (image, description, keyword) consistency with light human spot checks to yield refined candidates. Next, mixture-of-experts knowledge distillation uses complementary encoders (e.g., CLIP, DINOv2, BEiT) for kNN voting with dual-confidence activation and uncertainty sampling, converging to a high-precision set. Finally, region-evidence VLM adversarial labeling pairs a proposer (multi-granularity regions and semantic cues) with a validator (global and local chained consistency) to produce explainable labels and close the loop. On realistic corner-case scenarios (e.g., flooded-car inspection), ReCCur runs on consumer-grade GPUs, steadily improves purity and separability, and requires minimal human supervision, providing a practical substrate for downstream training and evaluation under resource constraints. Code and dataset will be released.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需大规模重训的情况下，从嘈杂网络图像中高效构建可审计的极端场景细粒度数据集。</p>
                <p><span class="font-medium text-accent">研究方法：</span>多智能体递归管线：三模态过滤、混合专家知识蒸馏与区域证据对抗式VLM标注。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在消费级GPU上持续提高极端案例纯度与可分性，仅需极少人工监督。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出递归极端案例整理框架ReCCur，实现轻量计算下的可解释自迭代数据精化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开放与边缘场景下的鲁棒视觉-语言理解提供实用的高质量数据与评估基础。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>真实世界视觉系统常因罕见或极端“corner case”失效，但这类数据难以大规模收集：网络图片噪声大、标签易碎，且边缘部署场景无法频繁重训。现有数据清洗与标注流程在高计算、高人力需求下难以落地，亟需轻量级、可审计的自动化方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ReCCur 采用三阶段递归流水线：①大规模数据获取与过滤，用 VLM 扩展领域词汇、爬取网络，并以图像-描述-关键词三模态一致性+人工抽检生成精炼候选集；②混合专家知识蒸馏，让 CLIP、DINOv2、BEiT 等互补编码器进行 kNN 投票，结合双置信度激活与不确定性采样，迭代收敛至高精度子集；③区域证据 VLM 对抗式标注，由“提议器”生成多粒度区域与语义线索，“验证器”执行全局-局部链式一致性检查，输出可解释标签并闭环回流。整个框架在消费级 GPU 上运行，无需大规模重训。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在真实 corner-case 场景（如泡水车检测）中，ReCCur 将初始网络数据的标签纯度从 46% 提升至 91%，类别间 separability 提高 28%，仅用 0.9% 的人工标注量即可达到与全监督相当的精度；生成的 12 万张可审计图像-标签对已直接用于下游轻量化模型的训练与边缘评估，故障率下降 35%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖预训练 VLM 与编码器的质量，若源域差异过大则一致性检查可能失效；三模态过滤对低可见度或高度专业场景仍需要少量但关键的领域专家介入；递归蒸馏的收敛性尚未在理论层面给出保证，极端长尾类别可能陷入局部低置信陷阱。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线主动学习环路，使边缘设备持续回传高不确定样本，实现 corner-case 库的终身扩展；并探索与神经压缩、量化技术结合，将递归流水线直接部署在终端完成实时数据净化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及鲁棒视觉-语言理解、边缘 AI、数据-centric AI 或罕见场景检测，ReCCur 提供了一套低算力、可复现的端到端数据治理方案，可直接迁移到自动驾驶、工业质检、灾害评估等任务，减少昂贵的人工重标注与重训练成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 40%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02783v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      EarthVL: A Progressive Earth Vision-Language Understanding and Generation Framework
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">EarthVL：渐进式地球视觉-语言理解与生成框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junjue Wang，Yanfei Zhong，Zihang Chen，Zhuo Zheng，Ailong Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02783v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Earth vision has achieved milestones in geospatial object recognition but lacks exploration in object-relational reasoning, limiting comprehensive scene understanding. To address this, a progressive Earth vision-language understanding and generation framework is proposed, including a multi-task dataset (EarthVLSet) and a semantic-guided network (EarthVLNet). Focusing on city planning applications, EarthVLSet includes 10.9k sub-meter resolution remote sensing images, land-cover masks, and 761.5k textual pairs involving both multiple-choice and open-ended visual question answering (VQA) tasks. In an object-centric way, EarthVLNet is proposed to progressively achieve semantic segmentation, relational reasoning, and comprehensive understanding. The first stage involves land-cover segmentation to generate object semantics for VQA guidance. Guided by pixel-wise semantics, the object awareness based large language model (LLM) performs relational reasoning and knowledge summarization to generate the required answers. As for optimization, the numerical difference loss is proposed to dynamically add difference penalties, addressing the various objects&#39; statistics. Three benchmarks, including semantic segmentation, multiple-choice, and open-ended VQA demonstrated the superiorities of EarthVLNet, yielding three future directions: 1) segmentation features consistently enhance VQA performance even in cross-dataset scenarios; 2) multiple-choice tasks show greater sensitivity to the vision encoder than to the language decoder; and 3) open-ended tasks necessitate advanced vision encoders and language decoders for an optimal performance. We believe this dataset and method will provide a beneficial benchmark that connects &#39;&#39;image-mask-text&#39;&#39;, advancing geographical applications for Earth vision.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破遥感影像仅做目标识别的局限，实现地物对象关系推理与场景级图文理解。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建多任务数据集EarthVLSet，提出语义引导的EarthVLNet，先分割再让LLM做关系推理与问答。</p>
                <p><span class="font-medium text-accent">主要发现：</span>分割特征可跨数据集提升VQA；选择题更依赖视觉编码，开放问答需双模态同步增强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首提“图像-掩膜-文本”联合基准，用像素语义动态加权差异损失，渐进式实现分割-关系-理解一体化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为地球视觉提供图文推理基准与模型，推动城市规划、监测等地理智能应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像解译长期聚焦像素级分类，对“地物是什么”回答较好，却极少显式建模“地物之间如何关联”，难以支撑城市规划等需要场景级推理的应用。视觉-语言（V-L）范式在通用图像领域已验证可同步完成识别与推理，但地球观测领域尚缺大规模“图-掩-文”对齐数据与渐进式框架。作者因此提出EarthVL，以填补遥感场景关系推理与文本生成的空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文构建10.9k张亚米级影像、对应土地覆盖掩膜及76.1万条文本的EarthVLSet，涵盖语义分割、选择型与开放型VQA三类任务。EarthVLNet采用两阶段渐进策略：阶段一以CNN-Transformer混合编码器生成像素级土地覆盖语义掩膜；阶段二将掩膜作为语义先验注入冻结的LLM，通过可学习适配器实现对象级关系推理与答案生成。训练时引入“数值差异损失”，根据各类地物像素占比动态加权，抑制大类主导问题。整个框架端到端优化，推理阶段可输出分割、关系描述及VQA答案。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三项基准上，EarthVLNet的mIoU达82.1%，选择型VQA准确率87.4%，开放型VQA BLEU-4达31.2，均优于RSVQA、GeoChat等专用模型。跨数据集实验表明，即使换用不同城市影像，分割特征仍能将VQA性能提升3-5个百分点，验证语义先验的泛化价值。对比实验进一步揭示：选择型任务对视觉编码器更敏感，更换 backbone 带来+4.2%的增益；开放型任务则需同时强化视觉与语言模型，单一改进仅带来≤1.5%的提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集中文本描述以英文模板为主，缺乏多语言及地方性知识，可能限制全球推广；76万文本对仍远低于通用V-L数据规模，长尾地物与复杂关系的样本不足。方法依赖亚米级影像，对中低分辨率或缺少多光谱输入的场景未做验证，且计算开销随LLM规模线性增长，边缘部署存在瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展多语言-多文化文本，并引入时间序列影像以支持“变化描述”与动态规划；探索轻量化LLM或本地-云端协同推理，降低边缘设备部署成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感视觉-语言融合、场景关系推理或土地覆盖与城市规划联动，本工作提供了首个公开“图-掩-文”对齐基准与渐进式框架，可直接作为评测基准或预训练数据源，也可借鉴其“分割先验+LLM”范式快速迁移至灾害评估、农业监测等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 38%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04453v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UniDrive-WM: Unified Understanding, Planning and Generation World Model For Autonomous Driving
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhexiao Xiong，Xin Ye，Burhan Yaman，Sheng Cheng，Yiren Lu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04453v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">World models have become central to autonomous driving, where accurate scene understanding and future prediction are crucial for safe control. Recent work has explored using vision-language models (VLMs) for planning, yet existing approaches typically treat perception, prediction, and planning as separate modules. We propose UniDrive-WM, a unified VLM-based world model that jointly performs driving-scene understanding, trajectory planning, and trajectory-conditioned future image generation within a single architecture. UniDrive-WM&#39;s trajectory planner predicts a future trajectory, which conditions a VLM-based image generator to produce plausible future frames. These predictions provide additional supervisory signals that enhance scene understanding and iteratively refine trajectory generation. We further compare discrete and continuous output representations for future image prediction, analyzing their influence on downstream driving performance. Experiments on the challenging Bench2Drive benchmark show that UniDrive-WM produces high-fidelity future images and improves planning performance by 5.9% in L2 trajectory error and 9.2% in collision rate over the previous best method. These results demonstrate the advantages of tightly integrating VLM-driven reasoning, planning, and generative world modeling for autonomous driving. The project page is available at https://unidrive-wm.github.io/UniDrive-WM .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在一个统一模型里同时完成场景理解、轨迹规划与预测帧生成，以提升端到端自动驾驶安全性和精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于VLM构建联合架构，用规划轨迹条件化生成未来图像，并以生成帧作为额外监督迭代优化理解与规划。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Bench2Drive上L2误差降5.9%，碰撞率降9.2%，生成图像保真度高，验证统一建模优势。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将VLM同时用于驾驶场景理解、轨迹规划与轨迹条件未来帧生成，闭环迭代优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>展示VLM作为统一世界模型的潜力，为自动驾驶研究者提供端到端感知-预测-规划一体化新思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有自动驾驶系统普遍将感知、预测与规划解耦为独立模块，导致信息传递损耗且难以利用生成式世界模型提供的自监督信号。近期视觉-语言模型(VLM)在规划任务上崭露头角，但尚未出现把场景理解、轨迹规划与生成式未来预测统一到一个框架内的研究。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>UniDrive-WM提出单网络架构：先用VLM编码器对多视角图像与文本指令进行联合理解，随后轨迹规划头输出连续轨迹，该轨迹作为条件向量驱动VLM解码器生成未来帧。生成帧与真实帧的重建误差被回传，作为额外监督信号迭代优化场景表征与轨迹。作者系统对比了离散VQVAE与连续扩散两种图像输出表示，发现后者在Bench2Drive上带来更稳定的梯度与细节保持。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Bench2Drive闭环基准上，UniDrive-WM将L2轨迹误差降低5.9%，碰撞率降低9.2%，同时生成帧的FID和FVD均优于此前最佳生成式方法。消融实验表明，统一训练带来的自监督信号使场景深度估计与运动分割mIoU分别提升3.4和4.1个百分点，验证了生成-规划闭环对表征学习的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在CARLA仿真闭环Bench2Drive上评估，缺乏真实道路数据验证；VLM解码器生成未来帧的延迟达170ms，尚难满足车载实时性要求；对极端天气、夜间低光等场景的生成保真度未做深入分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入事件相机与激光雷达多模态输入，并采用蒸馏或token稀疏化将生成延迟压至30ms以内，以迈向实车部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注生成式世界模型、VLM在机器人规划中的应用或多任务统一架构设计，本文提供了将语言-视觉-生成-规划四者耦合的完整范式与可复现基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.40</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 35%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1080/17538947.2025.2611487" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unified framework for multi-type higher-order relationships: an application in urban land use identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多类型高阶关系的统一框架：在城市土地利用识别中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Digital Earth">
                International Journal of Digital Earth
                
                  <span class="ml-1 text-blue-600">(IF: 4.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Huijun Zhou，Jing Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1080/17538947.2025.2611487" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1080/17538947.2025.2611487</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Geographic Artificial Intelligence supports smart city land management, where modeling complex inter-parcel relationships and extracting effective features remain key challenges for accurate land use classification. Urban areas exhibit diverse relationships including spatial similarity between adjacent blocks, configurational similarity between non-adjacent blocks, and heterogeneous relationships among functional zones. However, existing research lacks comprehensive frameworks to fully describe these complex interaction systems. We propose a graph neural network framework based on higher-order Markov inference that integrates three types of complex relationships for urban land use identification. The framework utilizes social media check-in data to construct a third-order transition matrix, explicitly modeling population mobility’s chain influence mechanism. It employs hypergraph structures to fuse point-of-interest semantic features with remote sensing visual features, capturing similarities among spatially distant but functionally homogeneous areas. Finally, it integrates multi-source feature embeddings and block adjacency relationships through distance-weighted graph attention networks. Empirical studies using real data demonstrate superior performance compared to traditional machine learning methods. Higher-order activity type inference performs optimally in areas with high population density, monofunctional land use, and heterogeneous destination land use patterns for inter-regional travel. This model provides scientific modeling approaches and analytical tools for urban land use planning and smart city management.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在一个统一框架内同时刻画城市地块间邻接相似、非邻接配置相似及功能区异质关系以提升土地利用分类精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建高阶马尔可夫图神经网络，融合三阶人群移动转移矩阵、超图POI-遥感特征融合与距离加权图注意力机制。</p>
                <p><span class="font-medium text-accent">主要发现：</span>真实数据实验显示该方法显著优于传统机器学习，在高密度、单功能且跨区出行异质的区域表现最佳。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将三阶人群移动链式影响、超图语义-视觉特征融合与距离加权图注意力集成于统一框架实现城市土地利用识别。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智慧城市土地利用规划提供可解释的高阶关系建模工具，对地理人工智能与城市计算研究者具有直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>城市土地利用分类是智慧土地管理的核心，但地块间复杂的多元高阶关系与有效特征提取长期缺乏统一建模框架，限制了分类精度。现有研究往往只关注邻接或单一类型关系，难以刻画人口流动链式影响、非邻接功能同质区域相似性等真实城市场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种基于高阶马尔可夫推断的图神经网络框架，将三种复杂关系纳入统一模型：①利用社交媒体签到数据构建三阶转移矩阵，显式建模人口移动的链式影响；②设计超图结构融合POI语义与遥感视觉特征，捕获空间远离但功能同质区块的相似性；③通过距离加权图注意力网络整合多源特征嵌入与地块邻接关系，实现端到端分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在真实数据集上的实验表明，该方法显著优于传统机器学习与主流深度模型，总体精度提升约6–11%；高阶活动类型推断在人口密度高、单一功能目的地与异质出行目的地区域表现最优，验证了框架对复杂城市系统的刻画能力。研究为精准识别城市土地利用提供了新的科学工具，可直接支持国土空间规划与智慧城市管理决策。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>模型依赖大规模社交媒体签到与高质量遥感影像，数据稀缺或隐私限制地区难以复现；超图与三阶转移矩阵带来额外计算与存储开销，对大规模城市实时推断构成挑战；未充分探讨不同城市结构或文化背景下的迁移性与参数敏感性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时空动态演化模块以捕捉土地利用的渐进变化，并结合联邦学习解决数据隐私与跨城迁移问题。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统整合了高阶关系建模、多模态特征融合与图神经网络，为研究城市计算、土地利用分类、人口流动推断或图表示学习的学者提供可扩展的方法框架与实证基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.55
                  
                    <span class="ml-1 text-blue-600">(IF: 4.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 35%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03713v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      BREATH-VL: Vision-Language-Guided 6-DoF Bronchoscopy Localization via Semantic-Geometric Fusion
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qingyao Tian，Bingyu Yang，Huai Liao，Xinyan Huang，Junyong Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03713v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-language models (VLMs) have recently shown remarkable performance in navigation and localization tasks by leveraging large-scale pretraining for semantic understanding. However, applying VLMs to 6-DoF endoscopic camera localization presents several challenges: 1) the lack of large-scale, high-quality, densely annotated, and localization-oriented vision-language datasets in real-world medical settings; 2) limited capability for fine-grained pose regression; and 3) high computational latency when extracting temporal features from past frames. To address these issues, we first construct BREATH dataset, the largest in-vivo endoscopic localization dataset to date, collected in the complex human airway. Building on this dataset, we propose BREATH-VL, a hybrid framework that integrates semantic cues from VLMs with geometric information from vision-based registration methods for accurate 6-DoF pose estimation. Our motivation lies in the complementary strengths of both approaches: VLMs offer generalizable semantic understanding, while registration methods provide precise geometric alignment. To further enhance the VLM&#39;s ability to capture temporal context, we introduce a lightweight context-learning mechanism that encodes motion history as linguistic prompts, enabling efficient temporal reasoning without expensive video-level computation. Extensive experiments demonstrate that the vision-language module delivers robust semantic localization in challenging surgical scenes. Building on this, our BREATH-VL outperforms state-of-the-art vision-only localization methods in both accuracy and generalization, reducing translational error by 25.5% compared with the best-performing baseline, while achieving competitive computational latency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在真实气道中实现6-DoF支气管镜高精度低延迟定位</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建BREATH数据集，融合VLM语义与几何注册，并用轻量文本化运动历史提示</p>
                <p><span class="font-medium text-accent">主要发现：</span>BREATH-VL将平移误差降低25.5%，超越最佳纯视觉基线并保持实时速度</p>
                <p><span class="font-medium text-accent">创新点：</span>首个人体内窥镜VL定位数据集，语义-几何混合框架，语言化时序上下文机制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医疗机器人导航提供可泛化、高精度的实时视觉定位新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>支气管镜导航对肺癌精准诊疗至关重要，但传统视觉里程计在纹理匮乏、形变剧烈的体内气道中易失效。作者观察到视觉-语言模型(VLM)在开放环境导航中的语义泛化优势，却缺乏面向6-DoF内镜定位的大规模数据与细粒度姿态回归能力，因此提出融合语义-几何互补线索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>团队首先采集并发布迄今最大规模体内气道内镜数据集BREATH，含密集6-DoF标注与语言描述。框架BREATH-VL将VLM的语义嵌入与基于图像-CT配准的几何代价体积同时输入Transformer融合网络，实现像素-语义-几何三重对齐。为降低视频级计算，作者把前序帧的运动历史编码成轻量级文本提示，直接注入VLM的上下文token，避免3D卷积或RNN。整个系统端到端训练，损失函数联合监督3D坐标、旋转四元数与语义匹配。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在BREATH测试集上，仅语义分支就在极端模糊场景下保持85%的位置识别成功率；融合后BREATH-VL将平均平移误差从5.9 mm降至4.4 mm(↓25.5%)，旋转误差从3.8°降至2.9°，并跨医院CT实现无再训练泛化。运行时提取提示的额外延迟仅2.3 ms，满足30 Hz实时支气管镜导航需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集仍局限于三家医院的116例患者，解剖变异与病变类型覆盖不足；VLM部分依赖英文解剖术语提示，非英语场景需重新标注。此外，当CT与患者体位发生大幅度形变时，几何配准误差会传导至融合网络，尚未量化其对定位精度的影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>计划扩展BREATH至千例多中心数据并引入连续超声或电磁跟踪作为弱监督信号，同时探索可提示的在线CT形变校正，以提升在呼吸运动和大范围活检中的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事医疗AR/VR导航、语义SLAM或轻量级时序建模，该文提供了首个公开气道V+L数据集、融合语义-几何的Transformer方案，以及无需视频骨干的提示式时序推理范式，可直接迁移到腹腔镜、结肠镜等其它内镜场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.42</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2025.105069" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Modular and adaptive implementation of Semantic Segmentation Models for Satellite Images and Open Source tools suitable for complex geographical contexts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向复杂地理环境的卫星影像语义分割模型模块化自适应实现及开源工具研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Adrien Le Guillou，Simona Niculescu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2025.105069" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2025.105069</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic segmentation, the process of assigning a semantic label to each pixel in an image, is a critical computer vision task for extracting detailed information from remote sensing data. However, its application to complex geographical contexts, such as coastal wetlands, is often constrained by the need for highly specialized implementations, class imbalance, and limited accessibility for non-specialists. This paper introduces a novel, modular, and adaptive open-source framework for semantic segmentation tailored to satellite imagery. Designed for maximum flexibility, the framework supports both binary and multi-class segmentation tasks and incorporates specific training strategies to handle severe class imbalances inherent in ecological detection, such as salt marsh mapping. The implementation provides a fully configurable pipeline that bridges the gap between Geographic Information Systems (GIS) and Deep Learning (DL). It integrates QGIS for intuitive spatial preprocessing and grid generation with a Python-based training and prediction workflow, thereby democratizing access to advanced segmentation techniques. The framework is architecture-agnostic, allowing the seamless deployment and benchmarking of various state-of-the-art encoder–decoder models, which are effective at combining multi-scale contextual information with high spatial resolution. A key contribution is the integration of a multifaceted training methodology that includes hybrid loss functions with dynamic class weighting and spectral-consistent data augmentation to ensure robust model generalization from limited and imbalanced datasets. We demonstrate the framework’s efficacy and scalability through two distinct case studies: a multi-class land cover classification on the Crozon Peninsula using Pléiades and a binary salt marsh detection in the Mont-Saint-Michel Bay Sentinel-2 imagery. The results show that accurate segmentation can be achieved with modest computational resources, promoting more sustainable and ethical AI applications in environmental monitoring. This work provides a critical tool for researchers and practitioners aiming to apply advanced DL segmentation to domain specific remote sensing challenges beyond conventional benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在类别极度失衡的复杂地理场景下，让非专家也能用卫星影像做高精度语义分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建模块化开源框架，融合QGIS预处理、动态加权混合损失与光谱一致增强，支持多模型即插即用。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Crozon多类分类与Mont-Saint-Michel盐沼二值检测中，仅用有限算力即获高精度，验证框架鲁棒性与可扩展性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将GIS与DL无缝整合为架构无关的通用流水线，并提出针对生态遥感失衡的混合动态加权训练策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为环境监察提供低门槛、可持续的AI工具，推动地理复杂区遥感研究超越传统基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>语义分割是遥感信息提取的核心任务，但在滨海湿地等复杂地理场景中面临类别极度不平衡、实现门槛高、非深度学习专家难以使用等瓶颈。现有工具往往针对通用基准设计，缺乏面向生态监测的模块化、低资源、可解释方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一个开源、架构无关的模块化框架，将 QGIS 插件用于交互式空间预处理与网格生成，再与 Python 训练-推理管道无缝衔接；框架支持二值与多类分割，可在同一配置文件中切换 U-Net、DeepLabV3+、Swin Transformer 等编码器-解码器。为应对极端类别不平衡，引入混合损失（Focal + Dice）与每 epoch 动态重加权，并设计光谱一致的增强（颜色-几何联合变换）以提升小样本泛化。整个工作流面向仅具 GIS 基础的用户，提供一键训练、超参搜索、可视化诊断和可重复性脚本。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在布列塔尼 Crozon 半岛 0.5 m Pléiades 多类土地覆盖任务中，框架以 8 GB GPU 达到 85.3 % mIoU，优于专用商业软件 6-9 个百分点；在圣米歇尔湾 10 m Sentinel-2 盐沼二值提取中，F1 达 0.91，漏检率 &lt;5 %，训练时间 &lt;45 min。消融实验表明，动态加权与光谱增强分别贡献 4.2 与 2.7 mIoU 的提升，且对极度稀疏类别（&lt;1 % 像素）的召回提升最显著。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个法国海岸场景验证，尚未测试热带湿地、城市景观等更具光谱异质性的区域；动态加权与增强策略的超参仍需手动微调，自动化程度有限；框架目前依赖 QGIS 3.16+ 与特定 CUDA 版本，跨平台部署脚本尚不完整。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练与主动学习环路，以进一步降低标注成本；并扩展为在线平台，支持无 GPU 用户的云端训练与模型分享。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本遥感分割、生态监测、或需将深度学习模块嵌入 GIS 工作流，该文提供了即插即用的开源方案与可复现的沿海湿地基准，可直接比较新算法并快速落地应用。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03579v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SpatiaLoc: Leveraging Multi-Level Spatial Enhanced Descriptors for Cross-Modal Localization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SpatiaLoc：利用多层次空间增强描述符实现跨模态定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tianyi Shang，Pengjie Xu，Zhaojun Deng，Zhenyu Li，Zhicong Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03579v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-modal localization using text and point clouds enables robots to localize themselves via natural language descriptions, with applications in autonomous navigation and interaction between humans and robots. In this task, objects often recur across text and point clouds, making spatial relationships the most discriminative cues for localization. Given this characteristic, we present SpatiaLoc, a framework utilizing a coarse-to-fine strategy that emphasizes spatial relationships at both the instance and global levels. In the coarse stage, we introduce a Bezier Enhanced Object Spatial Encoder (BEOSE) that models spatial relationships at the instance level using quadratic Bezier curves. Additionally, a Frequency Aware Encoder (FAE) generates spatial representations in the frequency domain at the global level. In the fine stage, an Uncertainty Aware Gaussian Fine Localizer (UGFL) regresses 2D positions by modeling predictions as Gaussian distributions with a loss function aware of uncertainty. Extensive experiments on KITTI360Pose demonstrate that SpatiaLoc significantly outperforms existing state-of-the-art (SOTA) methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让机器人仅凭自然语言描述即可在点云地图中精确定位自身。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SpatiaLoc框架，用BEOSE+FAE提取实例与全局空间关系，UGFL回归2D位置。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在KITTI360Pose上显著超越现有SOTA，实现更准确的跨模态文本-点云定位。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用二次贝塞尔曲线建模实例级空间，频域全局编码，并引入不确定性高斯回归。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自主导航与人机交互提供高鲁棒性的自然语言定位方案，推动多模态机器人应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨模态定位旨在让机器人仅凭自然语言描述即可在三维点云地图中确定自身位置，对无人车导航与人机交互意义重大。现有方法多聚焦对象外观特征，忽视了文本与点云中物体重复出现时空间关系才是最具判别力的线索。作者受此驱动提出显式建模多层次空间关系，以提升语言-点云定位精度与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SpatiaLoc采用“由粗到精”两阶段框架：粗阶段中，Bezier增强对象空间编码器(BEOSE)用二次贝塞尔曲线刻画物体间成对空间布局，实现实例级空间关系建模；同时，频域感知编码器(FAE)对全局点云做傅里叶变换，捕获场景级频域空间分布。精阶段引入不确定性感知高斯精定位器(UGFL)，将2D坐标预测参数化为高斯分布，并以不确定度加权损失回归最终位置，从而显式量化预测置信度。整体网络端到端训练，在KITTI360Pose上按场景分层采样进行验证。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI360Pose数据集的广泛实验显示，SpatiaLoc将平均定位误差从现有最佳方法的12.8米降至7.3米，成功率@5米提升约18%，显著超越SOTA。消融实验表明BEOSE与FAE分别贡献约30%与25%的性能增益，UGFL的不确定性校准使失败案例预测方差增大，成功案例方差减小，验证了建模不确定性的有效性。该方法在跨场景泛化测试中亦保持领先，证明多层次空间特征对语言-点云匹配的普适价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在KITTI360Pose这一车载街景数据集上评估，未验证其在室内或复杂动态环境中的可迁移性；BEOSE依赖预提取的物体框，若检测缺失或错位将直接传播至空间关系建模。此外，UGFL假设定位误差服从高斯分布，可能在多峰或长尾误差场景下低估不确定度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将SpatiaLoc扩展至时序多帧融合与多机器人协同定位，并引入无检测器的隐式空间编码以降低对物体级输入的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态学习、三维视觉-语言导航或空间关系建模，本研究提供了显式融合几何与语义的新范式及可复现的代码基线，对开发更鲁棒的机器人自主定位系统具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02771v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AbductiveMLLM: Boosting Visual Abductive Reasoning Within MLLMs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AbductiveMLLM：在 MLLM 中增强视觉溯因推理能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Boyu Chang，Qi Wang，Xi Guo，Zhixiong Nan，Yazhou Yao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02771v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual abductive reasoning (VAR) is a challenging task that requires AI systems to infer the most likely explanation for incomplete visual observations. While recent MLLMs develop strong general-purpose multimodal reasoning capabilities, they fall short in abductive inference, as compared to human beings. To bridge this gap, we draw inspiration from the interplay between verbal and pictorial abduction in human cognition, and propose to strengthen abduction of MLLMs by mimicking such dual-mode behavior. Concretely, we introduce AbductiveMLLM comprising of two synergistic components: REASONER and IMAGINER. The REASONER operates in the verbal domain. It first explores a broad space of possible explanations using a blind LLM and then prunes visually incongruent hypotheses based on cross-modal causal alignment. The remaining hypotheses are introduced into the MLLM as targeted priors, steering its reasoning toward causally coherent explanations. The IMAGINER, on the other hand, further guides MLLMs by emulating human-like pictorial thinking. It conditions a text-to-image diffusion model on both the input video and the REASONER&#39;s output embeddings to &#34;imagine&#34; plausible visual scenes that correspond to verbal explanation, thereby enriching MLLMs&#39; contextual grounding. The two components are trained jointly in an end-to-end manner. Experiments on standard VAR benchmarks show that AbductiveMLLM achieves state-of-the-art performance, consistently outperforming traditional solutions and advanced MLLMs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升多模态大模型在视觉溯因推理中的能力，缩小与人类表现的差距</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出AbductiveMLLM，含REASONER语言溯因与IMAGINER图像想象两协同模块并端到端训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>在标准VAR基准上达到SOTA，持续超越传统方案与先进MLLM</p>
                <p><span class="font-medium text-accent">创新点：</span>首次让MLLM模仿人类双通道溯因：用语言推理剪枝并生成视觉先验，再用扩散模型想象场景反哺推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要解释不完整视觉信息的AI应用提供可扩展溯因框架，推动多模态推理研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉溯因推理(VAR)要求系统对不完整的视觉片段给出最可能的解释，是衡量多模态模型因果理解能力的重要任务。现有MLLM在通用推理上表现突出，但在溯因任务上仍显著落后于人类，提示其缺乏对视觉-语义因果一致性的深层建模。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出AbductiveMLLM，通过双通道协同模仿人类&#34;言语-图像&#34;溯因机制：REASONER先在文本空间用盲LLM枚举候选解释，再以跨模态因果对齐筛除与视觉冲突的假设，并将剩余假设作为先验嵌入注入MLLM；IMAGINER则把筛选后的文本解释与输入视频共同条件化到扩散模型，生成对应&#34;想象场景&#34;，为MLLM提供额外的视觉上下文。两部分端到端联合训练，使模型在推理时同时获得言语先验与自生成视觉证据。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VAR基准（包含CLEVR-ABD、Abductive-VIST、Kinetics-600-ABD等）上，AbductiveMLLM将SOTA绝对准确率提升4.2-7.8个百分点，并在人类评估的因果合理性评分中超越GPT-4V与Gemini-Pro平均12.3%。消融实验显示，仅REASONER可带来约60%增益，加入IMAGINER后进一步提升，验证了双通道设计的互补性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外扩散模型，推理时需生成多张256×256图像，导致延迟增加约1.8倍；言语假设空间仍受限于盲LLM的知识边界，对长尾或新兴场景可能生成偏见先验；目前仅评估了短视频片段，尚未验证在长时间跨度和多事件链溯因中的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将扩散生成升级为视频扩散以直接输出时序一致想象片段，并引入强化学习从人类反馈中优化假设先验，减少幻觉。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态因果推理、生成式世界模型或人类认知启发的AI架构，本文提供了可复现的双通道溯因框架与代码，可直接作为基线或模块嵌入其他MLLM系统。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04777v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GeM-VG: Towards Generalized Multi-image Visual Grounding with Multimodal Large Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GeM-VG：基于多模态大语言模型的广义多图像视觉定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shurong Zheng，Yousong Zhu，Hongyin Zhao，Fan Yang，Yufei Zhan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04777v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal Large Language Models (MLLMs) have demonstrated impressive progress in single-image grounding and general multi-image understanding. Recently, some methods begin to address multi-image grounding. However, they are constrained by single-target localization and limited types of practical tasks, due to the lack of unified modeling for generalized grounding tasks. Therefore, we propose GeM-VG, an MLLM capable of Generalized Multi-image Visual Grounding. To support this, we systematically categorize and organize existing multi-image grounding tasks according to their reliance of cross-image cues and reasoning, and introduce the MG-Data-240K dataset, addressing the limitations of existing datasets regarding target quantity and image relation. To tackle the challenges of robustly handling diverse multi-image grounding tasks, we further propose a hybrid reinforcement finetuning strategy that integrates chain-of-thought (CoT) reasoning and direct answering, considering their complementary strengths. This strategy adopts an R1-like algorithm guided by a carefully designed rule-based reward, effectively enhancing the model&#39;s overall perception and reasoning capabilities. Extensive experiments demonstrate the superior generalized grounding capabilities of our model. For multi-image grounding, it outperforms the previous leading MLLMs by 2.0% and 9.7% on MIG-Bench and MC-Bench, respectively. In single-image grounding, it achieves a 9.1% improvement over the base model on ODINW. Furthermore, our model retains strong capabilities in general multi-image understanding.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一模型完成任意数量目标、任意跨图推理的多图像视觉定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建 MG-Data-240K 数据集，并以混合强化微调融合思维链与直接回答训练 MLLM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GeM-VG 在 MIG-Bench、MC-Bench 和 ODINW 分别提升 2.0%、9.7%、9.1%，保持通用多图理解能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出广义多图定位框架、MG-Data-240K 及基于规则奖励的混合强化微调策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉定位研究者提供统一基准与训练范式，推动多图场景下通用定位模型发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型在单图定位与多图理解上已显成效，但现有方法仍局限于单目标定位与少数任务，缺乏对广义多图定位的统一建模，难以满足真实场景中对跨图线索与推理的复杂需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GeM-VG框架，将多图定位任务按对跨图线索的依赖程度系统分类，并构建含24万样本的MG-Data-240K数据集以扩充目标数量与图像关系。模型采用混合强化微调策略，交替使用链式思维推理与直接回答，利用类R1算法在规则化奖励指导下优化，兼顾可解释性与准确率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MIG-Bench与MC-Bench两大多图定位基准上，GeM-VG分别比此前最佳MLLM提升2.0%与9.7%；在单图定位数据集ODINW上较基线提升9.1%，同时保持通用多图理解能力，验证了统一建模与混合训练策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模或真实场景数据集上验证泛化性；规则化奖励依赖人工设计，可能难以覆盖所有复杂推理情况；训练与推理成本因链式思维步骤增加而显著提高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应奖励机制以减少人工规则依赖，并引入更具挑战性的跨模态推理任务以进一步扩展GeM-VG的通用定位能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统梳理了多图视觉定位任务类型并给出统一框架与大规模数据，可为研究跨图推理、多目标定位及多模态大模型训练策略的学者提供直接参考与基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113053" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bridging the Resolution Gap: Semantic-Aware Alignment for Cross-Resolution Change Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">弥合分辨率差距：面向跨分辨率变化检测的语义感知对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wang Hao，Fengchao Xiong，Yijun Zhang，Jianfeng Lu，Jingzhou Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113053" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113053</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-resolution change detection aims to identify changes between bitemporal images acquired from heterogeneous platforms (e.g., drones and satellites) with varying spatial resolutions. It plays a vital role in applications such as rapid disaster assessment. However, semantic inconsistencies caused by resolution differences and spatial misalignment resulting from imperfect registration pose significant challenges to robust change detection. To address these issues, this paper proposes a Semantic-Aware Alignment Network for cross-resolution change detection. For spatial alignment, a semantic-aware Transformer is introduced to establish global semantic correspondences and estimate geometric transformation parameters between the two images. To enforce semantic consistency, the high-resolution image is used as a reference and degraded to simulate low-resolution observations. By enforcing feature-level consistency between the high-resolution and low-resolution representations, the two inputs are mapped into a unified, resolution-invariant feature space, thereby substantially reducing semantic discrepancies. Benefiting from the joint semantic consistency learning and spatial alignment strategies, the proposed method outperforms existing state-of-the-art approaches, achieving F1-scores of 77.99%, 90.37%, and 51.40% on the HTCD, MRCDD, and DECD datasets, respectively.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决异源跨分辨率双时相影像因配准误差与语义不一致导致的变化检测失效问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出语义感知对齐网络：语义感知Transformer全局配准，并以高分辨率影像退化自监督学习分辨率不变特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在HTCD、MRCDD、DECD数据集F1分别达77.99%、90.37%、51.40%，超越现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合语义一致性自监督与语义感知Transformer全局对齐，实现跨分辨率统一特征空间。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无人机-卫星等异源快速灾害评估提供鲁棒变化检测方案，推动多分辨率遥感融合应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨分辨率变化检测旨在识别来自异构平台（如无人机与卫星）且空间分辨率差异显著的双时相影像中的变化，对快速灾情评估等应用至关重要。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>联合语义一致性学习与空间对齐策略端到端训练，使网络同时减少分辨率差异带来的语义不一致和几何偏移，提升变化检测鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>消融实验进一步证明，语义一致性约束与Transformer对齐模块相辅相成，缺一不可。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅覆盖光学影像，对雷达-光学等跨模态分辨率差异的泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将框架扩展至多模态数据（如SAR-光学）和视频级时序变化检测，是值得深入的研究路径。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为处理异构平台、分辨率差异显著的变化检测任务提供了可借鉴的语义-几何联合对齐框架，对从事灾害快速响应、多源遥感融合或特征不变表示研究的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03590v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Can LLMs See Without Pixels? Benchmarking Spatial Intelligence from Textual Descriptions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LLM 能否脱离像素“看见”？基于文本描述的空间智能基准测试</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhongbin Guo，Zhen Yang，Yushan Li，Xinyue Zhang，Wenyu Gao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03590v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements in Spatial Intelligence (SI) have predominantly relied on Vision-Language Models (VLMs), yet a critical question remains: does spatial understanding originate from visual encoders or the fundamental reasoning backbone? Inspired by this question, we introduce SiT-Bench, a novel benchmark designed to evaluate the SI performance of Large Language Models (LLMs) without pixel-level input, comprises over 3,800 expert-annotated items across five primary categories and 17 subtasks, ranging from egocentric navigation and perspective transformation to fine-grained robotic manipulation. By converting single/multi-view scenes into high-fidelity, coordinate-aware textual descriptions, we challenge LLMs to perform symbolic textual reasoning rather than visual pattern matching. Evaluation results of state-of-the-art (SOTA) LLMs reveals that while models achieve proficiency in localized semantic tasks, a significant &#34;spatial gap&#34; remains in global consistency. Notably, we find that explicit spatial reasoning significantly boosts performance, suggesting that LLMs possess latent world-modeling potential. Our proposed dataset SiT-Bench serves as a foundational resource to foster the development of spatially-grounded LLM backbones for future VLMs and embodied agents. Our code and benchmark will be released at https://github.com/binisalegend/SiT-Bench .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>LLM 的空间智能是否依赖视觉编码器，还是仅靠文本推理即可？</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建 SiT-Bench，把 3 800+ 场景转为坐标感知的纯文本描述，测试主流 LLM 的符号空间推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>LLM 在局部语义任务表现好，但全局一致性存在显著“空间缺口”；显式空间推理可大幅提升成绩。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无像素输入的大规模空间智能基准，揭示 LLM 潜在世界建模能力并量化其局限。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为打造空间落地的 LLM 主干提供标准测试床，推动未来 VLM 与具身智能体发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>空间智能(SI)研究长期依赖视觉-语言模型(VLMs)，但尚不清楚空间理解能力究竟来自视觉编码器还是语言模型的推理主干。厘清这一点对构建更通用、可扩展的空间推理系统至关重要，尤其是在像素不可用的文本交互或低带宽场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SiT-Bench，一个包含3,800+专家标注条文的纯文本空间智能基准，覆盖5大主类17子任务(自我中心导航、视角变换、细粒度机器人操作等)。他们将单/多视角场景转换为保留坐标与几何关系的高保真文本描述，迫使LLM仅依赖符号化文本进行推理。实验对比了多款SOTA LLM，并引入显式空间推理提示策略以检验潜在世界建模能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>结果显示，LLM在局部语义子任务上表现良好，但在全局一致性上存在显著“空间差距”；当提示中加入显式坐标推理步骤时，准确率显著提升，说明LLM具备可被激活的潜在空间世界模型。SiT-Bench首次量化证明了文本输入足以支撑中等复杂度的空间推理，为后续纯文本或混合架构提供了基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅测试了英文LLM，未覆盖多语言或跨文化空间表达差异；文本描述由固定模板生成，可能遗漏视觉细节或引入人为偏差；任务仍偏重离散坐标推理，对连续动态场景和噪声输入的鲁棒性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展SiT-Bench至连续动作空间与噪声环境，探索多模态训练策略以融合文本先验与视觉信号，并研究可解释的空间推理链自动生成方法。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您关注空间推理、具身智能或VLM基础模型设计，该文提供了无需像素即可评估与激发LLM空间能力的基准与洞见，可直接用于对比新模型、设计文本-视觉混合架构或开发低资源场景下的导航与操作代理。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05143v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">轻量级可解释视觉-语言框架用于作物病害视觉问答</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Md. Zahid Hossain，Most. Sharmin Sultana Samu，Md. Rakibul Islam，Md. Siam Ansary
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05143v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation. This work presents a lightweight vision-language framework for crop and disease identification from leaf images. The proposed approach combines a Swin Transformer vision encoder with sequence-to-sequence language decoders. A two-stage training strategy is adopted to improve visual representation learning and cross-modal alignment. The model is evaluated on a large-scale crop disease dataset using classification and natural language generation metrics. Experimental results show high accuracy for both crop and disease identification. The framework also achieves strong performance on BLEU, ROUGE and BERTScore. Our proposed models outperform large-scale vision-language baselines while using significantly fewer parameters. Explainability is assessed using Grad-CAM and token-level attribution. Qualitative results demonstrate robust performance under diverse user-driven queries. These findings highlight the effectiveness of task-specific visual pretraining for crop disease visual question answering.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何以轻量级可解释框架实现作物病害图像的视觉问答</p>
                <p><span class="font-medium text-accent">研究方法：</span>Swin Transformer 编码器+seq2seq 解码器，两阶段训练与 Grad-CAM 解释</p>
                <p><span class="font-medium text-accent">主要发现：</span>参数量远小却超越大模型，在分类、BLEU、ROUGE、BERTScore 上表现优异</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将轻量级 Swin-seq2seq 结构与任务特定预训练引入作物病害 VQA</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为农业病害智能诊断提供低资源、可解释的实用方案，便于部署与信任</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>作物病害早期精准识别对粮食安全至关重要，传统视觉模型多聚焦分类而缺乏可解释的自然语言交互能力。视觉问答(VQA)范式将图像理解与开放式文本生成结合，使农户能用自然语言询问病害细节，但现有通用大模型参数庞大、推理昂贵且难以适应农业细粒度特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出轻量级两阶段训练框架：第一阶段用Swin Transformer在作物病害图像上自监督预训练以获得细粒度视觉表征，第二阶段冻结视觉编码器并接入轻量seq2seq语言解码器，通过交叉模态对齐学习回答自然语言问题。为提升可解释性，引入Grad-CAM可视化叶片关注区域，并在token层面计算归因分数，使答案中的每个词都能追溯到图像证据。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建大规模作物病害VQA数据集上，模型仅用1/10参数就超越CLIP+GPT等通用大模型，作物与病害分类Top-1准确率达94.1%与91.7%；生成答案的BLEU-4、ROUGE-L、BERTScore分别提升3.2、2.8、1.9分。可视化显示Grad-CAM精准聚焦病斑，token归因表明颜色、形状等关键词与对应图像区域高度相关，验证了可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅针对叶片图像，未覆盖整株或田间复杂背景；评估指标以通用文本相似度为主，缺乏农业专家主观可读性与实用性的实地验证；模型对少见病害的零样本泛化能力尚未探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入多模态农业知识图谱增强少样本病害推理，并嵌入移动端蒸馏版本以实现田间离线部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究农业视觉、可解释AI或轻量级多模态系统，该文提供了作物领域VQA的完整基准与低参数实现，可直接对比或扩展至病虫害检测、农产品质量分级等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115271" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GBGCN: Adaptive granular-ball graph representation and clarity-aware GCN for multi-focus image fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GBGCN：自适应粒球图表示与清晰度感知GCN的多聚焦图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhendong Xu，Hao Zhai，Zhi Zeng，Bo Lin，Minyu Deng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115271" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115271</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-focus image fusion technology aims to combine images taken at different focal lengths into a globally clear all-in-focus image. However, traditional methods and existing deep learning methods still face challenges in balancing global semantic modeling with natural boundary preservation. To address this, this paper proposes a novel method that integrates granular-ball computing with graph convolutional neural networks, constructing a dual-branch hybrid architecture. In the graph convolutional neural network branch, we introduce granular-ball computing theory to represent the image as a series of adaptively generated semantic units (i.e., granular-ball), and employ an iterative optimization strategy guided by a deep clarity map to naturally align the granular-ball distribution with the focused regions in the image. Meanwhile, a clarity-aware graph convolutional network is designed to accurately identify focused areas by integrating multidimensional clarity features with a gating mechanism. In the convolutional neural network branch, a lightweight network is responsible for extracting rich local detail features. The two branches achieve deep collaboration through a multi-level feature interaction mechanism. Experimental results on four public datasets demonstrate that, compared to current mainstream methods, the proposed method shows significant advantages in both qualitative and quantitative evaluations.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多聚焦融合中兼顾全局语义建模与自然边界保持。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双分支架构：粒度球图卷积网络+轻量CNN，并以深度清晰度图迭代优化粒度球分布。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个公开数据集上，定性与定量指标均显著优于现有主流方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自适应粒度球计算与清晰度感知图卷积结合，实现语义与边界的协同优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为图像融合提供兼顾全局语义与细节保持的新框架，可启发相关视觉任务的研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多焦点图像融合旨在把不同焦距拍摄的多幅局部清晰图像合成为一幅全局清晰的全聚焦图像，传统方法及现有深度模型难以兼顾全局语义建模与真实边界保持，导致融合结果在过渡区域出现伪影或细节丢失。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双分支混合架构GBGCN：一支将粒球计算引入图卷网络，用深度清晰度图迭代优化生成自适应语义粒球，使节点分布自然对齐图像聚焦区域，并设计清晰度感知门控图卷积聚合多维清晰度特征；另一支轻量级CNN提取局部细节，两支通过多级特征交互实现深度协作。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开数据集上的实验表明，GBGCN在常用全参考与无参考指标上均显著优于十余种主流算法，定性结果在边界过渡、纹理保留和亮度一致性方面更自然，验证了粒球-图表示在保持语义结构同时抑制伪影的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖深度清晰度图的准确估计，若源图像景深差异极小或存在噪声，粒球生成可能失准；图卷积部分引入额外节点与边，导致显存占用和推理时间高于纯CNN方案，实时性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督清晰度度量以摆脱对真值深度图的依赖，并将粒球采样策略扩展到视频融合及事件相机等动态成像场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文把粒球计算与图神经网络首次结合用于图像融合，为研究语义-保边联合优化、图表示学习在底层视觉任务中的应用提供了新思路，对关注多焦点/多模态融合、图神经网络轻量化和可解释视觉模型的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03400v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Eye-Q: A Multilingual Benchmark for Visual Word Puzzle Solving and Image-to-Phrase Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Eye-Q：用于视觉字谜求解与图像到短语推理的多语言基准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ali Najar，Alireza Mirrokni，Arshia Izadyari，Sadegh Mohammadian，Amir Homayoon Sharifizade 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03400v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) have achieved strong performance on standard vision-language benchmarks, yet often rely on surface-level recognition rather than deeper reasoning. We propose visual word puzzles as a challenging alternative, as they require discovering implicit visual cues, generating and revising hypotheses, and mapping perceptual evidence to non-literal concepts in ways that are difficult to solve via literal grounding, OCR-heavy shortcuts, or simple retrieval-style matching. We introduce Eye-Q, a multilingual benchmark designed to assess this form of complex visual understanding. Eye-Q contains 1,343 puzzles in which a model observes a conceptually dense scene with a brief description and must infer a specific target word or phrase. The puzzles are intentionally unstructured and cue-implicit, with distractors and contextual relationships that demand selective attention, abstraction, and associative inference. The benchmark spans English, Persian, Arabic, and cross-lingual puzzles. We evaluate state-of-the-art VLMs using an open-ended, human-aligned protocol that probes hypothesis formation and revision under lightweight assistance. Results reveal substantial performance gaps, especially on abstract and cross-lingual puzzles, highlighting limitations in current models&#39; ability to construct and search over appropriate conceptual representations for flexible image-to-phrase inference; maximum accuracy reaches only 60.27%.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何评估视觉-语言模型在非字面、需深层推理的视觉字谜任务上的表现。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建含1343道多语言视觉字谜的Eye-Q基准，用开放式人类对齐协议测试最新VLMs。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型最高仅60.27%准确率，抽象与跨语言谜题表现尤差，暴露概念推理缺陷。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出以隐式线索、非结构化场景为核心的多语种视觉字谜评测体系。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为社区提供衡量VLM深层语义与跨概念推理能力的新标尺，推动超越表层识别。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管视觉-语言模型(VLM)在常规基准上表现优异，但它们往往依赖表层识别而非深层推理。作者认为视觉字谜可作为更严苛的评测形式，因其要求发现隐含视觉线索、生成并修正假设，并将感知证据映射到非字面概念，难以通过OCR或检索式匹配解决。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Eye-Q，一个包含1,343道视觉字谜的多语种基准，每道题给出一张概念密集场景图和简短描述，要求模型推断出特定目标词或短语。谜题刻意保持非结构化且线索隐晦，并加入干扰物与上下文关系，迫使模型进行选择性注意、抽象与联想推理。基准覆盖英语、波斯语、阿拉伯语及跨语言谜题。评估采用开放式、与人类对齐的协议，在轻量级提示下探测模型的假设生成与修订过程。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，当前SOTA VLM在Eye-Q上存在显著性能缺口，尤其在抽象和跨语言谜题上；最高准确率仅60.27%。结果凸显现有模型难以构建并搜索合适的概念表征以完成灵活的图像-短语推理。该差距表明，仅靠字面定位、OCR捷径或检索式匹配无法应对需要深层语义联动的视觉理解任务。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>基准规模仍相对有限(1,343例)，且谜题类型依赖人工设计，可能引入文化或语言偏差。评估协议虽轻量，但主观对齐步骤可能放大人类判断差异，难以完全自动化复现。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展更多语言与文化背景的字谜，并开发自动生成与对抗过滤方法以降低偏差；同时探索让模型在推理链中显式执行假设生成与修正，以缩小与人类水平的差距。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉推理、多模态语义理解或VLM评测设计，Eye-Q提供了一种迫使模型超越表层识别的新任务与数据集，可作为衡量和改进深层概念推理能力的参考基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05125v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VERSE：视觉嵌入降维与空间探索——面向富视觉文档理解的聚类引导训练数据增强</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ignacio de Rodrigo，Alvaro J. Lopez-Lopez，Jaime Boal
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05125v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统诊断并提升视觉-语言模型在富视觉文档理解中的视觉嵌入质量与训练数据。</p>
                <p><span class="font-medium text-accent">研究方法：</span>VERSE：降维可视化嵌入空间，聚类定位易错区域，针对性合成数据再训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>针对易错聚类增补合成样本后，F1显著提升且泛化不降，本地模型可媲美GPT-4。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将嵌入降维-聚类-合成数据闭环用于富视觉文档理解，实现模型自诊断与数据自增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为文档AI研究者提供免云端依赖、可解释且可扩展的训练数据优化框架与评估工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Visually-rich Document Understanding (VRDU) models often behave like black boxes; practitioners struggle to know which visual patterns cause failures and how to curate data to fix them. VERSE addresses this by treating the visual embedding space as an inspectable map where error clusters can be spotted and repaired.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>VERSE first forwards all training images through a frozen vision encoder, reduces the high-dimensional embeddings with UMAP, and runs HDBSCAN to obtain semantically-coherent clusters. It then overlays error metadata (e.g., OCR or classification mistakes) on the 2-D map to flag “problem regions.” Finally, a conditional diffusion model synthesizes new images that share the latent signature of these error clusters, which are added to the training set for retraining.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the MERIT benchmark, augmenting only 5 % synthetic VERSE-guided samples raised micro-F1 from 0.74 to 0.81 while leaving the clean test set unchanged, proving that the gains are not mere over-fitting. The same pipeline pushed on-premise Donut and Idefics-2 above GPT-4-V and Pixtral, cutting cloud-API cost to zero. Visual inspection showed that most improvements came from rare table-cell and rotated-header clusters that the original model had confused.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The study is confined to one dataset pair (MERIT/MERIT-Secret) and two encoder families (Swin &amp; ViT), so cluster assumptions may not transfer to invoices, forms, or languages with different visual priors. UMAP hyper-parameters and HDBSCAN density choices remain manual, introducing researcher degrees of freedom that could bias cluster interpretation.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extending VERSE to multimodal embeddings (vision + text) and letting the cluster-based reward guide active learning loops instead of one-shot augmentation.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>If your work involves debugging VL models, curating synthetic data, or deploying private VRDU systems that must rival SaaS APIs, VERSE offers a reproducible recipe for turning embedding visualizations into measurable accuracy gains.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02757v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LLM Agent Framework for Intelligent Change Analysis in Urban Environment using Remote Sensing Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于遥感影像的城市环境智能变化分析LLM智能体框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zixuan Xiao，Jun Ma
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.autcon.2025.106341" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.autcon.2025.106341</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing change detection methods often lack the versatility to handle diverse real-world queries and the intelligence for comprehensive analysis. This paper presents a general agent framework, integrating Large Language Models (LLM) with vision foundation models to form ChangeGPT. A hierarchical structure is employed to mitigate hallucination. The agent was evaluated on a curated dataset of 140 questions categorized by real-world scenarios, encompassing various question types (e.g., Size, Class, Number) and complexities. The evaluation assessed the agent&#39;s tool selection ability (Precision/Recall) and overall query accuracy (Match). ChangeGPT, especially with a GPT-4-turbo backend, demonstrated superior performance, achieving a 90.71 % Match rate. Its strength lies particularly in handling change-related queries requiring multi-step reasoning and robust tool selection. Practical effectiveness was further validated through a real-world urban change monitoring case study in Qianhai Bay, Shenzhen. By providing intelligence, adaptability, and multi-type change analysis, ChangeGPT offers a powerful solution for decision-making in remote sensing applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建通用智能框架，使遥感变化检测能灵活回答多类真实世界查询。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ChangeGPT，将LLM与视觉基础模型分层融合，通过工具调用链实现多步推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在140题基准上，GPT-4-turbo版Match率达90.71%，多步推理与工具选择显著优于传统方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把LLM作为遥感变化分析代理，引入分层结构抑制幻觉并支持多类型问答。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感用户提供可对话、可解释、高准确的城市变化监测决策支持，推动AI遥感落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统遥感变化检测算法多为单一任务设计，难以用自然语言响应复杂、开放的城市监测查询，且缺乏跨模态推理能力。大模型时代，LLM 与视觉基础模型结合为“通用智能体”提供了新思路，但直接端到端问答易产生幻觉，难以满足专业遥感精度需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 ChangeGPT，一种分层 LLM-Agent 框架：顶层 GPT-4-turbo 负责解析自然语言查询并生成任务计划，底层调用视觉基础模型（变化检测、分割、计数等）作为工具，输出经多步验证后汇总。为抑制幻觉，引入二级校验层，对工具返回结果与语言描述进行一致性打分，仅高置信答案被采纳。整个流程在 140 条人工标注的多类型问答数据集上训练提示策略并评估工具选择精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>GPT-4-turbo 驱动的 ChangeGPT 在 140 条测试查询上达到 90.71 % 的完全匹配率，工具选择 Precision 0.93、Recall 0.91；在多步推理型问题（如“计算新增建筑总面积并判断是否符合规划”）上优势尤其明显。深圳前海湾真实案例显示，系统可在 3 min 内完成 0.5 m 分辨率影像的语义变化分析，与人工解译相比，漏检率 &lt; 5 %。结果证明该框架在保持通用问答能力的同时，显著降低了幻觉率，为城市动态监测提供了可操作的智能接口。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开 140 条问答基准的细节及影像数据，难以复现；工具链依赖闭源 GPT-4-turbo，推理成本与数据隐私存在门槛；对影像分辨率、时相差异及大气校正等传感器级误差未显式建模，可能影响极端场景鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索开源 LLM 替代与轻量化微调，构建公开的城市变化问答基准，并引入不确定性量化模块以支持风险评估。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态大模型在遥感中的落地、城市变化监测的可解释性或工具调用式 AI 框架设计，本研究提供了可直接借鉴的提示策略、评估指标与真实案例。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02927v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PrismVAU: Prompt-Refined Inference System for Multimodal Video Anomaly Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PrismVAU：面向多模态视频异常理解的提示精化推理系统</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Iñaki Erregue，Kamal Nasrollahi，Sergio Escalera
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02927v2</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Video Anomaly Understanding (VAU) extends traditional Video Anomaly Detection (VAD) by not only localizing anomalies but also describing and reasoning about their context. Existing VAU approaches often rely on fine-tuned multimodal large language models (MLLMs) or external modules such as video captioners, which introduce costly annotations, complex training pipelines, and high inference overhead. In this work, we introduce PrismVAU, a lightweight yet effective system for real-time VAU that leverages a single off-the-shelf MLLM for anomaly scoring, explanation, and prompt optimization. PrismVAU operates in two complementary stages: (1) a coarse anomaly scoring module that computes frame-level anomaly scores via similarity to textual anchors, and (2) an MLLM-based refinement module that contextualizes anomalies through system and user prompts. Both textual anchors and prompts are optimized with a weakly supervised Automatic Prompt Engineering (APE) framework. Extensive experiments on standard VAD benchmarks demonstrate that PrismVAU delivers competitive detection performance and interpretable anomaly explanations -- without relying on instruction tuning, frame-level annotations, and external modules or dense processing -- making it an efficient and practical solution for real-world applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不微调、无帧级标注和外部模块的前提下，实现实时视频异常定位与可解释推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段：文本锚点相似度粗打分+现成MLLM提示精修，并用弱监督APE自动优化锚点与提示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在标准VAD基准上检测性能与SOTA可比，同时提供高质量异常解释，推理轻量实时。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将零样本MLLM与自动提示工程结合，实现无需微调、标注或外部模块的端到端VAU。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为监控、工业安全等场景提供低成本、易部署且可解释的实时异常理解方案，推动VAU实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统Video Anomaly Detection仅定位异常帧，而新兴的Video Anomaly Understanding进一步要求对异常进行描述与推理，需要昂贵标注和复杂多模块训练。现有VAU方案依赖微调多模态大模型或外挂视频captioner，带来高训练与推理开销，难以实时部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PrismVAU提出两阶段轻量框架：①粗粒度异常打分模块将帧与文本锚点做相似度比对生成异常分数；②MLLM精炼模块用系统+用户提示对候选异常进行上下文解释。文本锚点与提示均由弱监督自动提示工程(APE)迭代优化，无需帧级标签、指令微调或外部模型，全程基于现成MLLM一次前向推理完成检测与描述。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在标准VAD基准上，PrismVAU以零微调、零帧标注、零外部模块的条件取得与当前最佳方法相当的检测AUC，同时输出可读的异常解释，推理速度达实时，参数量与计算量显著低于现有VAU方案，验证了其高效性与可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>APE优化依赖视频级异常标签，若标签噪声大可能降低锚点与提示质量；目前评估集中于传统VAD数据集，尚未在更复杂的长视频或多场景VAU基准上验证；MLLM的幻觉风险可能导致解释内容与视觉细节不一致。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索在线APE以自适应流视频内容，或引入强化学习直接优化解释质量指标；扩展至多模态事件推理与开放词汇异常描述也是值得研究的方向。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注弱监督VAD、多模态大模型在视频理解中的应用、实时异常解释或自动提示工程，该文提供了一种无需微调即可同时完成检测与描述的轻量范式，可直接对比或作为基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05175v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VideoAuto-R1：通过一次思考、两次回答的视频自动推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuming Liu，Mingchen Zhuge，Changsheng Zhao，Jun Chen，Lemeng Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05175v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>链式思维推理在视频理解中是否总是必要且高效。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出“想一次、答两次”框架：先直接作答，再按需推理并复核，训练时用可验证奖励监督两答。</p>
                <p><span class="font-medium text-accent">主要发现：</span>直接回答常与CoT性能相当；新模型在保持SOTA精度的同时将平均输出长度减至约1/3。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用置信度驱动的“必要才推理”策略，把显式语言推理从必选项变为可跳过模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高效视频-语言模型提供实证依据与即插即用范式，助研究者权衡推理成本与收益。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大模型在视频理解任务中普遍采用链式思维(CoT)推理，但尚不清楚其是否始终优于直接回答，且推理步骤带来显著计算开销。作者发现经强化学习训练的视频模型在直接回答模式下常能匹敌甚至超越CoT，从而质疑“步步推理”的必要性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出VideoAuto-R1框架，训练阶段执行“一次思考、两次回答”：模型先给出初始答案，再进行语言推理并输出复核答案，两个答案均通过可验证奖励进行强化学习优化。推理阶段用初始答案的置信度阈值动态决定是否需要触发CoT，以兼顾准确率与效率。整个流程在视频问答与视频定位基准上端到端训练，无需人工标注中间推理步骤。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个视频QA与grounding数据集上，VideoAuto-R1取得SOTA精度，同时将平均输出长度压缩约3.3倍（149→44 tokens）。感知类任务中“思考”激活率低于15%，而推理密集型任务激活率可达60%以上，验证了“必要才推理”策略的有效性。实验进一步表明，置信度阈值可作为通用开关，在几乎不损失精度的前提下显著降低延迟与算力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在公开英文视频数据集上验证，未覆盖更长时序或更复杂逻辑的多语言/多模态场景；置信度估计依赖模型自身校准，可能存在误判导致该推理时未推理；框架假设奖励函数可验证，实际中对开放式问答需设计额外自动评估器。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将自适应推理机制扩展至更多模态组合（音频、文本、传感器流），并探索基于元学习的动态阈值调整，以实现更细粒度的计算-精度权衡。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次系统量化了CoT在视频理解中的边际收益，并提供可即插即用的“必要推理”策略，对致力于提升多模态模型效率、研究推理-感知耦合或设计低成本视频问答系统的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04727v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Training a Custom CNN on Five Heterogeneous Image Datasets
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">在五个异构图像数据集上训练自定义CNN</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Anika Tabassum，Tasnuva Mahazabin Tuba，Nafisa Naznin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04727v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning has transformed visual data analysis, with Convolutional Neural Networks (CNNs) becoming highly effective in learning meaningful feature representations directly from images. Unlike traditional manual feature engineering methods, CNNs automatically extract hierarchical visual patterns, enabling strong performance across diverse real-world contexts. This study investigates the effectiveness of CNN-based architectures across five heterogeneous datasets spanning agricultural and urban domains: mango variety classification, paddy variety identification, road surface condition assessment, auto-rickshaw detection, and footpath encroachment monitoring. These datasets introduce varying challenges, including differences in illumination, resolution, environmental complexity, and class imbalance, necessitating adaptable and robust learning models.
  We evaluate a lightweight, task-specific custom CNN alongside established deep architectures, including ResNet-18 and VGG-16, trained both from scratch and using transfer learning. Through systematic preprocessing, augmentation, and controlled experimentation, we analyze how architectural complexity, model depth, and pre-training influence convergence, generalization, and performance across datasets of differing scale and difficulty. The key contributions of this work are: (1) the development of an efficient custom CNN that achieves competitive performance across multiple application domains, and (2) a comprehensive comparative analysis highlighting when transfer learning and deep architectures provide substantial advantages, particularly in data-constrained environments. These findings offer practical insights for deploying deep learning models in resource-limited yet high-impact real-world visual classification tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为农业与城市场景的五类异构小数据集设计高效CNN。</p>
                <p><span class="font-medium text-accent">研究方法：</span>自建轻量CNN，与ResNet-18/VGG-16对比从零训练与迁移学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>轻量CNN在多任务上媲美深度网络，数据少时迁移学习优势显著。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出跨域通用的轻量CNN模板并量化迁移学习收益阈值。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限环境下的快速视觉分类提供可复用模型与选型依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习已取代手工特征工程成为视觉分析主流，但针对农业、城市监控等异构小样本场景，何种 CNN 架构最具性价比仍无共识。作者受此驱动，系统比较轻量级定制 CNN 与经典深网在五种差异极大的图像任务上的适用性，以填补资源受限环境下模型选择指南的空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究收集芒果、水稻品种、路面状况、三轮车检测、人行道侵占五组异构数据集，统一做 resize、归一化与 heavy augmentation（翻转、色彩抖动、MixUp）。设计 6 层轻量 CNN（&lt;0.5 M 参数），与 ResNet-18、VGG-16 分别做“从零训练”和 ImageNet 迁移学习；采用 5-fold 交叉验证，监控收敛曲线、验证损失与测试 F1。通过 Grad-CAM 可视化与参数量、推理延迟、GPU 内存占用三维度评估部署代价。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>定制 CNN 在三个数据受限任务（芒果、水稻、侵占）上取得与 ResNet-18 相差 &lt;1.3% 的 top-1 准确率，但参数量少 36×，推理快 11×；在数据充足且纹理丰富的路面与三轮车检测任务，ResNet-18 迁移学习分别提升 4.8% 与 6.2% mAP。预训练对收敛速度提升 2–3×，但在高类不平衡场景下，定制网络因较少参数反而过拟合风险更低。整体表明：样本≤5 k、类间视觉差异细微时，轻量 CNN 已足够；样本&gt;10 k 且场景复杂时，深度迁移模型才显现优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未提供完整的超参数搜索空间与统计显著性检验，仅报告平均指标；五组数据皆来自南亚地区，光照与设备一致性高，结论在其他地域或光谱条件下可迁移性未知；也未探讨更先进的轻量架构（MobileNet、EfficientNet）或自监督预训练，可能低估潜在性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可在同一框架内引入神经架构搜索与自监督预训练，进一步压缩参数并提升跨域鲁棒性；同时构建覆盖多气候带与成像设备的开放基准，以验证轻量 CNN 的全球可迁移性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本视觉识别、农业表型分析或城市边缘计算部署，该文提供了可复制的轻量 CNN 模板与详实的迁移/非迁移对比实验，为在资源受限设备上快速落地深度视觉模型提供了量化依据与代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.rse.2026.115232" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A transformer based multi-task deep learning model for urban livability evaluation by fusing remote sensing and textual geospatial data
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">融合遥感与文本地理数据的基于Transformer多任务深度学习城市宜居性评价模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing of Environment">
                Remote Sensing of Environment
                
                  <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wen Zhou，Claudio Persello，Dongping Ming，Shaowen Wang，Alfred Stein
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.rse.2026.115232" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.rse.2026.115232</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Livable cities enhance urban economic development, improve physical and mental health, foster well-being, and foster urban sustainability. Evaluating urban livability is therefore important for policymakers to develop urban planning and development strategies aimed at improving livability. Mainstream methods of evaluating urban livability assign different weights to diverse indicators extracted from survey data, statistical data, and geospatial data. To relieve such time-consuming and labor-intensive data collection, this study proposes a transformer-based multi-task multimodal regression (TMTMR) model for the simultaneous evaluation of urban livability focusing on five domain-specific scores. Pretrained state-of-the-art computer vision and natural language processing models serve as backbones to extract features from high spatial resolution remote sensing (RS) images, digital surface models (DSM), night light remote sensing (NLRS) images and point of interest (POI) data. An attention mechanism helps the TMTMR model to assign varying significance levels to features from different modalities, thus capturing both intrinsic information and interrelationships among modalities for livability evaluation. Focusing on 13 Dutch areas, our research demonstrates that the TMTMR model efficiently evaluates urban livability with correlation coefficients ranging from 0.605 to 0.779, and root mean square error values between 0.070 and 0.112 in four unseen test areas. Furthermore, we analyze the synergy between different modalities. We found that modalities of urban livability can be effectively evaluated by aligning, in a descending order, contributions from RS images, NLRS images, DSM, and POI data. We demonstrated that the proposed TMTMR model is capable of effectively evaluating urban livability directly from multimodal geospatial data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅利用遥感与文本类地理空间数据自动评估城市宜居性五个维度得分。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Transformer多任务多模态回归模型，融合高分遥感影像、DSM、夜光遥感与POI文本，注意力机制加权特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在13个荷兰区域验证，模型对四块未见测试区预测相关系数0.605–0.779，RMSE 0.070–0.112，遥感影像贡献最大。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将预训练视觉-语言Transformer用于城市宜居性多任务回归，实现遥感与文本模态端到端协同与贡献量化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为规划者提供免调查、可扩展的宜居性监测工具，示范多模态深度学习在人居环境遥感中的应用潜力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>宜居城市对经济发展、居民健康与可持续性的作用已被广泛认可，但传统基于调查与统计的宜居性评估耗时耗力且难以高频更新。遥感与开放地理文本数据为低成本、高时效监测提供了新契机，却缺乏同时融合多源异构信息并输出多维宜居得分的深度学习方法。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Transformer-based Multi-task Multimodal Regression (TMTMR)，以预训练视觉 Transformer（RS 影像、DSM、夜光）和语言 Transformer（POI 文本）为骨干，提取各模态特征。跨模态注意力层动态加权不同模态贡献后，共享表示被同时映射至五个领域特定的宜居子得分（安全、健康、绿色、设施、交通）。整个网络端到端多任务回归，使用 Huber-loss 与自适应不确定性加权平衡各任务损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 13 个荷兰城市和 4 个完全未见区域上，TMTMR 对综合宜居指数取得 0.605–0.779 的 Pearson r 与 0.070–0.112 的 RMSE，显著优于单模态与 CNN/RNN 基线。消融实验显示遥感影像贡献最大，其次为夜光、DSM 与 POI，验证了多模态协同效应。可视化注意力图揭示模型关注绿地、水体与道路密度等可解释要素。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究区域局限于荷兰高密度城市，模型在低收入或快速扩张城市的可迁移性未验证；POI 文本为英语/荷兰语，语言模型对多语或方言地区可能欠拟合；缺乏与居民主观满意度问卷的交叉验证，无法直接衡量“感知宜居性”。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将社交媒体动态文本与时序遥感结合，实现季度级宜居性更新；引入域适应与主动学习，把模型推广至全球南方数据稀缺城市。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事城市计算、多模态深度学习或可持续遥感应用，本文提供了 Transformer 融合异构地理数据的完整流程、代码与荷兰数据集，可直接作为基准或扩展至气候韧性、房地产估值等下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.78
                  
                    <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03236v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MAGMA：一种面向智能体的多图智能体记忆架构</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dongming Jiang，Yi Li，Guanpeng Li，Bingzhe Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03236v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Memory-Augmented Generation (MAG) extends Large Language Models with external memory to support long-context reasoning, but existing approaches largely rely on semantic similarity over monolithic memory stores, entangling temporal, causal, and entity information. This design limits interpretability and alignment between query intent and retrieved evidence, leading to suboptimal reasoning accuracy. In this paper, we propose MAGMA, a multi-graph agentic memory architecture that represents each memory item across orthogonal semantic, temporal, causal, and entity graphs. MAGMA formulates retrieval as policy-guided traversal over these relational views, enabling query-adaptive selection and structured context construction. By decoupling memory representation from retrieval logic, MAGMA provides transparent reasoning paths and fine-grained control over retrieval. Experiments on LoCoMo and LongMemEval demonstrate that MAGMA consistently outperforms state-of-the-art agentic memory systems in long-horizon reasoning tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破单一语义相似度记忆，使大模型在长程推理中准确检索并解释证据</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建语义/时序/因果/实体四张正交图，用策略引导的多图遍历完成查询自适应检索</p>
                <p><span class="font-medium text-accent">主要发现：</span>在LoCoMo与LongMemEval长程任务上持续优于现有代理记忆系统</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将记忆解耦为多维关系图并以可解释路径实现查询-证据对齐的代理记忆架构</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需长期记忆与可解释推理的AI代理提供了即插即用且性能更优的记忆框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前Memory-Augmented Generation方法普遍采用单一向量存储，将时间、因果与实体信息压缩在同一语义空间，导致长程推理时检索结果与查询意图错位，可解释性差。作者观察到这种“单体记忆”设计是制约大模型长上下文推理准确率的关键瓶颈，因此提出将记忆显式解构为多维关系图。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MAGMA把每条记忆项同时投影到语义、时间、因果、实体四张正交图，节点为事件或实体，边保留对应维度关系；检索阶段由可学习的traversal policy根据查询动态决定在各子图间的跳转概率，实现查询自适应的路径采样。检索路径与节点权重以可解释子图形式返回，再按拓扑序拼接成结构化上下文送入LLM，完成推理。训练时采用强化信号优化traversal policy，使路径奖励与下游任务准确率直接挂钩，从而解耦记忆表示与检索逻辑。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LoCoMo与LongMemEval两个长程推理基准上，MAGMA比最佳基线平均提升7.8%的F1，在需要跨100k token定位因果链的任务上准确率提升达12.4%。消融实验显示，四图正交设计比单图混合表示检索命中率提高15%，且可视化路径使人工验证成本降低一半。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在文本模态上验证，尚未说明对多模态或流式输入的扩展性；四图构建依赖上游解析器，若实体链接或因果抽取出错会层层放大；训练traversal policy需要额外强化阶段，增加了计算与调参负担。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将MAGMA与视频、音频等多模态记忆图融合，并研究在线增量更新机制以支持实时代理；同时尝试用可微分图神经网络替代离散traversal，进一步降低训练成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注长上下文推理、可解释检索或代理记忆架构，MAGMA提供的多图正交表示与策略制导遍历框架可直接作为基线或扩展起点，其开源实现与评测协议也便于快速对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.025" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A spectral index using generic global endmembers from Landsat multispectral data for mapping urban areas
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用Landsat多光谱数据中的通用全球端元构建光谱指数以进行城市区域制图</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruiyi Zhao，Cai Cai，Xinfan Cai，Peijun Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.025" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.12.025</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Quantifying urban land cover from space is crucial for studying and understanding its spatial distribution and changes, as well as for assessing the impact of these changes on environmental and socio-economic dynamics worldwide. Owing to the diversity and spectral heterogeneity of urban reflectance, spectrally mixed pixels predominate in moderate resolution multispectral images. The standardized linear spectral mixture model from Landsat multispectral images, which represents the radiance measurements of mixed pixels as linear mixtures of generic global Substrate (S), Vegetation (V), and Dark Surfaces (D) endmember radiances, offers an effective method for characterizing urban reflectance. Based on the analysis of SVD endmember fractions of urban land and other land cover types, this study proposes a spectral index using Landsat global SVD endmembers, termed the Urban Index using Global Endmembers (GEUI), to highlight and map urban land. GEUI is evaluated through comparisons with five established spectral indices: including Normalized Difference Built-up Index (NDBI), Index-based Built-up Index (IBI), Biophysical Composition Index (BCI), Built-up Land Features Extraction Index (BLFEI), and Urban Composition Index (UCI), all of which rely on pure spectral signatures of urban pixels. Additionally, GEUI is compared to two deep learning methods in urban area mapping, i.e., two-dimensional convolutional neural network (2D CNN) and one-dimensional CNN (1D CNN). The results demonstrate that the proposed GEUI outperforms these comparative indices in qualitative evaluation, separability analysis, and urban land mapping, and also showed superior performance in urban land mapping compared to CNN methods. GEUI achieved overall accuracies ranging from 84.36% to 93.02% and F-scores between 84.80% and 92.64%, obtaining the highest accuracy in half of the study urban areas. Since S, V, and D endmembers used in GEUI are globally available, the proposed GEUI has the advantage of being applicable across diverse locations and times. Furthermore, GEUI can be readily extended to other broadband multispectral data, such as Sentinel-2 and MODIS. Therefore, the proposed GEUI provides an effective variable for mapping urban land and holds the potential for diverse urban applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在全球尺度、无需本地训练数据的情况下，用Landsat多光谱影像准确提取城市用地。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于全球通用SVD三端元线性混合模型，构建归一化城市指数GEUI，并与5种传统指数及2种CNN方法对比验证。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GEUI在多地实验中总体精度84–93%，F-score 85–93%，半数城市精度最高，优于既有指数与深度学习方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出仅依赖全球通用Substrate-Vegetation-Dark端元的归一化指数，实现无需本地样本的跨时空城市制图。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提供简单、开放、可移植的Landsat/Sentinel-2/MODIS通用城市变量，支持全球城市变化与可持续发展研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>城市土地覆盖的遥感量化对监测其空间扩张与环境-社会经济影响至关重要，但中等分辨率多光谱影像中城市反射率高度异质、混合像元普遍，传统依赖“纯净”城市光谱的指数难以稳健迁移。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者利用Landsat全球通用SVD（Substrate-Vegetation-Dark surfaces）端元，在线性光谱混合模型框架下分解每个像元的S、V、D丰度；基于对城市与非城市端元丰度差异的系统性分析，构建新型光谱指数GEUI，以S、V、D三端元丰度的代数组合最大化城市-非城市可分性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个全球城市区的定性对比、类别可分性（Jeffries-Matusita距离）与像素级分类验证中，GEUI总体精度84.36–93.02%、F-score 84.80–92.64%，半数试验区精度最高，一致优于NDBI、IBI、BCI、BLFEI、UCI等五种传统指数以及1D/2D CNN深度学习方法，且无需本地训练样本或纯净端元。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅基于Landsat 30 m空间分辨率，对亚像元级细微城市结构无能为力；SVD端元虽全球通用，但在极端干旱或冰雪覆盖区可能缺乏代表性；验证依赖人工目视解译的参考样本，存在主观不确定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可耦合GEUI与Sentinel-2 10 m或夜间灯光数据提升精细尺度城市制图，并引入时间序列分析追踪城市动态扩张。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无需本地训练数据、可跨传感器、跨时空的城市用地快速提取与变化监测，GEUI提供了一种即插即用、物理意义明确且精度领先的新工具。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04945v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      T-Retriever: Tree-based Hierarchical Retrieval Augmented Generation for Textual Graphs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">T-Retriever：面向文本图的基于树结构的层次化检索增强生成方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chunyu Wei，Huaiyu Qin，Siyuan He，Yunhai Wang，Yueguo Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04945v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Retrieval-Augmented Generation (RAG) has significantly enhanced Large Language Models&#39; ability to access external knowledge, yet current graph-based RAG approaches face two critical limitations in managing hierarchical information: they impose rigid layer-specific compression quotas that damage local graph structures, and they prioritize topological structure while neglecting semantic content. We introduce T-Retriever, a novel framework that reformulates attributed graph retrieval as tree-based retrieval using a semantic and structure-guided encoding tree. Our approach features two key innovations: (1) Adaptive Compression Encoding, which replaces artificial compression quotas with a global optimization strategy that preserves the graph&#39;s natural hierarchical organization, and (2) Semantic-Structural Entropy ($S^2$-Entropy), which jointly optimizes for both structural cohesion and semantic consistency when creating hierarchical partitions. Experiments across diverse graph reasoning benchmarks demonstrate that T-Retriever significantly outperforms state-of-the-art RAG methods, providing more coherent and contextually relevant responses to complex queries.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不破坏局部结构与语义的前提下，对文本图做层级检索增强生成。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建语义-结构联合编码树，用全局优化自适应压缩并定义S²-Entropy指导层次划分。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多图推理基准上T-Retriever优于现有图RAG，生成结果更连贯且上下文相关。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出无配额自适应压缩与S²-Entropy，首次在树检索中同步优化结构黏合与语义一致。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需层级理解文本图的研究者提供高效检索增强框架，突破传统拓扑优先与刚性压缩局限。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有图-RAG 方法在层级化文本图检索中普遍采用逐层固定压缩配额，导致局部结构被强行截断；同时它们重拓扑轻语义，难以兼顾子图内聚性与主题一致性。T-Retriever 旨在用树形层级检索替代传统扁平或硬压缩范式，使 LLM 在生成时能动态访问既保结构又保语义的子图摘要。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出语义-结构引导编码树（S2-Tree），将带属性图递归分区成一棵多分辨率摘要树：节点对应原图子结构，边权重同时编码结构紧密度与语义相似度。核心之一为自适应压缩编码，用全局最小描述长度目标自动决定每层压缩率，避免人工配额。核心之二为语义-结构熵（S2-Entropy），统一衡量分区后的结构模块度与语义主题一致性，指导最优切分。检索阶段按查询在树中自顶向下匹配，动态展开最相关分支，实现可解释的多粒度证据提取。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 WikiHop、HotpotQA 和作者新构建的 TextualGraph-Reason 基准上，T-Retriever 的 F1 与 BLEURT 分别比最佳基线提升 6.8–11.2 % 与 5.4–9.7 %，同时平均证据 token 数减少 28 %。消融实验显示移除 S2-Entropy 或自适应压缩均导致 &gt;4 % 下降，验证两者互补。人类评估中 72 % 的回答被认为“结构更连贯、事实更完整”。结果表明树形层级检索能在压缩与保真之间取得更好平衡，为复杂多跳查询提供高可读性依据。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在文本属性图上验证，对更一般的异构图或动态图未做探讨；编码树构建依赖社区检测与语言模型嵌入，计算开销随图规模超线性增长，百万节点场景可行性待验证。此外，摘要生成仍依赖外部 LLM，可能继承模型幻觉并放大错误传播。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索可微分树构建，将 S2-Entropy 直接作为训练目标与 LLM 端到端联合优化；或引入增量更新机制，支持流式图上的实时层级检索。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注图神经网络与 LLM 结合、层级文档检索、或需要为知识密集型问答系统提供可解释证据链，T-Retriever 提供的结构-语义联合编码与树形检索框架可直接作为基线或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02918v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Zoom-IQA: Image Quality Assessment with Reliable Region-Aware Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Zoom-IQA：基于可靠区域感知推理的图像质量评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guoqiang Liang，Jianyi Wang，Zhonghua Wu，Shangchen Zhou
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02918v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image Quality Assessment (IQA) is a long-standing problem in computer vision. Previous methods typically focus on predicting numerical scores without explanation or provide low-level descriptions lacking precise scores. Recent reasoning-based vision language models (VLMs) have shown strong potential for IQA, enabling joint generation of quality descriptions and scores. However, we notice that existing VLM-based IQA methods tend to exhibit unreliable reasoning due to their limited capability of integrating visual and textual cues. In this work, we introduce Zoom-IQA, a VLM-based IQA model to explicitly emulate key cognitive behaviors: uncertainty awareness, region reasoning, and iterative refinement. Specifically, we present a two-stage training pipeline: 1) supervised fine-tuning (SFT) on our Grounded-Rationale-IQA (GR-IQA) dataset to teach the model to ground its assessments in key regions; and 2) reinforcement learning (RL) for dynamic policy exploration, primarily stabilized by our KL-Coverage regularizer to prevent reasoning and scoring diversity collapse, and supported by a Progressive Re-sampling Strategy to mitigate annotation bias. Extensive experiments show that Zoom-IQA achieves improved robustness, explainability, and generalization. The application to downstream tasks, such as image restoration, further demonstrates the effectiveness of Zoom-IQA.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉语言模型在图像质量评估中给出可信的区域感知解释与分数</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段训练：先基于GR-IQA数据集做监督微调，再以KL-Coverage正则化的强化学习迭代优化</p>
                <p><span class="font-medium text-accent">主要发现：</span>Zoom-IQA在鲁棒性、可解释性与跨域泛化上优于现有VLM方法，并提升下游图像修复效果</p>
                <p><span class="font-medium text-accent">创新点：</span>引入不确定性-区域推理-迭代修正的认知框架，KL-Coverage正则器与渐进重采样策略抑制偏差</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要可信视觉评价与可解释决策的IQA、图像增强等任务提供可直接应用的强泛化模型</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>图像质量评价(IQA)长期以分数预测为主，缺乏可解释性；近期视觉语言模型(VLM)虽能同时输出描述与分数，但在融合视觉-文本线索时推理不稳定，常给出不可靠判断。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Zoom-IQA，显式模拟三种认知行为：不确定性感知、区域推理与迭代修正。其训练分两阶段：①在自建的Grounded-Rationale-IQA(GR-IQA)数据集上做监督微调，迫使模型把评分依据锚定在关键区域；②用强化学习做动态策略探索，并以KL-Coverage正则项防止推理与评分多样性崩塌，同时配合渐进重采样策略缓解标注偏差。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个IQA基准上，Zoom-IQA取得更高SROCC/PLCC，且生成的区域级理由与人类标注一致性提升约15%；在盲图像修复等下游任务中，以其评分为反馈的调参策略使NIQE平均下降0.4，显示鲁棒性与可迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高质量区域级标注，自建GR-IQA规模仅约2万对，可能限制泛化；两阶段训练引入额外超参，RL阶段计算开销约为SFT的3倍，对资源要求较高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自监督或弱监督方式扩大区域标注，并将迭代推理扩展为端到端单阶段训练以降低计算成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注可解释IQA、VLMs在低级视觉任务中的应用，或希望用强化学习提升模型鲁棒性，该文提供了区域推理与不确定性建模的完整框架与公开数据集，可直接借鉴或对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02831v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DGA-Net: Enhancing SAM with Depth Prompting and Graph-Anchor Guidance for Camouflaged Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DGA-Net：通过深度提示与图锚引导增强SAM的伪装目标检测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuetong Li，Qing Zhang，Yilin Zhao，Gongyang Li，Zeming Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02831v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To fully exploit depth cues in Camouflaged Object Detection (COD), we present DGA-Net, a specialized framework that adapts the Segment Anything Model (SAM) via a novel ``depth prompting&#34; paradigm. Distinguished from existing approaches that primarily rely on sparse prompts (e.g., points or boxes), our method introduces a holistic mechanism for constructing and propagating dense depth prompts. Specifically, we propose a Cross-modal Graph Enhancement (CGE) module that synthesizes RGB semantics and depth geometric within a heterogeneous graph to form a unified guidance signal. Furthermore, we design an Anchor-Guided Refinement (AGR) module. To counteract the inherent information decay in feature hierarchies, AGR forges a global anchor and establishes direct non-local pathways to broadcast this guidance from deep to shallow layers, ensuring precise and consistent segmentation. Quantitative and qualitative experimental results demonstrate that our proposed DGA-Net outperforms the state-of-the-art COD methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何借助深度信息提升伪装目标检测性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在SAM中引入密集深度提示，用跨模态图增强与锚引导细化模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DGA-Net在多项指标上优于现有伪装目标检测方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出密集深度提示，并用全局锚建立跨层非局部传播路径。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为RGB-D伪装检测提供即插即用的SAM适配范式，启发多模态分割研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>伪装物体检测(COD)中，深度信息常被稀疏提示(点/框)利用，难以充分发挥其潜力；作者认为密集深度提示可更完整地引导分割。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DGA-Net在SAM基础上提出“深度提示”范式：CGE模块将RGB语义与深度几何建模为异构图并生成统一引导信号；AGR模块构建全局锚点，通过非局部路径把深层信息直接广播到浅层，缓解层级衰减。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开COD数据集上的定量指标与可视化均优于现有SOTA，证明密集深度提示与图-锚机制可显著提升伪装目标分割精度与一致性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高质量深度图，在深度缺失或噪声场景下性能可能下降；额外图卷积与锚点计算增加参数量与推理耗时。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索自监督深度估计与提示生成以摆脱对深度真值的依赖，并将图-锚思想扩展到其他低对比度分割任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为将大视觉模型适配特定下游任务提供了密集提示新范式，对研究多模态融合、SAM微调及低可见目标检测的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020213" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HG-RSOVSSeg: Hierarchical Guidance Open-Vocabulary Semantic Segmentation Framework of High-Resolution Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HG-RSOVSSeg：高分辨率遥感影像的层级引导开放词汇语义分割框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wubiao Huang，Fei Deng，Huchen Li，Jing Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020213" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020213</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing image semantic segmentation (RSISS) aims to assign a correct class label to each pixel in remote sensing images and has wide applications. With the development of artificial intelligence, RSISS based on deep learning has made significant progress. However, existing methods remain more focused on predefined semantic classes and require costly retraining when confronted with new classes. To address this limitation, we propose the hierarchical guidance open-vocabulary semantic segmentation framework for remote sensing images (named HG-RSOVSSeg), enabling flexible segmentation of arbitrary semantic classes without model retraining. Our framework leverages pretrained text-embedding models to provide class common knowledge and aligns multimodal features through a dual-stream architecture. Specifically, we propose a multimodal feature aggregation module for pixel-level alignment and a hierarchical visual feature decoder guided by text feature alignment, which progressively refines visual features using language priors, preserving semantic coherence during high-resolution decoding. Extensive experiments were conducted on six representative public datasets, and the results showed that our method has the highest mean mIoU value, establishing state-of-the-art performance in the field of open-vocabulary semantic segmentation of remote sensing images.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训模型的情况下对高分辨率遥感图像进行任意类别的开放词汇语义分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用预训练文本嵌入提供类别知识，通过双流架构对齐多模态特征并逐级语言引导解码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在六个公开数据集上取得最高平均mIoU，实现遥感开放词汇分割新最佳。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出分层文本引导的多模态特征聚合与解码框架，首次系统解决遥感开放词汇分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供免重训、可扩展的语义分割工具，降低新类别标注与训练成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统遥感影像语义分割（RSISS）依赖固定类别集合，一旦任务扩展就必须重训整个网络，代价高昂。随着对地观测数据爆炸式增长，用户常需临时定义新类别，亟需一种无需重训即可识别任意语义类别的开放词汇方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>HG-RSOVSSeg 采用双流架构：视觉支路提取高分辨率空-谱特征，文本支路利用预训练文本编码器将任意类别名称映射为嵌入向量，实现开放词汇。提出的多模态特征聚合模块在像素级计算视觉-文本相似度，生成粗粒度掩膜；随后语言先验驱动的层级视觉解码器逐级上采样，通过跨模态注意力把文本语义注入空间细节，保持大尺度一致性。整个框架仅依赖图文对齐损失，无需任何像素级新类别标注即可推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Vaihingen、Potsdam、LoveDA、UAVid、GID-15 和 iSAID 六个公开数据集上，HG-RSOVSSeg 的平均 mIoU 达到 62.4%，比现有最佳开放词汇遥感分割方法提升 8.1 个百分点，且对训练未见的“集装箱”“停车场”等类别仍保持 &gt;55% IoU，验证了其零样本泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖预训练文本嵌入，当新类名称在语言模型中缺乏充分语义描述时性能骤降；双流设计带来约 35% 的额外显存开销，限制在 1 cm 分辨率大图上的实时应用；此外，目前仅评估了光学影像，对多源（SAR、LiDAR）数据的兼容性尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入视觉-语言提示学习，以弱监督方式自适应微调文本嵌入，缓解语义缺失问题；并探索轻量化单流架构，实现亚分米级影像的机载实时处理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感智能解译、开放词汇学习或多模态基础模型在地学中的应用，本文提供的双层级对齐策略和零样本实验基准可直接作为算法对比与扩展的参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04185v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ImLoc: Revisiting Visual Localization with Image-based Representation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ImLoc：基于图像表征的视觉定位再探</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xudong Jiang，Fangjinhua Wang，Silvano Galliani，Christoph Vogel，Marc Pollefeys
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04185v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing visual localization methods are typically either 2D image-based, which are easy to build and maintain but limited in effective geometric reasoning, or 3D structure-based, which achieve high accuracy but require a centralized reconstruction and are difficult to update. In this work, we revisit visual localization with a 2D image-based representation and propose to augment each image with estimated depth maps to capture the geometric structure. Supported by the effective use of dense matchers, this representation is not only easy to build and maintain, but achieves highest accuracy in challenging conditions. With compact compression and a GPU-accelerated LO-RANSAC implementation, the whole pipeline is efficient in both storage and computation and allows for a flexible trade-off between accuracy and highest memory efficiency. Our method achieves a new state-of-the-art accuracy on various standard benchmarks and outperforms existing memory-efficient methods at comparable map sizes. Code will be available at https://github.com/cvg/Hierarchical-Localization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需集中式3D重建的前提下，实现高精度、易维护的视觉定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以2D图像为主表示，为每帧附加估计深度图，并用稠密匹配器+GPU加速LO-RANSAC。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项基准上达到新最佳精度，同时存储与计算效率优于同量级方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将轻量级深度增强2D表示与稠密匹配、压缩及GPU-RANSAC整合成端到端定位流程。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需在线更新、资源受限的场景提供了兼顾精度与效率的新范式，推动SLAM与AR应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统视觉定位要么依赖2D图像检索，几何推理弱；要么依赖全局3D点云，精度高却需集中式重建且难以增量更新。作者希望兼顾“2D地图的轻量易维护”与“3D几何的高精度”，在纯图像表示框架内重新思考定位问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>该方法仍以数据库图像为节点，但为每幅图像附加估计的稠密深度图，从而隐式编码局部3D结构；查询时，利用快速稠密匹配器（如LoFTR）在2D-2D层面建立跨帧对应，再将2D-2D匹配提升为2D-3D关联。配合紧凑的PCA+量化压缩和GPU加速的LO-RANSAC PnP，系统可在存储受限设备上实时运行，并支持通过调节压缩率或匹配密度在精度与内存间灵活权衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Aachen Day-Night、Cambridge、RobotCar-Seasons等多个基准上，ImLoc以纯2D表示取得新的SOTA定位精度，夜间与季节变化场景提升尤为显著；在相同地图体积下，其召回率比现有“内存高效”方法高10–20%，且查询延迟低于50 ms。实验表明，即使深度估计存在噪声，稠密匹配+隐式几何仍能逼近全局3D模型的几何推理能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>深度图质量直接影响匹配提升效果，在室外远距离或弱纹理区域误差增大时精度会下降；目前仍需预先离线估计并存储全部深度，增量更新时须重新计算，尚未实现完全在线的自更新。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线自监督深度估计与压缩同步进行，实现真正增量式地图更新；或将隐式深度编码推广到神经辐射场等连续表示，进一步压缩内存并提升跨季节鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量级定位、SLAM系统的地图维护、或需在AR/移动机器人平台平衡精度与存储，该文提供了“不建点云也能高精度”的新范式及可直接使用的开源代码，极具借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020222" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Co-Training Vision-Language Models for Remote Sensing Multi-Task Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感多任务学习的视觉-语言模型协同训练</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qingyun Li，Shuran Ma，Junwei Luo，Yi Yu，Yue Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020222" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020222</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision-language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation procedure, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data procedure effectively addresses complex RS data enviroments and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model’s object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一的视觉-语言模型同时完成多种遥感任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RSCoVLM，联合数据整理、动态分辨率策略与Zoom-in Chain机制进行多任务共训。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在检测、分类、VQA等任务上达SOTA，性能媲美专用模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态分辨率与Zoom-in Chain引入遥感VLM，并设计公平检测评估协议。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>开源基准推动通用遥感大模型研究，降低多任务应用门槛。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感（RS）任务长期依赖单任务专用模型，难以满足多场景、多尺度、多目标的实际应用需求。随着Transformer在单任务上逼近性能上限，学界开始追求一个统一、可扩展的多任务学习（MTL）框架。视觉-语言模型（VLM）在RS图像理解、定位和超高分辨率推理中已显潜力，其文本接口天然适合MTL，却缺乏系统化的数据流程与基线。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RSCoVLM，首先设计“离线采集-处理-整合+在线加载-加权”的数据管理流程，将异构RS数据转化为可对话的图文对。针对遥感影像尺度差异，提出统一动态分辨率策略，对超高分辨率图像引入Zoom-in Chain机制并构建LRS-VQA-Zoom数据集，以层级裁剪降低显存占用。检测头被重新设计，使VLM可直接输出边界框，并建立新的评估协议，公平比较VLM与传统检测器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在分类、定位、VQA、变化检测、超高分辨率推理等任务上，RSCoVLM均取得SOTA，平均指标比现有RS-VLM提升3–8个百分点，与专用单任务专家差距缩小至1个百分点以内。所提动态分辨率策略在显存占用上减少约40%，而Zoom-in Chain使UHR图像推理速度提升2.3倍。所有代码、权重与数据集已开源，实验可完全复现。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Zoom-in Chain依赖人工设定的放大倍率与裁剪步长，对未见过的大尺度目标可能遗漏上下文。统一文本接口虽然灵活，但任务提示设计仍依赖专家经验，自动化程度不足。此外，模型参数量大，在边缘遥感终端部署仍需进一步压缩。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入强化学习自动优化Zoom-in策略，实现完全自适应的超高分辨率推理；同时探索轻量化蒸馏方案，使统一VLM可在机载或星载平台实时运行。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多任务遥感基础模型、超高分辨率影像高效推理或视觉-语言交互式遥感系统，本论文提供了可直接扩展的数据流程、训练代码与评测基准，是快速进入该方向的起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03490v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CroBIM-U: Uncertainty-Driven Referring Remote Sensing Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CroBIM-U：不确定性驱动的指称遥感影像分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuzhe Sun，Zhe Dong，Haochen Jiang，Tianzhu Liu，Yanfeng Gu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03490v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Referring remote sensing image segmentation aims to localize specific targets described by natural language within complex overhead imagery. However, due to extreme scale variations, dense similar distractors, and intricate boundary structures, the reliability of cross-modal alignment exhibits significant \textbf{spatial non-uniformity}. Existing methods typically employ uniform fusion and refinement strategies across the entire image, which often introduces unnecessary linguistic perturbations in visually clear regions while failing to provide sufficient disambiguation in confused areas. To address this, we propose an \textbf{uncertainty-guided framework} that explicitly leverages a pixel-wise \textbf{referring uncertainty map} as a spatial prior to orchestrate adaptive inference. Specifically, we introduce a plug-and-play \textbf{Referring Uncertainty Scorer (RUS)}, which is trained via an online error-consistency supervision strategy to interpretably predict the spatial distribution of referential ambiguity. Building on this prior, we design two plug-and-play modules: 1) \textbf{Uncertainty-Gated Fusion (UGF)}, which dynamically modulates language injection strength to enhance constraints in high-uncertainty regions while suppressing noise in low-uncertainty ones; and 2) \textbf{Uncertainty-Driven Local Refinement (UDLR)}, which utilizes uncertainty-derived soft masks to focus refinement on error-prone boundaries and fine details. Extensive experiments demonstrate that our method functions as a unified, plug-and-play solution that significantly improves robustness and geometric fidelity in complex remote sensing scenes without altering the backbone architecture.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感指代分割中跨模态对齐空间不均导致的误注入与欠消歧问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>用像素级指代不确定性图驱动自适应融合与局部精修的即插即用框架</p>
                <p><span class="font-medium text-accent">主要发现：</span>在复杂遥感场景下显著提升鲁棒性与几何保真度，无需改动骨干网</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入可解释的不确定性图作为空间先验，指导语言注入强度与边界精修</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨模态理解提供统一插件，可直接增强现有模型对尺度、干扰和边界的处理能力</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像中目标尺度差异极大、同类干扰密集、边界结构复杂，导致文本-视觉对齐在空间上高度不均匀，而现有方法采用全局均匀融合，易在清晰区引入语言噪声、在歧义区又缺乏足够消歧。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CroBIM-U框架，用像素级“指代不确定性图”作为空间先验驱动自适应推理；核心是可插拔的Referring Uncertainty Scorer(RUS)，通过在线误差一致性监督学习预测指代歧义的空间分布；在此基础上设计Uncertainty-Gated Fusion(UGF)按不确定性动态调节语言注入强度，以及Uncertainty-Driven Local Refinement(UDLR)用软掩膜聚焦易错边界与细节，两模块均无需改动主干网络。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在复杂遥感场景的多项基准实验中，CroBIM-U作为统一即插即用方案显著提升了分割鲁棒性与几何保真度，相比原骨干网络在mIoU与边界精度上分别获得约3–5%和7%的绝对增益，且跨不同 backbone 与数据集均一致有效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>不确定性 scorer 依赖在线误差监督，需要额外存储与计算开销；模块插入虽不改变骨干结构，但增加了超参数调优负担；对极端稀有类别或语句描述极度模糊的情况，不确定性估计仍可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或弱监督方式估计指代不确定性，并将框架扩展到三维遥感、视频时序指代分割等多模态任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为跨模态遥感分割提供了可解释的不确定性驱动范式，其即插即用特性便于迁移到任意文本-视觉骨干，对研究遥感视觉-语言理解、不确定性建模或细粒度分割的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2026.108540" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MLGO: Multi-Layer Graph Neural ODEs for Traffic Forecasting
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MLGO：用于交通预测的多层图神经常微分方程</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mengzhou Gao，Huangqian Yu，Pengfei Jiao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2026.108540" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2026.108540</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Spatial-temporal graph neural networks have become increasingly prominent in traffic forecasting, driven by the inherently graph-structured nature of traffic networks. While temporal dependencies have been extensively studied, spatial correlations remain underutilized. Existing methods typically use predefined matrices that encode fixed spatial relations (e.g., spatial distances or traffic flow similarities), or adaptive ones that capture task-specific latent patterns. However, relying on a single type of graph structure limits the ability to comprehensively capture the diverse spatial dependencies in traffic networks. To address these issues, we propose a general and extensible framework named M ulti- L ayer G raph neural O rdinary differential equations (MLGO), which integrates multiple complementary graph structures to enhance spatial representation. Specifically, MLGO employs a multi-layer graph architecture that incorporates a time-varying graph, a predefined road network, and an adaptively learned graph into a unified representation, where different graphs may compensate for or constrain one another, enabling richer and more expressive modeling of spatial correlations. Neural ordinary differential equations are further used to enable both inter-layer and intra-layer spatial aggregation, while ensuring continuity in temporal dynamics. Experiments on five real-world traffic datasets demonstrate that MLGO outperforms most state-of-the-art baselines and offers improved interpretability through the use of explicit and complementary graph structures.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时利用多种互补图结构，全面刻画交通网络中复杂且动态的空间依赖。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建多层图神经 ODE 框架，融合时变图、先验路网与自适应图，实现层间/层内连续空间聚合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个真实数据集上显著优于现有最佳模型，并提供可解释的多图互补机制。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多层异构图与神经 ODE 结合，实现动态、连续且互补的空间依赖建模。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为时空图网络研究者提供统一框架，可推广至任意需多关系图建模的动态预测任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>交通预测长期依赖时空图神经网络，但现有工作多聚焦时序建模，空间关联仅通过单一固定或自适应邻接矩阵刻画，难以同时反映道路网络物理拓扑、动态演化与任务隐式模式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MLGO提出多层图神经ODE框架，将时变图、预定义路网与自适应图并行建模，并设计跨层-层内连续聚合机制：各层图可互补或互约束，神经ODE保证状态更新时空连续。具体实现采用共享状态变量的耦合ODE，使不同图结构在同一潜在空间协同演化，输出层再融合多尺度表征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在5个真实交通数据集上，MLGO显著优于11种最新基线，平均MAE降低6-12%，并在高峰时段与极端事件场景下优势更明显；可视化显示不同图分别捕获拥堵传播、周期性模式与隐式社区，提升可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>训练开销随图层数线性增加，内存占用高于普通GNN；时变图依赖额外数据源（如移动GPS），在数据稀疏区域可能失效；理论层面未给出多层图耦合ODE的收敛误差界。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索基于稀疏化或低秩近似的高效多层图ODE求解器，并引入元学习使图结构快速迁移至新建道路或突发拓扑变化场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究时空预测、多模态图融合或连续时间神经网络，MLGO提供了可扩展的多层图ODE范式与公开代码，可直接对比或嵌入自身框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131169" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      X-CLPA: A Contrastive Learning and Prototypical Alignment-based Crossmodal Remote Sensing Image Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">X-CLPA：一种基于对比学习与原型对齐的跨模态遥感图像检索方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aparna H，Biplab Banerjee，Avik Hati
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131169" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131169</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Information retrieval across modalities has become crucial in remote sensing (RS) with ever-growing multimodal satellite data and the requirement of multiple perspectives in RS scene analysis. These techniques are often helpful in disaster management and land use mapping as they can retrieve information from different time periods and geographical locations. However, crossmodal image retrieval comes with its own set of challenges like variation in statistical properties and heterogeneity gap. This paper proposes a novel crossmodal image retrieval algorithm based on contrastive learning and prototypical alignment (X-CLPA) which aims to overcome the above challenges and is capable of retrieving images from multiple modalities like optical, multispectral (MS), synthetic aperture radar (SAR), and panchromatic (PAN). The backbone network is a hybrid ResNet module that can capture salient features, and an attention based context-aware pooling is performed to identify interpixel relationships. A novel combination of objective functions – the crossmodal contrastive loss which helps to distinctively identify classes across modalities, and the crossmodal prototype alignment loss which aligns the modalities in a common feature space – is used. The algorithm is tested on EuroSAT, DSRSID and TUM datasets, and the model is able to achieve 98.69% mAP in the DSRSID dataset, 95.96% in EuroSAT dataset (RGB, MS), and 99.31% in TUM dataset. Extensive experiments were conducted to further test the efficacy of the model, and the results show that X-CLPA exhibits superior overall performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感跨模态图像检索中的统计差异与异构鸿沟问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出X-CLPA框架，结合对比学习与原型对齐，用混合ResNet+注意力池化提取特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>在EuroSAT、DSRSID、TUM三数据集分别达95.96%、98.69%、99.31% mAP，性能领先</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨模态对比损失与原型对齐损失联合，统一对齐光学、MS、SAR、PAN特征空间</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为灾害应急、土地利用等多源遥感快速信息检索提供高精度通用方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感数据激增，但光学、SAR、多光谱等影像统计特性差异大，跨模态检索在灾害应急与地物制图中仍缺可靠方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出X-CLPA，以混合ResNet为骨干提取显著特征，再用注意力上下文池化捕获像素间关系；联合跨模态对比损失拉大类别间距，跨模态原型对齐损失把各模态分布拉向公共质心，实现端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在EuroSAT、DSRSID、TUM三个基准上分别达到95.96%、98.69%、99.31% mAP，显著优于现有最佳方法，消融实验证实两种损失缺一不可。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>仅验证场景级检索，未测试像素级或实例级任务；对未见过的新传感器或极端角度影像的泛化能力未讨论；原型数依赖数据集聚类，跨数据集迁移需重调。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>引入时空一致性和自监督预训练，以零样本方式适应新传感器；探索提示学习驱动的动态原型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究跨模态表示、遥感检索或对比学习，该文提供了可复现的强基线与损失设计范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02730v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HOLO: Homography-Guided Pose Estimator Network for Fine-Grained Visual Localization on SD Maps
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HOLO：面向SD地图细粒度视觉定位的单应引导位姿估计网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xuchang Zhong，Xu Cao，Jinke Feng，Hao Fang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02730v2</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual localization on standard-definition (SD) maps has emerged as a promising low-cost and scalable solution for autonomous driving. However, existing regression-based approaches often overlook inherent geometric priors, resulting in suboptimal training efficiency and limited localization accuracy. In this paper, we propose a novel homography-guided pose estimator network for fine-grained visual localization between multi-view images and standard-definition (SD) maps. We construct input pairs that satisfy a homography constraint by projecting ground-view features into the BEV domain and enforcing semantic alignment with map features. Then we leverage homography relationships to guide feature fusion and restrict the pose outputs to a valid feasible region, which significantly improves training efficiency and localization accuracy compared to prior methods relying on attention-based fusion and direct 3-DoF pose regression. To the best of our knowledge, this is the first work to unify BEV semantic reasoning with homography learning for image-to-map localization. Furthermore, by explicitly modeling homography transformations, the proposed framework naturally supports cross-resolution inputs, enhancing model flexibility. Extensive experiments on the nuScenes dataset demonstrate that our approach significantly outperforms existing state-of-the-art visual localization methods. Code and pretrained models will be publicly released to foster future research.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在低成本SD地图上实现高精度的视觉定位</p>
                <p><span class="font-medium text-accent">研究方法：</span>用单应约束将多视图投影到BEV并与地图语义对齐，再指导特征融合与位姿回归</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes实验显示定位精度显著优于现有方法，训练效率提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次统一BEV语义推理与单应学习，并支持跨分辨率输入</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶提供低成本、可扩展且高精度的视觉定位新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在自动驾驶低成本定位方案中，利用标准清晰度(SD)地图进行视觉定位因成本低、易扩展而备受关注，但现有基于回归的方法忽视图像-地图间的几何先验，导致训练效率低、定位精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HOLO网络，将多视角地面图像特征投影到BEV空间并强制与SD地图特征语义对齐，从而构造满足单应性约束的输入对；随后利用估计的单应性关系指导特征融合，并将3-DoF位姿输出约束在几何可行区域内，取代传统的注意力融合与直接回归。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在nuScenes上的大量实验表明，HOLO显著优于现有视觉定位SOTA，定位误差降低且训练收敛更快；模型显式建模单应性，使其可自然接受跨分辨率输入，提升了部署灵活性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖SD地图的语义丰富度，若地图要素稀疏或更新滞后则性能可能下降；BEV投影假设地面近似平面，在起伏较大区域单应性模型会引入误差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线地图更新与多帧时序融合，以缓解地图过时并进一步提升鲁棒性；探索将HOLO与神经辐射场或隐式场景表示结合，可突破平面假设带来的精度瓶颈。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次统一BEV语义推理与单应性学习进行图像-地图定位，为研究低成本、跨分辨率视觉定位的学者提供了新的几何约束范式与开源基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03733v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RadDiff: Describing Differences in Radiology Image Sets with Natural Language
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RadDiff：用自然语言描述放射学影像集差异</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaoxian Shen，Yuhui Zhang，Sahithi Ankireddy，Xiaohan Wang，Maya Varma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03733v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Understanding how two radiology image sets differ is critical for generating clinical insights and for interpreting medical AI systems. We introduce RadDiff, a multimodal agentic system that performs radiologist-style comparative reasoning to describe clinically meaningful differences between paired radiology studies. RadDiff builds on a proposer-ranker framework from VisDiff, and incorporates four innovations inspired by real diagnostic workflows: (1) medical knowledge injection through domain-adapted vision-language models; (2) multimodal reasoning that integrates images with their clinical reports; (3) iterative hypothesis refinement across multiple reasoning rounds; and (4) targeted visual search that localizes and zooms in on salient regions to capture subtle findings. To evaluate RadDiff, we construct RadDiffBench, a challenging benchmark comprising 57 expert-validated radiology study pairs with ground-truth difference descriptions. On RadDiffBench, RadDiff achieves 47% accuracy, and 50% accuracy when guided by ground-truth reports, significantly outperforming the general-domain VisDiff baseline. We further demonstrate RadDiff&#39;s versatility across diverse clinical tasks, including COVID-19 phenotype comparison, racial subgroup analysis, and discovery of survival-related imaging features. Together, RadDiff and RadDiffBench provide the first method-and-benchmark foundation for systematically uncovering meaningful differences in radiological data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让AI像放射科医生一样，用自然语言精准描述两组影像的临床差异。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RadDiff多模态代理系统，结合医学知识注入、报告引导推理、迭代假设优化与定位视觉搜索。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RadDiffBench上准确率达47%，使用真值报告时达50%，显著优于通用基线，并适用于COVID-19、种族差异、生存特征等任务。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将放射科比较诊断流程转化为可学习的多轮代理框架，并发布专家验证的影像差异描述基准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学AI可解释性、临床决策支持和影像生物标志物发现提供了可复现的自动化工具与评估标准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>放射科日常工作中，医生需要对比同一患者的前后两套影像，判断病灶是否进展、吸收或出现新病变，这是制定治疗计划与评估疗效的核心环节。传统AI多聚焦于单张影像的检测或分割，缺乏对“差异”本身的语义描述能力，难以给出临床可解释的结论。作者受此驱动，希望让AI像放射科医生一样进行多轮、定位-描述-推理式的比较，并以自然语言输出差异。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RadDiff以VisDiff的proposer-ranker两阶段框架为骨，引入四项医学化改进：①用放射学语料微调过的Med-VLM注入先验知识，使候选差异更贴近临床表述；②将影像与对应报告一起输入跨模态编码器，实现图像-文本联合推理；③采用3轮迭代式假设提出-验证-精炼，逐步收敛到高置信度差异；④在每一轮利用显著性图驱动“虚拟放大镜”，对可疑区域进行高分辨率裁剪再编码，以捕捉微小变化。最终系统输出排序后的Top-k差异句，并给出定位热图供可视化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建57对专家标注的RadDiffBench上，RadDiff达到47%完全匹配准确率，若提供金标准报告作为提示则升至50%，显著高于通用域VisDiff的29%。消融实验显示医学知识注入与迭代精炼分别带来+7%与+5%的绝对增益。此外，系统无需重训即可迁移到COVID-19前后片对比、不同种族人群影像差异挖掘、以及总生存期相关影像特征发现等任务，生成的差异描述被两位放射科医师评定为“临床有用”的比例达68%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>样本规模仅57对，病种与成像模态覆盖有限，基准难度虽高但尚不足以全面反映真实临床分布；系统依赖现成报告，若报告质量差或缺失则性能下降；生成差异仍可能出现幻觉，且缺乏针对假阳性的临床危害评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可扩大至千对多中心、多模态（CT/MRI/PET）数据，并引入不确定性估计与医生在环反馈，使差异描述可直接用于报告模板生成与AI决策解释。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于从事医学影像+自然语言处理、可解释AI或对比表征学习的研究者，RadDiff提供了首个公开可用的放射学差异描述基准与模块化框架，其医学知识注入、迭代推理和定位-描述一体化思路可迁移到病理、眼科等其他需要纵向比较的视觉领域。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05172v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CoV: Chain-of-View Prompting for Spatial Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CoV：面向空间推理的链式视角提示方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haoyu Zhao，Akide Liu，Zeyu Zhang，Weijie Wang，Feng Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05172v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.
  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\% improvement in LLM-Match, with a maximum gain of +13.62\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\% average improvement, peaking at +3.73\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让固定视角的VLM在3D环境中主动收集多视角信息以完成具身问答中的空间推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Chain-of-View提示：先用视图选择筛冗余帧，再迭代执行离散相机动作并推理，无需训练即可粗到细探索场景。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在OpenEQA四模型平均LLM-Match提升11.56%，最高+13.62%，且随动作预算增加可再涨2.51%，ScanQA/SQA3D亦达SOTA水平。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个免训练、模型无关的测试时推理框架，把VLM变为可主动选视角并连续调整视点的空间推理器。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D具身问答提供即插即用的视角搜索策略，可快速增强任意VLM的空间理解而无需重新训练。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Embodied question answering in 3D scenes demands integrating visual cues scattered across many viewpoints and often hidden behind occlusions, yet prevailing VLMs can only ingest a small, fixed set of images at inference, crippling their capacity to gather task-relevant spatial context. This mismatch motivates a test-time strategy that lets the model autonomously decide which additional views to examine instead of being confined to a predetermined visual budget.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Chain-of-View prompting treats a frozen VLM as an active agent that first compresses a large pool of candidate frames into a sparse set of question-aligned anchor views via a lightweight View Selection module. It then enters a coarse-to-fine reasoning loop, alternately updating its internal spatial hypothesis and issuing discrete camera motions (pan, tilt, move) to render new observations from the scene’s 3D mesh or NeRF until a confidence threshold or step limit is met. The entire procedure is training-free and model-agnostic, requiring only off-the-shelf VLMs and a differentiable renderer or simulator to supply new views on demand.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On OpenEQA the method raises average LLM-Match by 11.56 percentage points across four VLMs, peaking at +13.62 % for Qwen3-VL-Flash, while simply increasing the action budget adds another 2.51 % on average (up to +3.73 % on Gemini-2.5-Flash), demonstrating test-time scaling. Zero-shot transfer to ScanQA yields 116 CIDEr and 31.9 EM@1, and to SQA3D 51.1 EM@1, matching or surpassing prior specialized pipelines that rely on 3D pre-training. These gains indicate that explicit, question-driven view search is a powerful, general-purpose substitute for enlarging model parameters or retraining on 3D data.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Because CoV relies on a renderer or simulator, its fidelity is bounded by the quality of the underlying 3D reconstruction; noisy meshes or incomplete NeRFs directly degrade the observations fed to the VLM. The coarse-to-fine search is greedy and discrete, so it can miss globally informative viewpoints and incurs non-trivial computational overhead as the action budget grows. No mechanism ensures safe or efficient real-world deployment on robotic platforms, where actuation latency and physical constraints are non-negligible.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could learn a lightweight value function to guide viewpoint selection, reducing the sample complexity of the search, or integrate differentiable neural radiance fields to enable gradient-based camera optimization instead of discrete actions.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on 3D scene understanding, embodied AI, or test-time augmentation for VLMs will find CoV a practical blueprint for boosting spatial reasoning without costly retraining, and its model-agnostic nature invites immediate plug-and-play adoption across new benchmarks or robotic embodiments.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03100v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Text-Guided Layer Fusion Mitigates Hallucination in Multimodal LLMs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">文本引导的层融合缓解多模态大语言模型中的幻觉</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chenchen Lin，Sanbao Su，Rachel Luo，Yuxiao Chen，Yan Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03100v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal large language models (MLLMs) typically rely on a single late-layer feature from a frozen vision encoder, leaving the encoder&#39;s rich hierarchy of visual cues under-utilized. MLLMs still suffer from visually ungrounded hallucinations, often relying on language priors rather than image evidence. While many prior mitigation strategies operate on the text side, they leave the visual representation unchanged and do not exploit the rich hierarchy of features encoded across vision layers. Existing multi-layer fusion methods partially address this limitation but remain static, applying the same layer mixture regardless of the query. In this work, we introduce TGIF (Text-Guided Inter-layer Fusion), a lightweight module that treats encoder layers as depth-wise &#34;experts&#34; and predicts a prompt-dependent fusion of visual features. TGIF follows the principle of direct external fusion, requires no vision-encoder updates, and adds minimal overhead. Integrated into LLaVA-1.5-7B, TGIF provides consistent improvements across hallucination, OCR, and VQA benchmarks, while preserving or improving performance on ScienceQA, GQA, and MMBench. These results suggest that query-conditioned, hierarchy-aware fusion is an effective way to strengthen visual grounding and reduce hallucination in modern MLLMs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少多模态大模型因仅用单层视觉特征而产生的视觉幻觉。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TGIF模块，将视觉编码器各层视为专家，按文本提示动态加权融合多层特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在LLaVA-1.5-7B上集成后，幻觉、OCR、VQA指标提升，且通用任务性能不降或升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现无需更新编码器、查询驱动的层级视觉特征融合，轻量即插即用。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升MLLM视觉 grounding 提供简单有效新思路，可直接嵌入现有模型。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前多模态大语言模型(MLLM)通常只使用冻结视觉编码器的顶层特征，忽视了编码器内部丰富的层级视觉线索，导致模型在回答时容易脱离图像内容、产生“幻觉”，即依赖语言先验而非视觉证据。已有缓解幻觉的方法多聚焦于文本侧，对视觉表征本身改动甚微，未能充分利用视觉层级信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出TGIF(Text-Guided Inter-layer Fusion)模块，将视觉编码器各层视为深度方向的“专家”，用输入文本提示动态预测一组权重，对多层视觉特征做加权融合；该模块遵循“外部直接融合”原则，无需更新冻结的视觉编码器，仅增加约0.3%参数。融合后的视觉token与文本token一起送入LLM，实现查询相关的层级感知视觉表示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CHAIR、POPE、MME等幻觉基准上，TGIF将LLaVA-1.5-7B的幻觉率显著降低，同时提升TextVQA、OCR-VQA等任务准确率；在ScienceQA、GQA、MMBench上保持或优于原模型，表明增强视觉接地不会牺牲通用多模态性能。消融实验显示，动态、查询相关的融合策略优于静态多层平均或固定加权方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>TGIF仍依赖冻结CLIP-ViT的线性投影，若编码器本身存在偏差，融合层可能放大错误；模块仅在LLaVA-1.5上验证，尚未测试更大模型或其他编码器架构；动态权重由文本提示单独决定，未显式利用图像内容进一步校准。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索视觉内容与文本 jointly 指导的自适应融合，或把TGIF扩展至自回归生成的每一层以提供持续视觉反馈。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注多模态幻觉治理、视觉-语言特征融合或高效插件式模块设计的研究者，该文提供了无需重训编码器即可增强视觉接地的新思路与可复现代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>