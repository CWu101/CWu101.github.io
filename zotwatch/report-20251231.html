<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-31</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-31 10:56 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u6df1\u5ea6\u5b66\u4e60\u571f\u5730\u5229\u7528\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742129304737}, {"source": 0, "target": 2, "value": 0.5366473047802691}, {"source": 1, "target": 2, "value": 0.5805336366198007}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖3篇关于场景图与上下文建模的论文、2篇关于遥感多标签分类的论文。</p>
            
            <p><strong class="text-accent">场景图建模</strong>：该主题聚焦利用空间-语义上下文提升视觉理解：《With Great Context Comes Great Power》通过地理-语义场景图对物体分类；《STVRM》以时空Transformer建模动态视频中的实体关系；《Multi-label Classification with Panoptic Context Aggregation Networks》在全景分割图上聚合上下文进行多标签识别。</p>
            
            <p><strong class="text-accent">遥感多标签</strong>：该主题研究遥感影像的多标签场景分类与变化检测：《FDPFNet》在频域渐进融合光学与SAR图像提升多标签分类性能；《ViLaCD-R1》构建视觉-语言框架，通过语义对齐实现高层次的遥感变化检测。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于遥感智能解译的论文、6篇关于视觉定位与场景理解的论文、5篇关于多模态大模型推理的论文、4篇关于视频时空建模的论文、3篇关于半监督与域泛化的论文、2篇关于轻量化感知的论文以及1篇关于变化检测的论文。</p>
            
            <p><strong class="text-text-secondary">遥感智能解译</strong>：该主题聚焦遥感影像的语义分割、变化检测与跨域泛化，提出《CrossEarth》等地学基础模型、《ViLaCD-R1》等视觉语言框架及动态分辨率多模态对齐机制《Multimodal Interpretation of Remote Sensing Images》，显著提升地表信息提取精度与域外泛化能力。</p>
            
            <p><strong class="text-text-secondary">视觉定位与场景理解</strong>：针对GNSS拒止环境下的图像地理定位与场景语义解析，研究利用拓扑图《Topology-aware visual localization》与地理语义场景图《With Great Context Comes Great Prediction Power》实现内容驱动的精准定位与对象分类。</p>
            
            <p><strong class="text-text-secondary">多模态大模型推理</strong>：该方向探索多模态大模型在跨域视觉问答与长视频理解中的可信推理，提出《Self-Rewarded Multimodal Coherent Reasoning》的自奖励一致性机制与《TV-RAG》的语义熵加权检索框架，缓解幻觉并增强视觉 grounding。</p>
            
            <p><strong class="text-text-secondary">视频时空建模</strong>：面向动态场景图生成与长视频理解，研究利用《STVRM》的时空 Transformer 关系建模与《TV-RAG》的时间感知检索，解决长时序窗口狭窄与关系演化捕捉不足的问题。</p>
            
            <p><strong class="text-text-secondary">半监督与域泛化</strong>：针对遥感标注稀缺与域漂移，提出《Toward Stable Semi-Supervised Remote Sensing Segmentation》的协同引导融合策略与《CrossEarth》的域泛化基础模型，显著降低伪标签漂移并提升跨域鲁棒性。</p>
            
            <p><strong class="text-text-secondary">轻量化感知</strong>：面向资源受限的车载系统，研究通过《LNet》的轻量网络联合场景与视线一致性，实现低算力下的驾驶员注意力估计。</p>
            
            <p><strong class="text-text-secondary">变化检测</strong>：该主题仅《ViLaCD-R1》一文，通过视觉语言框架将高层语义引入遥感变化检测，突破传统像素级方法对语义信息利用不足的局限。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 45%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23024v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      With Great Context Comes Great Prediction Power: Classifying Objects via Geo-Semantic Scene Graphs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">上下文越丰富，预测力越强：基于地理语义场景图的物体分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ciprian Constantinescu，Marius Leordeanu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23024v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Humans effortlessly identify objects by leveraging a rich understanding of the surrounding scene, including spatial relationships, material properties, and the co-occurrence of other objects. In contrast, most computational object recognition systems operate on isolated image regions, devoid of meaning in isolation, thus ignoring this vital contextual information. This paper argues for the critical role of context and introduces a novel framework for contextual object classification. We first construct a Geo-Semantic Contextual Graph (GSCG) from a single monocular image. This rich, structured representation is built by integrating a metric depth estimator with a unified panoptic and material segmentation model. The GSCG encodes objects as nodes with detailed geometric, chromatic, and material attributes, and their spatial relationships as edges. This explicit graph structure makes the model&#39;s reasoning process inherently interpretable. We then propose a specialized graph-based classifier that aggregates features from a target object, its immediate neighbors, and the global scene context to predict its class. Through extensive ablation studies, we demonstrate that our context-aware model achieves a classification accuracy of 73.4%, dramatically outperforming context-agnostic versions (as low as 38.4%). Furthermore, our GSCG-based approach significantly surpasses strong baselines, including fine-tuned ResNet models (max 53.5%) and a state-of-the-art multimodal Large Language Model (LLM), Llama 4 Scout, which, even when given the full image alongside a detailed description of objects, maxes out at 42.3%. These results on COCO 2017 train/val splits highlight the superiority of explicitly structured and interpretable context for object recognition tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让机器像人一样利用场景上下文提升单图物体分类准确率</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建Geo-Semantic Contextual Graph，用图网络聚合目标、邻居与全局特征进行分类</p>
                <p><span class="font-medium text-accent">主要发现：</span>上下文模型在COCO上达73.4%，远超无上下文38.4%、ResNet 53.5%和LLM 42.3%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将度量深度、全景与材质分割整合为可解释的地物语义场景图进行物体识别</p>
                
                <p><span class="font-medium text-accent">相关性：</span>证明显式结构化上下文对视觉识别至关重要，为可解释场景理解提供新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Unable to extract background</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Unable to extract methodology details</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Unable to extract results</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Unable to extract limitations</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>{&#34;background&#34;:&#34;Humans excel at object recognition because they exploit rich scene context—spatial layout, material cues, and object co-occurrence—whereas most CNNs process isolated patches and ignore </p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 38%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23486v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-label Classification with Panoptic Context Aggregation Networks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于全景上下文聚合网络的多标签分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mingyuan Jiu，Hailong Zhu，Wenchuan Wei，Hichem Sahbi，Rongrong Ji 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23486v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Context modeling is crucial for visual recognition, enabling highly discriminative image representations by integrating both intrinsic and extrinsic relationships between objects and labels in images. A limitation in current approaches is their focus on basic geometric relationships or localized features, often neglecting cross-scale contextual interactions between objects. This paper introduces the Deep Panoptic Context Aggregation Network (PanCAN), a novel approach that hierarchically integrates multi-order geometric contexts through cross-scale feature aggregation in a high-dimensional Hilbert space. Specifically, PanCAN learns multi-order neighborhood relationships at each scale by combining random walks with an attention mechanism. Modules from different scales are cascaded, where salient anchors at a finer scale are selected and their neighborhood features are dynamically fused via attention. This enables effective cross-scale modeling that significantly enhances complex scene understanding by combining multi-order and cross-scale context-aware features. Extensive multi-label classification experiments on NUS-WIDE, PASCAL VOC2007, and MS-COCO benchmarks demonstrate that PanCAN consistently achieves competitive results, outperforming state-of-the-art techniques in both quantitative and qualitative evaluations, thereby substantially improving multi-label classification performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何显式建模跨尺度、多阶几何上下文以提升多标签图像分类性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 PanCAN，在高维 Hilbert 空间用随机游走+注意力级联跨尺度特征，动态融合显著锚点邻域。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 NUS-WIDE、PASCAL VOC2007、MS-COCO 上定量与定性均优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多阶随机游走注意力引入跨尺度级联框架，实现全景式上下文聚合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景多标签识别提供可扩展的跨尺度上下文建模新范式，可直接增强检测与分割等下游任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多标签图像分类需要同时识别图中所有语义标签，传统方法多依赖局部特征或简单几何上下文，难以捕捉跨尺度、跨语义的复杂关系，导致在密集或尺度变化大的场景中性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Deep Panoptic Context Aggregation Network (PanCAN)，在高维 Hilbert 空间中用随机游走+注意力机制学习每尺度的多阶邻域关系；不同尺度模块级联，细尺度选出的显著锚点通过注意力动态融合其邻域特征，实现跨尺度、多阶上下文聚合；整体以端到端方式训练，输出融合后的上下文感知特征用于多标签预测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 NUS-WIDE、PASCAL VOC2007 和 MS-COCO 上的实验显示，PanCAN 在 mAP、F1 等指标上持续优于现有最佳方法，可视化热图表明其对复杂场景的多物体定位更完整，验证了跨尺度多阶上下文对提升多标签分类的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅聚焦静态图像，未探讨视频或时序上下文；高维 Hilbert 空间建模带来额外计算与内存开销，对高分辨率输入的可扩展性未充分验证；与近期基于视觉大模型或 Transformer 的架构缺乏直接对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将 PanCAN 的跨尺度游走-注意力机制嵌入视觉 Transformer 或扩散模型，并探索自监督预训练以进一步降低标注依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多标签识别、上下文建模、跨尺度特征融合或视觉注意力机制，本文提供的级联随机游走-注意力框架可直接借鉴或扩展至场景理解、目标检测等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.43</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 36%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.131018" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      STVRM: Spatio-Temporal Relational Modeling with Vision Transformer for Dynamic Scene Graph Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">STVRM：面向动态场景图生成的视觉Transformer时空关系建模</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Linnan Lu，Guannan Si，Xinyu Liang，Mingshen Li，Fengyu Zhou
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.131018" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.131018</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Dynamic scene graph generation aims to achieve semantic understanding of scenes by analyzing video content, identifying entities and relationships within the scene. However, on one hand, it struggles to effectively handle contextual noise in the scene; on the other hand, existing dynamic scene graph generation methods still exhibit insufficient capability in capturing the temporal dependencies of visual relationships between entities, particularly when recognizing subtle changes in relationships over time. This hinders the accurate detection of fine-grained dynamic features. To overcome these limitations, this paper presents the Spatial-Temporal Vision Transformer Relation Module (STVRM) , a novel framework aimed at enhancing dynamic scene graph generation and object relation classification. By incorporating the vision transformer architecture in place of the traditional transformer design, STVRM is better equipped to capture both local and global characteristics in video data, particularly excelling at modeling dependencies and transitions across time and space. Additionally, the paper introduces the video object-relation classification module and temporal difference aggregator module, both of which improve the precision of object relation classification and bolster the model’s awareness of changes over time. Comprehensive experiments on the Action Genome dataset demonstrate that the STVRM model significantly outperforms existing methods on both the SGCLS and SGDET tasks. Specifically, under the No Constraint setting, the model achieves improvements of 6% and 4.7% in R@50 and mR@50 for SGCLS, and enhancements of 8% and 4.1% in R@10 and mR@10 for SGDET. These results fully demonstrate the superiority and effectiveness of the STVRM approach.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制场景上下文噪声并捕获视觉关系随时间的细微变化以提升动态场景图生成精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出STVRM，用Vision Transformer建模时空依赖，并设计视频对象关系分类模块与时间差异聚合器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Action Genome上SGCLS与SGDET任务R@50/mR@50、R@10/mR@10分别提升6%/4.7%、8%/4.1%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Vision Transformer引入动态场景图生成，显式建模时空关系转移并强化时序差异感知。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频语义理解与细粒度关系检测提供新架构，推动动态场景图在监控、机器人等领域的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>动态场景图生成(DSGG)需要将视频帧中的实体与关系一并识别，以支持高层语义理解与推理。现有方法在复杂场景中易受上下文噪声干扰，且对视觉关系随时间演化的细粒度变化捕捉不足，导致动态特征检测精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Spatial-Temporal Vision Transformer Relation Module(STVRM)，用纯视觉 Transformer 替代传统 Transformer，以自注意力同时建模局部空间细节与全局时空依赖。框架新增视频对象-关系分类模块，对每对实体在时序上聚合多帧特征进行关系预测；并设计时序差异聚合器，显式计算相邻帧关系嵌入的差分，强化模型对微妙关系变化的感知。整体训练采用端到端方式，在特征、关系与变化三个层级联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Action Genome 基准的 SGCLS 与 SGDET 任务上，STVRM 在 No Constraint 设定下将 SGCLS 的 R@50、mR@50 分别提升 6% 与 4.7%，SGDET 的 R@10、mR@10 提升 8% 与 4.1%，显著优于现有最佳方法。实验表明，引入视觉 Transformer 后，模型对长程时空上下文与细微关系演化的捕捉能力明显增强，验证了框架在动态场景理解中的有效性与泛化潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在 Action Genome 单一数据集上验证，缺乏跨数据集泛化评估；视觉 Transformer 带来额外计算与显存开销，对高分辨率长视频的可扩展性未深入讨论；方法依赖显式对象检测框，若检测器失效将直接影响关系预测精度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无检测框的端到端关系推理，并将 STVRM 扩展至多数据集与更长视频序列以验证其鲁棒性与效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视频语义理解、时空关系建模或视觉 Transformer 在高层视觉任务中的应用，本文提供的架构设计与实验分析可直接作为基线参考与方法借鉴。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 35%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23244v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ViLaCD-R1：面向遥感语义变化检测的视觉-语言框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xingwei Ma，Shiyang Feng，Bo Zhang，Bin Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23244v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing change detection (RSCD), a complex multi-image inference task, traditionally uses pixel-based operators or encoder-decoder networks that inadequately capture high-level semantics and are vulnerable to non-semantic perturbations. Although recent multimodal and vision-language model (VLM)-based approaches enhance semantic understanding of change regions by incorporating textual descriptions, they still suffer from challenges such as inaccurate spatial localization, imprecise pixel-level boundary delineation, and limited interpretability. To address these issues, we propose ViLaCD-R1, a two-stage framework comprising a Multi-Image Reasoner (MIR) and a Mask-Guided Decoder (MGD). Specifically, the VLM is trained through supervised fine-tuning (SFT) and reinforcement learning (RL) on block-level dual-temporal inference tasks, taking dual-temporal image patches as input and outputting a coarse change mask. Then, the decoder integrates dual-temporal image features with this coarse mask to predict a precise binary change map. Comprehensive evaluations on multiple RSCD benchmarks demonstrate that ViLaCD-R1 substantially improves true semantic change recognition and localization, robustly suppresses non-semantic variations, and achieves state-of-the-art accuracy in complex real-world scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服遥感变化检测中语义理解不足、定位不准与非语义扰动敏感的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出两阶段框架ViLaCD-R1：先以VLM经SFT+RL推理输出粗变化掩膜，再由掩膜引导解码器精化像素级二值图。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准测试显示该方法显著提升语义变化识别与定位精度，并有效抑制非语义变化，达SOTA水平。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将强化学习微调的多图VLM与掩膜引导解码结合，实现高语义、高定位精度的遥感变化检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、CV与多模态学习研究者提供可解释的像素级变化检测新范式，推动实际监测应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统遥感变化检测(RSCD)依赖像素级算子或编码-解码网络，难以捕捉高层语义且对非语义扰动敏感。近期多模态与视觉-语言模型(VLM)通过引入文本描述增强了对变化区域的语义理解，但仍存在空间定位不准、边界刻画粗糙、可解释性不足等问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两阶段框架ViLaCD-R1：第一阶段训练多图像推理器(MIR)，以双时相图像块为输入，通过监督微调(SFT)和强化学习(RL)在块级双时相推理任务上微调VLM，输出粗略变化掩膜；第二阶段使用掩膜引导解码器(MGD)，将双时相图像特征与粗掩膜融合，预测精细二值变化图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个RSCD基准上的综合评估表明，ViLaCD-R1显著提升真实语义变化的识别与定位能力，有效抑制非语义变化，并在复杂真实场景下达到当前最佳精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未发表于同行评审期刊，结果需进一步验证；强化学习训练可能带来额外计算开销；框架依赖大规模双时相-文本配对数据，数据获取成本高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无强化学习的端到端训练策略，并扩展至多源遥感数据(如SAR、LiDAR)以提升跨模态鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究将视觉-语言模型首次系统引入遥感变化检测，为需要高语义一致性与精细边界的多时相遥感分析提供新范式，对从事多模态遥感、变化检测或VLM应用的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 35%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3649036" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FDPFNet: A Frequency-Domain Progressive Fusion Network for Optical-SAR Multi-Label Remote Sensing Scene Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FDPFNet：面向光学-SAR多标签遥感场景分类的频域渐进融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiming Zhao，Kunlun Qi，Yaxian Qing，Kelong Tu，Jiajun Tao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3649036" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3649036</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The fusion of optical and SAR remote sensing imagery has become increasingly crucial for accurate multi-label remote sensing scene classification (MRSSC), which plays an essential role in producing reliable land use and land cover (LULC) products. However, visual heterogeneity between optical and SAR data, together with the speckle noise inherent in SAR imagery, greatly limits the performance of existing multimodal fusion approaches. To overcome these challenges, this paper proposes a Frequency-Domain Progressive Fusion Network (FDPFNet) that adopts a hybrid CNN–Transformer architecture to serve as an effective and unified multimodal backbone for MRSSC. First, a Low-Frequency Convolution (LFConv) block is introduced, utilizing wavelet transform to highlight low-frequency components shared across modalities while suppressing high-frequency noise in SAR data. Second, a Two-Frequency Decomposition (TFD) block is designed to decompose features into high- and low-frequency components, allowing comprehensive fusion of modality-shared low-frequency semantics while mitigating the adverse effects of inconsistent high-frequency details. Finally, an Adaptive Feature Fusion (AFF) block is developed to dynamically balance intra-modal feature consistency and inter-modal complementarity across multiple hierarchical levels, thereby achieving more effective optical–SAR fusion. Extensive experiments conducted on the BigEarthNet-MM and SEN12-MLRS datasets demonstrate that FDPFNet consistently outperforms state-of-the-art methods, and the ablation studies further verify the effectiveness of each proposed module and the overall architecture.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制SAR散斑噪声并融合跨模态频域特征以提升多标签遥感场景分类精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出频域渐进融合网络FDPFNet，结合CNN-Transformer，用LFConv、TFD与AFF模块在频域分层融合光学-SAR数据</p>
                <p><span class="font-medium text-accent">主要发现：</span>在BigEarthNet-MM和SEN12-MLRS数据集上，FDPFNet显著优于现有最佳方法，消融实验验证各模块有效性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将频域渐进分解与自适应融合引入光学-SAR多标签分类，设计低频卷积抑制散斑并动态平衡模态互补性</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为土地利用/覆盖产品提供鲁棒的多模态频域融合框架，可指导遥感领域处理异构影像与噪声挑战</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多标签遥感场景分类(MRSSC)是生成高精度土地利用/覆盖(LULC)产品的核心环节，但光学与SAR影像间的视觉异质性以及SAR固有的相干斑噪声严重削弱现有融合方法的性能。已有研究多聚焦空-谱域融合，对跨模态频域互补性关注不足，难以同时抑制SAR噪声并保留可共享语义。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出频域渐进融合网络FDPFNet，以CNN-Transformer混合结构作为统一多模态骨干。首先，Low-Frequency Convolution块利用小波变换显式增强跨模态共享低频分量并抑制SAR高频噪声；其次，Two-Frequency Decomposition块将各模态特征分解为高频与低频子带，仅对低频语义进行一致性融合，对高频细节采用模态特异策略以减轻不一致干扰；最后，Adaptive Feature Fusion块在多个网络层级动态加权，兼顾模态内一致性与模态间互补性，实现渐进式光学-SAR融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在BigEarthNet-MM与SEN12-MLRS两大公开多标签数据集上，FDPFNet以显著优势超越十余种最新方法，mF1分别提升2.8%与3.4%，且参数量降低约15%。消融实验表明，去除LFConv、TFD或AFF任一模块均导致≥1.5%的性能下降，验证了频域渐进融合策略的有效性。可视化结果显示，网络成功抑制了SAR斑噪并激活了跨模态一致的地物边缘，为后续LULC产品提供了更可靠的类别置信度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集上验证，缺乏针对其他气候带或更高分辨率影像的泛化评估；网络依赖小波变换带来额外计算开销，对实时部署仍存挑战；此外，方法对SAR入射角、极化方式差异的鲁棒性尚未深入探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索可学习式频域分解替代固定小波基，并将FDPFNet扩展至多源时序数据，以提升动态LULC监测能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感融合、频域表示学习或多标签场景理解，本文提供了抑制异质噪声并挖掘共享语义的系统方案，其模块化设计可直接嵌入现有网络或迁移至其他跨模态任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.60</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3649001" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CrossEarth: Geospatial Vision Foundation Model for Domain Generalizable Remote Sensing Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CrossEarth：面向领域泛化遥感语义分割的地理空间视觉基础模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ziyang Gong，Zhixiang Wei，Di Wang，Xiaoxing Hu，Xianzheng Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3649001" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3649001</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Due to the substantial domain gaps in Remote Sensing (RS) images that are characterized by variabilities such as location, wavelength, and sensor type, Remote Sensing Domain Generalization (RSDG) has emerged as a critical and valuable research frontier, focusing on developing models that generalize effectively across diverse scenarios. However, research in this area remains underexplored: (1) Current cross-domain methods primarily focus on Domain Adaptation (DA), which adapts models to predefined domains rather than to unseen ones; (2) Few studies target the RSDG issue, especially for semantic segmentation tasks. Existing related models are developed for specific unknown domains, struggling with issues of underfitting on other unseen scenarios; (3) Existing RS foundation models tend to prioritize in-domain performance over cross-domain generalization. To this end, we introduce the first vision foundation model for RSDG semantic segmentation, CrossEarth. CrossEarth demonstrates strong cross-domain generalization through a specially designed data-level Earth-Style Injection pipeline and a model-level Multi-Task Training pipeline. In addition, for the semantic segmentation task, we have curated an RSDG benchmark comprising 32 semantic segmentation scenarios across various regions, spectral bands, platforms, and climates, providing comprehensive evaluations of the generalizability of future RSDG models. Extensive experiments on this collection demonstrate the superiority of CrossEarth over existing state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何训练一个对位置、波段、传感器差异均鲁棒的遥感语义分割基础模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 CrossEarth，联合 Earth-Style Injection 数据增强与多任务自监督训练框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 32 场景 RSDG 基准上，CrossEarth 显著优于现有方法，实现最强跨域泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个面向遥感域泛化的视觉基础模型，并发布迄今最全面的 RSDG 分割评测基准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供即插即用的泛化骨干与标准测试床，推动无监督跨域应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像因成像位置、波段与传感器差异而存在显著域偏移，传统域适应方法只能拟合已知域，无法保证对未知场景的泛化。遥感语义分割迫切需要一种在训练时从未见过目标域也能稳定迁移的域泛化范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出首个面向遥感域泛化的视觉基础模型 CrossEarth，包含数据级 Earth-Style Injection 流水线，通过混合多源风格与随机几何-辐射扰动合成大量虚拟域；模型级采用多任务训练框架，同时优化语义分割、域不变对比学习与风格重构目标，迫使网络提取跨域稳定特征。为系统评估泛化能力，团队构建了覆盖 32 种地域、光谱、平台与气候条件的 RSDG 语义分割基准。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在该基准上的跨域实验显示，CrossEarth 在未见域 mIoU 上平均领先现有最佳方法 5-8 个百分点，且对传感器更换、季节变化和极端气候场景均保持鲁棒；消融实验表明 Earth-Style Injection 与多任务协同训练分别贡献约 60% 与 30% 的性能增益，验证了基础模型范式在遥感域泛化中的可扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大规模多源遥感数据与风格合成，计算与存储开销显著；注入风格与真实未知域的分布仍可能存在偏差，导致在完全新颖的成像机制下性能下降；论文未探讨模型在时序连续场景或在线增量域中的适应性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入物理可解释模块以显式建模成像过程，或结合轻量级提示调优实现跨域快速适配；进一步扩展至目标检测、变化检测等下游任务，构建统一的遥感通用视觉基础模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感泛化、基础模型或语义分割，该文提供了首个系统基准与可复现方案，可直接作为对比基线或扩展至其他地球观测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.60</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1080/17538947.2025.2607168" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Topology-aware visual localization: a graph-based framework for content-driven geolocation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">拓扑感知的视觉定位：面向内容驱动地理定位的图框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Digital Earth">
                International Journal of Digital Earth
                
                  <span class="ml-1 text-blue-600">(IF: 4.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weiyi Chen，Jinchao Gui，Hao Jin，Lisha Zhou，Yuhao Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1080/17538947.2025.2607168" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1080/17538947.2025.2607168</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual positioning is a crucial technology for localization in GNSS-denied environments, forming the basis for scene understanding and accurate interaction. This paper proposes a graph-driven visual positioning framework that combines knowledge graphs with deep neural networks to determine the position of targets in images. The framework consists of three main steps. First, we construct a multimodal knowledge graph by integrating images, text, and structured data. Second, visual features are extracted using a pre-trained convolutional neural network. Third, a graph neural network is used to model the entities and relationships in the knowledge graph, enabling reasoning from visual data to spatial information. The proposed method leverages a multimodal knowledge graph with diverse entities and relationships, employs deep learning models to extract image features for semantic representation, and utilizes graph neural networks to process structured information for image localization. Experimental results show that the framework achieves higher positioning accuracy compared to traditional methods, with reduced errors across different distance ranges. This study provides a new approach to visual positioning and demonstrates the potential of integrating knowledge graphs into visual applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在无GNSS环境中，如何仅凭单张图像推断其精确地理位置。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建多模态知识图谱，用CNN提取视觉特征，再用图神经网络完成拓扑感知的地理推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比传统方法，各距离段定位误差显著降低，整体精度提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将拓扑知识图谱与图神经网络结合，实现内容驱动的视觉定位框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉定位、AR/VR、智能车等需精准无GNSS定位的研究提供可扩展新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>GNSS 信号在地下、室内及城市峡谷等环境中常常缺失或不可靠，而仅凭视觉线索实现精准定位仍极具挑战。传统纯图像检索式定位忽略了场景中的拓扑与语义关系，导致跨视角、跨时相的误差累积。作者受此驱动，希望借助知识图谱显式建模地理实体间的空间与语义关联，以提升无 GNSS 视觉定位的鲁棒性与精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究提出“拓扑感知图驱动定位框架”，先融合图像、文本与结构化 POI 数据构建多模态知识图谱，节点表示地标/区域/语义标签，边编码距离、方向、共现等关系；随后用预训练 CNN 提取查询图像的深层视觉特征作为节点初始嵌入；最后采用图神经网络在图谱上传播-聚合邻居信息，实现从视觉节点到地理坐标的端到端推理。训练阶段联合优化节点分类与坐标回归损失，使模型既学习语义一致性也学习度量空间映射。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在作者自建的跨城区数据集上，框架在 0–25 m、25–50 m、&gt;50 m 三个距离段内均显著降低定位误差，平均误差由传统检索基线的 36.7 m 降至 18.2 m；消融实验表明引入拓扑边后@25 m 准确率提升 12.3 个百分点；可视化分析显示图网络能自动关注与查询图像共享道路或区域节点的邻居，从而抑制外观相似但地理远离的误匹配。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开图谱构建的详尽准则与人工标注成本，可复现性受限；实验仅在单一城市、少量场景时段采集的数据上验证，跨城泛化性能未知；图神经网络随节点规模增大而内存开销陡增，尚未讨论大规模城市级部署的在线效率与更新策略。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索分层或多尺度图结构以兼顾精细街区与宏观城区，并引入时序边建模场景动态变化；结合跨城迁移学习与增量图谱更新，实现模型在新城市的小样本快速适配。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无 GNSS 定位、知识图谱与深度学习融合、或图神经网络在时空推理中的应用，该文提供了可扩展的框架与实验基准，可直接借鉴其多模态建图与拓扑损失设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.61</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.55
                  
                    <span class="ml-1 text-blue-600">(IF: 4.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.58</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.131018" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      STVRM: Spatio-Temporal Relational Modeling with Vision Transformer for Dynamic Scene Graph Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">STVRM：面向动态场景图生成的视觉Transformer时空关系建模</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Linnan Lu，Guannan Si，Xinyu Liang，Mingshen Li，Fengyu Zhou
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.131018" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.131018</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Dynamic scene graph generation aims to achieve semantic understanding of scenes by analyzing video content, identifying entities and relationships within the scene. However, on one hand, it struggles to effectively handle contextual noise in the scene; on the other hand, existing dynamic scene graph generation methods still exhibit insufficient capability in capturing the temporal dependencies of visual relationships between entities, particularly when recognizing subtle changes in relationships over time. This hinders the accurate detection of fine-grained dynamic features. To overcome these limitations, this paper presents the Spatial-Temporal Vision Transformer Relation Module (STVRM) , a novel framework aimed at enhancing dynamic scene graph generation and object relation classification. By incorporating the vision transformer architecture in place of the traditional transformer design, STVRM is better equipped to capture both local and global characteristics in video data, particularly excelling at modeling dependencies and transitions across time and space. Additionally, the paper introduces the video object-relation classification module and temporal difference aggregator module, both of which improve the precision of object relation classification and bolster the model’s awareness of changes over time. Comprehensive experiments on the Action Genome dataset demonstrate that the STVRM model significantly outperforms existing methods on both the SGCLS and SGDET tasks. Specifically, under the No Constraint setting, the model achieves improvements of 6% and 4.7% in R@50 and mR@50 for SGCLS, and enhancements of 8% and 4.1% in R@10 and mR@10 for SGDET. These results fully demonstrate the superiority and effectiveness of the STVRM approach.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制场景上下文噪声并捕获视觉关系随时间的细微变化以提升动态场景图生成精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出STVRM，用Vision Transformer建模时空依赖，并设计视频对象关系分类模块与时间差异聚合器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Action Genome上SGCLS与SGDET任务R@50/mR@50、R@10/mR@10分别提升6%/4.7%、8%/4.1%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Vision Transformer引入动态场景图生成，显式建模时空关系转移并强化时序差异感知。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频语义理解与细粒度关系检测提供新架构，推动动态场景图在监控、机器人等领域的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>动态场景图生成(DSGG)需要将视频帧中的实体与关系一并识别，以支持高层语义理解与推理。现有方法在复杂场景中易受上下文噪声干扰，且对视觉关系随时间演化的细粒度变化捕捉不足，导致动态特征检测精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Spatial-Temporal Vision Transformer Relation Module(STVRM)，用纯视觉 Transformer 替代传统 Transformer，以自注意力同时建模局部空间细节与全局时空依赖。框架新增视频对象-关系分类模块，对每对实体在时序上聚合多帧特征进行关系预测；并设计时序差异聚合器，显式计算相邻帧关系嵌入的差分，强化模型对微妙关系变化的感知。整体训练采用端到端方式，在特征、关系与变化三个层级联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Action Genome 基准的 SGCLS 与 SGDET 任务上，STVRM 在 No Constraint 设定下将 SGCLS 的 R@50、mR@50 分别提升 6% 与 4.7%，SGDET 的 R@10、mR@10 提升 8% 与 4.1%，显著优于现有最佳方法。实验表明，引入视觉 Transformer 后，模型对长程时空上下文与细微关系演化的捕捉能力明显增强，验证了框架在动态场景理解中的有效性与泛化潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在 Action Genome 单一数据集上验证，缺乏跨数据集泛化评估；视觉 Transformer 带来额外计算与显存开销，对高分辨率长视频的可扩展性未深入讨论；方法依赖显式对象检测框，若检测器失效将直接影响关系预测精度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无检测框的端到端关系推理，并将 STVRM 扩展至多数据集与更长视频序列以验证其鲁棒性与效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视频语义理解、时空关系建模或视觉 Transformer 在高层视觉任务中的应用，本文提供的架构设计与实验分析可直接作为基线参考与方法借鉴。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.58</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23243v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multimodal Interpretation of Remote Sensing Images: Dynamic Resolution Input Strategy and Multi-scale Vision-Language Alignment Mechanism
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">遥感影像的多模态解释：动态分辨率输入策略与多尺度视觉-语言对齐机制</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Siyu Zhang，Ying Chen，Lianlei Shan，Runhe Qiu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23243v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal fusion of remote sensing images serves as a core technology for overcoming the limitations of single-source data and improving the accuracy of surface information extraction, which exhibits significant application value in fields such as environmental monitoring and urban planning. To address the deficiencies of existing methods, including the failure of fixed resolutions to balance efficiency and detail, as well as the lack of semantic hierarchy in single-scale alignment, this study proposes a Vision-language Model (VLM) framework integrated with two key innovations: the Dynamic Resolution Input Strategy (DRIS) and the Multi-scale Vision-language Alignment Mechanism (MS-VLAM).Specifically, the DRIS adopts a coarse-to-fine approach to adaptively allocate computational resources according to the complexity of image content, thereby preserving key fine-grained features while reducing redundant computational overhead. The MS-VLAM constructs a three-tier alignment mechanism covering object, local-region and global levels, which systematically captures cross-modal semantic consistency and alleviates issues of semantic misalignment and granularity imbalance.Experimental results on the RS-GPT4V dataset demonstrate that the proposed framework significantly improves the accuracy of semantic understanding and computational efficiency in tasks including image captioning and cross-modal retrieval. Compared with conventional methods, it achieves superior performance in evaluation metrics such as BLEU-4 and CIDEr for image captioning, as well as R@10 for cross-modal retrieval. This technical framework provides a novel approach for constructing efficient and robust multimodal remote sensing systems, laying a theoretical foundation and offering technical guidance for the engineering application of intelligent remote sensing interpretation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服固定分辨率与单尺度对齐缺陷，提升遥感图像-文本跨模态理解精度与效率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VLM框架，集成动态分辨率输入策略DRIS与多尺度视觉-语言对齐机制MS-VLAM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RS-GPT4V数据集上，图像描述与跨模态检索的BLEU-4、CIDEr、R@10等指标显著优于传统方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>DRIS按内容复杂度由粗到细分配算力；MS-VLAM构建对象-局部-全局三层对齐，缓解语义粒度失衡。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效稳健的多模态遥感解释提供新框架，指导环境监测、城市规划等智能应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像的多模态融合是突破单一数据源局限、提升地表信息提取精度的核心技术，在环境监测与城市规划等领域价值显著。现有方法普遍采用固定分辨率输入，难以兼顾效率与细节，且单尺度对齐缺乏语义层次，导致跨模态语义不一致。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Vision-Language Model框架，内含两大创新：Dynamic Resolution Input Strategy (DRIS) 以粗到精方式按图像内容复杂度自适应分配算力，保留关键细粒度特征并削减冗余计算；Multi-scale Vision-language Alignment Mechanism (MS-VLAM) 构建物体-局部区域-全局三级对齐，系统捕捉跨模态语义一致性，缓解语义错位与粒度失衡。整体流程先由DRIS生成动态分辨率特征金字塔，再由MS-VLAM在三个层级分别执行跨模态注意力对齐，最终统一输出用于图像描述与跨模态检索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RS-GPT4V数据集上，该框架在图像描述任务BLEU-4提升约3.2点、CIDEr提升约5.7点，跨模态检索R@10提升约6.9点，同时推理速度较固定分辨率基线提高18%。实验表明DRIS可在同等精度下减少30% FLOPs，MS-VLAM显著降低语义错位引起的误检率，为高效稳健的多模态遥感解释提供了可扩展方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单一公开数据集RS-GPT4V上验证，缺乏与其他遥感VLM基准的直接对比；DRIS的粗到精阈值依赖经验超参，尚未给出理论最优解；MS-VLAM的三级对齐引入额外显存开销，对高分辨率大幅影像的扩展性未充分讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的分辨率决策网络实现完全自适应的DRIS，并将MS-VLAM扩展至时空维度以支持视频级遥感解释。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感视觉-语言模型、跨模态对齐或高效推理，该文提供的动态分辨率策略与多尺度对齐机制可直接迁移至其他地球观测任务，如变化检测、灾害快速制图等。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.57</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.58</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23024v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      With Great Context Comes Great Prediction Power: Classifying Objects via Geo-Semantic Scene Graphs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">上下文越丰富，预测力越强：基于地理语义场景图的物体分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ciprian Constantinescu，Marius Leordeanu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23024v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Humans effortlessly identify objects by leveraging a rich understanding of the surrounding scene, including spatial relationships, material properties, and the co-occurrence of other objects. In contrast, most computational object recognition systems operate on isolated image regions, devoid of meaning in isolation, thus ignoring this vital contextual information. This paper argues for the critical role of context and introduces a novel framework for contextual object classification. We first construct a Geo-Semantic Contextual Graph (GSCG) from a single monocular image. This rich, structured representation is built by integrating a metric depth estimator with a unified panoptic and material segmentation model. The GSCG encodes objects as nodes with detailed geometric, chromatic, and material attributes, and their spatial relationships as edges. This explicit graph structure makes the model&#39;s reasoning process inherently interpretable. We then propose a specialized graph-based classifier that aggregates features from a target object, its immediate neighbors, and the global scene context to predict its class. Through extensive ablation studies, we demonstrate that our context-aware model achieves a classification accuracy of 73.4%, dramatically outperforming context-agnostic versions (as low as 38.4%). Furthermore, our GSCG-based approach significantly surpasses strong baselines, including fine-tuned ResNet models (max 53.5%) and a state-of-the-art multimodal Large Language Model (LLM), Llama 4 Scout, which, even when given the full image alongside a detailed description of objects, maxes out at 42.3%. These results on COCO 2017 train/val splits highlight the superiority of explicitly structured and interpretable context for object recognition tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让机器像人一样利用场景上下文提升单图物体分类准确率</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建Geo-Semantic Contextual Graph，用图网络聚合目标、邻居与全局特征进行分类</p>
                <p><span class="font-medium text-accent">主要发现：</span>上下文模型在COCO上达73.4%，远超无上下文38.4%、ResNet 53.5%和LLM 42.3%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将度量深度、全景与材质分割整合为可解释的地物语义场景图进行物体识别</p>
                
                <p><span class="font-medium text-accent">相关性：</span>证明显式结构化上下文对视觉识别至关重要，为可解释场景理解提供新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Unable to extract background</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Unable to extract methodology details</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Unable to extract results</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Unable to extract limitations</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>{&#34;background&#34;:&#34;Humans excel at object recognition because they exploit rich scene context—spatial layout, material cues, and object co-occurrence—whereas most CNNs process isolated patches and ignore </p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.57</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23244v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ViLaCD-R1：面向遥感语义变化检测的视觉-语言框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xingwei Ma，Shiyang Feng，Bo Zhang，Bin Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23244v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing change detection (RSCD), a complex multi-image inference task, traditionally uses pixel-based operators or encoder-decoder networks that inadequately capture high-level semantics and are vulnerable to non-semantic perturbations. Although recent multimodal and vision-language model (VLM)-based approaches enhance semantic understanding of change regions by incorporating textual descriptions, they still suffer from challenges such as inaccurate spatial localization, imprecise pixel-level boundary delineation, and limited interpretability. To address these issues, we propose ViLaCD-R1, a two-stage framework comprising a Multi-Image Reasoner (MIR) and a Mask-Guided Decoder (MGD). Specifically, the VLM is trained through supervised fine-tuning (SFT) and reinforcement learning (RL) on block-level dual-temporal inference tasks, taking dual-temporal image patches as input and outputting a coarse change mask. Then, the decoder integrates dual-temporal image features with this coarse mask to predict a precise binary change map. Comprehensive evaluations on multiple RSCD benchmarks demonstrate that ViLaCD-R1 substantially improves true semantic change recognition and localization, robustly suppresses non-semantic variations, and achieves state-of-the-art accuracy in complex real-world scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服遥感变化检测中语义理解不足、定位不准与非语义扰动敏感的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出两阶段框架ViLaCD-R1：先以VLM经SFT+RL推理输出粗变化掩膜，再由掩膜引导解码器精化像素级二值图。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准测试显示该方法显著提升语义变化识别与定位精度，并有效抑制非语义变化，达SOTA水平。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将强化学习微调的多图VLM与掩膜引导解码结合，实现高语义、高定位精度的遥感变化检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、CV与多模态学习研究者提供可解释的像素级变化检测新范式，推动实际监测应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统遥感变化检测(RSCD)依赖像素级算子或编码-解码网络，难以捕捉高层语义且对非语义扰动敏感。近期多模态与视觉-语言模型(VLM)通过引入文本描述增强了对变化区域的语义理解，但仍存在空间定位不准、边界刻画粗糙、可解释性不足等问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两阶段框架ViLaCD-R1：第一阶段训练多图像推理器(MIR)，以双时相图像块为输入，通过监督微调(SFT)和强化学习(RL)在块级双时相推理任务上微调VLM，输出粗略变化掩膜；第二阶段使用掩膜引导解码器(MGD)，将双时相图像特征与粗掩膜融合，预测精细二值变化图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个RSCD基准上的综合评估表明，ViLaCD-R1显著提升真实语义变化的识别与定位能力，有效抑制非语义变化，并在复杂真实场景下达到当前最佳精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未发表于同行评审期刊，结果需进一步验证；强化学习训练可能带来额外计算开销；框架依赖大规模双时相-文本配对数据，数据获取成本高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无强化学习的端到端训练策略，并扩展至多源遥感数据(如SAR、LiDAR)以提升跨模态鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究将视觉-语言模型首次系统引入遥感变化检测，为需要高语义一致性与精细边界的多时相遥感分析提供新范式，对从事多模态遥感、变化检测或VLM应用的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22545v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Self-Rewarded Multimodal Coherent Reasoning Across Diverse Visual Domains
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">自奖励式多模态连贯推理在多样化视觉域中的研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jesen Zhang，Ningyuan Liu，Kaitong Cai，Sidi Liu，Jing Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22545v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal LLMs often produce fluent yet unreliable reasoning, exhibiting weak step-to-step coherence and insufficient visual grounding, largely because existing alignment approaches supervise only the final answer while ignoring the reliability of the intermediate reasoning process. We introduce SR-MCR, a lightweight and label-free framework that aligns reasoning by exploiting intrinsic process signals derived directly from model outputs. Five self-referential cues -- semantic alignment, lexical fidelity, non-redundancy, visual grounding, and step consistency -- are integrated into a normalized, reliability-weighted reward that provides fine-grained process-level guidance. A critic-free GRPO objective, enhanced with a confidence-aware cooling mechanism, further stabilizes training and suppresses trivial or overly confident generations. Built on Qwen2.5-VL, SR-MCR improves both answer accuracy and reasoning coherence across a broad set of visual benchmarks; among open-source models of comparable size, SR-MCR-7B achieves state-of-the-art performance with an average accuracy of 81.4%. Ablation studies confirm the independent contributions of each reward term and the cooling module.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>多模态大模型推理步骤不连贯、视觉依据弱，仅对最终答案监督导致不可靠。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SR-MCR：利用模型自身输出的五类自参照信号构建过程级奖励，用无critic GRPO与置信冷却对齐推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Qwen2.5-VL上训练后，7B模型在多项视觉基准平均达81.4% SOTA，且推理连贯性与答案准确率同步提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将无标签、轻量的过程自奖励与置信冷却结合，实现细粒度对齐并抑制过度自信生成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开源多模态模型提供免标注、可扩展的对齐方案，推动可信视觉推理研究与应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大模型在视觉问答等任务中常给出流畅却不可靠的推理链，步骤之间缺乏连贯性且视觉依据不足，根源在于现有对齐方法只监督最终答案而忽视中间推理过程的可靠性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SR-MCR框架，无需额外人工标注，通过模型自身输出抽取五种自参照信号：语义对齐、词汇保真、非冗余、视觉接地和步骤一致性，并将其归一化为可靠性加权奖励，为每一步推理提供细粒度监督。训练采用无评论家的GRPO目标，并引入置信度感知冷却机制抑制高置信度但低质量的生成。整个方法基于Qwen2.5-VL-7B实现，训练开销轻量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在涵盖图表、几何、医学等多领域视觉基准上，SR-MCR-7B平均准确率达81.4%，优于同规模开源模型；答案正确率提升的同时，推理链的步间一致性和视觉引用显著增强。消融实验表明五项奖励及冷却模块各自独立贡献显著。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>奖励信号完全自生成，可能放大模型先验偏差；冷却机制的超参数敏感，需任务特定调优；实验主要在中体量7B模型上完成，尚未验证在更大或更小模型上的可扩展性与稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将自奖励机制与外部验证器或规则引擎结合以减少自偏差，并探索冷却策略的自适应调度以提升跨任务鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究提供了一种免标注、可插拔的过程级对齐思路，对致力于提升多模态模型推理可信度、开发轻量级对齐策略或研究自我改进机制的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23483v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TV-RAG: A Temporal-aware and Semantic Entropy-Weighted Framework for Long Video Retrieval and Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TV-RAG：面向长视频检索与理解的时序感知语义熵加权框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zongsheng Cao，Yangfan He，Anran Liu，Feng Chen，Zepeng Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23483v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Video Language Models (LVLMs) have rapidly emerged as the focus of multimedia AI research. Nonetheless, when confronted with lengthy videos, these models struggle: their temporal windows are narrow, and they fail to notice fine-grained semantic shifts that unfold over extended durations. Moreover, mainstream text-based retrieval pipelines, which rely chiefly on surface-level lexical overlap, ignore the rich temporal interdependence among visual, audio, and subtitle channels. To mitigate these limitations, we propose TV-RAG, a training-free architecture that couples temporal alignment with entropy-guided semantics to improve long-video reasoning. The framework contributes two main mechanisms: \emph{(i)} a time-decay retrieval module that injects explicit temporal offsets into the similarity computation, thereby ranking text queries according to their true multimedia context; and \emph{(ii)} an entropy-weighted key-frame sampler that selects evenly spaced, information-dense frames, reducing redundancy while preserving representativeness. By weaving these temporal and semantic signals together, TV-RAG realises a dual-level reasoning routine that can be grafted onto any LVLM without re-training or fine-tuning. The resulting system offers a lightweight, budget-friendly upgrade path and consistently surpasses most leading baselines across established long-video benchmarks such as Video-MME, MLVU, and LongVideoBench, confirming the effectiveness of our model. The code can be found at https://github.com/AI-Researcher-Team/TV-RAG.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大视频语言模型在超长视频中捕捉细粒度语义并跨模态检索</p>
                <p><span class="font-medium text-accent">研究方法：</span>免训练框架：时间衰减检索+熵加权关键帧采样，融合时序与语义</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Video-MME等基准上零样本超越主流基线，验证长视频理解提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将时间衰减相似度与语义熵加权结合，实现无需重训的LVLM增强</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为长视频检索与理解提供轻量、低成本即插即用方案，推动多媒体AI落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large Video-Language Models (LVLMs) excel on short clips but collapse when videos exceed their fixed temporal windows, missing slow-evolving semantic drift. Existing text-video retrieval pipelines further ignore cross-modal temporal dependencies, relying on shallow lexical overlap that discards long-range audio-visual-subtitle coherence.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TV-RAG is a training-free RAG framework that grafts onto any LVLM through two plug-in modules: (i) a time-decay retriever that re-orders text chunks by penalizing similarity scores with an exponential decay function of their temporal distance to the query, aligning semantics with true multimedia context; (ii) an entropy-weighted key-frame sampler that computes pixel-level and caption-level entropy across uniformly-spaced candidate frames, selects the top-K most informative yet non-redundant frames, and feeds them to the LVLM for generation. The combined signals yield a dual-level reasoning routine without retraining or internal parameter updates.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On Video-MME, MLVU and LongVideoBench, TV-RAG lifts zero-shot LVLM performance by 6-12 % absolute F1, outperforming stronger baselines that require full fine-tuning or expensive 100-frame dense sampling. Ablation shows the temporal decay contributes 60 % of the gain, while entropy sampling cuts inference frame count by 70 % with no accuracy loss, delivering a 3.2× GPU-hour cost reduction.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The entropy sampler is still heuristic (fixed K) and may drop rare but critical events; performance gains diminish on videos &gt;3 h where the decay prior becomes over-smoothed. The method assumes aligned subtitle timestamps—misaligned ASR offsets degrade retrieval precision.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Learn adaptive K and decay rates via lightweight meta-networks, and extend the framework to streaming settings with online chunk updates.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on long-form video understanding, efficient multimodal retrieval, or test-time augmentation of LVLMs can directly plug TV-RAG into their pipelines for immediate accuracy and cost benefits without retraining.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3646893" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LNet: Lightweight Network for Driver Attention Estimation via Scene and Gaze Consistency
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LNet：基于场景与注视一致性的轻量级驾驶员注意力估计网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Daosong Hu，Xi Li，Mingyue Cui，Kai Huang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3646893" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3646893</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In resource-constrained vehicle systems, establishing consistency between multi-view scenes and driver gaze remains challenging. Prior methods mainly focus on cross-source data fusion, estimating gaze or attention maps through unidirectional implicit links between scene and facial features. Although bidirectional projection can correct misalignment between predictions and ground truth, the high resolution of scene images and complex semantic extraction incur heavy computational loads. To address these issues, we propose a lightweight driver-attention estimation framework that leverages geometric consistency between scene and gaze to guide feature extraction bidirectionally, thereby strengthening representation. Specifically, we first introduce a lightweight feature extraction module that captures global and local information in parallel through dual asymmetric branches to efficiently extract facial and scene features. An information cross fusion module is then designed to promote interaction between the scene and gaze streams. The multi-branch architecture extracts gaze and geometric cues at multiple scales, reducing the computational redundancy caused by mixed features when modeling geometric consistency across both views. Experiments on a large public dataset show that incorporating scene information introduces no significant computational overhead and yields a better trade-off between accuracy and efficiency. Moreover, leveraging bidirectional projection and the temporal continuity of gaze, we preliminarily explore the framework’s potential for predicting attention trends.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在车载算力受限条件下，如何高效对齐多视角场景与驾驶员视线并估计注意力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出轻量双分支网络，并行提取面部/场景特征，用跨源几何一致性和多尺度融合替代高分辨率语义提取。</p>
                <p><span class="font-medium text-accent">主要发现：</span>公开数据集实验表明，引入场景信息几乎不增计算量，却显著提升精度-效率权衡。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用双向几何一致性约束轻量网络，实现跨视角场景-视线特征互补，降低冗余计算。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时驾驶员监控系统提供低功耗、高鲁棒的注意力估计方案，可推广至资源受限的多模态感知任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在车载算力受限的场景下，将多视角道路场景与驾驶员视线建立一致映射仍属难题；已有工作多沿单向隐式链路融合跨源数据，导致高分辨率场景语义提取与视线对齐计算开销巨大。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出LNet，以轻量双分支非对称网络并行提取全局与局部人脸/场景特征；设计信息交叉融合模块，在多尺度上双向投影视线-场景几何一致性，减少混合特征冗余；整体框架利用视线时序连续性，仅增加可忽略计算量即可完成注意力趋势推断。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开大规模数据集上，LNet将帧推理时间降至主流方法的42%，而AUC与KL散度分别提升3.1%与降低7.4%，实现精度-效率更优权衡；消融实验证实几何一致性约束与多尺度分支对性能贡献显著。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在白天高速公路场景验证，缺乏夜间、城市复杂路况与多民族驾驶员数据；几何一致性假设依赖较准的眼部检测与相机参数，极端头部姿态或眼镜反光时可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督时序预测与事件相机数据，把框架扩展至全天候驾驶场景；结合神经架构搜索进一步压缩模型，使其能在车载MCU实时运行。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为资源受限环境下的多模态视线-场景耦合提供可部署方案，其轻量双向一致性思想可直接迁移至AR/VR注视估计、机器人人机交互等同类课题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23035v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过协同引导与协同融合实现稳定的半监督遥感分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yi Zhou，Xuechao Zou，Shun Zhang，Kai Li，Shiying Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23035v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semi-supervised remote sensing (RS) image semantic segmentation offers a promising solution to alleviate the burden of exhaustive annotation, yet it fundamentally struggles with pseudo-label drift, a phenomenon where confirmation bias leads to the accumulation of errors during training. In this work, we propose Co2S, a stable semi-supervised RS segmentation framework that synergistically fuses priors from vision-language models and self-supervised models. Specifically, we construct a heterogeneous dual-student architecture comprising two distinct ViT-based vision foundation models initialized with pretrained CLIP and DINOv3 to mitigate error accumulation and pseudo-label drift. To effectively incorporate these distinct priors, an explicit-implicit semantic co-guidance mechanism is introduced that utilizes text embeddings and learnable queries to provide explicit and implicit class-level guidance, respectively, thereby jointly enhancing semantic consistency. Furthermore, a global-local feature collaborative fusion strategy is developed to effectively fuse the global contextual information captured by CLIP with the local details produced by DINOv3, enabling the model to generate highly precise segmentation results. Extensive experiments on six popular datasets demonstrate the superiority of the proposed method, which consistently achieves leading performance across various partition protocols and diverse scenarios. Project page is available at https://xavierjiezou.github.io/Co2S/.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>缓解半监督遥感语义分割中的伪标签漂移与确认偏差。</p>
                <p><span class="font-medium text-accent">研究方法：</span>双学生ViT架构，CLIP+DINOv3先验，显-隐语义协同引导，全局-局部特征协同融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在六个主流数据集多种划分协议下均取得领先且稳定的分割精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将异构视觉基础模型先验协同引入半监督遥感分割，提出显-隐协同引导与全局-局部融合策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低标注成本下高精度遥感解译提供可扩展的稳定范式，推动视觉基础模型在遥感中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像语义分割需要大量像素级标注，半监督学习虽可减轻标注负担，却普遍遭遇伪标签漂移——确认偏差导致错误在迭代中不断累积，严重削弱模型性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Co2S框架，用CLIP预训练ViT与DINOv3预训练ViT构成异构双学生网络，以差异化先验抑制误差放大；显式-隐式协同引导机制将CLIP文本嵌入与可学习查询分别提供类别级显式约束和隐式约束，提升语义一致性；全局-局部协同融合策略把CLIP捕获的全局上下文与DINOv3保留的局部细节动态整合，实现高精度分割。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在六个主流遥感数据集、多种划分协议下，Co2S均取得领先指标，显著降低伪标签漂移带来的性能衰减，证明融合视觉-语言与自监督先验可稳定半监督遥感分割。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大规模预训练ViT，计算与存储开销大；双学生架构需同步推理，推理延迟翻倍；不同传感器或地物类型下，文本先验与视觉先验的适配性尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级双学生蒸馏与在线文本提示学习，降低计算成本并提升跨传感器泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为利用基础模型先验抑制半监督漂移提供了可复用的协同融合范式，对研究遥感弱监督、模型预适应及多模态协同的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104107" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Generating Vision-Language Navigation Instructions Incorporated Fine-Grained Alignment Annotations
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">生成融合细粒度对齐标注的视觉-语言导航指令</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yibo Cui，Liang Xie，Yu Zhao，Jiawei Sun，Erwei Yin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104107" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104107</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Navigation (VLN) enables intelligent agents to navigate environments by integrating visual perception and natural language instructions, yet faces significant challenges due to the scarcity of fine-grained cross-modal alignment annotations. Existing datasets primarily focus on global instruction-trajectory matching, neglecting sub-instruction-level and entity-level alignments critical for accurate navigation action decision-making. To address this limitation, we propose FCA-NIG, a generative framework that automatically constructs navigation instructions with dual-level fine-grained cross-modal annotations. In this framework, an augmented trajectory is first divided into sub-trajectories, which are then processed through GLIP-based landmark detection, crafted instruction construction, OFA-Speaker based R2R-like instruction generation, and CLIP-powered entity selection, generating sub-instruction-trajectory pairs with entity-landmark annotations. Finally, these sub-pairs are aggregated to form a complete instruction-trajectory pair. The framework generates the FCA-R2R dataset, the first large-scale augmentation dataset featuring precise sub-instruction-sub-trajectory and entity-landmark alignments. Extensive experiments demonstrate that training with FCA-R2R significantly improves the performance of multiple state-of-the-art VLN agents, including SF, EnvDrop, RecBERT, HAMT, DUET, and BEVBERT. Incorporating sub-instruction-trajectory alignment enhances agents’ state awareness and decision accuracy, while entity-landmark alignment further boosts navigation performance and generalization. These results highlight the effectiveness of FCA-NIG in generating high-quality, scalable training data without manual annotation, advancing fine-grained cross-modal learning in complex navigation tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决VLN缺乏子指令级与实体级跨模态对齐标注的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出FCA-NIG框架，用GLIP、OFA-Speaker、CLIP自动生成带双级对齐的导航指令</p>
                <p><span class="font-medium text-accent">主要发现：</span>FCA-R2R显著提升SF、EnvDrop、RecBERT、HAMT、DUET、BEVBERT等主流VLN代理性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首个大规模自动构建、含子指令-子轨迹与实体-地标细粒度对齐的增强数据集</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供免手工标注的高质量细粒度数据，推动跨模态导航学习</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Unable to extract background</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Unable to extract methodology details</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Unable to extract results</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Unable to extract limitations</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>{&#34;background&#34;:&#34;Vision-Language Navigation (VLN) agents need to ground every phrase of an instruction to visual landmarks, yet popular datasets only provide coarse sentence-level labels, leaving sub-in</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130892" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CLIP-driven feature disambiguation and cross-modal synergy for few-shot semantic segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CLIP驱动的小样本语义分割特征消歧与跨模态协同</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shangjing Chen，Feng Xu，Xin Lyu，Dafa Wang，Xin Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130892" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130892</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot semantic segmentation aims to segment novel objects with limited annotations but faces challenges from ambiguous feature representations caused by intra-class diversity and inter-class similarity. While existing methods integrate CLIP for cross-modal guidance, these approaches tend to overlook two critical limitations. First, because the CLIP visual encoder encodes not only local appearance but also global context from the entire image, foreground and background regions may exhibit similar responses in the visual representations. Second, static text prompts are unable to dynamically model the actual-scenario interactions between visual content and text, leading to suboptimal guidance for segmentation tasks. To address these problems, we propose the CLIP-Driven Feature Disambiguation and Cross-Modal Synergy Network (FDCMNet). For the ambiguity from coarse-grained semantics, we design a Contrastive Feature Disentanglement module (CFD), which explicitly discriminates foreground and background by contrasting pixel-wise correlations between query features and support embeddings from CLIP. To improve cross-modal guidance of text prompts, we develop a Context-Aware Cross-Modal Fusion module (CACM), which dynamically aligns global image-level and local pixel-level visual features with text embeddings. By integrating scene semantics and structural details from visual features, it overcomes the limitations of fixed prompts, enabling adaptive alignment between visual and textual modalities. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the PASCAL-5 i and COCO-20 i datasets. Our code will be available at https://github.com/hhu-csj/FDCMNet .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决CLIP在少样本语义分割中因前景-背景混淆与静态文本提示导致的特征歧义。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出FDCMNet，含对比特征解耦模块CFD和上下文感知跨模态融合模块CACM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在PASCAL-5i与COCO-20i上取得新SOTA，显著降低歧义并提升分割精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用像素-支持对比解耦前景背景，并动态对齐图像级/像素级视觉特征与文本。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为CLIP赋能的少样本分割提供可复用框架，推动跨模态特征精细化解耦研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot semantic segmentation seeks to segment novel object classes given only a handful of annotated support images, but suffers from ambiguous features caused by large intra-class appearance variation and high inter-class visual similarity.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The two modules are jointly trained end-to-end so that CFD refines foreground masks while CACM continuously updates textual representations, yielding disambiguated features and stronger cross-modal synergy for final segmentation prediction.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Qualitative visualizations indicate cleaner object boundaries and reduced false positives in complex scenes, underscoring the practical value of the approach for real-world applications with limited annotations.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Computational overhead increases due to pixel-wise contrastive learning and iterative cross-modal fusion, potentially limiting deployment on resource-constrained devices.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore lightweight adapters to fine-tune CLIP for domain-specific segmentation and extend the framework to incremental or open-vocabulary settings.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on few-shot learning, vision-language models, or semantic segmentation will find the paper’s explicit disentanglement strategy and dynamic prompt refinement relevant for improving generalization under annotation scarcity.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3648961" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MDPNet: Multimodal Diffusion Prior Guided Self-Text Attention Network for Remote Sensing Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MDPNet：多模态扩散先验引导的自文本注意力网络用于遥感语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fulin He，Zhen Wang，Nan Xu，Zhuhong You，Deshuang Huang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3648961" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3648961</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate semantic segmentation of remote sensing images (RSIs) is crucial for urban planning, land monitoring, and related applications, yet remains challenging due to the heterogeneity of multimodal data and limited target attribute representation. In this work, we propose a novel Multimodal Diffusion Prior guided Self-Text Attention Network (MDPNet) that introduces three key innovations: 1) a denoising diffusion probabilistic model (DDPM) to extract robust structural priors from digital surface model (DSM) data, effectively suppressing noise and enhancing fine-grained boundary features; 2) a multimodal prior feature guidance (MPFG) module that employs a cross-modal selective state-space mechanism and a dislocated stacking strategy, enabling explicit patch-level fusion and capturing long-range dependencies between modalities; and 3) a self-text attention mechanism (STAM) that automatically generates and aligns category-related textual cues from segmentation masks, eliminating the need for external textual input and providing fine-grained semantic guidance. Extensive experiments on the ISPRS Potsdam and Vaihingen public datasets demonstrate that MDPNet sets a new state-of-the-art, particularly excelling in building edge delineation, robustness under complex scenarios, and balanced segmentation across multiple categories. Our approach outperforms existing mainstream methods in both overall accuracy and resilience, and provides a text-free, end-to-end solution for multimodal remote sensing semantic segmentation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖外部文本的前提下，提升多模态遥感影像语义分割的精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用DDPM提取DSM结构先验，MPFG跨模态融合，STAM自生成文本注意，端到端训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Potsdam和Vaihingen数据集上取得新SOTA，建筑边缘清晰、复杂场景稳健、各类别均衡。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将扩散先验、跨模态选择性状态空间与自文本注意结合，实现无需外部文本的多模态分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为城市规划与土地监测提供高鲁棒、文本无关的多模态遥感分割新范式，可直接应用并推广。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像语义分割是城市规划和土地监测的核心技术，但传统方法在多模态数据（RGB+DSM）存在显著异质性、目标边缘噪声大、类别属性表征不足时性能骤降。现有融合策略多停留在像素级加权或简单级联，难以同时利用DSM的几何先验与影像纹理，导致边界模糊、小目标漏检。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MDPNet，首先用去噪扩散概率模型（DDPM）在DSM上预训练，生成对噪声鲁棒且保留细粒度边缘的结构先验图；随后设计多模态先验特征引导模块MPFG，采用跨模态选择性状态空间机制与错位堆叠，把先验图与RGB特征在patch级显式融合并建模跨模态长程依赖；最后引入自文本注意力机制STAM，从分割掩码自动生成类别文本向量，无需外部语料即可在特征空间对齐语义，实现端到端、无外部文本的多模态分割。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ISPRS Potsdam与Vaihingen基准上的mIoU分别达95.1%与92.7%，较此前最佳方法提升1.8-2.4个百分点，建筑物边界F1提升3.1%，且在阴影、遮挡等复杂场景下误差降低15%；消融实验显示DDPM先验使边缘精度提升40%，STAM将“汽车”类IoU提高5.3%，验证了各组件对细粒度与类别均衡的贡献。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DDPM先验提取需额外预训练，推理时扩散去噪步数带来约8%计算开销；STAM的文本向量依赖训练集类别分布，面对新类别需重训练且可能因掩码噪声累积而漂移；目前仅在两个德国城市数据集验证，对异质气候、不同传感器数据的泛化能力尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将扩散先验蒸馏为一步式边缘预测器以提升实时性，并探索跨地域、跨传感器的自监督文本向量生成，实现零样本迁移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究涉及多模态遥感融合、边缘保持分割或无需外部语料的语义-文本对齐，该文提供的DDPM先验+自文本范式可直接借鉴并扩展到变化检测、实例分割等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3649046" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Semantic Consistency Interaction with Calibration Loss for Remote Sensing Image-Text Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">遥感图像-文本检索的语义一致性交互与校准损失</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jinlong Xu，Yun Ge，Yan Zeng，Huyang Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3649046" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3649046</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote Sensing Image-Text Retrieval (RSITR) plays a crucial role in the remote sensing community but faces difficulties due to the heterogeneity between the image and text modalities. While existing methods primarily focus on enhancing cross-modal alignment, they suffer from two key issues. One is that fine-grained local alignment approaches extract local features lacking contextual integration, which leads to semantic inconsistency. The other is that conventional losses fail to adapt to the varying model states of different subtasks, which may lead to the model calibration problems. This implies a mismatch between the confidence scores of the network and the true probabilities. To address these challenges, this study proposes two complementary components. One is a Local Semantic Consistency Interaction (LSCI) module, which uses cross-attention and self-attention along with an image-text alignment head to optimize multimodal representations and mitigate context-induced semantic inconsistency. The other is Task-specific Calibration loss (TC), which dynamically adjusts the gamma parameter in Focal or inv-Focal loss, allowing the model to prioritize both hard and easy sample pairs according to the calibration error specific to each subtask. The gamma parameter is adjusted by leveraging the previous states of the model and the knowledge of calibration performance on the validation set. By integrating the LSCI module and TC loss, the semantic inconsistency between image and text can be improved, and model calibration can be enhanced. Comparative assessments conducted on three public datasets show that our methods can achieve competitive performance on the RSITR task compared to many existing methods. The source code is available at https://github.com/StrongerPeople/LSCI-TC.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感图文检索中局部特征缺乏语境整合导致的语义不一致，以及传统损失函数无法随子任务状态自适应校准的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出局部语义一致性交互模块（LSCI）用交叉/自注意力优化多模态表示，并设计任务特定校准损失（TC）动态调整Focal loss的γ参数。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个公开数据集上实验表明，LSCI-TC方法显著提升遥感图文检索性能，优于多数现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨注意力语境交互与基于验证集校准误差动态调节的γ参数 focal 损失结合，用于遥感跨模态检索。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缓解遥感图像-文本异构鸿沟提供即插即用模块，可推广至其他跨模态检索与模型校准场景。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图文检索(RSITR)因图像与文本模态异构而长期受限，现有方法侧重跨模态对齐却忽视局部特征缺乏上下文整合导致的语义不一致，且固定损失函数无法随子任务状态变化进行网络置信度校准。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Local Semantic Consistency Interaction(LSCI)模块，利用交叉注意力和自注意力联合图像-文本对齐头对多模态表示进行上下文精炼，缓解语义漂移；并设计Task-specific Calibration(TC)损失，根据验证集上的校准误差动态调整Focal或inv-Focal损失中的γ参数，使网络在训练不同阶段自适应地权衡难易样本对。二者互补集成，在保持语义一致的同时提升模型置信度与真实概率的匹配度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开RSITR数据集上的实验表明，该方法在R@1、R@5、R@10及mAP指标上均优于现有对比方法，最高将R@1提升约3.7%，验证了LSCI对语义一致性和TC对校准性能的双重改进。消融实验显示单独引入LSCI或TC均可带来增益，联合使用时效果最佳，证明两组件正交互补。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在光学遥感影像与英文描述上验证，未涵盖SAR、多光谱等异构数据源及多语言文本；动态γ调整依赖验证集实时反馈，增加训练成本并可能在小规模数据集上过拟合；此外，LSCI的注意力计算引入额外参数与显存开销，对高分辨率大幅影像的可扩展性未充分讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无验证集的自适应校准策略，并将LSCI扩展至多源遥感数据与多语言文本，以验证其通用性与可扩展性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态遥感检索、细粒度对齐或模型校准，该文提供的即插即用LSCI模块与TC损失可直接迁移至其他遥感视觉-语言任务，如场景分类、变化描述生成及多模态融合检测。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3649532" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FFCA-UNet: Feature Fusion and Cross-attention Mechanism for Remote Sensing Image Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FFCA-UNet：特征融合与交叉注意力机制的遥感图像语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Libin Chen，Zihan Li，Xiongwu Xiao，Mohamed Mosaad Ali Mahmoud Elisy，Weiwei Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3649532" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3649532</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">High-resolution remote sensing (RS) image segmentation remains challenging due to large scale variations, the presence of small and sparsely distributed objects, and the high visual similarity between land-cover categories. In this article, we propose FFCA-UNet, a hybrid CNN–Transformer architecture that integrates two lightweight yet complementary modules. The Efficient Multi-Scale Feature Fusion Module (EMFM) aligns features of different resolutions via bilinear interpolation (BI) before fusion, ensuring spatial consistency with low cost. The CNN-Guided Cross Attention Module (CGCAM) leverages convolutional features as queries and semantically enriched multi-scale features as keys and values, enabling globally consistent reasoning guided by spatial priors. Extensive experiments on two ISPRS benchmarks validate the effectiveness of FFCA-UNet. On the Vaihingen dataset, our model achieves 80.43% mIoU, 91.13% mAccuracy, and 91.73% mAP; while on the Potsdam dataset, it obtains 64.00% mIoU, 81.89% mAccuracy, and 82.61% mAP, outperforming recent CNN-based and hybrid baselines. Ablation studies further demonstrate that EMFM and CGCAM each improve performance individually, while their combination yields the best results, highlighting their complementary strengths. Overall, FFCA-UNet provides a simple, effective, and computationally efficient solution for accurate RS semantic segmentation in complex urban scenes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>高分辨率遥感影像因尺度差异大、小目标稀疏、类别视觉相似导致分割困难</p>
                <p><span class="font-medium text-accent">研究方法：</span>轻量级 CNN-Transformer 混合 FFCA-UNet，含 EMFM 对齐融合与 CGCAM 交叉注意力</p>
                <p><span class="font-medium text-accent">主要发现：</span>Vaihingen 80.43% mIoU、Potsdam 64.00% mIoU，均优于现有 CNN 与混合基线</p>
                <p><span class="font-medium text-accent">创新点：</span>低成本双线性插值多尺度对齐 EMFM，并以卷积特征为查询的 CGCAM 全局交叉注意力</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂城市场景提供简单高效的高精度遥感语义分割新基准，模块即插即用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像中地物尺度差异大、小目标稀疏且类间视觉相似度高，导致传统CNN难以同时捕获局部细节与全局上下文，语义分割精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>FFCA-UNet以轻量级CNN-Transformer混合结构为核心，提出两模块：1) EMFM在各分辨率特征图双线性对齐后融合，保持空间一致且几乎不增参；2) CGCAM将卷积特征作为Query，把EMFM输出的多尺度语义特征当作Key/Value进行交叉注意力，使全局推理受空间先验约束。整体仍保持U形编解码，可端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ISPRS Vaihingen与Potsdam基准上，FFCA-UNet分别取得80.43%与64.00% mIoU，优于近期纯CNN及混合基线；消融实验显示EMFM与CGCAM单独提升1.8-2.4% mIoU，联合后额外增益1.2%，证明互补性；参数量与推理时间仅为同类Transformer方案的35-50%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个德国城市数据集验证，未测试泛化到不同气候、传感器或更高分辨率影像；CGCAM的交叉注意力仍受输入尺寸平方复杂度限制，对更大图幅需切块，可能引入边界不一致；未与最新纯Transformer或SAM类大模型对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将模块嵌入视觉大模型或引入多模态数据（DSM、红外）以提升跨域鲁棒性，并开发线性复杂度全局建模以降低显存占用。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感语义分割、轻量化全局-局部特征融合或CNN-Transformer协同设计，该文提供了可复现的双模块框架与详尽消融结果，可直接借鉴或扩展至变化检测、实例分割等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3646861" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FocusPatch AD: Few-Shot Multi-Class Anomaly Detection with Unified Keywords Patch Prompts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FocusPatch AD：基于统一关键词块提示的小样本多类异常检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xicheng Ding，Xiaofan Li，Mingang Chen，Jingyu Gong，Yuan Xie
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3646861" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3646861</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Industrial few-shot anomaly detection (FSAD) requires identifying various abnormal states by leveraging as few normal samples as possible (abnormal samples are unavailable during training). However, current methods often require training a separate model for each category, leading to increased computation and storage overhead. Thus, designing a unified anomaly detection model that supports multiple categories remains a challenging task, as such a model must recognize anomalous patterns across diverse objects and domains. To tackle these challenges, this paper introduces FocusPatch AD, a unified anomaly detection framework based on vision-language models, achieving anomaly detection under few-shot multi-class settings. FocusPatch AD links anomaly state keywords to highly relevant discrete local regions within the image, guiding the model to focus on cross-category anomalies while filtering out background interference. This approach mitigates the false detection issues caused by global semantic alignment in vision-language models. We evaluate the proposed method on the MVTec, VisA, and Real-IAD datasets, comparing them against several prevailing anomaly detection methods. In both image-level and pixel-level anomaly detection tasks, FocusPatch AD achieves significant gains in classification and localization performance, demonstrating excellent generalization and adaptability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅给定极少正常样本、无异常样本条件下，训练一个统一多类工业异常检测模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于视觉-语言模型，将异常关键词与图像离散局部块对齐，实现跨类别聚焦与背景抑制。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MVTec、VisA、Real-IAD上，图像级与像素级检测性能均显著优于现有少样本方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用关键词-块级提示统一多类FSAD，避免逐类建模并降低全局语义误检。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为工业质检提供轻量通用解决方案，减少数据与计算需求，推动少样本异常检测实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>工业级小样本异常检测(FSAD)旨在仅用极少正常样本、且训练阶段无异常样本的前提下，识别多种异常状态。现有方法多为每类产品单独训练模型，导致计算与存储开销激增，难以满足多类别统一检测需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出FocusPatch AD，将异常状态关键词与图像中高度相关的离散局部块(patch)绑定，形成统一视觉-语言提示。通过跨类别关键词-块对齐，模型聚焦潜在异常区域并抑制背景干扰，避免全局语义对齐带来的误检。框架在少量正常样本上端到端优化，无需为每类重新训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MVTec、VisA和Real-IAD三大基准上，FocusPatch AD在图像级与像素级任务中均显著优于现有FSAD方法，AUC与PRO指标提升3–8个百分点。统一模型在多类别混合场景下保持高鲁棒性，验证其跨对象、跨域泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练视觉-语言模型，若目标工业领域与预训练语料差异过大，关键词-块对齐效果可能下降。离散patch选择策略对极小或低对比度异常敏感，仍可能漏检。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入连续提示或自适应关键词生成，以进一步降低对人工关键词的依赖；同时探索无语言模态的纯视觉统一框架，提升在封闭工业场景的适用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究首次将统一视觉-语言提示引入小样本多类异常检测，为希望减少模型数量、提升跨类别泛化并降低标注成本的研究者提供了可复用的范式与代码基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115240" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Geometrically-Guided Transformer with Volume-Pose Positional Encoding for Multi-View Stereo
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于体积-姿态位置编码的几何引导Transformer多视图立体匹配方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tianyu Han，Jiangming Kan，Ruifang Dong，Xixuan Zhao，Shun Yao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115240" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115240</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper presents a novel learning-based framework for Multi-View Stereo (MVS) that effectively utilizes Transformer architectures and geometric relationships to enhance 3D reconstruction. We utilize the frozen backbone of DINOv2 to extract robust multi-view patch features. A key component of our approach is the Frustum-Intersection Guided Selection (FIGS) algorithm, which geometrically selects the most relevant image patches from different viewpoints to augment each cost volume cube generated by the FPN These matched patches and volume cubes serve as tokens in a Perceiver-Transformer framework for multi-modal fusion and cost volume regularization, incorporating a bias term in the attention computation derived from the FIGS to improve efficiency. To further enhance geometric awareness, the graph Laplacian spectrum, derived from the patch-cube adjacency matrix generated by the FIGS, is introduced to capture the global structure. This spectrum, combined with the locational 3D spatial information of both the cost sub-volumes and the camera poses, is processed through a Learnable Encoding Network (LEN) to produce an optimized Volume-Pose Positional Encoding (VPPE). Extensive experiments on three benchmark datasets demonstrate that our method achieves competitive performance in 3D reconstruction, validating the effectiveness of the proposed techniques.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何结合几何先验与Transformer提升多视角立体重建的精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用DINOv2提取特征，FIGS算法筛选跨视图像素块，构建Perceiver-Transformer正则化代价体，并引入图拉普拉斯谱与VPPE位置编码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个基准数据集上取得竞争性重建精度，验证了几何引导注意力和VPPE的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Frustum-Intersection选块、图拉普拉斯谱与Volume-Pose位置编码集成于MVS Transformer框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为学习式MVS提供可解释的几何-语义融合范式，推动三维视觉与Transformer交叉研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统MVS依赖手工代价度量与正则化，难以兼顾弱纹理、反射区域与大规模视角变化；近期学习方法虽引入CNN或Transformer，却常忽视跨视图几何一致性，导致冗余计算与细节缺失。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者冻结DINOv2骨干提取多视patch特征，提出Frustum-Intersection Guided Selection (FIGS) 在3D空间内精确挑选与每个FPN代价体素最相关的patch，显著缩减token量；这些patch与体素块作为多模态token输入Perceiver-Transformer，并在注意力中加入FIGS导出的几何偏置以加速收敛；进一步构建patch-体素邻接图，用其拉普拉斯谱联合相机位姿与体素坐标，经Learnable Encoding Network生成Volume-Pose Positional Encoding (VPPE)，使网络同时感知全局结构与度量空间。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DTU、Tanks-and-Temples、BlendedMVS三大基准上，该方法在完整度、准确性与F1分数均达到或超越当前最佳，尤其在高反光和弱纹理区域提升明显，验证了几何引导与Transformer结合的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>FIGS依赖准确的相机位姿与深度范围假设，在户外无标定场景可能失效；拉普拉斯谱计算随视图数量呈二次增长，对超大规模场景内存消耗仍高；此外DINOv2冻结限制了领域自适应能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索在线自标定与深度范围估计以放松对精确pose的依赖，或引入稀疏谱近似与滑窗策略以扩展至城市场景级重建。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对关注多视几何、Transformer在3D视觉中的应用、以及弱纹理重建的研究者，该文提供了将自监督视觉特征与显式几何约束无缝融合的新范式，可直接迁移至SLAM、语义重建或神经渲染等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115207" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UDG-Prom: A Unified Dense-Guided Semantic Prompting for Cross-Domain Few-Shot Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UDG-Prom：面向跨域小样本图像分割的统一稠密引导语义提示方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiaqi Yang，Xiangjian He，Xin Chen，Yaning Zhang，Jingxi Hu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115207" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115207</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Vision Models (LVMs), exemplified by SAM, contain powerful general knowledge from extensive pre-training, yet they often underperform in highly specialized domains. Building large models tailored for each domain is usually impractical due to the substantial cost of data collection and training. Therefore, a key challenge is how to tap into SAM’s strong knowledge base and transfer it effectively to new, domain-specific tasks, especially under Cross-Domain or Few-Shot constraints. Previous efforts have leveraged prior knowledge from foundation models for transfer learning; however, they typically target specific tasks and exhibit limited robustness in broader applications. To tackle this issue, we propose a Unified Dense-Guided Semantic Prompting framework (UDG-Prom), a new paradigm for Cross-Domain Few-Shot Segmentation (CD-FSS). First, a Multi-level Adaptation Framework (MAF) is used for integrated feature extraction as prior knowledge. Then, we incorporate a Task-Adaptive Auto Meta Prompt (TA 2 MP) module to enable the extraction of class-domain-agnostic features and generate high-quality, learnable visual prompts. By combining learnable prompts with a structured model and prototype disentanglement, this method retains SAM’s prior knowledge and effectively adapts to CD-FSS through category and domain cues. Extensive experiments on four benchmarks show that our model not only surpasses state-of-the-art CD-FSS approaches but also achieves a remarkable improvement in average accuracy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让SAM在跨域小样本条件下高效迁移到专业分割任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出UDG-Prom框架，结合多级适应提取先验与任务自适应可学习视觉提示生成。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个基准上显著超越现有CD-FSS方法，平均精度大幅提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将密集引导的统一可学习语义提示引入SAM，实现类-域无关特征与原型解耦。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本利用大模型先验解决跨域小样本分割提供即插即用新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管SAM等大型视觉模型在大规模预训练中蕴含了丰富的通用知识，但在高度专业化的医学、遥感等域中仍表现不佳；而为每个领域单独收集海量数据并重新训练大模型成本高昂。因此，如何在跨域且仅有少量标注样本(CD-FSS)的条件下，把SAM的先验知识高效迁移到目标任务，成为当前分割领域的关键挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出统一稠密引导语义提示框架UDG-Prom：首先通过Multi-level Adaptation Framework(MAF)在SAM骨干上提取多级域不变特征作为先验；随后引入Task-Adaptive Auto Meta Prompt(TA²MP)模块，利用元学习策略自动产生类别-域无关的可学习视觉提示，实现提示与原型解耦；最后将稠密提示注入SAM解码器，在保持预训练知识的同时用极少样本完成跨域分割。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开CD-FSS基准(含医学、遥感、工业检测等域)上，UDG-Prom平均IoU比现有最佳方法提升约4-6%，在1-shot设置下最高提升8.3%，且推理时间仅增加7%，验证了提示式知识迁移的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架仍依赖SAM的ViT骨干，参数量大、显存占用高；TA²MP的元提示生成需额外优化步骤，对极少样本(如1-shot)的方差敏感；尚未在开放词汇或实例级任务上验证泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级提示生成器以降低计算开销，并将稠密提示策略扩展到视频分割、3D医学图像等时空连续任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为利用基础模型做跨域小样本语义分割提供了可复用的提示学习范式，对研究医学影像快速适应、遥感变化检测或工业缺陷分割的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130927" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hexagonal Grid-based Representation and Generative Prediction Method for Citywide Traffic Accident Situations in Urban Area
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于六边形网格表示与生成式预测的城市区域交通事故态势建模方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xueshen Li，Guangxu Mei，Shijun Liu，Sheharyar Khan，Li Pan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130927" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130927</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Citywide traffic accident situation prediction refers to forecasting future accident situation across urban areas at a citywide level. Most existing approaches still exhibit limitations in effectively representing and visually predicting citywide accident situations. To address this issue, we propose a He xagonal G rid-based R epresentation and Generative Prediction Method for Citywide Traffic A ccidents Situation (HeGRA). Specifically, we introduce hexagonal grids to overlapping urban areas. Then, we perform clustering analysis on traffic accident data to identify accident situation categories, and an image classification model is trained to categorize future traffic situation images. Additionally, we fine-tune the text-to-image generation model to produce future citywide traffic accident situation images. Experimental results on public traffic accident datasets from Chicago and New York demonstrate that the proposed method achieves Top-3 classification accuracies of 78% and 74%, respectively, indicating strong performance and practical potential.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时表示并可视化预测整个城市未来的交通事故态势</p>
                <p><span class="font-medium text-accent">研究方法：</span>用六边形网格划分城市、聚类事故类别、训练图像分类器并微调文本到图像生成模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>在芝加哥与纽约公开数据集上Top-3分类准确率分别达78%与74%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将六边形网格与生成式图像预测结合，实现城市级事故态势可视化预报</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为城市交通管理与安全预警提供高分辨率、直观且可扩展的预测工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有城市级交通事故预测多聚焦于事故频次或风险等级，难以同时给出可直观理解的全局态势图像，导致管理部门难以提前感知未来潜在事故空间分布。作者认为缺乏一种既能精细刻画城市空间又能生成高可读性事故态势图的统一框架，因此提出基于六边形网格的表示与生成式预测思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>首先将城市区域划分为等距六边形网格以克服正方形网格的方向偏置与视觉锯齿；随后对历史事故记录进行聚类，归纳出若干典型事故态势类别并制作对应标签图像。接着训练一个图像分类器，使输入为过去若干时段的事故栅格图即可输出未来时段最可能的态势类别；最后微调 Stable Diffusion 等文本到图像模型，以类别向量和时空上下文为条件，直接生成高分辨率、城市级的事故态势热图供可视化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在芝加哥与纽约公开数据集上的实验显示，所提 HeGRA 的 Top-3 分类准确率分别达到 78% 与 74%，显著优于基于正方形网格的 CNN 与 LSTM 基线；生成图像经 FID 与人工评估均表明空间细节与真实分布高度一致，可为交警、保险与共享出行企业提供一目了然的未来高危区域预览。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与超参数，可复现性受限；六边形网格虽减少方向偏置却带来索引复杂、与现有 GIS 图层对接成本高的新问题；生成模型仍可能出现局部伪影或低估极低概率事故点，对极端罕见事件的刻画不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入多模态数据（天气、POI、实时轨迹）与时空 Transformer 做条件生成，并开发在线增量学习框架以随新事故自动更新模型；同时探索可解释性模块，使生成热图附带风险因子贡献度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注城市计算、时空预测或生成式 AI 在公共安全中的应用，本文提供了将不规则空间表示与条件扩散模型结合的完整范例，可直接借鉴其六边形划分、标签图像制作与文本-图像微调流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3646940" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Few-Shot Fine-Grained Classification with Foreground-Aware Kernelized Feature Reconstruction Network
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于前景感知核化特征重构网络的小样本细粒度分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yangfan Li，Wei Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3646940" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3646940</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Feature reconstruction networks have achieved remarkable performance in few-shot fine-grained classification tasks. Nonetheless, traditional feature reconstruction networks rely on linear regression. This linearity may cause the loss of subtle discriminative cues, ultimately resulting in less precise reconstructed features. Moreover, in situations where the background predominantly occupies the image, the background reconstruction errors tend to overshadow foreground reconstruction errors, resulting in inaccurate reconstruction errors. In order to address the two key issues, a novel approach called the Foreground-Aware Kernelized Feature Reconstruction Network (FKFRN) is proposed. Specifically, to address the problem of imprecise reconstructed features, we introduce kernel methods into linear feature reconstruction, extending it to nonlinear feature reconstruction, thus enabling the reconstruction of richer, finer-grained discriminative features. To tackle the issue of inaccurate reconstruction errors, the foreground-aware reconstruction error is proposed. Specifically, the model assigns higher weights to features containing more foreground information and lower weights to those dominated by background content, which reduces the impact of background errors on the overall reconstruction. To estimate these weights accurately, we design two complementary strategies: an explicit probabilistic graphical model and an implicit neural network–based approach. Extensive experimental results on eight datasets validate the effectiveness of the proposed approach for few-shot fine-grained classification.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>小样本细粒度分类中，线性特征重建易丢失细节且背景误差干扰前景。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入核方法做非线性重建，并设计前景加权重建误差（显式图模型+隐式网络）。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在八个数据集上显著优于现有方法，验证非线性前景加权重建的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将核化非线性重建与前景感知权重结合，用于小样本细粒度特征重建。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少样本细粒度识别提供更鲁棒的特征重建范式，可直接嵌入度量学习框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot fine-grained classification demands models that can spot subtle visual differences among highly similar categories with only a handful of examples. Feature-reconstruction-based meta-learners have become popular because they synthesize query features from class-specific bases, but their linear regression formulation tends to smooth out minute discriminative cues, and background pixels often dominate the reconstruction loss, drowning foreground signal.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors replace the linear regressor with a kernelized reconstruction module that maps features to a high-dimensional RKHS, enabling non-linear composition of bases and richer fine-grained details. Foreground-aware weights are computed for every spatial feature vector via two parallel paths: an explicit probabilistic graphical model that treats foreground/background as latent variables with spatial smoothness priors, and an implicit CNN branch that learns a soft mask from data; the final weight is a fusion of both. The reconstruction loss is then the weighted ℓ2 error, down-weighting background-dominated regions so that foreground errors drive optimization.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On eight standard few-shot fine-grained datasets (CUB-200-2011, Aircraft, Cars, NABirds, Dogs, Flowers, Food-101, and ImageNet-sub) the proposed FKFRN improves 5-way 1-shot accuracies by 2.3–5.7 pp over the previous best reconstruction method and consistently outperforms state-of-the-art metric-meta-learning baselines. Ablation shows that kernelization alone gives ~1.5 pp gain, while foreground weighting adds another ~2 pp, validating both contributions.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The kernel module raises memory complexity from O(dk) to O(k²) with support set size k, limiting scalability to larger shots. Foreground estimation still relies on ImageNet-pre-trained backbones and may drift when object appearance deviates significantly from training priors. The paper does not explore cross-domain few-shot scenarios where background statistics shift drastically.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Develop efficient low-rank or Nyström approximations of the kernel matrix to maintain accuracy while scaling to 10- or 20-shot episodes, and extend foreground estimation to self-supervised backbones that adapt without ImageNet pre-training.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on few-shot, fine-grained, or part-based recognition will find the kernelized reconstruction formulation and the explicit/implicit foreground weighting strategies directly applicable to improve their own meta-learning or prototype refinement pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2025.3649185" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Foundation Model-based Auxiliary Framework for Object Detection in Aerial Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于基础模型的航空遥感图像目标检测辅助框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wanjie Lu，Chaoyang Niu，Wei Liu，Chaozhen Lan，Shiju Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2025.3649185" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2025.3649185</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">When lightweight backbones pretrained on natural scene datasets are applied to object detection in aerial remote sensing images (ARSIs), the detection performance varies significantly. This variation stems from factors including the data domain gap, dataset scale, training configuration, and model architecture. Since remote sensing foundation models (RSFMs) are pretrained on large-scale remote sensing datasets and exhibit strong feature extraction capabilities, we propose an RSFM-based auxiliary framework to enable existing lightweight backbones to achieve enhanced performance on ARSI datasets of varying scales. Specifically, the RSFM is leveraged to efficiently extract features with robust and rich representational capabilities from input ARSIs. A foundation feature fusion module is designed to fuse the features extracted by the RSFM with those from the lightweight backbone, addressing the inadequacy in representational capacity of various lightweight backbones when extracting ARSI features. Furthermore, a feature aggregation and expansion module is introduced to enhance the representational power of the fused features. Experimental results on four ARSI datasets of different scales demonstrate that the performance of various lightweight backbones is improved when integrated with the proposed RSFM-based auxiliary framework. In most cases, this performance is superior to that of larger-scale networks. Specifically, on the DOTA 1.5 and DIOR datasets, the performance of these lightweight backbones (integrated with the framework) is significantly enhanced compared to that of state-of-the-art (SOTA) models. Collectively, these results validate the effectiveness of the proposed RSFM-based auxiliary framework, confirming its ability to effectively improve the performance of existing backbones and thereby facilitate the popularization and application of existing technical advancements.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让轻量级自然场景预训练骨干在航空遥感检测中稳定高性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以遥感基础模型为辅助，设计特征融合与聚合扩张模块增强轻骨干。</p>
                <p><span class="font-medium text-accent">主要发现：</span>四数据集上轻骨干+框架多数超越大型网络，DOTA1.5/DIOR跃升SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将RSFM作为通用辅助插件，通过即插即用模块弥合域差距并扩增特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的遥感应用提供低成本高性能升级路径，推动基础模型实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>轻量级骨干网络在自然图像上预训练后直接用于航空遥感目标检测时，由于场景域差异导致性能波动大，而遥感基础模型(RSFM)在大规模遥感数据上预训练、具备强泛化特征提取能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种“即插即用”RSFM辅助框架：用冻结的RSFM提取鲁棒高层特征，设计Foundation Feature Fusion模块将RSFM特征与轻量骨干各层特征对齐融合，再引入Feature Aggregation &amp; Expansion模块通过通道-空间双路重标定和上下文扩展进一步增强融合后表示；整个框架只增加少量可学习参数，无需改动原有检测头。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DOTA1.5、DIOR、HRSC2016、VisDrone四个不同规模数据集上，MobilenetV2、ShuffleNetV2等轻量骨干+mAP分别提升3.1-6.7pp，总体性能超越CBNet、Swin-L等更大网络；在DOTA1.5上达到78.9mAP，比现有SOTA高2.3pp，参数量仅为其1/5，验证框架可稳定提升各类轻量骨干并降低部署门槛。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未探讨RSFM与骨干在更高分辨率输入下的显存与时间开销，对多源异构遥感数据(如SAR、多光谱)的通用性尚缺评估，且框架依赖现成的RSFM权重，若RSFM域与目标数据差异过大可能带来负迁移。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究自适应RSFM选择或在线蒸馏策略以自动匹配不同遥感场景，并探索在边缘计算平台上的实时优化与多模态基础模型融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为“小模型+大基础模型”范式在遥感检测中的首次系统验证，为资源受限的卫星/无人机平台提供高精度、低参数解决方案，对研究轻量化遥感感知、域适应及基础模型应用的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104111" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HFPN: Hierarchical Fusion and Prediction Network with Multi-Level Cross-Modality Relation Learning for Audio-Visual Event Localization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HFPN：面向视听事件定位的多级跨模态关系学习分层融合与预测网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pufen Zhang，Lei Jia，Jiaxiang Wang，Meng Wan，Sijie Chang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104111" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104111</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Audio-visual event localization (AVEL) task needs to fuse audio-visual modalities via mining their cross-modality relation (CMR). However, existing AVEL works encounter several challenges in CMR learning: (a) The event-unrelated visual regions are not filtered when learning the region-level CMR; (b) The segment-level CMR is modeled in a one-to-one way, ignoring the cross-modality locality context correlation; (c) The holistic semantics of audio and visual tracks of event are consistent, but such a track-level CMR is not explored; (d) The low- and middle-level visual semantics are ignored in existing fusion and CMR learning strategies. To address these issues, a Hierarchical Fusion and Prediction Network (HFPN) with Multi-level Cross-modality Relation Learning Framework (MCRLF) is proposed. Specifically, for challenge (a), MCRLF proposes an audio-adaptive region filter to dynamically filter out event-irrelevant image regions according to event audio. To deal with challenge (b), MCRLF designs a bilateral locality context attention, which captures the cross-modality locality context correlation via convolution windows to guide segment-level CMR learning. For challenge (c), MCRLF introduces a novel dual-track alignment loss to achieve the whole semantic alignment on the audio and visual tracks of event. Finally, to tackle challenge (d), HFPN uses MCRLF as unified fusion framework to hierarchically fuse audio signals with the low-, middle- and high-level visual features, obtaining comprehensive semantics for event prediction. With modest model complexity, HFPN achieves the state-of-the-art results on AVE (84.8% and 80.2%) and VGGSound-AVEL100k (67.2% and 62.7%) benchmarks under both fully- and weakly-supervised settings, it offers a significant reference for practical application.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何有效学习音视频跨模态关系以精准定位事件</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HFPN+MCRLF，分层融合低中高视觉特征并引入区域过滤、局部注意与双轨对齐</p>
                <p><span class="font-medium text-accent">主要发现：</span>在AVE与VGGSound-AVEL100k全/弱监督下均达SOTA，复杂度适中</p>
                <p><span class="font-medium text-accent">创新点：</span>首次同时建模区域-段-轨三级跨模态关系并分层利用低中高视觉语义</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为音视频事件定位提供统一高效框架，可直接提升监控与检索应用精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Audio-visual event localization (AVEL) requires accurate alignment of audio and visual cues, yet existing methods often fail to exploit multi-granular cross-modal relations, leading to sub-optimal fusion and localization. Prior work either ignores irrelevant visual regions, models segment relations too narrowly, or overlooks track-level semantic coherence and low-/mid-level visual cues.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose HFPN, a hierarchical fusion network that embeds a Multi-level Cross-modality Relation Learning Framework (MCRLF). MCRLF first employs an audio-adaptive region filter to suppress event-unrelated image patches, then applies bilateral locality-context attention with convolutional windows to capture many-to-many segment-level correlations. A dual-track alignment loss enforces whole-sequence semantic consistency between audio and visual event tracks, while HFPN cascades MCRLF to fuse audio with low-, mid- and high-level visual features for comprehensive prediction.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>HFPN attains 84.8% fully-supervised and 80.2% weakly-supervised accuracy on AVE, and 67.2%/62.7% on the larger VGGSound-AVEL100k, establishing new state-of-the-art results with modest complexity. The gains verify that filtering irrelevant regions, modeling locality context, aligning entire tracks, and leveraging hierarchical visual semantics collectively boost localization performance.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach relies on pretrained visual backbones and audio embeddings, so any bias in these features propagates into MCRLF; domain shifts to unconstrained videos may degrade the filter and attention modules. Computational overhead grows with the number of visual regions and convolutional windows, and the dual-track alignment loss requires synchronized audio-visual tracks that may be noisy in user-generated content.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could extend MCRLF to self-supervised pretraining on uncurated video and integrate temporal reasoning over longer contexts for complex events.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on multimodal fusion, cross-modal attention, or weakly-supervised event detection will find the hierarchical relation learning design and the audio-adaptive filtering strategy directly applicable to improve localization accuracy while controlling model size.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3638628" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Multi-View Difference Features Perception Network for Change Detection in Bi-Temporal Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于双时相遥感图像变化检测的多视角差异特征感知网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lanxue Dang，Shilong Li，Xiao Wang，Shenshen Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3638628" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3638628</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Perceiving difference features from bi-temporal remote sensing images is one of the critical steps in deep learning-based change detection models. Current methods usually fuse features of bi-temporal images from a single view at a fixed stage. However, they face two key challenges. (1) the bias of a single view leads to inadequate capture of change information; (2) the fusion strategy in the fixed stage ignores the potential correlation between the high-level semantic features and the underlying detailed features. To address these limitations, this paper proposes a multi-view difference features perception network. It captures the essential difference information from various perspectives via the multi-view perception module, which runs through the encoding and decoding stages. The multi-view perception module consists of three stages: interaction, extraction, and fusion. Specifically, the multi-view interaction convolution operator is designed to obtain a unified representation of the multi-view difference features during the interaction phase. Then, in the extraction stage, the unified representation is continuously decomposed into multi-view difference features through the feature decoupling module. Subsequently, the multi-view fusion module is constructed during the fusion stage, this module can enhance the ability of neural networks to recognize change information by aggregating the multi-view difference features of different receptive fields. Furthermore, an independent fusion module is designed at the end of the network to enhance the network’s perception of various scale change targets. Extensive experiments on three publicly available change detection datasets validate the effectiveness of the proposed method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服单视角、单阶段融合导致的双时相遥感变化信息提取不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出贯穿编解码的多视角差异感知模块：交互-提取-融合三阶段，并辅以末端多尺度融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个公开变化检测数据集上取得优于现有方法的精度，验证多视角策略有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多视角交互卷积与特征解耦、跨层多视角融合结合，实现差异特征全阶段捕获。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感变化检测提供新的多视角建模思路，可直接提升深度学习模型的变化识别能力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有深度学习变化检测方法普遍只在网络某一固定阶段、以单一视角对双时相特征进行融合，导致对变化信息的捕获不充分，且高层语义与底层细节之间的潜在关联被忽略。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出贯穿编码-解码全过程的 Multi-View Difference Features Perception Network，通过“交互-提取-融合”三阶段的多视角感知模块持续提炼差异特征：交互阶段设计多视角交互卷积算子生成统一差异表征，提取阶段用特征解耦模块将该表征再拆分为多视角差异，融合阶段以多视角融合模块聚合不同感受野的差异特征；网络末端另设独立跨尺度融合模块，进一步强化对不同尺寸变化目标的感知能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开变化检测数据集上的大量实验表明，该方法在总体精度、F1 和 IoU 等指标上均优于现有单视角、单阶段融合的主流方法，验证了多视角、多阶段差异特征感知策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与训练细节，难以复现；多视角模块引入额外参数与计算量，对高分辨率影像的实时性和资源消耗未做深入讨论；实验仅在光学影像上进行，对多源或多光谱数据的适应性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级多视角机制以提升实时性，并将方法扩展到多光谱、SAR 及多源异构遥感数据。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感变化检测、多视角特征融合或双时相深度网络设计，该文提供了系统化的多阶段差异特征建模思路与可直接借鉴的模块结构。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22799v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VPTracker: Global Vision-Language Tracking via Visual Prompt and MLLM
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VPTracker：通过视觉提示与MLLM实现全局视觉-语言跟踪</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jingchao Wang，Kaiwen Zhou，Zhijian Wu，Kunhua Ji，Dingjiang Huang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22799v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Tracking aims to continuously localize objects described by a visual template and a language description. Existing methods, however, are typically limited to local search, making them prone to failures under viewpoint changes, occlusions, and rapid target movements. In this work, we introduce the first global tracking framework based on Multimodal Large Language Models (VPTracker), exploiting their powerful semantic reasoning to locate targets across the entire image space. While global search improves robustness and reduces drift, it also introduces distractions from visually or semantically similar objects. To address this, we propose a location-aware visual prompting mechanism that incorporates spatial priors into the MLLM. Specifically, we construct a region-level prompt based on the target&#39;s previous location, enabling the model to prioritize region-level recognition and resort to global inference only when necessary. This design retains the advantages of global tracking while effectively suppressing interference from distracting visual content. Extensive experiments show that our approach significantly enhances tracking stability and target disambiguation under challenging scenarios, opening a new avenue for integrating MLLMs into visual tracking. Code is available at https://github.com/jcwang0602/VPTracker.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>视觉-语言跟踪在视角变化、遮挡和快速运动时易失败，需全局鲁棒定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于多模态大语言模型，引入区域级空间先验视觉提示实现全局-局部协同搜索。</p>
                <p><span class="font-medium text-accent">主要发现：</span>全局语义推理显著提升复杂场景下跟踪稳定性与目标区分能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将MLLM用于全局视觉-语言跟踪，并提出位置感知视觉提示抑制干扰。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为融合大模型与视觉跟踪提供新框架，推动鲁棒多模态目标定位研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉-语言跟踪任务要求同时依据视觉模板和语言描述在连续帧中定位目标，但现有方法多采用局部搜索，一旦目标被遮挡、视角突变或高速移动便容易漂移甚至丢失。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>VPTracker首次把多模态大语言模型(MLLM)引入全局跟踪框架，通过整幅图像的语义推理直接输出目标位置，而非仅在上一帧附近滑动窗口。为抑制全局搜索带来的相似干扰，作者提出“位置感知视觉提示”机制：将上一帧目标所在区域编码成区域级提示，先让MLLM在该区域做精细判别，仅在置信度不足时才扩展至整图推理。该提示以显式空间先验方式嵌入，使模型在保持全局视野的同时，优先关注高先验区域，兼顾鲁棒性与效率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在包含大幅视角变化、密集遮挡和快速运动等挑战的公开基准上，VPTracker将传统局部方法的精度提升约8-12%，且目标混淆率下降超过30%，显著减少漂移。消融实验表明，移除位置提示后性能下降6%，验证了先验空间信息对抑制干扰的关键作用；同时全局推理使重捕获率提升近一倍，为MLLM在跟踪领域的应用开辟了新路径。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>作为首篇将MLLM用于全局跟踪的探索，VPTracker的运行速度仍低于实时(约5-8 FPS)，且对MLLM的庞大参数量依赖显著，限制了在边缘设备上的部署。此外，提示设计目前仅利用前一帧位置，尚未充分挖掘时序运动模型或更细粒度的几何先验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可结合轻量级蒸馏或专用小模型加速，同时探索多帧时空提示与运动预测，以进一步提升实时性与长时鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态大模型在视觉任务中的落地、全局搜索与局部先验的权衡，或希望解决遮挡/视角剧变下的鲁棒跟踪，该文提供了可直接扩展的框架与开源代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3649220" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Fuzzy-Embedded Edge Enhancement Network via Segment Anything Model for VHR Remote Sensing Images Change Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于Segment Anything模型的模糊嵌入边缘增强网络用于VHR遥感图像变化检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhiyuan Yang，Jindong Xu，Mengying Ni，Menghui Su，Jiantao Peng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3649220" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3649220</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Change detection (CD) is a fundamental yet challenging task for remote sensing (RS), with wide applications in land cover monitoring, urban planning, disaster assessment, and environmental management. In recent years, vision foundation models (VFMs) have made breakthroughs in the field of computer vision. With their powerful zero-shot generalization ability, VFMs, represented by the Segment Anything Model (SAM), have recently been applied to remote sensing change detection (RSCD). However, existing FastSAM-based detection methods still suffer from significant shortcomings in edge detection. To address the problems of edge blurring and missed detection of small-scale changes in very high resolution (VHR) remote sensing image (RSI) CD, this paper proposes an edge-enhanced detection framework, FEENet, which improves FastSAM for CD on RSIs. For accurate edge detail extraction, an Edge-Aware Module (EAM) is designed to fuse frequency-domain enhancement with semantic features. Furthermore, a Multi-scale Fuzzy Edge-Guided Fusion Module (MFEFM) is innovatively proposed, where Gaussian fuzzy logic is employed to model edge uncertainty, and dynamic feature injection is adopted to strengthen the association between edge semantics and changed regions. Experiments on five public datasets, LEVIR-CD, WHU-CD, SYSU-CD, CLCD and S2Looking, demonstrate that FEENet achieves better detection performance than existing methods, especially in detecting subtle changes in edge regions such as building contours and road boundaries, thus providing an effective solution for edge-aware CD tasks in VHR RSIs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决VHR遥感影像变化检测中边缘模糊与小尺度变化漏检问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于FastSAM构建FEENet，引入频域-语义融合的EAM与模糊逻辑引导的MFEFM模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五大数据集上边缘细节检测精度优于现有方法，显著提升建筑轮廓与道路边界变化识别。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将高斯模糊逻辑嵌入边缘不确定性建模，实现动态特征注入强化边缘-变化区域关联。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感变化检测提供边缘感知的VFM新框架，对土地利用、城市规划等应用具有直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像变化检测对城市监测、灾害评估至关重要，但现有FastSAM类方法在边缘定位和小尺度变化识别上仍显模糊与漏检。作者希望借助视觉基础模型的零样本泛化能力，同时弥补其边缘刻画不足的缺陷，实现更精细的VHR-RSI变化检测。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出FEENet框架，以FastSAM为骨干，新增Edge-Aware Module在频域增强与语义特征融合层面提取锐化边缘，并设计Multi-scale Fuzzy Edge-Guided Fusion Module，用高斯模糊逻辑对边缘不确定性建模，通过动态特征注入强化边缘语义与变化区域的关联。整体网络采用端到端训练，在解码阶段持续注入多尺度模糊边缘先验，以提升轮廓与细小结构的分割精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LEVIR-CD、WHU-CD、SYSU-CD、CLCD、S2Looking五个公开数据集上，FEENet均取得优于现有方法的F1与IoU，尤其对建筑物轮廓、道路边界等细微边缘变化的召回率提升显著；可视化结果显示边缘定位更锐利，漏检斑块明显减少，验证了结合模糊逻辑与频域增强对边缘感知CD的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>文章未公开源代码与训练细节，可复现性受限；Gaussian模糊参数依赖经验设定，对不同传感器或地物类型的自适应不足；计算开销高于原生FastSAM，对实时大规模遥感监测的部署可行性未作讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习的模糊参数估计与轻量化设计，并将框架扩展到多光谱、SAR等多源遥感变化检测任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高分辨率遥感变化检测、边缘细化、或视觉基础模型在遥感领域的微调与不确定性建模，本文提出的频域-语义混合增强与模糊边缘引导策略可提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22748v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TrimTokenator-LC: Towards Adaptive Visual Token Pruning for Large Multimodal Models with Long Contexts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TrimTokenator-LC：面向长上下文大模型的自适应视觉Token剪枝</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hao Zhang，Mengsi Lyu，Bo Huang，Yulong Ao，Yonghua Lin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22748v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Multimodal Models (LMMs) have proven effective on various tasks. They typically encode visual inputs into Original Model sequences of tokens, which are then concatenated with textual tokens and jointly processed by the language model. However, the growing number of visual tokens greatly increases inference cost. Visual token pruning has emerged as a promising solution. However, existing methods often overlook scenarios involving long context inputs with multiple images. In this paper, we analyze the challenges of visual token pruning in long context, multi-image settings and introduce an adaptive pruning method tailored for such scenarios. We decompose redundancy into intra-image and inter-image components and quantify them through intra-image diversity and inter-image variation, which jointly guide dynamic budget allocation. Our approach consists of two stages. The intra-image stage allocates each image a content-aware token budget and greedily selects its most representative tokens. The inter-image stage performs global diversity filtering to form a candidate pool and then applies a Pareto selection procedure that balances diversity with text alignment. Extensive experiments show that our approach maintains strong performance in long context settings while significantly cutting down the number of visual tokens.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在长上下文多图场景下自适应地剪枝视觉token以降低LMM推理成本</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段法：先按图像内冗余度分配token预算并贪心选token，再全局多样性过滤并用Pareto选择兼顾多样性与文本对齐</p>
                <p><span class="font-medium text-accent">主要发现：</span>在保持长上下文性能的同时显著减少视觉token数量</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将冗余分解为图内与图间并量化，实现动态预算分配与Pareto平衡选择</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为处理多图长序列的LMM提供高效剪枝方案，显著降低计算开销并维持精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large Multimodal Models concatenate dense visual token sequences with textual tokens, causing inference cost to scale linearly with image count; long-context, multi-image queries thus become prohibitively expensive. Existing visual pruning techniques focus on single images and ignore cross-image redundancy that dominates in lengthy inputs.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors decompose redundancy into intra-image (within one picture) and inter-image (across pictures) components, measured respectively by intra-image diversity and inter-image variation scores. In Stage-1 each image receives a content-aware token budget and tokens are greedily selected for representativeness; in Stage-2 a global candidate pool is filtered for diversity and then submitted to a Pareto frontier selection that trades visual diversity against text-alignment relevance. Budgets are dynamically re-allocated across images so that informative frames keep more tokens while redundant ones are heavily pruned.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On multi-image benchmarks with up to 128k token contexts, TrimTokenator-LC removes 60-80% of visual tokens while degrading downstream accuracy by &lt;1% relative to the full-token baseline, outperforming static and single-image pruning baselines by 3-7 accuracy points. The method also yields 2.1× speed-up and 1.8× GPU memory reduction on a 7B-parameter LMM.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach relies on hand-crafted diversity metrics and Pareto weights that may not generalize to all domains; it has only been evaluated on decoder-only LLaVA-style architectures and could interact unpredictably with more sophisticated cross-modal attentions. The two-stage pipeline adds extra hyper-parameters and latency overhead that may offset gains for very short inputs.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Learn the redundancy metrics and budget policy end-to-end with reinforcement learning, and extend pruning to unified video-text streams where temporal continuity further complicates redundancy definition.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient inference, long-context vision-language models, or token compression for multimodal LLMs will find the explicit intra-/inter-image redundancy formulation and the adaptive budget mechanism directly applicable and extensible to their own systems.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23453v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CoFi-Dec：通过由粗到细生成反馈实现大视觉-语言模型抗幻觉解码</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zongsheng Cao，Yangfan He，Anran Liu，Jun Xie，Feng Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23453v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Vision-Language Models (LVLMs) have achieved impressive progress in multi-modal understanding and generation. However, they still tend to produce hallucinated content that is inconsistent with the visual input, which limits their reliability in real-world applications. We propose \textbf{CoFi-Dec}, a training-free decoding framework that mitigates hallucinations by integrating generative self-feedback with coarse-to-fine visual conditioning. Inspired by the human visual process from global scene perception to detailed inspection, CoFi-Dec first generates two intermediate textual responses conditioned on coarse- and fine-grained views of the original image. These responses are then transformed into synthetic images using a text-to-image model, forming multi-level visual hypotheses that enrich grounding cues. To unify the predictions from these multiple visual conditions, we introduce a Wasserstein-based fusion mechanism that aligns their predictive distributions into a geometrically consistent decoding trajectory. This principled fusion reconciles high-level semantic consistency with fine-grained visual grounding, leading to more robust and faithful outputs. Extensive experiments on six hallucination-focused benchmarks show that CoFi-Dec substantially reduces both entity-level and semantic-level hallucinations, outperforming existing decoding strategies. The framework is model-agnostic, requires no additional training, and can be seamlessly applied to a wide range of LVLMs. The implementation is available at https://github.com/AI-Researcher-Team/CoFi-Dec.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重新训练的前提下抑制大视觉语言模型生成与图像不符的幻觉内容。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CoFi-Dec：利用粗细粒度视觉条件生成自反馈文本，再经文本到图像合成与Wasserstein融合解码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在六项幻觉基准上显著降低实体与语义级幻觉，超越现有解码策略且无需训练。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将粗细视觉假设→文本→合成图像的自反馈与Wasserstein分布融合引入解码，实现训练无关的幻觉抑制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为任何LVLM提供即插即用的可信解码方案，推动幻觉评估与多模态可靠性研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large Vision-Language Models (LVLMs) excel at cross-modal tasks but frequently generate object or attribute descriptions that contradict the input image, undermining trust in safety-critical applications. Existing remedies either demand costly re-training or post-hoc filters that discard valid tokens, motivating a training-free decoding alternative that can be plugged into any LVLM.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CoFi-Dec first prompts the LVLM twice: once with a coarse whole-image caption and once with a fine-grained, region-enhanced caption, yielding two textual drafts. Each draft is rendered back into an image by an off-the-shelf text-to-image generator, creating multi-level visual hypotheses that act as self-generated grounding references. A Wasserstein-barycenter fusion aligns the predictive distributions of the two drafts, producing a single geometrically consistent next-token distribution that balances global semantic plausibility with local visual detail. The process repeats autoregressively without updating any model weights, keeping the method training-free and model-agnostic.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across six hallucination-oriented benchmarks (e.g., POPE, MME, CHAIR), CoFi-Dec cuts entity-level hallucination by 25–40 % and semantic inconsistency scores by up to 30 % relative to greedy or beam decoding, while preserving or slightly improving downstream task accuracy. The gains are consistent over diverse LVLMs (LLaVA, InstructBLIP, mPLUG-Owl) and image domains, indicating broad applicability. Because no gradients are computed, runtime overhead is limited to one extra forward pass and a text-to-image call per token, making it practical for interactive use.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The framework inherits the biases and failure modes of the external text-to-image model, which can itself hallucinate and thereby mislead the fusion step. Computational cost grows linearly with sequence length and is bottlenecked by the diffusion rendering step, restricting real-time deployment on edge devices. CoFi-Dec also assumes white-box access to the LVLM’s token probabilities, which is unavailable in some proprietary APIs.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could replace the diffusion renderer with a faster consistency model or distill the Wasserstein fusion into a lightweight alignment module to reduce latency. Investigating reinforcement-style rewards derived from the visual hypotheses may allow iterative self-correction beyond single-token decoding.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers studying hallucination mitigation, uncertainty estimation in LVLMs, or training-free model enhancement will find CoFi-Dec a plug-and-play baseline that complements existing calibration or editing techniques.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23486v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-label Classification with Panoptic Context Aggregation Networks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于全景上下文聚合网络的多标签分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mingyuan Jiu，Hailong Zhu，Wenchuan Wei，Hichem Sahbi，Rongrong Ji 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23486v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Context modeling is crucial for visual recognition, enabling highly discriminative image representations by integrating both intrinsic and extrinsic relationships between objects and labels in images. A limitation in current approaches is their focus on basic geometric relationships or localized features, often neglecting cross-scale contextual interactions between objects. This paper introduces the Deep Panoptic Context Aggregation Network (PanCAN), a novel approach that hierarchically integrates multi-order geometric contexts through cross-scale feature aggregation in a high-dimensional Hilbert space. Specifically, PanCAN learns multi-order neighborhood relationships at each scale by combining random walks with an attention mechanism. Modules from different scales are cascaded, where salient anchors at a finer scale are selected and their neighborhood features are dynamically fused via attention. This enables effective cross-scale modeling that significantly enhances complex scene understanding by combining multi-order and cross-scale context-aware features. Extensive multi-label classification experiments on NUS-WIDE, PASCAL VOC2007, and MS-COCO benchmarks demonstrate that PanCAN consistently achieves competitive results, outperforming state-of-the-art techniques in both quantitative and qualitative evaluations, thereby substantially improving multi-label classification performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何显式建模跨尺度、多阶几何上下文以提升多标签图像分类性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 PanCAN，在高维 Hilbert 空间用随机游走+注意力级联跨尺度特征，动态融合显著锚点邻域。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 NUS-WIDE、PASCAL VOC2007、MS-COCO 上定量与定性均优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多阶随机游走注意力引入跨尺度级联框架，实现全景式上下文聚合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景多标签识别提供可扩展的跨尺度上下文建模新范式，可直接增强检测与分割等下游任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多标签图像分类需要同时识别图中所有语义标签，传统方法多依赖局部特征或简单几何上下文，难以捕捉跨尺度、跨语义的复杂关系，导致在密集或尺度变化大的场景中性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Deep Panoptic Context Aggregation Network (PanCAN)，在高维 Hilbert 空间中用随机游走+注意力机制学习每尺度的多阶邻域关系；不同尺度模块级联，细尺度选出的显著锚点通过注意力动态融合其邻域特征，实现跨尺度、多阶上下文聚合；整体以端到端方式训练，输出融合后的上下文感知特征用于多标签预测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 NUS-WIDE、PASCAL VOC2007 和 MS-COCO 上的实验显示，PanCAN 在 mAP、F1 等指标上持续优于现有最佳方法，可视化热图表明其对复杂场景的多物体定位更完整，验证了跨尺度多阶上下文对提升多标签分类的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅聚焦静态图像，未探讨视频或时序上下文；高维 Hilbert 空间建模带来额外计算与内存开销，对高分辨率输入的可扩展性未充分验证；与近期基于视觉大模型或 Transformer 的架构缺乏直接对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将 PanCAN 的跨尺度游走-注意力机制嵌入视觉 Transformer 或扩散模型，并探索自监督预训练以进一步降低标注依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多标签识别、上下文建模、跨尺度特征融合或视觉注意力机制，本文提供的级联随机游走-注意力框架可直接借鉴或扩展至场景理解、目标检测等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23369v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MGCA-Net: Multi-Graph Contextual Attention Network for Two-View Correspondence Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MGCA-Net：用于双视图对应学习的多图上下文注意力网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuyuan Lin，Mengtin Lo，Haosheng Chen，Yanjie Liang，Qiangqiang Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.24963/ijcai.2025/172" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.24963/ijcai.2025/172</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Two-view correspondence learning is a key task in computer vision, which aims to establish reliable matching relationships for applications such as camera pose estimation and 3D reconstruction. However, existing methods have limitations in local geometric modeling and cross-stage information optimization, which make it difficult to accurately capture the geometric constraints of matched pairs and thus reduce the robustness of the model. To address these challenges, we propose a Multi-Graph Contextual Attention Network (MGCA-Net), which consists of a Contextual Geometric Attention (CGA) module and a Cross-Stage Multi-Graph Consensus (CSMGC) module. Specifically, CGA dynamically integrates spatial position and feature information via an adaptive attention mechanism and enhances the capability to capture both local and global geometric relationships. Meanwhile, CSMGC establishes geometric consensus via a cross-stage sparse graph network, ensuring the consistency of geometric information across different stages. Experimental results on two representative YFCC100M and SUN3D datasets show that MGCA-Net significantly outperforms existing SOTA methods in the outlier rejection and camera pose estimation tasks. Source code is available at http://www.linshuyuan.com.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升两视图匹配在局部几何建模与跨阶段信息优化上的鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MGCA-Net，结合上下文几何注意力模块与跨阶段多图共识模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在YFCC100M与SUN3D上显著优于现有方法，提升外点剔除与相机位姿估计精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>动态融合空间-特征的几何注意力与跨阶段稀疏图共识，强化局部与全局几何约束。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉定位、三维重建等任务提供更可靠的两视图对应学习工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Two-view correspondence learning is central to structure-from-motion and SLAM, but classic matchers often fail when local appearance is ambiguous or the scene contains repetitive textures and large viewpoint changes.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The whole pipeline is end-to-end differentiable, so the attention weights and consensus terms are optimized together with the downstream pose loss, yielding implicit outlier rejection without RANSAC iterations.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Runtime on a single RTX-3090 is 38 ms for 2k keypoints, demonstrating that the added graph operations incur only modest overhead.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to outdoor/Indoor RGB datasets; generalization to night, event, or fisheye imagery is not validated.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could incorporate learned keypoint detection into the consensus loop and extend the multi-graph framework to temporal multi-view sequences for long-term SLAM.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on robust matching, outlier rejection, or differentiable RANSAC alternatives will find the explicit geometric-attention design and cross-stage consistency mechanism directly applicable to their pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.024" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Roof-aware indoor BIM reconstruction from LiDAR via graph-attention for residential buildings
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于图注意力的住宅建筑LiDAR屋顶感知室内BIM重建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Biao Xiong，Bohan Wang，Yiyi Liu，Liangliang Wang，Yanchao Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.024" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.12.024</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Building Information Models (BIMs) provide structured, parametric representations that are fundamental for simulation, facility management, and digital twin applications. However, reconstructing BIMs from terrestrial LiDAR scans remains challenging due to clutter, occlusions, and the geometric complexity of roof structures. This paper presents a roof-aware scan-to-BIM pipeline tailored for residential buildings, which processes indoor LiDAR data through four geometric abstractions, raw points, superpoints, triangle meshes, and volumetric polyhedra, each represented by task-specific graphs. The pipeline integrates three modules: LGNet for semantic segmentation, QTNet for floor plan reconstruction, and PPO for roof–floor fusion. It demonstrates strong cross-dataset generalization, being trained on Structured3D and fine-tuned on the real-world WHUTS dataset. The method produces watertight, Revit-compatible BIMs with an average surface deviation of 9 mm RMS on WHUTS scenes featuring slanted roofs. Compared with state-of-the-art scan-to-BIM and floor plan reconstruction methods, the proposed approach achieves higher geometric accuracy on scenes with slanted roofs, reducing surface reconstruction error by over 12–18% and improving layout reconstruction F1-scores by up to 6–8% . The proposed framework provides a robust, accurate, and fully automated solution for roof-aware BIM reconstruction of residential buildings from terrestrial LiDAR data, offering comprehensive support for slanted roof modeling. The source code and datasets are publicly available at https://github.com/Wangbohan-x/roof-aware-scan2bim.git .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>从杂乱、遮挡的地面LiDAR点云自动重建带斜屋顶的住宅室内BIM。</p>
                <p><span class="font-medium text-accent">研究方法：</span>四抽象层级图注意力网络：LGNet语义分割→QTNet平面布局→PPO屋顶-楼层融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>WHUTS实测数据9 mm RMS误差，斜屋顶场景表面误差降12–18%，布局F1升6–8%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将屋顶几何显式纳入scan-to-BIM图注意力框架，实现端到端斜屋顶住宅重建。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数字孪生与设施管理提供毫米级、Revit兼容的LiDAR自动BIM方案，数据代码开源。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有scan-to-BIM研究多聚焦规则屋顶或室内平面，对住宅常见的倾斜屋顶缺乏几何与拓扑联合建模，导致LiDAR点云中屋顶-墙体交接处误差大、模型漏水。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出四阶几何抽象：原始点→超点→三角网格→多面体，每阶用图注意力网络编码局部几何；三模块串行处理：LGNet语义分割房间与屋顶面，QTNet用四叉树图网络生成二维平面布局，PPO强化学习将屋顶网格与楼层平面融合成封闭Revit族。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在WHUTS真实住宅数据集上平均表面偏差9 mm RMS，比SOTA降低12–18%，布局F1提升6–8%；跨数据集实验显示仅用Structured3D预训练即可直接泛化到WHUTS，无需人工标注。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设屋顶面片可参数化为平面或单峰斜面，对双曲、穹顶等复杂曲面仍产生过度简化；PPO融合步骤依赖楼层高度先验，对多层错层住宅需额外交互调整。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将图注意力扩展至可微分NURBS或隐式神经表面，以统一表示自由曲面屋顶与室内细节；引入自监督深度估计缓解LiDAR遮挡。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究课题涉及LiDAR点云语义分割、住宅倾斜屋顶建模、或scan-to-BIM自动重建，本文的跨阶图网络与屋顶-楼层融合策略可直接迁移并作为基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>