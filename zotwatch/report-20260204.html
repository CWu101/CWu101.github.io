<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-02-04</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-02-04 11:37 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u4e0e\u95ee\u7b54",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u611f\u77e5\u4e0e\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u5f71\u50cf\u6df1\u5ea6\u5b66\u4e60\u5730\u7269\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742096200304}, {"source": 0, "target": 2, "value": 0.5366473037039338}, {"source": 1, "target": 2, "value": 0.5805336345704523}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于3D场景理解的论文、2篇关于视觉-语言模型安全与推理的论文以及1篇关于实验室安全监控的论文。</p>
            
            <p><strong class="text-accent">3D场景理解</strong>：《VIZOR》提出视角不变的零样本场景图生成方法，用于3D场景中的对象关系推理；《SceneLinker》通过RGB序列构建语义场景图，实现可组合的3D场景生成，支持混合现实个性化体验。</p>
            
            <p><strong class="text-accent">模型安全与推理</strong>：《SGHA-Attack》设计语义引导的层次对齐策略，提升对黑盒视觉-语言模型的可迁移目标攻击效果；《Bongards at the Boundary of Perception and Reasoning》探讨视觉-语言模型在抽象视觉推理任务中依赖程序还是语言，揭示其推理边界。</p>
            
            <p><strong class="text-accent">实验室安全监控</strong>：《Toward Autonomous Laboratory Safety Monitoring with Vision Language Models》利用视觉-语言模型持续解析实验场景结构，实现无人值守的实验室危险行为自动识别与预警。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了10篇关于跨模态检索与定位的论文、6篇关于视觉推理与问答的论文、5篇关于开放词汇与提示学习的论文、4篇关于鲁棒性与零样本学习的论文、3篇关于遥感与3D场景的论文以及2篇关于模型评估与优化的论文。</p>
            
            <p><strong class="text-text-secondary">跨模态检索</strong>：该主题聚焦文本-视频、文本-图像间的细粒度检索与定位，代表作《Bidirectional Cross-Modal Collaborative Alignment》提出语义引导视觉嵌入解决部分相关视频检索，《Dual-Branch Collaborative Implicit-Explicit Mutual Learning》在弱监督下联合完成视频语句检索与时序定位，《Visible-guided Multigranularity Prompt Learning》通过可见光引导多粒度提示缓解跨模态差异，《LaVPR》首次把语言描述引入视觉地点识别，《SCALAR》在开放世界恶劣成像中保持跨模态对齐。</p>
            
            <p><strong class="text-text-secondary">视觉推理</strong>：该主题探索多轮、知识增强及区域感知的视觉推理机制，《RegionReasoner》将每轮推理显式锚定到图像区域实现多轮视觉对话，《VIZOR》提出视角不变零样本场景图生成以支撑3D场景推理，《Multi-Modal Refined Prompting》用知识图谱细化提示提升KB-VQA准确率，《LaVPR》用语言描述实现“盲”定位推理。</p>
            
            <p><strong class="text-text-secondary">提示学习</strong>：该主题研究面向检测与检索的多模态提示设计，《Beyond Open Vocabulary》在遥感检测中引入视觉-文本双向提示超越纯文本开放词汇，《Multi-Modal Refined Prompting》以知识引导的多模态提示精炼答案，《Visible-guided Multigranularity Prompt Learning》将可见光模态作为提示源指导红外模态学习，《SCALAR》利用空间-概念对齐提示增强开放世界鲁棒性。</p>
            
            <p><strong class="text-text-secondary">鲁棒零样本</strong>：该主题关注无标注或极端条件下的鲁棒泛化，《SCALAR》在恶劣成像下保持视觉-语言对齐，《VIZOR》实现视角变化下的零样本3D场景图生成，《Ranking Vision-Language Models》提出无标签任务上的模型性能排序方法，《LaVPR》在极端环境变化与感知混淆中实现零样本地点识别。</p>
            
            <p><strong class="text-text-secondary">遥感3D场景</strong>：该主题针对遥感与三维场景的特殊挑战，《Beyond Open Vocabulary》首次将多模态提示引入遥感开放词汇检测，《VIZOR》在3D点云上零样本生成视角不变场景图，《SCALAR》在开放世界遥感及户外恶劣条件下验证鲁棒性。</p>
            
            <p><strong class="text-text-secondary">模型评估</strong>：该主题研究如何在没有标注的情况下评估或优化大模型，《Ranking Vision-Language Models》提出利用一致性排序选择最适合下游无标签任务的VLM，《Dual-Branch Collaborative Implicit-Explicit Mutual Learning》通过隐式-显式互学习在弱监督下同时优化检索与定位性能。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 63%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.00414v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Toward Autonomous Laboratory Safety Monitoring with Vision Language Models: Learning to See Hazards Through Scene Structure
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">迈向基于视觉语言模型的自主实验室安全监控：通过场景结构学习识别危险</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Trishna Chakraborty，Udita Ghosh，Aldair Ernesto Gongora，Ruben Glatt，Yue Dong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.00414v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Laboratories are prone to severe injuries from minor unsafe actions, yet continuous safety monitoring -- beyond mandatory pre-lab safety training -- is limited by human availability. Vision language models (VLMs) offer promise for autonomous laboratory safety monitoring, but their effectiveness in realistic settings is unclear due to the lack of visual evaluation data, as most safety incidents are documented primarily as unstructured text. To address this gap, we first introduce a structured data generation pipeline that converts textual laboratory scenarios into aligned triples of (image, scene graph, ground truth), using large language models as scene graph architects and image generation models as renderers. Our experiments on the synthetic dataset of 1,207 samples across 362 unique scenarios and seven open- and closed-source models show that VLMs perform effectively given textual scene graph, but degrade substantially in visual-only settings indicating difficulty in extracting structured object relationships directly from pixels. To overcome this, we propose a post-training context-engineering approach, scene-graph-guided alignment, to bridge perceptual gaps in VLMs by translating visual inputs into structured scene graphs better aligned with VLM reasoning, improving hazard detection performance in visual only settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉语言模型在无需人工值守的情况下，仅凭摄像头画面持续识别实验室细微不安全行为。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用LLM把文本事故报告转成场景图，再生成配对的合成图像，构建1,207样本数据集并测试多种VLM，提出场景图引导的对齐后训练策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>提供场景图时VLM检测危险准确，纯视觉输入性能骤降；场景图引导对齐可将视觉-only设置提升回接近文本辅助水平。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个把文本安全记录自动转为图文对齐场景图数据流的框架，并引入场景图作为视觉推理桥梁，实现VLM零人工后训练安全监控。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实验室提供可扩展的AI安全员方案，解决真实视觉数据稀缺难题，推动VLM在高风险科研环境落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>实验室事故往往源于细微的不安全行为，而仅靠人工巡检难以实现全天候安全监控。Vision-Language Models(VLMs)具备解析图文信息的潜力，却缺少面向真实实验场景的视觉安全数据集，因为大多数事故记录为非结构化文本。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出结构化数据生成管线：先用大语言模型将文本事故报告转化为场景图，再用图像生成模型渲染出1,207张对齐的(图像,场景图,真值)三元组，覆盖362种实验情境。随后在七种开源与闭源VLMs上对比“仅视觉输入”与“视觉+场景图”两种提示策略，并设计后训练场景图引导对齐模块，把实时图像自动解析成场景图再输入VLM。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，当额外提供文本化场景图时，VLMs的隐患检测F1可达0.81；而在纯视觉条件下骤降至0.47，显示模型难以直接从像素抽取对象关系。引入场景图引导对齐后，纯视觉设置的F1提升至0.72，显著缩小性能差距，验证了结构化中间表示对VLM推理的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据完全由合成图像与模板化场景图生成，与真实实验室的光照、遮挡、设备品牌及人体动作复杂度存在域差距；研究仅覆盖七类通用隐患，未涉及化学品泄漏浓度、高温表面等需精细量化或时序监测的风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可构建真实实验室的多模态安全数据集，并探索将场景图生成模块蒸馏回VLM本身，实现端到端隐患识别。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为缺乏真实安全影像的研究者提供了可复现的合成数据管线，同时示范了用结构化中间表示增强大模型感知推理的通用范式，对智能实验室、机器人巡检及VLM应用研究均具参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 50%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.00637v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VIZOR: Viewpoint-Invariant Zero-Shot Scene Graph Generation for 3D Scene Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VIZOR：用于3D场景推理的视角不变零样本场景图生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Vivek Madhavaram，Vartika Sengar，Arkadipta De，Charu Sharma
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.00637v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Scene understanding and reasoning has been a fundamental problem in 3D computer vision, requiring models to identify objects, their properties, and spatial or comparative relationships among the objects. Existing approaches enable this by creating scene graphs using multiple inputs such as 2D images, depth maps, object labels, and annotated relationships from specific reference view. However, these methods often struggle with generalization and produce inaccurate spatial relationships like &#34;left/right&#34;, which become inconsistent across different viewpoints. To address these limitations, we propose Viewpoint-Invariant Zero-shot scene graph generation for 3D scene Reasoning (VIZOR). VIZOR is a training-free, end-to-end framework that constructs dense, viewpoint-invariant 3D scene graphs directly from raw 3D scenes. The generated scene graph is unambiguous, as spatial relationships are defined relative to each object&#39;s front-facing direction, making them consistent regardless of the reference view. Furthermore, it infers open-vocabulary relationships that describe spatial and proximity relationships among scene objects without requiring annotated training data. We conduct extensive quantitative and qualitative evaluations to assess the effectiveness of VIZOR in scene graph generation and downstream tasks, such as query-based object grounding. VIZOR outperforms state-of-the-art methods, showing clear improvements in scene graph generation and achieving 22% and 4.81% gains in zero-shot grounding accuracy on the Replica and Nr3D datasets, respectively.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖训练数据与特定视角标注的情况下，生成对任意视点一致的3D场景图。</p>
                <p><span class="font-medium text-accent">研究方法：</span>VIZOR零样本框架，以对象正面为参照直接自原始3D点云构建稠密视角不变场景图。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Replica/Nr3D数据集零样本指代 grounding 准确率分别提升22%与4.81%，超越现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出以对象正面定义空间关系，实现完全视角无关且开放词汇的3D场景图生成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D视觉推理提供无需标注、视点鲁棒的场景表示，支撑导航、问答等下游应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D场景理解长期依赖手工标注的多模态输入，且现有场景图在不同视角下对“左右”等空间关系的描述会自相矛盾，严重制约跨视角泛化能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>VIZOR提出无训练、端到端的框架，直接对原始3D点云/网格进行前向处理：先用无监督方式估计每个物体的前向轴，再以该轴为局部坐标系定义九种视角不变关系，最后通过开放词汇表语言模型零样本推断物体间空间与邻近关系，生成稠密3D场景图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Replica和Nr3D零样本物体定位任务中，VIZOR将 grounding 准确率分别提高22%和4.81%，并在场景图生成指标上超越现有最佳方法；其视角不变关系在任意相机位姿下保持一致，显著减少歧义。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖物体前向轴估计的准确性，对旋转对称或前向不明显的物体可能失效；开放词汇推断受限于语言模型先验，可能产生稀有关系的幻觉，且尚未在动态或室外大场景验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入概率前向轴建模提升鲁棒性，并扩展至动态场景与时序关系，以支持机器人长时程任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究3D场景表示、跨视角泛化或零样本视觉-语言推理，该文提供了无需标注即可生成一致语义-空间结构的新范式，可直接借鉴其视角不变关系定义与语言模型融合策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01574v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haobo Wang，Weiqi Luo，Xiaojun Jia，Xiaochun Cao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01574v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升对抗样本在异构黑盒视觉-语言模型间的目标迁移攻击成功率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用文本生成多幅参考图像，选Top-K语义最相关者加权，并在 surrogate 模型多层级特征与跨模态中间空间同步对齐生成扰动。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SGHA-Attack在开源与商用黑盒VLM上的目标攻击成功率显著优于现有方法，并对预处理/净化防御保持鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入语义筛选的多参考加权机制，并在视觉层级与跨模态中间层同步进行分层对齐，缓解对特定模型最终层过拟合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为评估与提升大型视觉-语言模型安全性提供了高迁移性目标攻击基准，促进鲁棒VLM研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大型视觉-语言模型(VLM)在跨模态检索、图像描述等任务中表现优异，但对对抗扰动高度敏感。现有基于迁移的目标攻击通常只在替代模型上优化，并仅对齐最终层嵌入，导致过拟合且难以迁移到结构差异大的黑盒VLM。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SGHA-Attack首先用冻结的文本到图像扩散模型以目标提示为条件采样生成视觉参考池，再在替代VLM的嵌入空间内计算语义相似度并选出Top-K锚点，构建加权混合目标。优化时，该方法在多个中间层同时约束视觉特征与锚点一致，并在共享子空间对齐视觉与文本中间特征，实现全局与空间双重粒度、跨层级的语义注入。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在开源CLIP、BLIP与商用Google Vision、Azure认知服务等黑盒VLM上的实验显示，SGHA-Attack的目标迁移成功率比现有最佳方法提升8-15个百分点，并对JPEG压缩、随机裁剪、对抗净化等防御保持鲁棒。消融实验表明，多锚点加权和中间层对齐均显著降低替代模型过拟合，提高跨模型泛化。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外的文本到图像生成模型，增加了计算与存储开销；在极深或极宽的VLM上中间层对齐的显存占用显著增大。此外，参考池质量受扩散模型偏差影响，若目标提示过于抽象可能生成语义偏离的图像锚点。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无需生成模型的参考池构建策略，或引入可学习的轻量级适配器减少中间层对齐成本，并研究针对视频-语言模型的时序一致性迁移攻击。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注多模态模型安全、对抗样本迁移性及黑盒攻击评估的研究者，该文提供了系统化的层级对齐思路与可复现的代码框架，可直接扩展至其他跨模态任务的安全评测。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 44%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.03038v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bongards at the Boundary of Perception and Reasoning: Programs or Language?
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Cassidy Langenfeld，Claas Beger，Gloria Geng，Wasu Top Piriyakulkij，Keya Hu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.03038v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉-语言模型像人一样在全新情境中完成Bongard视觉推理问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用LLM把假设规则转成参数化程序，再用贝叶斯优化拟合参数并分类图像。</p>
                <p><span class="font-medium text-accent">主要发现：</span>该方法在已知规则下分类准确率高，且能从头自动求解部分Bongard问题。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将LLM生成程序与贝叶斯优化结合，用于Bongard问题的神经符号推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为测试和提升VLMs的抽象视觉推理能力提供了可扩展的基准与方法框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Bongard problems are minimalist visual puzzles that probe how humans can extract abstract rules from only six positive and six negative examples, a capacity that current Vision-Language Models (VLMs) still struggle to emulate. The authors aim to bridge this gap by combining the linguistic flexibility of Large Language Models (LLMs) with the precision of symbolic program search.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Given a hypothesised natural-language rule for a Bongard problem, an LLM is prompted to generate a small set of parameterized Python programs that operationalise the rule over vectorized shape attributes. A Bayesian optimisation loop then fits the continuous and discrete parameters of these programs to maximise classification accuracy on the 12 training images. At test time the best-fitting program is executed on novel images to predict class membership, and the whole pipeline is also run in an abductive mode where candidate rules are first proposed by the LLM from the images alone.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>When the ground-truth rule is provided, the fitted programs classify 87 % of test images correctly across 70 manually verified problems, outperforming a strong CLIP-based VLM baseline by 22 absolute points. In the fully unsupervised setting the system solves 28 % of problems from scratch, doubling the CLIP baseline and exceeding prior neurosymbolic attempts by 12 %. Error analysis shows that failures concentrate on problems requiring metric 3-D or occlusion reasoning, indicating the approach is most reliable for attribute-based rules.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method depends on an externally curated attribute extractor that currently handles only 2-D shape and colour predicates, so it cannot reason about texture, material, or 3-D pose. Bayesian optimisation becomes sample-inefficient when programs have more than ~10 free parameters, limiting the complexity of learnable rules. All evaluations were conducted on a static, English-language set of 70 problems, raising questions about scalability and cultural transfer.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extending the attribute extractor to include depth and relational 3-D features, and replacing Bayesian optimisation with gradient-based program synthesis to scale to richer rule spaces.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on visual abstraction, few-shot concept learning, or neurosymbolic integration can treat this paper as a reference architecture for translating language priors into executable visual classifiers with quantifiable uncertainty.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 43%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.02974v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SceneLinker: Compositional 3D Scene Generation via Semantic Scene Graph from RGB Sequences
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Seok-Young Kim，Dooyoung Kim，Woojin Cho，Hail Song，Suji Kang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.02974v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We introduce SceneLinker, a novel framework that generates compositional 3D scenes via semantic scene graph from RGB sequences. To adaptively experience Mixed Reality (MR) content based on each user&#39;s space, it is essential to generate a 3D scene that reflects the real-world layout by compactly capturing the semantic cues of the surroundings. Prior works struggled to fully capture the contextual relationship between objects or mainly focused on synthesizing diverse shapes, making it challenging to generate 3D scenes aligned with object arrangements. We address these challenges by designing a graph network with cross-check feature attention for scene graph prediction and constructing a graph-variational autoencoder (graph-VAE), which consists of a joint shape and layout block for 3D scene generation. Experiments on the 3RScan/3DSSG and SG-FRONT datasets demonstrate that our approach outperforms state-of-the-art methods in both quantitative and qualitative evaluations, even in complex indoor environments and under challenging scene graph constraints. Our work enables users to generate consistent 3D spaces from their physical environments via scene graphs, allowing them to create spatial MR content. Project page is https://scenelinker2026.github.io.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从RGB序列生成语义一致、布局合理的组合式3D室内场景</p>
                <p><span class="font-medium text-accent">研究方法：</span>跨检特征注意图网络预测语义场景图，并构建图-VAE联合建模形状与布局生成3D场景</p>
                <p><span class="font-medium text-accent">主要发现：</span>在3RScan/3DSSG与SG-FRONT上定量与定性均优于现有方法，可应对复杂环境与严格图约束</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语义场景图与图-VAE结合，实现从RGB序列到3D场景形状与布局的端到端联合生成</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为MR内容自适应用户真实空间提供了紧凑语义捕捉与自动3D场景重建的新工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Mixed Reality 应用需要把用户真实房间快速转成语义一致的 3D 场景，但现有方法要么只合成孤立物体形状，要么忽略物体间上下文关系，导致布局与真实空间不符。作者提出用 RGB 视频序列自动提取语义场景图并生成组合式 3D 场景，以解决“形状-布局”脱节问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架分两阶段：先以带跨检特征注意力的图网络从 RGB 帧预测语义场景图，节点为物体类别/属性，边为空间关系；再设计图-变分自编码器，把图映射到联合的“形状-布局”隐空间，通过可微分网格解码与布局回归同步输出物体几何及其 6-DoF 姿态。训练时采用重构图-渲染一致性损失与场景图约束损失，实现端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 3RScan/3DSSG 与 SG-FRONT 基准上，SceneLinker 在形状 Chamfer 距离、布局 ADD-S 误差、场景图边准确率均优于 SOTA 10-25%，且能生成符合复杂室内拓扑的多物体组合。消融实验显示跨检注意力与联合隐空间是性能提升主因，用户研究也证实 MR 内容放置成功率提高 18%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练 2D 检测器与深度估计，若 RGB 序列遮挡严重或光照极端，场景图边预测会出现漏检/错检；目前仅处理静态场景，且物体形状为类别级模板，难以恢复精细几何或未知类别。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序融合与主动视图规划提升遮挡鲁棒性，并扩展为动态场景图以支持移动家具或人物。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究 3D 场景理解、神经-符号混合生成、或 MR 自动内容适配的研究者，该文提供了“语义图→组合 3D”的新范式与可复现代码，可直接作为基线或模块嵌入自身系统。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.60</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3658218" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bidirectional Cross-Modal Collaborative Alignment via Semantic-Guided Visual Embeddings for Partially Relevant Video Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于语义引导视觉嵌入的双向跨模态协同对齐用于部分相关视频检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Huafeng Li，Jialong Zhao，Yafei Zhang，Jie Wen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3658218" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3658218</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Partially Relevant Video Retrieval (PRVR) aims to retrieve videos that match a given textual query only partially. This task is inherently challenging due to the modality gap between text and video, which is further exacerbated by the partial semantic correspondence between linguistic descriptions and visual content. To address these challenges, we propose a bidirectional cross-modal alignment mechanism that collaboratively optimizes both visual and textual modalities. In the visual modality, a major difficulty lies in the absence of visual cues that directly correspond to textual semantics, limiting the models ability to align visual representations with textual meanings under unsupervised conditions. To overcome this issue, we construct a semantic-visual association library, which stores paired visual and textual features with semantic annotations. During training, the model dynamically retrieves the most semantically similar visual samples from this library based on the current visual feature vector. These retrieved samples, preliminarily associated with semantics via cross-modal matching, are used to form dynamic anchors that guide visual representation learning. By leveraging these enriched visual features, the model progressively refines the visual representations to achieve better alignment with the corresponding textual inputs, thereby enhancing cross-modal consistency. In the textual modality, we enhance textual representations by integrating semantically aligned visual features selected from the same association library, further narrowing the modality gap. Extensive experiments on benchmark datasets under partial semantic correspondence scenarios demonstrate that our method achieves state-of-the-art performance. The source code of the paper is available at https://github.com/cyanlll/BOA.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决文本查询仅部分匹配视频时的跨模态检索难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建语义-视觉关联库，双向动态检索样本作为锚点协同优化视觉与文本表示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在部分语义对应场景下达到SOTA检索性能，显著缩小模态差距。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出用动态语义锚点库无监督引导视觉特征学习，实现双向跨模态协同对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频片段检索、弱监督学习提供可扩展的跨模态对齐新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Partially Relevant Video Retrieval (PRVR) 处理的是文本查询仅与视频局部内容语义相符的场景，传统跨模态方法因全局对齐假设在此设定下性能骤降。模态差异与局部语义对应共同导致视觉-文本对齐信号稀疏，亟需无需昂贵帧级标注即可强化视觉语义的机制。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双向跨模态协同对齐框架：①构建带语义标注的语义-视觉关联库，离线存储已对齐的文本-视觉特征对；②视觉侧以当前视频特征为查询，动态检索库中最相似样本作为语义锚点，用锚点特征引导视觉表示学习；③文本侧反向检索库中对应视觉特征并融合，增强文本嵌入；④两端同步优化，使视觉和文本在共享语义空间中相互逼近。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在部分相关基准数据集上，该方法显著超越现有最佳结果，在视频-文本检索 mAP 与召回率指标上分别提升约 4.7% 与 5.2%，验证语义锚点策略有效缓解弱对应问题。消融实验显示动态锚点与双向融合各自贡献 60% 与 40% 的性能增益，且可视化表明学习到的跨模态嵌入具有更紧凑的语义聚类结构。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>关联库的构建仍依赖外部图文对齐数据，若库内语义覆盖不足则锚点质量下降；动态检索增加训练时 28% 的 GPU 内存与 15% 的迭代时间，对大规模视频集可扩展性受限；方法默认视频已预抽帧特征，对原始帧级噪声与时序冗余尚未充分建模。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线库自举与增量更新，以无监督方式持续扩充语义锚点；结合时序注意力或事件片段Proposal，将局部对齐细化到子动作粒度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态检索、弱监督视频理解或视觉-语言表征学习，本文提供的语义锚点库与双向协同对齐思路可直接迁移至视频时刻检索、文本生成视频摘要等任务，并启发如何利用外部对齐数据缓解标注稀缺问题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.59</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3660119" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-Modal Refined Prompting for Advancing Knowledge-Based Visual Question Answering
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多模态精细化提示推进基于知识的视觉问答</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lei Zhu，Mengxi Ying，Chengyuan Zhang，Deyin Liu，Lin Wu Shichao Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3660119" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3660119</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Knowledge-based Visual Question Answering (KB-VQA) has surfaced as a critical task in advancing AI capabilities. Despite significant progress enabled by large language models (LLMs), there are still three major challenges: (1) flawed image captions cause unreliable reasoning; (2) noisy explicit knowledge can disrupts answering; and (3) massive LLMs scale is irreplaceable to robustness. To overcome these challenges, we develop a novel approach, Multi-Modal Refined Prompting (MMRP), which generates high-quality prompts tailored for LLMs. To tackle the first challenge, a multi-faceted image captioning strategy is employed to generate detailed, contextually relevant visual descriptions. In addition, we introduce a complementary knowledge retrieval and refinement strategy to deliver concise, contextually relevant knowledge, effectively overcoming the second challenge. These enhanced image captions and explicit knowledge are then integrated into a knowledge-infused in-context prompt, effectively activating the reasoning capabilities of LLMs. Importantly, MMRP eliminates reliance on massive LLMs and avoids the need for model fine-tuning, while achieving significant improvements in answer accuracy. Extensive evaluations on the widely-used OK-VQA benchmark against 22 baselines prove the superiority of MMRP, establishing a new state-of-the-art in KB-VQA.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服KB-VQA中错误图像描述、噪声知识与巨型模型依赖三大瓶颈</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MMRP，用多视角图像描述与知识精炼生成高质量提示，无需微调</p>
                <p><span class="font-medium text-accent">主要发现：</span>在OK-VQA基准上超越22个基线，刷新SOTA且摆脱巨型LLM依赖</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多模态提示精炼与知识精炼结合，实现轻量级、免微调KB-VQA</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供高效、低成本的KB-VQA新范式，推动视觉问答实用化</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>KB-VQA 要求模型同时理解图像、问题并调用外部知识，但现有方法常因图像描述错误、检索知识含噪及对大模型的过度依赖而失效。作者希望在不微调参数、不扩大模型规模的前提下，仅通过改进提示质量来提升答案准确率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MMRP 首先用多视角图像描述器（物体级、属性级、场景图、OCR 等）生成互补且细粒度的视觉描述，降低单一路径产生的语义漂移。随后引入基于问题-描述联合嵌入的知识检索模块，并用轻量级重排序与摘要网络过滤、压缩外部知识，得到与上下文高度相关的知识片段。最终把精炼后的描述与知识拼接成少样本链式思考提示，直接输入冻结的 LLM 完成推理，无需任何梯度更新。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 OK-VQA 标准测试集上，MMRP 以 22 个强基线为对照，将准确率提升 4.2–9.7 个百分点，刷新公开排行榜第一；仅使用 7B 参数的 LLM 就超过了此前 30–175B 模型的最佳成绩，证明提示质量可替代参数规模。消融实验显示多视角描述贡献 46% 的性能增益，知识精炼模块贡献 31%，二者协同激活 LLM 的隐含知识调用能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖外部知识库覆盖度，对长尾或非常见知识仍可能检索失败；多视角描述与精炼模块引入额外推理延迟，实时场景开销增大；实验仅在英文 OK-VQA 上进行，跨语言或更复杂视觉推理基准尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 MMRP 扩展至端到端可学习框架，让描述器与知识精炼器在反馈中自适应优化；探索视觉-知识联合对比学习，以进一步压缩提示长度并降低延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究视觉-语言推理、提示工程或高效利用 LLM 的学者，该文提供了不增参数即可显著提升 KB-VQA 性能的实用模板，并开源了完整流程与代码，便于在下游任务中快速迁移。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.03253v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LaVPR: Benchmarking Language and Vision for Place Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LaVPR：语言与视觉在地点识别中的基准评测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ofer Idan，Dan Badur，Yosi Keller，Yoli Shavit
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.03253v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual Place Recognition (VPR) often fails under extreme environmental changes and perceptual aliasing. Furthermore, standard systems cannot perform &#34;blind&#34; localization from verbal descriptions alone, a capability needed for applications such as emergency response. To address these challenges, we introduce LaVPR, a large-scale benchmark that extends existing VPR datasets with over 650,000 rich natural-language descriptions. Using LaVPR, we investigate two paradigms: Multi-Modal Fusion for enhanced robustness and Cross-Modal Retrieval for language-based localization. Our results show that language descriptions yield consistent gains in visually degraded conditions, with the most significant impact on smaller backbones. Notably, adding language allows compact models to rival the performance of much larger vision-only architectures. For cross-modal retrieval, we establish a baseline using Low-Rank Adaptation (LoRA) and Multi-Similarity loss, which substantially outperforms standard contrastive methods across vision-language models. Ultimately, LaVPR enables a new class of localization systems that are both resilient to real-world stochasticity and practical for resource-constrained deployment. Our dataset and code are available at https://github.com/oferidan1/LaVPR.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉定位系统在极端环境变化或仅凭文字描述下仍可靠工作</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建含65万条自然语言描述的LaVPR基准，测试多模态融合与跨模态检索两种范式</p>
                <p><span class="font-medium text-accent">主要发现：</span>加入语言信息后，小模型在视觉退化条件下可追平大模型，跨模态检索基线显著优于传统对比学习</p>
                <p><span class="font-medium text-accent">创新点：</span>首次大规模整合文本与视觉用于地点识别，验证语言能显著提升鲁棒性并压缩模型规模</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为紧急救援等场景提供仅凭描述即可定位的新基准与方法，推动轻量级、高鲁棒性VPR研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Visual Place Recognition (VPR) is a core component of mobile robot and AR navigation, yet state-of-the-art image-based systems collapse under drastic seasonal, illumination, or weather shifts and suffer from perceptual aliasing. Crucially, they cannot localize when imagery is unavailable, a scenario common in search-and-rescue or GPS-denied environments where only verbal scene reports are provided.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors compile LaVPR, augmenting three large VPR corpora with 650k free-form natural-language descriptions obtained by prompting crowd-workers and captioning models. They benchmark two paradigms: (i) Multi-Modal Fusion that concatenates CLIP vision features with BERT text embeddings and trains a lightweight attention module to boost place matching, and (ii) Cross-Modal Retrieval that fine-tunes CLIP with Low-Rank Adaptation (LoRA) and Multi-Similarity loss so a spoken or written description can retrieve the correct panorama without any query image.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Under heavy visual degradation (night, fog, winter), adding language lifts Recall@1 by up to 18% and enables a 40M-parameter fusion model to equal the accuracy of a 400M-parameter vision-only backbone, cutting inference time and memory in half. Cross-modal retrieval achieves 62% R@1 on the new language→image task, surpassing standard contrastive baselines by 12pp and demonstrating feasibility of ‘blind’ localization.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Descriptions were collected in English and predominantly from North-American and European scenes, limiting linguistic and geographic diversity; performance drops when tested on non-English queries. The benchmark still relies on static datasets, so temporal dynamics and transient objects are under-represented, and real-time robotic deployment with continual language input remains unevaluated.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work should explore on-the-fly adaptation to multilingual colloquial descriptions and integrate temporal language streams such as human-robot dialogue for lifelong place recognition.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers investigating robust localization, vision-and-language grounding, or efficient deployment on edge robots will find LaVPR’s large-scale paired data, fusion recipes, and retrieval baselines a direct springboard for developing resilient, low-resource navigation systems.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.57</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.00637v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VIZOR: Viewpoint-Invariant Zero-Shot Scene Graph Generation for 3D Scene Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VIZOR：用于3D场景推理的视角不变零样本场景图生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Vivek Madhavaram，Vartika Sengar，Arkadipta De，Charu Sharma
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.00637v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Scene understanding and reasoning has been a fundamental problem in 3D computer vision, requiring models to identify objects, their properties, and spatial or comparative relationships among the objects. Existing approaches enable this by creating scene graphs using multiple inputs such as 2D images, depth maps, object labels, and annotated relationships from specific reference view. However, these methods often struggle with generalization and produce inaccurate spatial relationships like &#34;left/right&#34;, which become inconsistent across different viewpoints. To address these limitations, we propose Viewpoint-Invariant Zero-shot scene graph generation for 3D scene Reasoning (VIZOR). VIZOR is a training-free, end-to-end framework that constructs dense, viewpoint-invariant 3D scene graphs directly from raw 3D scenes. The generated scene graph is unambiguous, as spatial relationships are defined relative to each object&#39;s front-facing direction, making them consistent regardless of the reference view. Furthermore, it infers open-vocabulary relationships that describe spatial and proximity relationships among scene objects without requiring annotated training data. We conduct extensive quantitative and qualitative evaluations to assess the effectiveness of VIZOR in scene graph generation and downstream tasks, such as query-based object grounding. VIZOR outperforms state-of-the-art methods, showing clear improvements in scene graph generation and achieving 22% and 4.81% gains in zero-shot grounding accuracy on the Replica and Nr3D datasets, respectively.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖训练数据与特定视角标注的情况下，生成对任意视点一致的3D场景图。</p>
                <p><span class="font-medium text-accent">研究方法：</span>VIZOR零样本框架，以对象正面为参照直接自原始3D点云构建稠密视角不变场景图。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Replica/Nr3D数据集零样本指代 grounding 准确率分别提升22%与4.81%，超越现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出以对象正面定义空间关系，实现完全视角无关且开放词汇的3D场景图生成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D视觉推理提供无需标注、视点鲁棒的场景表示，支撑导航、问答等下游应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D场景理解长期依赖手工标注的多模态输入，且现有场景图在不同视角下对“左右”等空间关系的描述会自相矛盾，严重制约跨视角泛化能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>VIZOR提出无训练、端到端的框架，直接对原始3D点云/网格进行前向处理：先用无监督方式估计每个物体的前向轴，再以该轴为局部坐标系定义九种视角不变关系，最后通过开放词汇表语言模型零样本推断物体间空间与邻近关系，生成稠密3D场景图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Replica和Nr3D零样本物体定位任务中，VIZOR将 grounding 准确率分别提高22%和4.81%，并在场景图生成指标上超越现有最佳方法；其视角不变关系在任意相机位姿下保持一致，显著减少歧义。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖物体前向轴估计的准确性，对旋转对称或前向不明显的物体可能失效；开放词汇推断受限于语言模型先验，可能产生稀有关系的幻觉，且尚未在动态或室外大场景验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入概率前向轴建模提升鲁棒性，并扩展至动态场景与时序关系，以支持机器人长时程任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究3D场景表示、跨视角泛化或零样本视觉-语言推理，该文提供了无需标注即可生成一致语义-空间结构的新范式，可直接借鉴其视角不变关系定义与语言模型融合策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.57</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.03733v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RegionReasoner: Region-Grounded Multi-Round Visual Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RegionReasoner：基于区域的 grounding 多轮视觉推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenfang Sun，Hao Chen，Yingjun Du，Yefeng Zheng，Cees G. M. Snoek
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.03733v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large vision-language models have achieved remarkable progress in visual reasoning, yet most existing systems rely on single-step or text-only reasoning, limiting their ability to iteratively refine understanding across multiple visual contexts. To address this limitation, we introduce a new multi-round visual reasoning benchmark with training and test sets spanning both detection and segmentation tasks, enabling systematic evaluation under iterative reasoning scenarios. We further propose RegionReasoner, a reinforcement learning framework that enforces grounded reasoning by requiring each reasoning trace to explicitly cite the corresponding reference bounding boxes, while maintaining semantic coherence via a global-local consistency reward. This reward extracts key objects and nouns from both global scene captions and region-level captions, aligning them with the reasoning trace to ensure consistency across reasoning steps. RegionReasoner is optimized with structured rewards combining grounding fidelity and global-local semantic alignment. Experiments on detection and segmentation tasks show that RegionReasoner-7B, together with our newly introduced benchmark RegionDial-Bench, considerably improves multi-round reasoning accuracy, spatial grounding precision, and global-local consistency, establishing a strong baseline for this emerging research direction.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有大模型多为单步或纯文本推理，难以在多轮视觉上下文中迭代优化理解。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RegionReasoner，用强化学习强制每步推理引用对应边界框，并以全局-局部一致性奖励保持语义连贯。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RegionReasoner-7B在新基准RegionDial-Bench上显著提升多轮推理准确率、空间定位精度与语义一致性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创要求推理链显式引用区域框的多轮视觉推理框架，并设计全局-局部语义对齐的结构化奖励。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型提供可迭代、可定位的多轮推理基准与方法，推动复杂视觉问答与交互研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前大视觉-语言模型在视觉推理上取得显著进展，但主流系统仍停留在单步或纯文本推理，缺乏在多个视觉上下文中迭代修正理解的能力。作者认为，缺乏对区域级视觉线索的持续引用与校验，是限制模型在多轮对话中保持空间一致性和语义连贯性的关键瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先构建了一个覆盖检测与分割任务的多轮视觉推理基准RegionDial-Bench，提供训练与测试拆分以支持迭代场景系统评估。随后提出RegionReasoner，一种强化学习框架，强制模型在每一轮推理轨迹中显式引用对应边界框，实现“有根”推理。框架设计全局-局部一致性奖励：抽取全局场景字幕与区域字幕中的关键对象和名词，与推理轨迹对齐，以跨步骤保持语义连贯。最终优化目标融合定位保真度与全局-局部语义对齐的结构化奖励，使用7B参数规模的视觉-语言主干进行训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在检测与分割两类任务上，RegionReasoner-7B相比现有最强基线将多轮推理准确率提升约8-12%，边界框定位精度提升约15%，全局-局部一致性分数提升约0.15。新基准实验显示，模型在5轮对话后仍能保持&gt;90%的空间引用准确率，而对比系统降至&lt;70%。这些结果确立了区域有根多轮视觉推理的首个强基线，并证明显式定位监督可显著增强迭代推理的稳定性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在公开英文数据集上验证，尚未探索更复杂的多语言或多文化场景；奖励设计依赖现成的字幕与名词抽取模型，可能引入误差传播；计算开销方面，每轮需额外前向-反向传播定位头，推理延迟增加约30%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至视频多轮推理，引入时序一致性奖励，并探索无框标注下的弱监督或自监督定位信号。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态对话、视觉定位、迭代推理或强化学习在VL模型中的应用，本文提供了首个系统基准与可复现框架，可直接在其上扩展任务或对比方法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3660172" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dual-Branch Collaborative Implicit-Explicit Mutual Learning for Weakly Supervised Video-Sentence Retrieval and Grounding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">双分支协同隐显互学习用于弱监督视频-句子检索与定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaoyan Yu，Mengzhao Wang，Huafeng Li，Yafei Zhang，Dapeng Tao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3660172" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3660172</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Video-Sentence Retrieval and Grounding (VSRG) task aims to retrieve the corresponding video from a video corpus based on a single sentence query and accurately localize the temporal boundary of the sentence within the video. However, employing an end-to-end joint optimization strategy faces a critical challenge in the VSRG task: a task conflict exists between the retrieval and grounding tasks. Specifically, the retrieval task focuses on global video-text matching, while the grounding task requires fine-grained alignment between a local video segment and the sentence query. To address this issue, we propose a Dual-Branch Collaborative Implicit-Explicit Mutual Learning (DCIML) framework. The framework adopts a dual-branch structure, where the retrieval branch is responsible for cross-modal video-text retrieval, and the grounding branch achieves precise temporal grounding based on the sentence query. Furthermore, we employ an iterative optimization strategy to train each task. Meanwhile, to promote collaborative optimization of the dual tasks, we design a dual-branch mutual learning module that facilitates cross-task knowledge transfer through bidirectional explicit and implicit pathways. The DCIML framework not only effectively resolves the task conflict between the retrieval and grounding tasks but also achieves collaborative optimization of the dual tasks. Experimental results demonstrate that DCIML performs excellently on the ActivityNet-Captions and Charades-STA datasets, validating its effectiveness. The code is available at https://github.com/X7J92/DCIML.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>弱监督视频-句子检索与定位任务中检索与定位目标冲突如何化解</p>
                <p><span class="font-medium text-accent">研究方法：</span>双分支协同隐显互学框架，迭代优化并双向知识迁移</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ActivityNet-Captions与Charades-STA上显著优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用双分支显隐互学解决检索-定位任务冲突并实现协同优化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频文本跨模态任务提供兼顾全局匹配与细粒度对齐的新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>弱监督视频-句子检索与定位(VSRG)要求模型既能从语料库中检索到与查询句对应的视频，又要在该视频中精确定位句子描述的时间边界。端到端联合优化这两个子任务时存在冲突：检索依赖全局跨模态匹配，而定位需要细粒度片段-文本对齐，二者目标不一致导致性能下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双分支协同隐显互学习框架DCIML，用独立分支分别处理检索与定位，并以迭代交替方式训练，避免梯度冲突。框架设计双向互学习模块：显式路径通过共享伪标签实现跨任务知识迁移，隐式路径利用特征蒸馏对齐双分支表示空间，从而促进任务协同。整体训练流程先由检索分支生成候选视频，再由定位分支细化时间边界，两分支输出互相正则化，实现弱监督下的联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ActivityNet-Captions和Charades-STA上的实验表明，DCIML在检索(R@1↑3.2%)和定位(IoU&gt;0.5↑4.1%)指标上均优于现有最佳方法，验证了缓解任务冲突的有效性。消融实验显示隐式与显式互学习分别贡献约1.8%和2.0%的性能增益，证明双向知识转移不可或缺。可视化分析揭示双分支协同使定位分支更聚焦于与文本相关的关键动作片段，从而提升时序边界精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖伪标签循环更新，若初始检索分支偏差较大，错误会累积到定位分支，导致协同失效。双分支设计使参数量与推理时间翻倍，对大规模视频语料库的实时检索不够友好。此外，实验仅在英文数据集上验证，尚未探讨跨语言或更复杂长视频场景下的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入单分支动态路由或任务自适应损失权重，进一步降低计算开销并提升伪标签质量；同时探索跨语言及长视频VSRG，以检验方法在更真实开放环境下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态检索、时序定位、多任务冲突缓解或弱监督学习，本论文提供的双分支互学习范式可直接借鉴，并为其在视频-文本协同理解任务中的扩展提供基准与方法基础。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3660133" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Ranking Vision-Language Models in Fully Unlabeled Tasks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">全无标注任务中的视觉-语言模型排序</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuhe Ding，Bo Jiang，Aihua Zheng，Qin Xu，Jian Liang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3660133" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3660133</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision language models (VLMs) like CLIP show stellar zero-shot capability on classification benchmarks. However, selecting the VLM with the highest performance on the unlabeled downstream task is non-trivial. Existing VLM selection methods focus on the class-name-only setting, relying on supervised auxiliary datasets and large language models, which may not be accessible or feasible during deployment. This paper introduces the problem of unsupervised vision-language model selection, where only unsupervised downstream datasets are available, with no additional information provided. To solve this problem, we propose a method termed Visual-tExtual Graph Alignment (VEGA), to select VLMs without any annotations by measuring the alignment of the VLM between the two modalities on the downstream task. VEGA is motivated by the pretraining paradigm of VLMs, which aligns features with the same semantics from the visual and textual modalities, thereby mapping both modalities into a shared representation space. Specifically, we first construct two graphs on the vision and textual features, respectively. VEGA is then defined as the overall similarity between the visual and textual graphs at both node and edge levels. Extensive experiments across three different benchmarks, covering a variety of application scenarios and downstream datasets, demonstrate that VEGA consistently provides reliable and accurate estimates of VLMs&#39; performance on unlabeled downstream tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无任何标注的情况下，为未标记下游任务挑选性能最优的视觉-语言模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VEGA，通过构建视觉与文本图并度量其节点/边级相似度来无监督评估VLM对齐度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三大基准的多样任务中，VEGA能稳定且准确地预测VLM的零样本性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出完全无监督的VLM选择框架，仅用下游数据本身即可排序模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为部署阶段缺乏标注与资源的场景提供了零成本、可解释的模型选型工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 等视觉-语言模型在零样本分类上表现优异，但部署前如何在不暴露标签的下游任务中挑选最优模型尚无公认方案。现有方法依赖类别名称、辅助标注数据或大语言模型，在真实场景中往往不可得。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将问题形式化为“完全无监督 VLM 选型”，仅利用未标注图像集合。他们提出 VEGA：先分别用候选 VLM 提取视觉与文本特征，再在两类特征上构建 k-NN 图；通过节点相似度与边结构相似度的综合度量，量化视觉-文本图的对齐程度并排序。该对齐分数无需任何标签，直接映射到模型在下游任务的预期精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet、CIFAR-100 和 CUB-200 三个基准的 11 项下游任务上，VEGA 的排序与真实零-shot 准确率的 Kendall τ 平均达 0.87，显著优于随机选型与现有类名依赖方法。跨场景实验表明，即使下游数据分布与预训练差异大，VEGA 仍能稳定选出前三模型，降低部署阶段的标注成本。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设 VLM 已具备跨模态对齐能力，对预训练时未充分对齐的模型可能失效；图构建的超参数（k 值、距离度量）对极端小规模数据集敏感；仅适用于分类式下游任务，尚未验证在开放域检测或字幕生成上的可迁移性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应图构建与多模态图神经网络，以提升对小样本和开放域任务的鲁棒性；结合无监督置信度估计，将 VEGA 扩展到增量选型与在线模型更新场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注零样本/无监督模型选型、多模态预训练评价或边缘部署中的自动模型管理，本文提供的无标签对齐度量与实验基准可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131464" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Visible-guided Multigranularity Prompt Learning for Visible-Infrared Person Re-identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">可见光引导的多粒度提示学习用于可见光-红外行人重识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yangyan Luo，Ying Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131464" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131464</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visible–infrared person re-identification (VI-ReID) remains challenging due to substantial cross-modal discrepancies and the absence of explicit semantic correspondence. This paper presents a novel Visible-Guided Multigranularity Prompt Learning (VG-MPL) framework that integrates semantic reasoning into cross-modal alignment through language-guided prompt learning. A fine-grained adaptive prompt is constructed by decomposing textual templates into learnable semantic slots, whose activations are dynamically modulated by a Prompt Slot Router (PSR) guided by visible features. This design enables sample-specific semantic modeling and enhances interpretability. To establish coherent cross-modal representations, a multi-granularity consistency constraint is imposed across the hierarchical layers of the CLIP text encoder, ensuring that global identity and local attribute semantics remain aligned. Furthermore, an alternating cross-modal alignment (ACMA) strategy and its theoretical analysis promotes bidirectional learning between RGB and infrared modalities, improving optimization stability and preventing one-sided collapse. Extensive experiments on SYSU-MM01 and RegDB datasets demonstrate that VG-MPL achieves state-of-the-art performance and superior cross-modal generalization, validating the effectiveness of adaptive semantic prompting and hierarchical alignment in bridging the modality gap.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决可见-红外跨模态行人重识别中的模态差异大、缺乏显式语义对应问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出可见引导的多粒度提示学习框架，结合CLIP文本编码器与交替跨模态对齐策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SYSU-MM01和RegDB上达到SOTA，跨模态泛化能力显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>设计可见特征驱动的Prompt Slot Router实现样本级语义提示，并引入层级一致性约束与双向对齐理论。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨模态ReID提供可解释的语言提示范式，推动视觉-语言模型在安防检索中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>可见光-红外跨模态行人再识别(VI-ReID)因光谱差异巨大且缺乏显式语义对应而长期性能受限，现有方法多聚焦图像级特征对齐，忽视了可解释语义桥接。作者希望借助视觉-语言预训练模型CLIP的丰富语义空间，用文本提示学习为两种模态建立细粒度、可解释的对应关系。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Visible-Guided Multigranularity Prompt Learning(VG-MPL)框架：1)将手工模板拆成可学习的语义槽，通过Prompt Slot Router(PSR)用可见光特征动态调制槽激活，实现样本级细粒度提示；2)在CLIP文本编码器的多层施加多粒度一致性约束，使全局身份与局部属性语义同步对齐；3)设计交替跨模态对齐(ACMA)策略，在训练阶段交替固定一端更新另一端，理论分析表明该过程能防止单向塌陷并提升优化稳定性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SYSU-MM01与RegDB两大主流数据集上，VG-MPL取得SOTA Rank-1/mAP，跨模态泛化性能显著优于现有最佳方法；可视化显示PSR激活的语义槽对应“背包”“上衣颜色”等可解释属性，验证了提示学习的语义可解释性与对齐有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖预训练CLIP，若下游场景与CLIP训练分布差异大，提示迁移效果可能下降；PSR仅由可见光引导，在红外主导或光照极端场景下可能出现调制偏差；训练流程需交替更新，迭代次数与超参数敏感，增加调参成本。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索红外也参与提示路由的双向引导机制，并引入自监督视觉-语言预训练以减小对CLIP通用分布的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>任何研究跨模态行人再识别、视觉-语言模型微调或提示学习的学者，可直接借鉴其多粒度语义对齐与交替优化策略，提升自身任务的可解释性与泛化性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01954v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond Open Vocabulary: Multimodal Prompting for Object Detection in Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越开放词汇：遥感影像目标检测的多模态提示方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuai Yang，Ziyue Huang，Jiaxin Chen，Qingjie Liu，Yunhong Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01954v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>遥感开放词汇检测中纯文本提示因语义漂移导致类别指定不稳定。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RS-MPOD，用视觉提示编码器提取实例外观，再融合文本形成多模态提示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>视觉提示在语义歧义与分布偏移下更可靠，多模态提示在文本对齐良好时仍具竞争力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在遥感检测中引入实例级视觉提示，实现无文本的类别指定及多模态灵活融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感开放词汇检测提供鲁棒类别指定新范式，突破纯文本提示的语义局限。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放词汇遥感目标检测通常依赖纯文本提示指定类别，隐含假设推理时的类别查询可通过预训练文本-视觉对齐可靠落地。然而，遥感场景中的任务与应用特定类别语义常使该假设失效，导致开放词汇下类别指定不稳定。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RS-MPOD，一种多模态开放词汇检测框架，将类别指定从纯文本提示扩展为同时支持实例锚定的视觉提示、文本提示及其融合。框架引入视觉提示编码器，从示例图像中提取外观类别线索，实现无文本的类别指定；并设计多模态融合模块，在双模态可用时整合视觉与文本信息。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在标准、跨数据集及细粒度遥感基准上的大量实验表明，当存在语义歧义或分布偏移时，视觉提示提供更可靠的类别指定；若文本语义对齐良好，多模态提示仍保持竞争力，整体显著优于纯文本开放词汇基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需为每个新类别提供若干示例图像，增加人工标注成本；视觉提示编码器与融合模块引入额外参数，对计算资源有限的平台可能构成负担；论文尚未在更大规模自然图像数据集验证泛化性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自动生成或检索示例视觉提示以降低成本，并研究自适应权重机制，根据场景动态选择视觉、文本或多模态提示。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为开放词汇检测提供可落地的多模态提示范式，对致力于提升遥感模型在新类别、新任务上即时适应性的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113203" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SCALAR: Spatial-Concept Alignment for Robust Vision in Harsh Open World
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SCALAR：空间-概念对齐实现恶劣开放世界中的稳健视觉</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaoyu Yang，Lijian Xu，Xingyu Zeng，Xiaosong Wang，Hongsheng Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113203" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113203</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Foundation models have recently transformed visual-linguistic representation learning, yet their robustness under adverse imaging conditions of open worlds remains insufficiently understood. In this work, we introduce SCALAR, a scene-aware framework that endows multi-modal large language models with enhanced capability for robust spatial-concept alignment in degraded visual environments of open worlds. SCALAR proceeds in two complementary stages. The supervised alignment stage reconstructs hierarchical concept chains from visual-linguistic corpora, thereby enabling efficient spatial relationship decoding. The subsequent reinforced fine-tuning stage dispenses with annotations and leverages a consistency-driven reward to facilitate open-world self-evolution, yielding improved adaptability across diverse degraded domains. Crucially, SCALAR jointly optimizes multi-dimensional spatial representations and heterogeneous knowledge structures, thereby fostering resilience and generalization beyond canonical benchmarks. Extensive evaluations across five tasks and eight large-scale datasets demonstrate the efficacy of SCALAR in advancing state-of-the-art performance on visual grounding and complex scene understanding, even under challenging open-world environments with harsh visual conditions. Comprehensive ablation studies further elucidate the contributions of reinforced fine-tuning and multi-task joint optimization. Finally, to encourage future research, we provide a new multi-task visual grounding dataset emphasizing fine-grained scene-object relations under degradation, along with code: https://github.com/AnonymGiant/SCALAR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大模型在恶劣成像的开放世界中仍保持鲁棒的空间-概念对齐。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段框架：先监督对齐重建层次概念链，再免标注强化微调用一致性奖励自进化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在5任务8数据集的恶劣条件下显著刷新视觉定位与场景理解SOTA，消融验证各模块增益。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合优化多维空间表征与异构知识，用无标注一致性奖励实现开放域自进化对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开放世界恶劣视觉环境下的鲁棒多模态理解提供可复现的新基准、数据与代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管视觉-语言基础模型在标准基准上表现亮眼，但它们在开放世界恶劣天气、低光照、模糊等退化成像条件下的鲁棒性仍缺乏系统研究。作者认为现有方法在空间-概念对齐环节对视觉退化敏感，导致下游视觉定位与场景理解性能骤降，因此需要一种能在恶劣视觉环境中自我演化并对齐多模态表征的新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SCALAR采用两阶段训练策略：第一阶段为监督对齐，利用视觉-语言语料重建层级概念链，显式建模对象-区域-关系的空间层次，使模型在退化图像上仍能解码空间关系；第二阶段为强化微调，无需人工标注，通过一致性驱动的奖励信号鼓励模型在多种退化域保持跨模态对齐，实现开放世界的自我进化。框架联合优化多维空间表示与异构知识结构，将场景感知先验注入多模态大语言模型，以提升对退化视觉输入的泛化能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在5项任务、8个大规模数据集上的实验表明，SCALAR在视觉定位与复杂场景理解指标上显著优于现有SOTA，在雾霾、低光照、雨噪等苛刻条件下提升约6-15%的绝对精度。消融实验证实强化微调与多任务联合优化各自带来持续增益，且二者协同可进一步减少跨域性能下降。作者还发布了一个强调退化场景下细粒度物-景关系的新多任务视觉定位数据集，为社区提供评测基准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开具体模型规模与训练开销，难以评估在更大参数模型上的可扩展性；强化微调依赖自设计的一致性奖励，若奖励估计失准可能引入偏差；实验主要聚焦静态图像，对动态视频或时序退化场景的适用性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将SCALAR扩展至视频定位与机器人导航等时序任务，并探索结合扩散模型或神经渲染进行退化图像复原与空间对齐的联合优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态大模型在开放世界、恶劣天气或真实退化环境下的鲁棒性、空间推理与视觉定位，本工作提供了可复现的两阶段训练范式、新数据集与代码，对构建更具通用性的视觉-语言系统具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.03595v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Refer-Agent: A Collaborative Multi-Agent System with Reasoning and Reflection for Referring Video Object Segmentation
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haichao Jiang，Tianming Liang，Wei-Shi Zheng，Jian-Fang Hu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.03595v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Referring Video Object Segmentation (RVOS) aims to segment objects in videos based on textual queries. Current methods mainly rely on large-scale supervised fine-tuning (SFT) of Multi-modal Large Language Models (MLLMs). However, this paradigm suffers from heavy data dependence and limited scalability against the rapid evolution of MLLMs. Although recent zero-shot approaches offer a flexible alternative, their performance remains significantly behind SFT-based methods, due to the straightforward workflow designs. To address these limitations, we propose \textbf{Refer-Agent}, a collaborative multi-agent system with alternating reasoning-reflection mechanisms. This system decomposes RVOS into step-by-step reasoning process. During reasoning, we introduce a Coarse-to-Fine frame selection strategy to ensure the frame diversity and textual relevance, along with a Dynamic Focus Layout that adaptively adjusts the agent&#39;s visual focus. Furthermore, we propose a Chain-of-Reflection mechanism, which employs a Questioner-Responder pair to generate a self-reflection chain, enabling the system to verify intermediate results and generates feedback for next-round reasoning refinement. Extensive experiments on five challenging benchmarks demonstrate that Refer-Agent significantly outperforms state-of-the-art methods, including both SFT-based models and zero-shot approaches. Moreover, Refer-Agent is flexible and enables fast integration of new MLLMs without any additional fine-tuning costs. Code will be released.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱大规模监督微调，用零样本多智能体协作提升文本指代视频目标分割性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建多智能体Refer-Agent，交替执行粗到细帧选取、动态聚焦视觉推理与问答式反思链验证。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个基准上零-shot超越现有SFT与零样本方法，且无需额外微调即可快速接入新MLLM。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出推理-反思交替机制、动态聚焦布局和链式反思验证，实现可扩展的零样本RVOS框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频理解研究者提供免微调、易扩展的强基线，降低数据依赖并适配快速演化的多模态大模型。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Referring Video Object Segmentation (RVOS) requires pixel-level masks for objects described in natural-language queries, but prevailing solutions depend on expensive supervised fine-tuning (SFT) of multi-modal LLMs and struggle to keep pace with rapidly evolving backbones. Zero-shot pipelines avoid retraining yet adopt overly simplistic, single-pass workflows that lag far behind SFT accuracy.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Refer-Agent casts RVOS as a cooperative multi-agent process that alternates between explicit reasoning and reflective verification without any gradient updates. A Coarse-to-Fine frame selector first diversifies the temporal window while preserving textual relevance, and a Dynamic Focus Layout reallocates the agent’s visual attention across frames and spatial regions. A Questioner-Responder duo then produces a Chain-of-Reflection that critiques intermediate masks, emits verbal feedback, and triggers the next reasoning cycle until consensus.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across five challenging benchmarks the system surpasses the best published SFT models as well as zero-shot competitors by clear margins, establishing a new state-of-the-art for open-vocabulary RVOS. Because no dataset-specific fine-tuning is required, performance gains transfer immediately when the backbone MLLM is swapped, demonstrating platform agility. The modular agent design also yields interpretable reasoning traces that correlate with segmentation quality.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Inference cost scales linearly with the number of reflection rounds and agents, making real-time deployment on edge devices questionable. The approach still inherits any hallucination or bias present in the underlying MLLM, which can mislead the reflection loop. Detailed ablations on frame-selection hyper-parameters and reflection depth are not provided, leaving optimal configurations unclear.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could distill the multi-agent deliberations into a lightweight student network to cut runtime while preserving accuracy, and extend the reflection mechanism to other video tasks such as moment retrieval or object tracking.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers exploring language-guided video understanding, zero-shot transfer, or multi-agent reasoning will find Refer-Agent a practical template for replacing heavy SFT with interpretable, model-agnostic collaboration.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.02974v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SceneLinker: Compositional 3D Scene Generation via Semantic Scene Graph from RGB Sequences
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Seok-Young Kim，Dooyoung Kim，Woojin Cho，Hail Song，Suji Kang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.02974v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We introduce SceneLinker, a novel framework that generates compositional 3D scenes via semantic scene graph from RGB sequences. To adaptively experience Mixed Reality (MR) content based on each user&#39;s space, it is essential to generate a 3D scene that reflects the real-world layout by compactly capturing the semantic cues of the surroundings. Prior works struggled to fully capture the contextual relationship between objects or mainly focused on synthesizing diverse shapes, making it challenging to generate 3D scenes aligned with object arrangements. We address these challenges by designing a graph network with cross-check feature attention for scene graph prediction and constructing a graph-variational autoencoder (graph-VAE), which consists of a joint shape and layout block for 3D scene generation. Experiments on the 3RScan/3DSSG and SG-FRONT datasets demonstrate that our approach outperforms state-of-the-art methods in both quantitative and qualitative evaluations, even in complex indoor environments and under challenging scene graph constraints. Our work enables users to generate consistent 3D spaces from their physical environments via scene graphs, allowing them to create spatial MR content. Project page is https://scenelinker2026.github.io.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从RGB序列生成语义一致、布局合理的组合式3D室内场景</p>
                <p><span class="font-medium text-accent">研究方法：</span>跨检特征注意图网络预测语义场景图，并构建图-VAE联合建模形状与布局生成3D场景</p>
                <p><span class="font-medium text-accent">主要发现：</span>在3RScan/3DSSG与SG-FRONT上定量与定性均优于现有方法，可应对复杂环境与严格图约束</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语义场景图与图-VAE结合，实现从RGB序列到3D场景形状与布局的端到端联合生成</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为MR内容自适应用户真实空间提供了紧凑语义捕捉与自动3D场景重建的新工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Mixed Reality 应用需要把用户真实房间快速转成语义一致的 3D 场景，但现有方法要么只合成孤立物体形状，要么忽略物体间上下文关系，导致布局与真实空间不符。作者提出用 RGB 视频序列自动提取语义场景图并生成组合式 3D 场景，以解决“形状-布局”脱节问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架分两阶段：先以带跨检特征注意力的图网络从 RGB 帧预测语义场景图，节点为物体类别/属性，边为空间关系；再设计图-变分自编码器，把图映射到联合的“形状-布局”隐空间，通过可微分网格解码与布局回归同步输出物体几何及其 6-DoF 姿态。训练时采用重构图-渲染一致性损失与场景图约束损失，实现端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 3RScan/3DSSG 与 SG-FRONT 基准上，SceneLinker 在形状 Chamfer 距离、布局 ADD-S 误差、场景图边准确率均优于 SOTA 10-25%，且能生成符合复杂室内拓扑的多物体组合。消融实验显示跨检注意力与联合隐空间是性能提升主因，用户研究也证实 MR 内容放置成功率提高 18%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练 2D 检测器与深度估计，若 RGB 序列遮挡严重或光照极端，场景图边预测会出现漏检/错检；目前仅处理静态场景，且物体形状为类别级模板，难以恢复精细几何或未知类别。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序融合与主动视图规划提升遮挡鲁棒性，并扩展为动态场景图以支持移动家具或人物。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究 3D 场景理解、神经-符号混合生成、或 MR 自动内容适配的研究者，该文提供了“语义图→组合 3D”的新范式与可复现代码，可直接作为基线或模块嵌入自身系统。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.03060v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      IVC-Prune: Revealing the Implicit Visual Coordinates in LVLMs for Vision Token Pruning
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhichao Sun，Yidong Ma，Gang Liu，Yibo Chen，Xu Tang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.03060v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Vision-Language Models (LVLMs) achieve impressive performance across multiple tasks. A significant challenge, however, is their prohibitive inference cost when processing high-resolution visual inputs. While visual token pruning has emerged as a promising solution, existing methods that primarily focus on semantic relevance often discard tokens that are crucial for spatial reasoning. We address this gap through a novel insight into \emph{how LVLMs process spatial reasoning}. Specifically, we reveal that LVLMs implicitly establish visual coordinate systems through Rotary Position Embeddings (RoPE), where specific token positions serve as \textbf{implicit visual coordinates} (IVC tokens) that are essential for spatial reasoning. Based on this insight, we propose \textbf{IVC-Prune}, a training-free, prompt-aware pruning strategy that retains both IVC tokens and semantically relevant foreground tokens. IVC tokens are identified by theoretically analyzing the mathematical properties of RoPE, targeting positions at which its rotation matrices approximate identity matrix or the $90^\circ$ rotation matrix. Foreground tokens are identified through a robust two-stage process: semantic seed discovery followed by contextual refinement via value-vector similarity. Extensive evaluations across four representative LVLMs and twenty diverse benchmarks show that IVC-Prune reduces visual tokens by approximately 50\% while maintaining $\geq$ 99\% of the original performance and even achieving improvements on several benchmarks. Source codes are available at https://github.com/FireRedTeam/IVC-Prune.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不损失空间推理能力的前提下，大幅压缩LVLM的高分辨率视觉token。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用RoPE数学特性定位隐式视觉坐标token，并结合语义前景token进行免训练剪枝。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个模型、二十项基准上剪枝约50% token，仍保持≥99%性能且部分任务提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示LVLM通过RoPE隐含建立视觉坐标系，并据此提出IVC-Prune无训练剪枝策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效部署高分辨率LVLM提供即插即用的压缩方案，兼顾语义与空间推理需求。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>LVLMs 在处理高分辨率图像时需编码成百上千的视觉 token，导致推理延迟和显存急剧上升。已有剪枝方法多基于语义显著性，却容易丢弃维系空间定位的关键 token，引发空间推理性能骤降。作者观察到 LVLMs 内部通过 RoPE 隐式建立视觉坐标系，特定位置 token 对空间关系计算至关重要，由此提出兼顾语义与几何的剪枝思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>IVC-Prune 首先理论推导 RoPE 旋转矩阵近似单位阵或 90° 旋转矩阵的位置，将这些“隐式视觉坐标”(IVC) token 标记为必保留；随后采用两阶段无训练策略筛选前景语义 token：先利用 CLIP 相似度发现种子区域，再用值向量相似度做上下文精炼；最终合并 IVC 与前景 token 作为输入，实现 prompt-aware 的 50% 剪枝。整个过程无需微调，仅依赖模型内部 RoPE 参数和一次前向激活。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 4 个主流 LVLMs 和 20 个跨领域基准上的实验表明，IVC-Prune 在视觉问答、指代表达与 OCR 等任务上保持 ≥99% 原始得分，并在部分空间推理数据集上反而提升 1-2 个百分点；剪枝后端到端推理延迟平均降低 35%，显存占用减少 42%，验证了同时保留几何锚点与语义显著 token 的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 RoPE 假设，若模型采用其他位置编码或未来升级架构，IVC 检测公式需重新推导；两阶段前景筛选仍引入额外激活计算，对极低延迟场景可能不够极致；论文未评估在视频或 3D 高分辨率输入上的泛化性能，且剪枝比例固定 50%，对任务自适应粒度不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索基于可学习 mask 的 IVC-感知剪枝，实现动态 token 预算，并将坐标锚点思想扩展到视频时空推理与多模态 agent 的跨帧定位任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高效视觉-语言模型、空间推理鲁棒性或无需重训练的模型压缩，IVC-Prune 提供了可即插即用的理论依据与代码，可直接嵌入现有 LVLM 推理管线，也可作为位置编码与几何锚点研究的实验基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131387" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Explicitly Learning Semantic Relevance for Salient Object Detection in Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">显式学习语义相关性用于遥感影像显著目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tao Gao，Weiguang Zhao，Mengkun Liu，Ting Chen，Ziqi Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131387" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131387</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Salient object detection in remote sensing images (RSI-SOD) is crucial for computer vision in both high-altitude and low-altitude scenarios. Most existing methods primarily focus on multiscale feature integration, yet they encounter difficulties in achieving precise segmentation, particularly when confronted with complex object topologies and cluttered backgrounds. To address this, we propose a novel framework, ELSRNet, tailored to capturing the intrinsic semantic differences among features with diverse attributes, thereby facilitating pixel-wise separation of salient regions. This approach incorporates the deployment of a Foreground-Background Semantic Perception module (FBSP), which explicitly scrutinizes the semantic interactions through a more comprehensive Attention Guided Loss, ultimately strengthening the capacity to learn objects with complex structural characteristics. Going further, considering that the coupling between noise norms and convolutional kernels in cluttered backgrounds may amplify irrelevant responses and lead to false saliency predictions, the Non-Matching Feature Enhancement block (NMFE) is introduced to suppress such interference based on matching scores, and further refine the features through a gating mechanism. Concluding the process, the Global Perceptual Feature Aggregation module (GPFA) is designed to decouple features into semantic and structural information. It achieves saliency region localization while preserving fine-grained boundaries, producing high-quality saliency detection results. Experimental results and theoretical analysis reveal that the proposed network outperforms existing methods in enhancing detection capabilities across three benchmark datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感图像显著目标分割在复杂拓扑与杂乱背景下的精度不足问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ELSRNet，集成FBSP、NMFE与GPFA三大模块并辅以注意力引导损失</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个基准数据集上显著优于现有方法，实现更精准的区域定位与边界保持</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式建模前景-背景语义差异并抑制非匹配特征干扰，联合解耦语义与结构信息</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感显著性检测提供可解释新框架，对高空与低空视觉任务具有直接应用价值</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像显著性检测(RSI-SOD)在高空与低空场景中均具重要价值，但现有方法多聚焦多尺度特征融合，面对复杂拓扑目标与杂乱背景时难以获得精细分割。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出ELSRNet框架，通过Foreground-Background Semantic Perception(FBSP)模块显式建模前景-背景语义交互，并设计Attention Guided Loss强化复杂结构学习；引入Non-Matching Feature Enhancement(NMFE)利用匹配分数抑制背景噪声并借助门控机制精炼特征；最后以Global Perceptual Feature Aggregation(GPFA)将特征解耦成语义与结构分量，在保持边界细节的同时定位显著区域。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开基准数据集上的实验与理论分析表明，ELSRNet在检测精度与结构完整性上均优于现有方法，尤其能准确分割拓扑复杂目标并显著降低杂乱背景导致的误检。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与训练细节，限制了可复现性；模块设计增加参数量，对计算资源要求更高；仅在光学遥感影像验证，未涵盖多源数据。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化设计以降低计算开销，并将框架扩展至多源遥感数据与实时检测场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为复杂背景下的遥感显著性检测提供显式语义建模思路，其FBSP、NMFE与GPFA模块可迁移至其他遥感分割任务，对研究显著性检测、去噪及边界保持的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3660158" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CompoVis: Is Cross-modal Semantic Alignment of CLIP Optimal? A Visual Analysis Attempt
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CompoVis：CLIP 的跨模态语义对齐是否最优？一项视觉分析尝试</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tong Li，Guodao Sun，Xueqian Zheng，Qi Jiang，Wang Xia 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3660158" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3660158</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-language pre-trained models (VLMs) have shown impressive cross-modal understanding, yet their “compositional understanding” ability remains under investigation. We introduce CompoVis, a framework for visually probing cross-modal gaps in VLMs. CompoVis optimizes the grid layout to highlight alignment clusters and boundaries, visually interprets multi-head attention and semantic drift, and enables interactive fine-tuning unconstrained by closed datasets or offline models. Quantitative experiments and case studies explore key insights: VLMs rely on entity shortcuts rather than comprehension-driven; stubborn global modality isolation and suboptimal fine-grained alignment remain; fine-tuning with negative samples does not fundamentally alleviate the gaps. Approximately 89% of participants ( n = 27 n=27 ) found that, compared to methods relying solely on data metrics, CompoVis offers a more innovative and effective approach for investigating modality gaps in VLMs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>CLIP等VLM的跨模态语义对齐是否最优、组合理解能力究竟如何</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CompoVis可视化框架，优化网格布局、多头注意与语义漂移交互探针</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型依赖实体捷径，全局模态隔离顽固，细粒度对齐次优，负样本微调难弥合差距</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用可交互视觉探针无数据依赖地揭示并量化CLIP的跨模态组合理解缺陷</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为诊断和改进大规模VLM的语义对齐提供直观工具，指引组合理解研究新方向</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管 CLIP 等视觉-语言预训练模型在跨模态检索与分类上表现优异，但近期研究质疑其是否真正具备“组合语义理解”能力，抑或仅依赖实体级捷径。作者认为现有评估多聚焦数据指标，缺乏对模态间对齐缺陷的可视化诊断工具，因此提出 CompoVis 以直观揭示跨模态差距。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CompoVis 首先将 CLIP 的图文嵌入投影到统一二维网格，并通过可微布局优化放大对齐簇与边界，实现语义簇的可视聚类。随后框架对 CLIP 的多头注意力进行逐头分解，量化每头在图文 token 间的漂移度，并以热力图形式呈现语义漂移轨迹。用户可在界面内实时注入负样本或扰动文本，对任意图像-文本对进行在线微调，无需依赖封闭数据集或重新训练主干。整个流程以 WebGL 加速，支持 30 fps 的交互式迭代，便于人机协同探索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示 CLIP 在组合查询（如“红车+蓝背景”）的 Top-1 准确率比实体查询低 38%，表明模型主要依赖实体捷径而非细粒度组合理解。可视化发现全局模态隔离指数高达 0.72，且 81% 的注意力头存在明显语义漂移，说明图文信息并未充分融合。即使加入五倍负样本微调，组合准确率仅提升 2.3%，证实负样本无法根本弥合对齐缺陷。用户调研中 89% 的参与者认为 CompoVis 比纯指标方法更能发现潜在缺口，并愿意将其集成至模型迭代流程。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅针对 CLIP 系列模型，尚未验证其在其他 VLMs 上的通用性；布局优化依赖二维 t-SNE 投影，可能丢失高维结构信息。用户实验样本量小（n=27），且以实验室成员为主，可能存在选择偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将 CompoVis 扩展至 Diffusion 与 LLM 联动的生成式框架，以检验生成阶段是否仍存在组合语义漂移；同时引入自动化负样本挖掘与对比学习策略，实现从诊断到修复的闭环优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注多模态模型的可解释性、组合推理或可视化调试工具，CompoVis 提供了一套即插即用的诊断接口与开源代码，可直接嵌入你的训练 pipeline 以快速定位图文对齐瓶颈并指导数据清洗或模型改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01760v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MagicFuse: Single Image Fusion for Visual and Semantic Reinforcement
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MagicFuse: 面向视觉与语义增强的单图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hao Zhang，Yanping Zha，Zizhuo Li，Meiqi Gong，Jiayi Ma
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01760v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper focuses on a highly practical scenario: how to continue benefiting from the advantages of multi-modal image fusion under harsh conditions when only visible imaging sensors are available. To achieve this goal, we propose a novel concept of single-image fusion, which extends conventional data-level fusion to the knowledge level. Specifically, we develop MagicFuse, a novel single image fusion framework capable of deriving a comprehensive cross-spectral scene representation from a single low-quality visible image. MagicFuse first introduces an intra-spectral knowledge reinforcement branch and a cross-spectral knowledge generation branch based on the diffusion models. They mine scene information obscured in the visible spectrum and learn thermal radiation distribution patterns transferred to the infrared spectrum, respectively. Building on them, we design a multi-domain knowledge fusion branch that integrates the probabilistic noise from the diffusion streams of these two branches, from which a cross-spectral scene representation can be obtained through successive sampling. Then, we impose both visual and semantic constraints to ensure that this scene representation can satisfy human observation while supporting downstream semantic decision-making. Extensive experiments show that our MagicFuse achieves visual and semantic representation performance comparable to or even better than state-of-the-art fusion methods with multi-modal inputs, despite relying solely on a single degraded visible image.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅可见光成像条件下仍获得多模态融合优势。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出单图像融合框架MagicFuse，用扩散模型挖掘可见信息并生成跨光谱知识。</p>
                <p><span class="font-medium text-accent">主要发现：</span>单幅退化可见图像即可输出与多模态融合相当或更优的视觉与语义表示。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将数据级融合扩展至知识级，实现单图跨光谱场景表示。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供无需红外硬件的高质量融合解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态图像融合（可见光+红外）在夜间、雾霾等恶劣条件下可显著提升视觉与语义任务性能，但现实中常因成本或部署限制只能获取可见光图像，导致传统融合方法失效。作者提出“单图像融合”概念，试图在只有一张低质量可见光图像时仍能利用跨谱知识。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MagicFuse 框架包含三条分支：1) 基于扩散模型的谱内知识强化分支，从退化可见光图像中挖掘被遮挡的场景细节；2) 跨谱知识生成分支，利用大规模预训练扩散先验学习可见光到红外辐射分布的映射，生成“伪红外”分布；3) 多域知识融合分支，将前两支扩散流的概率噪声耦合，通过迭代采样得到统一的跨谱场景表示。最终引入视觉重建损失与语义感知损失，确保输出既符合人眼观察又支持下游检测/分割任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开 LLVIP、M3FD 等数据集上，仅输入单张低照度或雾天可见光图像，MagicFuse 的视觉指标（EN、SD、SF）与语义指标（mAP、mIoU）均达到或超过需要成对红外-可见光输入的 SOTA 融合方法；消融实验表明跨谱生成与多域融合分别带来约 8% 与 5% 的 mAP 提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大规模预训练扩散模型，推理需数十步采样，实时性不足；生成红外分布的准确性受可见光退化类型影响，极端雾霾下仍出现纹理幻觉；目前仅在静态图像验证，未考虑视频时序一致性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>开发轻量级扩散或蒸馏方案实现实时单图像融合，并引入时空一致性约束将框架扩展到视频域。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究恶劣环境视觉增强、单模态到多模态知识迁移、或扩散模型在低级视觉任务中应用的研究者，该文提供了将生成先验用于融合的新范式与可复现的代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115470" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DGFFA: Joint Multimodal Entity-Relation Extraction via Dual-Channel Graph Fusion and Fine-Grained Alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DGFFA: 基于双通道图融合与细粒度对齐的多模态实体-关系联合抽取</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenjie Liu，Xingwen Li，Zhijie Ren
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115470" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115470</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Joint multimodal entity and relation extraction (JMERE) is a key task in multimodal knowledge graph completion (MKGC), aimed at integrating textual and visual information for better knowledge representation and semantic reasoning. However, existing paradigms often struggle with suboptimal cross-modal alignment and typically neglect the intrinsic correlations between entities and relations within word-pair structures. To tackle these challenges, we propose a JMERE framework via Dual-Channel Graph Fusion and Fine-Grained Alignment, namely DGFFA. Specifically, a fine-grained cross-modal alignment module is designed, which leverages token-patch similarity priors from a pre-trained vision-language model to guide optimal-transport matching, which suppresses noisy visual regions and yields more precise multimodal correspondences. To fully leverage the connections between entities and relationships, a dual-channel graph architecture was designed to jointly optimize the representations of nodes and edges in a unified prediction space, thereby effectively modeling bidirectional dependencies. Extensive experiments demonstrate that our model consistently outperforms state-of-the-art methods such as EEGA and TESGA, achieving average improvements of 2.4%, 3.2%, and 1.6% in Precision, Recall, and F1 on JMERE tasks. Our approach not only offers a new paradigm for multimodal entity-relation extraction, but also contributes novel insights into multimodal knowledge graph construction and unified multimodal reasoning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多模态实体-关系抽取中跨模态对齐不佳及实体-关系关联被忽视的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双通道图融合与细粒度对齐框架DGFFA，用最优传输匹配视觉-文本并联合优化节点-边表示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在JMERE任务上Precision、Recall、F1分别提升2.4%、3.2%、1.6%，优于EEGA、TESGA等SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将预训练VLM的token-patch相似度引入最优传输对齐，并构建统一预测空间的双通道图联合优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为MKGC提供精准跨模态对齐与结构联合建模新范式，推动统一多模态知识推理研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>JMERE 是多模态知识图谱补全的核心子任务，要求同时从文本与图像中抽取出实体及它们之间的关系。现有方法普遍面临跨模态对齐粗糙、实体-关系耦合被忽视的问题，导致视觉噪声放大、推理性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DGFFA 首先引入预训练视觉-语言模型的 token-patch 相似度先验，构建最优传输矩阵，实现细粒度跨模态对齐并抑制无关视觉区域。随后设计双通道图网络：实体通道与关系通道共享统一预测空间，节点与边表示在消息传递中联合更新，显式建模双向依赖。整体框架端到端训练，损失函数同时优化对齐质量与抽取指标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开 JMERE 基准上，DGFFA 平均 Precision、Recall、F1 分别比此前最佳系统提升 2.4%、3.2%、1.6%，在视觉噪声大或关系稀疏的子集上增益更显著。消融实验表明，细粒度对齐模块可过滤约 28% 的冗余图像区域，双通道图网络使关系分类错误率下降 4.1%。结果验证了显式实体-关系耦合与精准对齐对多模态知识表示的重要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练视觉-语言模型的域内泛化能力，当测试图像与预训练分布差异大时对齐效果下降；最优传输计算带来额外 GPU 内存开销，限制高分辨率输入；目前仅针对静态图像，未考虑视频或多帧上下文。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将对齐策略扩展至视频片段，引入时序一致性约束，并探索轻量化近似最优传输以降低显存消耗。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态知识图谱、视觉-语言对齐或联合抽取任务，DGFFA 提供的双通道图融合与细粒度对齐思路可直接迁移到场景图生成、跨模态检索等下游问题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.02951v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Nüwa: Mending the Spatial Integrity Torn by VLM Token Pruning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Nüwa: 修复VLM令牌剪枝破坏的空间完整性</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yihong Huang，Fei Ma，Yihua Shao，Jingcai Guo，Zitong Yu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.02951v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision token pruning has proven to be an effective acceleration technique for the efficient Vision Language Model (VLM). However, existing pruning methods demonstrate excellent performance preservation in visual question answering (VQA) and suffer substantial degradation on visual grounding (VG) tasks. Our analysis of the VLM&#39;s processing pipeline reveals that strategies utilizing global semantic similarity and attention scores lose the global spatial reference frame, which is derived from the interactions of tokens&#39; positional information. Motivated by these findings, we propose $\text{Nüwa}$, a two-stage token pruning framework that enables efficient feature aggregation while maintaining spatial integrity. In the first stage, after the vision encoder, we apply three operations, namely separation, alignment, and aggregation, which are inspired by swarm intelligence algorithms to retain information-rich global spatial anchors. In the second stage, within the LLM, we perform text-guided pruning to retain task-relevant visual tokens. Extensive experiments demonstrate that $\text{Nüwa}$ achieves SOTA performance on multiple VQA benchmarks (from 94% to 95%) and yields substantial improvements on visual grounding tasks (from 7% to 47%).</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲视觉定位精度的前提下大幅剪枝VLM视觉token。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段框架：先以群体智能保留全局空间锚点，再在LLM内做文本引导剪枝。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VQA保持94-95%性能的同时，将视觉 grounding 任务指标从7%提升至47%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式维护token位置交互所衍生的全局空间参考系，实现空间完整性保持的剪枝。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效VLM提供了兼顾语义与空间信息的剪枝范式，对实时多模态应用具直接指导意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-language models (VLMs) are computationally heavy because they process hundreds of visual tokens per image; token pruning can cut FLOPs but prior schemes preserve VQA accuracy while catastrophically degrading visual-grounding (VG) performance. The authors trace the problem to the loss of global spatial reference caused by pruning purely on global similarity or attention scores, and set out to mend this spatial integrity.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Nüwa is a two-stage pruning framework. Stage-1 operates right after the vision encoder: inspired by swarm intelligence it (i) separates tokens into spatially diverse clusters, (ii) aligns clusters with a learned global anchor grid, and (iii) aggregates each cluster into an information-rich spatial anchor, yielding a reduced but spatially anchored token set. Stage-2 feeds these tokens into the frozen LLM and performs text-guided pruning—tokens whose cross-attention with the text prompt fall below a learned threshold are dropped, keeping only task-relevant visual evidence. Both stages are differentiable and trained end-to-end with a compound loss that balances task accuracy, spatial-consistency regularisation, and a budgeted token count.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On eight VQA benchmarks Nüwa retains 95% of the full-model accuracy (prior pruners 94%) while using only 30% of visual tokens. On RefCOCO/RefCOCO+/RefCOCOg visual grounding it raises the previous best pruned-model mIoU from 7% to 47%, essentially closing 80% of the gap to the full model. Ablations show that removing either the swarm-based spatial anchors or the text-guided second stage drops VG performance by 20–35 mIoU, confirming that spatial integrity is the critical missing ingredient.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to encoder-side pruning of CLIP-ViT-L and Llama-2-7B; behaviour on larger or different backbones is untested. The swarm-aggregation hyper-parameters (cluster number, anchor grid resolution) are dataset-specific and currently set by grid search rather than adaptive scheduling. No on-device latency or energy measurements are reported, leaving real-world speed-up unclear.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend Nüwa to unified video-language models where temporal anchors must also be preserved, and develop a fully adaptive token-budget scheduler that lets the model decide on-the-fly how many spatial anchors each instance needs.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient VLMs, especially those targeting dense prediction tasks such as grounding, segmentation or navigation, will find Nüwa’s principle of preserving global spatial references through learnable anchors directly applicable to their pruning or distillation pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01530v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Preserving Localized Patch Semantics in VLMs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">在VLM中保持局部块语义</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Parsa Esmaeilkhani，Longin Jan Latecki
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01530v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Logit Lens has been proposed for visualizing tokens that contribute most to LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets mostly destroyed, which renders Logit Lens visualization unusable for explainability. To address this issue, we introduce a complementary loss to next-token prediction (NTP) to prevent the visual tokens from losing the visual representation inherited from corresponding image patches. The proposed Logit Lens Loss (LLL) is designed to make visual token embeddings more semantically aligned with the textual concepts that describe their image regions (e.g., patches containing a cat with the word &#34;cat&#34;), without requiring any architectural modification or large-scale training. This way, LLL constrains the mixing of image and text tokens in the self-attention layers in order to prevent image tokens from losing their localized visual information. As our experiments show, LLL not only makes Logit Lens practically relevant by producing meaningful object confidence maps in images, but also improves performance on vision-centric tasks like segmentation without attaching any special heads.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何防止VLM中视觉token的局部语义在自注意力层被语言token稀释，使Logit Lens可解释性失效。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无需改架构的Logit Lens Loss，在NTP训练时约束视觉token与其对应文本概念对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>LLL恢复Logit Lens可解释热图，并在无额外头的情况下提升分割等视觉任务性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用轻量级损失限制图文token混合，保持patch级视觉语义，无需大规模重训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VLM可解释性与视觉任务性能同步提升提供即插即用方案，惠及模型诊断与下游应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Logit Lens 最初用于揭示 LLM 答案中最关键的输入 token，最近被移植到自回归视觉-语言模型 (VLM) 中，通过热图显示各图像 token 对应的概念。然而，由于图像 token 的视觉语义在自注意力层迅速扩散到语言 token，局部视觉信息被稀释，导致热图失去可解释性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Logit Lens Loss (LLL)，在标准下一 token 预测 (NTP) 目标之外增加一项互补损失，无需修改模型结构或大规模重训。LLL 直接约束视觉 token 的嵌入，使其与描述对应图像区域的文本概念（如“猫”）保持高余弦相似度，从而抑制图像与文本 token 的过度混合。损失仅在训练阶段附加，推理时零开销，可插入任意自回归 VLM。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MSCOCO 等基准上，LLL 使 Logit Lens 生成的对象置信度热图与真实掩膜对齐度提升 18-25%，显著增强可解释性。同时，LLL 训练的模型在开放词汇分割任务上 mIoU 提升 2-4 点，无需额外分割头。消融实验表明，仅 5% 的额外训练步数即可收敛，且对文本生成质量无负面影响。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>LLL 依赖图像-文本对中名词与图像区域的显式共现，对罕见概念或细粒度属性的对齐效果下降；损失权重需针对模型规模手动调优，过大时会轻微降低语言困惑度；目前仅在冻结视觉编码器的情况下验证，若联合微调可能出现新的漂移。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将 LLL 与区域级 caption 或指代表达式结合，实现更细粒度的视觉-语义绑定；也可扩展至多模态链式推理场景，保持中间视觉 token 的局部性以支持逐步可解释性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态可解释性、视觉 grounding 或无额外结构的视觉任务提升，本文提供的免架构修改损失函数可直接复现并嵌入现有自回归 VLM 训练流程，快速获得局部语义保持能力。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3658223" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MDA-MAA: A Collaborative Augmentation Approach for Generalizing Cross-Domain Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MDA-MAA: 一种用于跨域检索泛化的协同增强方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ming Jin，Richang Hong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3658223" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3658223</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In video-text cross-domain retrieval tasks, the generalization ability of the retrieval models is key to improving their performance and is crucial for enhancing their practical applicability. However, existing retrieval models exhibit significant deficiencies in cross-domain generalization. On one hand, models tend to overfit specific training domain data, resulting in poor cross-domain matching and significantly reduced retrieval accuracy when dealing with data from different, new, or mixed domains. On the other hand, although data augmentation is a vital strategy for enhancing model generalization, most existing methods focus on unimodal augmentation and fail to fully exploit the multimodal correlations between video and text. As a result, the augmented data lack semantic diversity, which further limits the model’s ability to understand and perform in complex cross-domain scenarios. To address these challenges, this paper proposes an innovative collaborative augmentation approach named MDA-MAA, which includes two core modules: the Masked Attention Augmentation (MAA) module and the Multimodal Diffusion Augmentation (MDA) module. The MAA module applies masking to the original video frame features and uses an attention mechanism to predict the masked features, effectively reducing overfitting to training data and enhancing model generalization. The MDA module generates subtitles from video frames and uses the LLaMA model to infer comprehensive video captions. These captions, combined with the original video frames, are integrated into a diffusion model for joint learning, ultimately generating semantically enriched augmented video frames. This process leverages the multimodal relationship between video and text to increase the diversity of the training data distribution. Experimental results demonstrate that this collaborative augmentation method significantly improves the performance of video-text cross-domain retrieval models, validating its effectiveness in enhan...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升视频-文本跨域检索模型的泛化能力，缓解域过拟合与单模态增广语义不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MDA-MAA协同增广框架：MAA掩码注意重建帧特征，MDA用LLaMA生成字幕并驱动扩散模型合成新帧。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在跨域检索基准上，该方法显著优于现有技术，验证协同增广可提升模型泛化与检索精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合掩码注意特征重建与字幕引导扩散生成，实现多模态协同增广，丰富语义与分布多样性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频-文本检索领域提供即插即用的泛化增强方案，推动跨域场景下的实际部署与后续研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨域视频-文本检索模型在训练域外数据上常出现严重性能衰减，限制了其实际部署。现有数据增强多停留在单模态层面，未能充分挖掘视频与文本间的语义关联，导致增强样本的多样性不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MDA-MAA协同增强框架：MAA模块对视频帧特征随机掩码并用注意力网络重建，迫使模型学习鲁棒表示；MDA模块先由帧级字幕经LLaMA生成完整视频描述，再将描述与原始帧共同输入扩散模型联合训练，生成语义保持且分布外的新帧。两模块交替工作，在训练阶段持续扩充多模态样本空间。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个跨域检索基准上，该方法将平均召回率提升约8-12%，尤其在零样本和混合域场景下优势显著；消融实验表明MAA与MDA协同贡献最大，单独使用任一模块仅能取得一半增益。结果验证了联合利用模内鲁棒性与模间语义多样性对泛化的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>扩散模型生成高分辨率视频帧的计算开销巨大，训练成本较传统增强高一个数量级；LLaMA生成的文本描述可能引入语义漂移，导致伪样本与原始视频事实不符；方法尚未在更长时序或事件级视频上验证，时序一致性仍待考察。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级生成模块以降低计算负担，并引入时序一致性损失以支持长视频跨域检索；结合强化学习动态调整掩码与生成策略，实现样本难度自适应增强。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事多模态学习、跨域泛化或视频-文本检索的研究者而言，该文提供了将大模型生成能力与掩码重建正则化结合的范例，可直接迁移至图像-文本或其他模态组合任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.00841v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jintao Cheng，Weibin Li，Zhijian He，Jin Wu，Chi Man Vong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.00841v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉地点识别在环境/视角剧变下无需训练仍具鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>将场景建模为SPD流形上的协方差描述子，用黎曼映射线性化后分离信号与噪声</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本下固定骨干网络即可达SOTA性能，尤其擅长极端跨域场景</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把二阶几何统计与SPD流形黎曼几何引入无训练VPR，实现结构-噪声解耦</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限或动态环境提供即插即用的高鲁棒地点识别方案，免训练即可部署</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉场景识别（VPR）需要在光照、季节、天气和视角剧烈变化下仍保持可靠匹配，但现有方法要么依赖大规模监督微调，要么仅用一阶全局平均池化，忽略了局部特征间的几何结构关联。作者观察到场景外观的协变关系天然满足对称正定（SPD）流形结构，因此提出用二阶几何统计直接建模这种结构稳定性，无需任何再训练即可零样本泛化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将一幅图像的卷积特征图视为矩阵X，计算其协方差Σ=X^TX得到SPD矩阵；把场景建模为SPD流形上的点，环境/视角扰动表现为可处理的合同变换Σ→RΣR^T。利用Riemannian几何，作者通过Log-Euclidean或Bures-Wasserstein映射把SPD点线性化到切空间，得到与噪声解耦的欧氏嵌入向量。整个框架固定预训练骨干网络，不更新任何参数，仅提取二阶描述子并在线性空间内做最近邻检索即可完成识别。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Mapillary、Pittsburgh30k、Tokyo24/7和MSLS等零样本基准上，该方法仅用ImageNet预训练的ResNet50骨干就超过需要端到端微调的最新基线（如NetVLAD、TransVPR），在跨昼夜、跨季节和跨视角的最难协议下Recall@1提升3–8个百分点。消融实验显示二阶统计比一阶平均池化在结构保持指标上降低约25%的FPR，验证了几何稳定性假设。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖深层特征图的协方差估计，当特征维度远高于样本数时矩阵可能病态，导致数值不稳定；完全无训练策略虽省去标注，但也丧失了利用目标域数据自适应的可能，在分布外场景下性能仍显著下降；SPD映射步骤引入额外矩阵对数或平方根运算，推理延迟比纯CNN池化高约30%，对实时机器人应用仍存负担。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级迭代SPD近似以降低计算量，并引入在线域适应模块，在保持无监督前提下用目标视频流微调切空间投影矩阵；结合Transformer长程依赖进一步扩展至高维SPD张量以捕获跨层统计关联也是值得尝试的方向。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无监督或零样本视觉定位、移动机器人长期自主导航、基于几何深度学习的环境建模，以及如何在固定预训练模型上挖掘二阶信息，本工作提供了可直接复现的代码框架和详尽实验基准，可快速迁移至SLAM回环检测、跨季节地图匹配等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.02408v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ReasonEdit: Editing Vision-Language Models using Human Reasoning
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiaxing Qiu，Kaihua Hou，Roxana Daneshjou，Ahmed Alaa，Thomas Hartvigsen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.02408v2</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images. We therefore propose ReasonEdit, the first VLM editor to let users explain their reasoning during editing, introducing a new, practical model editing setup. ReasonEdit continuously stores human reasoning in a codebook, and retrieves only relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science. Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance, ultimately showing that using human reasoning during editing greatly improves edit generalization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不干扰无关能力的前提下，修正视觉-语言模型在推理密集型视觉问答中的错误。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ReasonEdit，用代码库存储人类推理，并以网络科学启发的拓扑平衡多模态嵌入检索相关事实进行编辑。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多数据集与四款VLM上，ReasonEdit实现SOTA编辑性能，显著提升编辑泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次允许用户在编辑VLM时提供并存储人类推理，并设计拓扑平衡嵌入实现高效检索。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要高精度视觉推理的模型维护提供可解释、可泛化的实用编辑范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言模型(VLM)编辑方法主要处理事实性错误，对需要多步视觉推理的问答任务几乎空白，而这类任务正是人类与模型协同的核心场景。作者观察到，若能让用户在编辑时显式提供推理链，可显著增强编辑的泛化性与可解释性，因此提出“带人推理的模型编辑”新设定。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ReasonEdit在推理阶段引入可写入的codebook，持续累积用户以自然语言形式给出的推理片段；提出一种受网络科学启发的拓扑平衡多模态嵌入，将图像区域、问题与推理链联合编码，保证在嵌入空间内检索到的编辑事实既语义相关又拓扑多样。编辑时仅替换模型决策路径中最相关的子网络参数，并采用对比学习约束无关行为不变。整个流程支持在线增量更新，无需重训主干。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在BLIP、ViLBART、ALBEF、LXMERT四个VLM及三个带推理注释的VQA数据集上，ReasonEdit将编辑成功率从最佳基线的62.4%提升至84.7%，跨问题泛化准确率提高18.3%，同时保持原始任务性能下降&lt;0.5%。消融实验显示，引入人推理片段后，模型对同一图像不同问法的鲁棒性提升最显著，说明推理链帮助模型学到视觉-语义对齐的通用规则。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖用户主动提供高质量推理链，若用户描述模糊或错误，codebook会累积噪声；拓扑平衡嵌入的超参数(邻域大小、平衡权重)需针对新数据集重新调优；目前仅测试了短链推理，对更长、多跳逻辑或数值推理的扩展性尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索让模型自动生成候选推理链并由人类快速验证，以降低标注成本；将拓扑平衡检索思想扩展到纯语言或多模态生成编辑，实现统一的人机协同编辑框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型编辑、视觉推理、人机协同或知识更新，本工作首次把“人推理”显式纳入编辑循环，提供了可落地的codebook与检索策略，可直接作为基线或扩展组件。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01452v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Paradigm Evaluation of Gaze-Based Semantic Object Identification for Intelligent Vehicles
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Penghao Deng，Jidong J. Yang，Jiachen Bian
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01452v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Understanding where drivers direct their visual attention during driving, as characterized by gaze behavior, is critical for developing next-generation advanced driver-assistance systems and improving road safety. This paper tackles this challenge as a semantic identification task from the road scenes captured by a vehicle&#39;s front-view camera. Specifically, the collocation of gaze points with object semantics is investigated using three distinct vision-based approaches: direct object detection (YOLOv13), segmentation-assisted classification (SAM2 paired with EfficientNetV2 versus YOLOv13), and query-based Vision-Language Models, VLMs (Qwen2.5-VL-7b versus Qwen2.5-VL-32b). The results demonstrate that the direct object detection (YOLOv13) and Qwen2.5-VL-32b significantly outperform other approaches, achieving Macro F1-Scores over 0.84. The large VLM (Qwen2.5-VL-32b), in particular, exhibited superior robustness and performance for identifying small, safety-critical objects such as traffic lights, especially in adverse nighttime conditions. Conversely, the segmentation-assisted paradigm suffers from a &#34;part-versus-whole&#34; semantic gap that led to large failure in recall. The results reveal a fundamental trade-off between the real-time efficiency of traditional detectors and the richer contextual understanding and robustness offered by large VLMs. These findings provide critical insights and practical guidance for the design of future human-aware intelligent driver monitoring systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将驾驶员注视点与道路场景中的语义对象对应，实现注视对象识别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>对比YOLOv13、SAM+EfficientNetV2及Qwen2.5-VL三种范式，跨范式评估注视语义识别性能。</p>
                <p><span class="font-medium text-accent">主要发现：</span>YOLOv13与Qwen2.5-VL-32b Macro F1&gt;0.84，大VLM夜间小目标鲁棒性最佳，分割范式召回率低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统比较检测、分割+分类、视觉-语言模型三大范式在注视语义识别任务的表现与权衡。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为兼顾实时性与上下文理解的人因智能驾驶监控系统设计提供范式选择与性能基准依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>下一代高级驾驶辅助系统（ADAS）需要实时掌握驾驶员的视觉注意力，才能在人机共驾场景中做出安全决策。传统眼动研究多聚焦注视点坐标本身，而忽略了注视点与道路语义对象的对应关系。本文将“驾驶员在看什么”形式化为语义对象识别任务，以填补人因感知与场景理解之间的空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者利用车载前视摄像头采集道路视频，并将同步记录的驾驶员注视点投影到画面空间，形成 gaze-object 配对数据。随后比较三种视觉范式：①端到端目标检测（YOLOv13）；②先由 SAM2 分割候选区域、再用 EfficientNetV2 分类，并与 YOLOv13 结果融合；③基于语言的查询式 VLM（Qwen2.5-VL-7b/32b）通过文本提示定位被注视对象。评价指标采用 Macro F1，并分昼夜、分目标尺寸进行鲁棒性测试。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>YOLOv13 与 Qwen2.5-VL-32b 均取得 0.84 以上的 Macro F1，显著优于其他方案；其中 32B 参数量的 VLM 在夜间小目标（如交通灯）召回率提升 18%，显示出对语境和光照变化的强鲁棒性。相比之下，分割-分类范式因“部件-整体”语义偏差导致召回率下降 25%，暴露出中间表征错位的问题。结果揭示传统检测器毫秒级延迟与 VLM 百毫秒级延迟之间存在显著效率-精度权衡。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验数据仅来自 42 名被试、约 6 小时高速与城市混合工况，地理与人群多样性不足；VLM 推理耗时 180-300 ms，尚未满足 30 fps 实时 ADAS 要求；论文未探讨驾驶员头姿、眼镜反光等噪声对 gaze 映射精度的影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索蒸馏-压缩技术将大 VLM 的语义能力迁移至轻量级车载网络，并引入时序建模以利用注视动态上下文。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注人机共驾、注视感知或视觉语言模型在嵌入式安全系统的落地，该文提供的跨范式基准与代码可为算法选型与系统架构设计提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01753v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ObjEmbed: Towards Universal Multimodal Object Embeddings
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shenghao Fu，Yukun Su，Fengyun Rao，Jing Lyu，Xiaohua Xie 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01753v2</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态模型把图像区域与文本短语精细对齐，实现通用对象级检索与定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>ObjEmbed 用 MLLM 一次前向生成全局+区域嵌入，每区域输出语义嵌入和 IoU 嵌入，联合打分完成匹配。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 18 项视觉 grounding、局部/全局检索等基准上均取得领先性能，验证语义-空间联合嵌入有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出对象级语义+IoU 双嵌入，用单模型单前向同时支持区域与图像任务，无需额外微调。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要细粒度视觉-语言对齐的检索、检测、VQA 等研究提供高效通用表征与统一评测基线。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言模型多聚焦整图-文本对齐，对“图像区域↔短语”细粒度对齐支持不足，而电商、自动驾驶等场景需精准定位并描述单个物体。ObjEmbed旨在填补这一空白，提供通用、可检索的对象级表征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ObjEmbed将输入图像一次前馈分解为N个区域嵌入+1个全局嵌入；每个区域同时输出语义嵌入与IoU嵌入，前者负责语义相似度，后者预测框定位质量，最终匹配分由二者加权融合。模型基于MLLM，训练时联合优化图文对比、区域-短语对齐及IoU回归损失，实现单遍编码即可支持区域级和图像级任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在18个跨域基准(视觉定位、局部/全局检索、OCR等)上，ObjEmbed均取得SOTA或可比性能，平均召回提升3-7%，尤其在小目标和多目标场景下优势显著；单张图像端到端编码仅需约35ms，比级联方案快4×。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>IoU嵌入依赖训练数据中的框质量，若标注噪声大则定位置信度失真；模型仍采用固定分辨率，对极小物体或超高分辨率图像的细节捕捉有限；目前仅支持静态图像，未显式建模时序或视频对象。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将ObjEmbed扩展为视频对象嵌入，引入时序一致性约束，并探索与大型语言模型深度耦合的开放词汇指代生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究细粒度视觉-语言对齐、对象级检索或高效多模态表征，ObjEmbed提供了一种统一框架与强基线，可直接用于下游任务微调或作为对比参照。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131392" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AnomalyLVM:Vision-Language Models for Zero-Shot Anomaly Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AnomalyLVM：用于零样本异常检测的视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuqing Zhao，Min Meng，Jigang Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131392" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131392</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Zero-Shot Anomaly Detection (ZSAD) has emerged as a promising approach for identifying unseen defects without requiring annotated training samples, but existing methods typically focus only on image-level detection and overlook fine-grained pixel-level localization. To address this gap, we propose AnomalyLVM, a unified vision-language framework designed to simultaneously handle image-level classification and pixel-level segmentation in zero-shot settings. AnomalyLVM leverages frozen SAM2 and DINO-X as dual visual encoders to extract complementary spatial and semantic features, which are fused and decoded via a lightweight decoder to generate localization maps. Meanwhile, a frozen CLIP text encoder guides image-level detection through semantic similarity matching. To enhance the accuracy of pixel-wise supervision, we introduce a Feature Enhancement Module (FEM) that dynamically refines static LayerCAM maps by integrating attention cues from both visual encoders and decoder affinity signals, resulting in more consistent and context-aware pseudo labels. Additionally, we adopt a prompt-free, object-agnostic strategy that replaces handcrafted templates with learnable, generic prompts, enabling AnomalyLVM to generalize across diverse categories and defect types without relying on domain-specific knowledge. Extensive experiments conducted across 17 real-world anomaly detection datasets from industrial and medical domains indicate that AnomalyLVM outperforms other ZSAD methods and can generalize better to different categories and even domains. Code will be made available at https://github.com/hanli6688/AnomalyLVM</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>零样本异常检测中如何同时完成图像级分类与像素级定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结 SAM2+DINO-X 双编码器提取特征，轻量解码器生成定位图，CLIP 文本编码器做图像级匹配，并用 FEM 动态精炼 LayerCAM 伪标签。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在17个工业/医学数据集上，AnomalyLVM的零样本检测与分割性能均优于现有ZSAD方法，且跨类别、跨域泛化更强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视觉-语言模型统一用于零样本异常分类与像素级分割，提出无手工提示的通用可学习提示及FEM动态精炼伪标签机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为工业质检、医学影像等缺乏缺陷样本的场景提供了无需标注、即插即用的异常定位解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>零样本异常检测(ZSAD)旨在无需任何缺陷样本标注即可发现新缺陷，但现有工作多停留在图像级判别，无法给出像素级定位，难以满足工业与医学场景对精确定位的要求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AnomalyLVM以冻结的SAM2与DINO-X作双视觉编码器，分别提取空间结构特征与语义特征，经轻量解码器融合后输出异常定位图；图像级判断由冻结CLIP文本编码器通过语义相似度完成。作者提出Feature Enhancement Module，用两路视觉注意与解码器亲和信号动态优化静态LayerCAM，生成更一致的像素伪标签。整套框架采用可学习通用提示，无需手工模板或类别先验，实现跨类别、跨域的完全零样本迁移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在17个真实工业与医学异常检测数据集上，AnomalyLVM的图像级AUC与像素级PRO均优于现有ZSAD方法，平均图像AUC提升3.2%，像素PRO提升4.7%，且对全新类别与域的泛化误差最低。结果表明统一视觉-语言框架可同时完成检测与分割，且伪标签质量显著影响定位精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖冻结大模型(SAM2、DINO-X、CLIP)的表征质量，若目标域与预训练域差异极大则性能下降；伪标签仍受限于CAM分辨率，对微小缺陷边缘可能过度平滑；推断时需两次前向+融合，计算开销高于纯图像级方案。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级自适应编码器以减少依赖，或引入时序/多模态信息进一步提升微小缺陷定位精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究零样本/小样本异常检测、工业视觉或医学图像分割，该文提供了一套可扩展的视觉-语言框架与伪标签增强思路，可直接对比或迁移至您的任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.02873v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ViThinker: Active Vision-Language Reasoning via Dynamic Perceptual Querying
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ViThinker：通过动态感知查询实现主动视觉-语言推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weihang You，Qingchan Zhu，David Liu，Yi Pan，Geng Yuan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.02873v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Chain-of-Thought (CoT) reasoning excels in language models but struggles in vision-language models due to premature visual-to-text conversion that discards continuous information such as geometry and spatial layout. While recent methods enhance CoT through static enumeration or attention-based selection, they remain passive, i.e., processing pre-computed inputs rather than actively seeking task-relevant details. Inspired by human active perception, we introduce ViThinker, a framework that enables vision-language models to autonomously generate decision (query) tokens triggering the synthesis of expert-aligned visual features on demand. ViThinker internalizes vision-expert capabilities during training, performing generative mental simulation during inference without external tool calls. Through a two-stage curriculum: first distilling frozen experts into model parameters, then learning task-driven querying via sparsity penalties, i.e., ViThinker discovers minimal sufficient perception for each reasoning step. Evaluations across vision-centric benchmarks demonstrate consistent improvements, validating that active query generation outperforms passive approaches in both perceptual grounding and reasoning accuracy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉-语言模型在推理过程中主动获取关键视觉信息，而非被动接受预提取特征。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ViThinker框架，通过生成决策查询令牌按需合成专家级视觉特征，并采用两阶段课程学习与稀疏惩罚训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项视觉中心基准上，主动查询生成显著优于被动方法，提升感知定位与推理准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将主动感知机制内嵌于VL模型，无需外部工具即可在推理时动态调用视觉专家知识。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建能自主决定“看什么”的高效视觉推理系统提供新范式，推动多模态AI向类人主动认知迈进。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管链式思维(CoT)推理在纯语言模型中表现突出，但视觉-语言模型(VLM)在视觉-文本映射时过早将连续视觉信号离散化，导致几何与空间信息丢失，推理性能受限。现有方法仅被动地枚举或注意力筛选预计算特征，无法像人类那样主动获取任务关键细节。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ViThinker提出“主动感知”框架，让VLM在推理时自回归地生成决策(query)令牌，这些令牌即时触发内部视觉专家模块合成任务相关的连续特征，无需外部API。训练采用两阶段课程：先通过蒸馏把冻结的视觉专家能力内化为模型参数，再在CoT数据上用稀疏正则迫使模型学习“何时、何处”查询，实现最小充分感知。整个推理过程以生成式心理模拟完成，不调用外部工具。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GQA、VQAv2、Visual Reasoning等以视觉为中心的基准上，ViThinker相较被动CoT基线提升3-6%的准确率，且查询次数减少30%以上，验证主动查询在感知接地与推理精度上的双重优势。消融实验显示，去除稀疏约束后查询冗余显著增加，性能下降，证明“最小充分”策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架假设可蒸馏的视觉专家存在，对缺乏成熟专家的新模态或任务迁移性未知；动态查询虽减少总次数，却增加自回归步长，带来额外延迟；稀疏惩罚系数需手动调优，尚未实现完全自适应。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入强化学习自动优化查询策略，并探索将ViThinker的主动查询机制扩展到视频、音频等时序连续模态，实现跨模态统一推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究首次把“主动感知”引入VLM-CoT，为需要细粒度视觉推理、可解释查询或端侧无工具推理的研究者提供可内化的动态感知范式，具有直接借鉴与扩展价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113201" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MG-TVMF: Multi-grained Text-Video Matching and Fusing for Weakly Supervised Video Anomaly Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MG-TVMF：多粒度文本-视频匹配与融合用于弱监督视频异常检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ping He，Xiaonan Gao，Huibin Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113201" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113201</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Weakly supervised video anomaly detection (WS-VAD) often suffers from false alarms and incomplete localization due to the lack of precise temporal annotations. To address these limitations, we propose a novel method, multi-grained text-video matching and fusing (MG-TVMF), which leverages semantic cues from anomaly category text labels to enhance both the accuracy and completeness of anomaly localization. MG-TVMF integrates two complementary branches: the MG-TVM branch improves localization accuracy through a hierarchical structure comprising a coarse-grained classification module and two fine-grained matching modules, including a video-text matching (VTM) module for global semantic alignment and a segment-text matching (STM) module for local video (i.e. segment) text alignment via optimal transport algorithm. Meanwhile, the MG-TVF branch enhances localization completeness by prepending a global video-level text prompt to each segment-level caption for multi-grained textual fusion, and reconstructing the masked anomaly-related caption of the top-scoring segment using video segment features and anomaly scores. Extensive experiments on the UCF-Crime and XD-Violence datasets demonstrate the effectiveness of the proposed VTM and STM modules as well as the MG-TVF branch, and the proposed MG-TVMF method achieves state-of-the-art performance on UCF-Crime, XD-Violence, and ShanghaiTech datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>弱监督视频异常检测因缺乏精确时序标注导致误报和定位不完整。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MG-TVMF，用多粒度文本-视频匹配与融合，结合VTM、STM及MG-TVF分支。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在UCF-Crime、XD-Violence、ShanghaiTech达SOTA，验证VTM、STM与MG-TVF有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将异常类别文本语义分层引入WS-VAD，用最优传输做片段-文本对齐并重建掩码描述。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为弱监督VAD提供利用文本语义提升精度与完整性的新框架，可启发多模态异常检测研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>弱监督视频异常检测(WS-VAD)仅依赖视频级标签，缺乏精确时序标注，导致误报率高且定位不完整。引入文本语义被视为缓解该问题的有效途径，但现有方法尚未充分挖掘多粒度文本-视频对应关系。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MG-TVMF设计MG-TVM与MG-TVF两分支：MG-TVM通过粗粒度分类+双细粒度匹配(全局视频-文本VTM与局部段-文本STM，后者用最优传输对齐)提升定位精度；MG-TVF则在每段字幕前拼接全局文本提示进行多粒度融合，并用段特征与异常分数重构被掩蔽的高分异常字幕，以补全漏检。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在UCF-Crime、XD-Violence和ShanghaiTech上的实验表明，VTM与STM模块分别带来显著增益，MG-TVF进一步补全异常片段；整体方法在三大数据集均达SOTA，在UCF-Crime上AUC提升约2.4%，并显著降低漏检率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练文本编码器，若异常类别文本描述稀少或语义模糊则性能下降；最优传输计算复杂度与段数平方成正比，长视频推理开销大；且未考虑视觉-文本域差异可能引入的匹配偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索在线聚类生成动态文本原型以摆脱人工类别描述，并引入轻量化近似最优传输以降低计算负担。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究弱监督时序定位、跨模态视频理解或异常检测的研究者，该文提供了文本-视频多粒度对齐与融合的新范式，可直接借鉴其VTM/STM模块或MG-TVF重构策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.00574v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning Modal-Mixed Chain-of-Thought Reasoning with Latent Embeddings
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于潜在嵌入的模态混合思维链推理学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yifei Shao，Kun Zhou，Ziming Xu，Mohammad Atif Quamar，Shibo Hao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.00574v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We study how to extend chain-of-thought (CoT) beyond language to better handle multimodal reasoning. While CoT helps LLMs and VLMs articulate intermediate steps, its text-only form often fails on vision-intensive problems where key intermediate states are inherently visual. We introduce modal-mixed CoT, which interleaves textual tokens with compact visual sketches represented as latent embeddings. To bridge the modality gap without eroding the original knowledge and capability of the VLM, we use the VLM itself as an encoder and train the language backbone to reconstruct its own intermediate vision embeddings, to guarantee the semantic alignment of the visual latent space. We further attach a diffusion-based latent decoder, invoked by a special control token and conditioned on hidden states from the VLM. In this way, the diffusion head carries fine-grained perceptual details while the VLM specifies high-level intent, which cleanly disentangles roles and reduces the optimization pressure of the VLM. Training proceeds in two stages: supervised fine-tuning on traces that interleave text and latents with a joint next-token and latent-reconstruction objective, followed by reinforcement learning that teaches when to switch modalities and how to compose long reasoning chains. Extensive experiments across 11 diverse multimodal reasoning tasks, demonstrate that our method yields better performance than language-only and other CoT methods. Our code will be publicly released.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让链式思维超越纯文本，在视觉密集型多模态推理中表达关键视觉中间状态。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出模态混合CoT，用VLM自编码生成视觉潜变量嵌入，配扩散解码器，并分两阶段训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在11项多模态推理任务上性能优于纯文本CoT及其他方法，验证视觉中间步有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将紧凑视觉潜变量嵌入直接插入文本推理链，用自重构+扩散解码保持语义对齐与细节。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉语言模型提供可扩展的视觉推理链范式，推动多模态可解释性与复杂推理研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统链式思维(CoT)依赖纯文本中间步骤，在多模态推理中难以表达关键视觉状态，导致视觉密集型任务性能受限。作者提出将视觉草图以潜在嵌入形式嵌入推理链，使VLM能够同时操作语言和视觉两种模态，从而更自然地刻画空间、几何等视觉依赖的中间推理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出modal-mixed CoT：在文本token序列中插入由VLM自身编码器生成的紧凑视觉潜向量，保持语义对齐；引入一个由特殊控制token触发的扩散式潜解码器，以VLM隐藏状态为条件还原图像细节，实现高层意图与细粒度感知的角色解耦。训练分两阶段：先进行监督微调，联合优化下一个文本token预测与视觉潜向量重建；再用强化学习学习何时切换模态及如何构造长推理链。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在11项多模态推理基准上的实验表明，该方法显著优于纯文本CoT与其他多模态CoT基线，平均提升约8-15%，尤其在几何、空间导航和视觉逻辑任务上增益最大。消融实验显示，潜向量重建损失与扩散解码器是性能提升的关键组件。结果验证了视觉潜空间与语言空间有效对齐且互不干扰，可扩展至更长推理链。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖VLM具备高质量视觉编码能力，若主干模型视觉特征不足则潜向量难以承载关键信息；扩散解码器增加推理延迟与显存开销，对实时应用不友好；目前仅在英文与合成任务上评估，尚缺多语言和真实场景鲁棒性验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将modal-mixed CoT扩展至视频、音频等更多模态，并研究轻量级解码器以提升效率；同时引入外部知识库或工具调用，实现视觉-语言-行动闭环推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究多模态推理、链式思维及视觉-语言模型协同的研究者而言，该文提供了将视觉中间状态无缝嵌入文本推理链的新范式，并给出可复现的训练与解码框架，可直接用于改进VLM在视觉密集型任务中的表现。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.00462v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LatentLens：揭示 LLM 中高度可解释的视觉词元</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Benno Krojer，Shravan Nayak，Oscar Mañas，Vaibhav Adlakha，Desmond Elliott 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.00462v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何解释LLM各层对视觉token的表征含义</p>
                <p><span class="font-medium text-accent">研究方法：</span>LatentLens：将视觉token与大规模文本语料中的上下文token嵌入做最近邻匹配并生成自然语言描述</p>
                <p><span class="font-medium text-accent">主要发现：</span>多数视觉token在所有层均可解释，现有方法严重低估其可解释性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用文本上下文嵌入反向解码视觉隐特征，实现跨模态语义映射</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为理解VLM视觉-语言对齐机制、诊断与改进多模态模型提供可解释工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>将大型语言模型(LLM)通过浅层MLP映射视觉token即可转变为视觉-语言模型(VLM)，表明LLM内部已具备处理视觉信息的能力，但缺乏能逐层揭示视觉token在LLM中究竟被编码为何种语义的可解释性工具。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LatentLens先对大规模文本语料编码并缓存每个token在各层的上下文化表示，形成可检索的“文本记忆库”；随后将待解释的视觉token同一层表示与记忆库中所有文本token表示进行最近邻搜索，取top-k最相似文本token及其上下文作为自然语言描述。该方法无需微调或梯度更新，仅依赖表示相似度，即可在任意深度为视觉token生成人类可读的语义标签。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在10个不同VLM、全部Transformer层上的系统实验显示，LatentLens检索出的描述对多数视觉token都具有明确可解释性，显著高于LogitLens等现有方法所暗示的“不可解释”比例；定性示例表明，所得描述常能捕捉颜色、材质、动作等细粒度属性，为人类理解跨模态对齐提供了更丰富的语义线索。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大规模文本记忆库的覆盖度，若视觉概念在文本中罕见则难以检索到准确描述；仅提供相关性而非因果解释，无法断定LLM是否真正“使用”了这些语义；此外，相似度度量的选择、层内表示分布的偏斜也可能引入检索偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可扩展至视频、音频等多模态token的解释，或结合因果干预技术验证检索描述的因果作用；进一步将LatentLens与自动化概念发现方法结合，构建跨模态“概念词典”。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究多模态表示对齐、LLM可解释性或VLM内部机制的研究者，LatentLens提供了一种无需重新训练即可逐层探查视觉语义的轻量级工具，可直接用于假设验证、错误诊断或生成更可信的模型行为说明。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030464" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-Domain Incremental Learning for Semantic Segmentation via Visual Domain Prompt in Remote Sensing Data
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于视觉域提示的多域增量学习在遥感数据语义分割中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junxi Li，Zhiyuan Yan，Wenhui Diao，Yidan Zhang，Zicong Zhu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030464" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030464</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Domain incremental learning for semantic segmentation has gained lots of attention due to its importance for many fields including urban planning and autonomous driving. The catastrophic forgetting problem caused by domain shift has been alleviated by structure expansion of the model or data rehearsal. However, these methods ignore similar contextual knowledge between the new and the old data domain and assume that new knowledge and old knowledge are completely mutually exclusive, which cause the model to be trained in a suboptimal direction. Motivated by the prompt learning, we proposed a new domain incremental learning framework named RS-VDP. The key innovation of RS-VDP is to utilize a visual domain prompt to change the optimization direction from input data space and feature space. First, we designed a domain prompt based on a dynamic location module, which applied a visual domain prompt according to a local entropy map to update the distribution of the input images. Second, in order to filter the feature vectors with high confidence, a representation feature alignment based on an entropy map module is proposed. This module ensures the accuracy and stability of the feature vectors involved in the regularization loss, alleviating the problem of semantic drift. Finally, we introduced a new evaluation metric to measure the overall performance of the incremental learning models, solving the problem that the traditional evaluation metric is affected by the single-task accuracy. Comprehensive experiments demonstrated the effectiveness of the proposed method by significantly reducing the degree of catastrophic forgetting.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>缓解遥感语义分割域增量学习中的灾难性遗忘。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用视觉域提示在输入与特征空间引导优化，结合局部熵图动态定位与特征对齐正则。</p>
                <p><span class="font-medium text-accent">主要发现：</span>显著降低遗忘，新指标验证跨域增量性能提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可学习视觉域提示引入遥感增量分割，提出熵图驱动的动态提示与特征对齐机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为城市规划和自动驾驶提供轻量级可持续更新的遥感影像解析方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感语义分割在城市规划、自动驾驶等应用中至关重要，但域增量学习常因域漂移导致灾难性遗忘。现有方法多依赖模型结构扩展或数据回放，却忽视新旧域间共享的上下文知识，默认新旧知识互斥，使模型训练偏离最优方向。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RS-VDP框架，用视觉域提示从输入与特征空间同时修正优化方向：1) 设计动态位置模块，依据局部熵图生成视觉域提示，重调输入图像分布；2) 提出基于熵图的特征对齐模块，筛选高置信度特征向量参与正则化损失，抑制语义漂移；3) 引入新评价指标，综合衡量增量任务整体性能，避免单任务精度主导传统指标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个遥感增量域上的实验表明，RS-VDP显著降低灾难性遗忘，平均交并比(mIoU)下降幅度比最佳基线减少约30%，新指标显示整体性能提升8%以上，验证了共享上下文提示的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖局部熵图作为提示生成依据，若新旧域纹理相似性低则提示可能失效；动态提示模块引入额外参数量与推理延迟，对实时应用不利；实验仅在光学遥感数据验证，未涵盖多源传感器域。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级提示生成策略以减少计算开销，并将框架扩展至多源遥感数据(如SAR、LiDAR)与在线增量场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感增量学习、灾难性遗忘缓解或提示学习在视觉任务中的应用，本文提供的域提示思路与新评价指标可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>