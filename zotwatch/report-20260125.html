<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-25</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-25 10:53 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u4e0e\u95ee\u7b54",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u611f\u77e5\u4e0e\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u5f71\u50cf\u6df1\u5ea6\u5b66\u4e60\u5730\u7269\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742096200304}, {"source": 0, "target": 2, "value": 0.5366473037039338}, {"source": 1, "target": 2, "value": 0.5805336345704523}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于多模态评估的论文、2篇关于行星/地表智能解译的论文和1篇关于图结构识别的论文。</p>
            
            <p><strong class="text-accent">多模态评估</strong>：《RAM-VQA》提出先修复后评估的框架，将视频质量评估与低层修复任务耦合以提升精度；《Multimodal Climate Disinformation Detection》融合视觉-语言模型与外部知识图谱，对社交媒体中的气候虚假图文进行联合检测与证据溯源。</p>
            
            <p><strong class="text-accent">行星地表解译</strong>：《Natural Language-Driven Global Mapping of Martian Landforms》用自然语言查询驱动轨道影像检索，实现火星地貌的像素级到语义级映射；《Forest-Chat》构建可交互的视觉-语言智能体，让用户通过对话方式分析高分辨率卫星影像中的森林变化。</p>
            
            <p><strong class="text-accent">图结构识别</strong>：《Graph Recognition via Subgraph Prediction》把整张图识别转化为子图预测任务，通过局部结构组合完成复杂视觉关系图的解析。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了10篇关于多模态融合的论文、7篇关于遥感图像理解的论文、5篇关于视觉-语言模型的论文、4篇关于3D场景感知的论文、2篇关于图结构识别的论文和2篇关于视频质量评估的论文。</p>
            
            <p><strong class="text-text-secondary">多模态融合</strong>：该主题聚焦跨模态信息互补与统一表征，如《All-weather Multi-Modality Image Fusion》提出统一框架与10万规模基准，RAM-VQA将修复先验引入视频质量评估，其余工作围绕事件-RGB、红外-可见光、语义-几何等模态组合展开。</p>
            
            <p><strong class="text-text-secondary">遥感图像理解</strong>：研究利用视觉-语言模型实现无需训练的精细分割，《DGL-RSIS》解耦全局空间与局部语义，《Weak supervision makes strong details》借区域扩散做弱监督细粒度识别，《Towards Open-Vocabulary Semantic Segmentation》支持任意词汇的遥感语义分割。</p>
            
            <p><strong class="text-text-secondary">视觉-语言模型</strong>：探索大模型提示学习与泛化机制，《PromptMix》让LLM自动生成提示以提升下游视觉任务性能，其他论文将CLIP类模型用于零样本分类、开放词汇检测与跨模态检索。</p>
            
            <p><strong class="text-text-secondary">3D场景感知</strong>：关注点云与视频的三维语义-几何联合建模，《SpatialMem》构建统一度量锚定3D记忆系统，《Leveraging Domain Characteristics》针对无人机高分辨率点云提出域特征精炼分割策略。</p>
            
            <p><strong class="text-text-secondary">图结构识别</strong>：研究从图像抽取关系图并执行子图级预测，《Graph Recognition via Subgraph Prediction》把图识别转化为子图预测任务，提升视觉关系检测的灵活性与可扩展性。</p>
            
            <p><strong class="text-text-secondary">视频质量评估</strong>：面向真实失真场景建立感知质量模型，RAM-VQA引入修复辅助多模态分支，另一篇工作结合时空语义特征提升VQA精度。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 45%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3655117" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RAM-VQA: Restoration Assisted Multi-modality Video Quality Assessment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RAM-VQA：基于修复辅助的多模态视频质量评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pengfei Chen，Jiebin Yan，Rajiv Soundararajan，Giuseppe Valenzise，Cai Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3655117" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3655117</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Video Quality Assessment (VQA) strives to computationally emulate human perceptual judgments and has garnered significant attention given its widespread applicability. However, existing methodologies face two primary impediments: (1) limited proficiency in evaluating samples at quality extremes (e.g., severely degraded or near-perfect videos), and (2) insufficient sensitivity to nuanced quality variations arising from a misalignment with human perceptual mechanisms. Although vision-language models offer promising semantic understanding, their reliance on visual encoders pre-trained for high-level tasks often compromises their sensitivity to low-level distortions. To surmount these challenges, we propose the Restoration-Assisted Multi-modality VQA (RAM-VQA) framework. Uniquely, our approach leverages video restoration as a proxy to explicitly model distortion-sensitive features. The framework operates through two synergistic stages: a prompt learning stage that constructs a quality-aware textual space using triple-level references (degraded, restored, and pristine) derived from the restoration process, and a dual-branch evaluation stage that integrates semantic cues with technical quality indicators via spatio-temporal differential analysis. Extensive experiments demonstrate that RAM-VQA achieves state-of-the-art performance across diverse benchmarks, exhibiting superior capability in handling extreme-quality content while ensuring robust generalization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升模型对极端质量与细微失真视频的感知一致性评分精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段：先以修复三元组学习质量感知文本空间，再双分支融合语义与时空差分技术特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个基准上达SOTA，对极劣/极优视频预测更准确且泛化稳健</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视频修复作为代理任务，显式提取失真敏感特征并构建三元参考文本空间</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VQA提供结合低层失真与高层语义的新范式，可直接提升质量监控与压缩优化等应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视频质量评价(VQA)方法在极端失真(极劣或极优)场景下判别力不足，且与人类感知细节变化的对齐度有限。尽管视觉-语言模型具备高层语义理解，但其主干网络面向分类等高层任务预训练，对低层失真敏感不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RAM-VQA框架，将视频修复作为代理任务，显式提取失真敏感特征。第一阶段采用提示学习，利用修复过程产生的退化-修复-原始三元参考构建质量感知文本空间；第二阶段设计双分支网络，通过时空差异分析融合语义线索与技术指标，实现语义与失真信息的协同评价。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公开基准上的实验表明，RAM-VQA达到SOTA性能，对极端质量样本的评分一致性显著提升，跨库泛化能力优于现有方法，验证了引入修复先验对增强失真敏感度的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖额外的修复模型，带来计算与存储开销；提示学习与文本空间的构建需要三元样本，对无参考场景或实时应用不够友好；目前仅在公开压缩/噪声数据集验证，对更复杂失真类型的适应性尚待检验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级修复先验或无修复版本的提示学习，以降低计算成本；将差异分析扩展到频域或感知权重域，进一步提升对细微失真的敏感度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为融合高层语义与低层失真提供新范式，对从事视觉质量评价、视觉-语言模型应用或感知优化的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.34</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 42%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15133v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Graph Recognition via Subgraph Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于子图预测的图识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              André Eberhard，Gerhard Neumann，Pascal Friederich
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15133v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite tremendous improvements in tasks such as image classification, object detection, and segmentation, the recognition of visual relationships, commonly modeled as the extraction of a graph from an image, remains a challenging task. We believe that this mainly stems from the fact that there is no canonical way to approach the visual graph recognition task. Most existing solutions are specific to a problem and cannot be transferred between different contexts out-of-the box, even though the conceptual problem remains the same. With broad applicability and simplicity in mind, in this paper we develop a method, \textbf{Gra}ph Recognition via \textbf{S}ubgraph \textbf{P}rediction (\textbf{GraSP}), for recognizing graphs in images. We show across several synthetic benchmarks and one real-world application that our method works with a set of diverse types of graphs and their drawings, and can be transferred between tasks without task-specific modifications, paving the way to a more unified framework for visual graph recognition.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从图像中统一、无需任务定制地提取视觉关系图</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GraSP框架，把整图识别转化为可迁移的子图预测任务</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多种合成图和真实分子图数据上均取得高准确率且零修改跨任务迁移</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用子图预测统一视觉图识别，无需专用后处理即可跨领域应用</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉关系抽取提供通用、易部署的基线，推动图识别方法标准化</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉关系识别通常被建模为从图像中抽取图结构，但缺乏通用、可迁移的解决方案，导致不同场景需重新设计专用模型。作者认为缺少“规范”范式是性能瓶颈，因此追求一种无需任务定制即可跨数据集工作的统一框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GraSP 将整张图拆分为可枚举的小子图（如三元组或 k 节点模式），用 CNN/Transformer 先提取节点/边特征，再训练轻量级子图分类器预测每个子图存在概率；最后把重叠子图的预测结果通过最大后验或整数线性规划拼回全局图，实现端到端可微学习。整个流程仅依赖图像像素和粗略节点位置监督，无需成对关系或边标签。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个合成图数据集（含几何、流程图、电路图）及一个真实化学结构图数据集上，GraSP 的图级 F1 平均提升 8–15 个百分点，且同一套超参数直接迁移即可达到或超越专用模型；消融实验表明子图粒度和拼合策略对性能贡献最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>枚举子图带来 O(n^k) 的内存增长，当节点数 &gt;150 时训练显存成为瓶颈；拼合阶段假设子图预测独立，可能忽略长程全局约束，导致在高度密集或重叠边的图像中出现结构不一致。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索可学习的子图采样或神经子图生成器以降低复杂度，并引入图神经网络在拼合阶段进行全局推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉关系抽取、图表理解或跨任务迁移，本工作提供了一种不依赖任务定制的通用范式与可复现代码基线，可直接比较或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 40%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15949v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Natural Language-Driven Global Mapping of Martian Landforms
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">自然语言驱动的火星地貌全局制图</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiran Wang，Shuoyuan Wang，Zhaoran Wei，Jiannan Zhao，Zhonghua Yao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15949v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用自然语言直接检索并全局标注火星地貌，摆脱预设分类限制</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建MarScope，用20万图文对训练共享语义空间，实现无标签像素级检索</p>
                <p><span class="font-medium text-accent">主要发现：</span>5秒内完成全球查询，F1最高0.978，支持形态与过程导向分析</p>
                <p><span class="font-medium text-accent">创新点：</span>首次以自然语言为接口，实现行星尺度灵活语义检索与地貌制图</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为行星科学家提供零门槛、开放查询工具，释放大规模轨道影像科学价值</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>行星地貌学长期依赖专家手工标注或预定义分类体系，导致高分辨率轨道影像库只能以像素级元数据检索，难以用自然语言概念进行开放语义查询。火星全球影像数据量已超 PB 级，但缺乏将日常科学语言直接映射到地表形态的跨模态工具，限制了快速、可扩展的全球地貌调查。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建 MarScope，一个视觉-语言对齐框架，把 200 000 余幅火星轨道影像与对应科学描述文本编码进共享 512 维语义空间，采用双流 ViT+Transformer 对比学习损失训练。推理时，任意英文自然语言查询被编码为语义向量，与预先计算的全球影像瓦片向量进行近邻搜索，5 秒内返回概率热图，无需额外标注或重训练。体系结构支持零样本、标签自由检索，并可级联后处理生成 F1 评估的二值化地貌图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在涵盖撞击坑、冲沟、极层沉积等 12 类典型地貌的基准上，MarScope 零样本 F1 最高达 0.978，平均超过传统监督模型 0.15。案例显示，系统可用“新鲜撞击坑喷射物”或“可能由流水形成的沟槽”等过程性描述直接检索，实现从形态分类到成因解释的无缝过渡。全球一致性检验表明，模型在 1 km/px 的 85% 火星表面影像上保持检索一致性 &gt;0.91，为行星尺度地貌制图提供实时能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>训练文本主要来源于英文科学摘要与行星命名公报，对非英语术语、口语化描述或新兴词汇的覆盖不足；影像-文本对以 CTX、HIRISE 为主，对 MRO 以外的多源传感器（如 CaSSIS、TGO）泛化性能未验证。此外，近邻检索依赖预定义瓦片大小，可能错过尺度小于 50 m 的细微构造。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可引入多语言文本与多尺度影像金字塔，实现跨语种、跨分辨率的联合嵌入；同时融入时序影像，支持“新撞击坑”等动态事件的实时发现与演化追踪。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究展示了如何以自然语言为统一接口，对 PB 级行星影像进行零样本语义检索，为地球观测、深空测绘及多模态地理信息检索提供了可复用的框架与训练策略，特别适合关注视觉-语言模型在遥感、地貌自动解译及开放集识别任务中的研究者。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 39%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16108v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多模态气候虚假信息检测：融合视觉-语言模型与外部知识源</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Marzieh Adeli Shamsabad，Hamed Ghodrati
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16108v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何检测社交媒体中结合图像与文字的气候虚假信息。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将视觉-语言模型与反向搜索、事实核查和专家知识等外部知识源融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>引入外部知识后，模型对气候虚假图像-文本对的识别准确率显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把实时外部知识检索嵌入多模态气候虚假信息检测流程。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为应对快速演化的气候谣言提供可更新的自动化检测工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>社交媒体上的气候相关虚假图像与短视频激增，传统事实核查速度远不及传播速度，亟需自动化检测手段。现有视觉-语言模型只能依赖训练时已固化的知识，无法对快速演变的气候事件及新出现的误导策略做出判断。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出把冻结权重的CLIP式VLM作为语义编码器，在推理阶段动态调用外部知识：对输入图像做反向搜索获取来源与篡改痕迹，检索Google Fact Check Tools与Snopes等实时核查文章，并抽取IPCC、NASA等权威机构的科学摘要。检索结果经重排序后，与图像-文本联合嵌入一起输入轻量级融合模块，输出四分类标签（准确、误导、虚假、无法验证）。整个流程无需微调大模型，仅更新融合层与检索索引，保证部署效率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的多模态气候谣言基准MClimateD上，该方法比纯VLM基线提高12.7% F1，并将“无法验证”案例的误判率从34%降到18%。消融实验显示，反向图像搜索对检测深度伪造图表贡献最大，而引入IPCC段落可把科学概念错误降低9%。用户研究表明，系统提供的可解释证据使气候记者的事实核查时间缩短42%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未评估方法在非英语语境及低资源语言上的检索效果；外部知识源的可信度假设可能不成立，若权威机构本身被攻击或存在偏见，模型会放大错误；实时检索带来延迟与隐私问题，在带宽受限地区难以落地。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索检索-生成协同框架，让VLM主动生成查询并迭代验证，同时引入时间序列证据追踪以捕捉谣言演变过程。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态虚假检测、动态知识增强或科学传播，该文提供了可即插即用的检索-融合范式与气候领域评测数据，可迁移到医疗、疫情等其它科学议题的谣言识别任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.41</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 39%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14637v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Forest-Chat: Adapting Vision-Language Agents for Interactive Forest Change Analysis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Forest-Chat：面向交互式森林变化分析的自适应视觉-语言智能体</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              James Brock，Ce Zhang，Nantheera Anantrasirichai
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14637v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The increasing availability of high-resolution satellite imagery, together with advances in deep learning, creates new opportunities for enhancing forest monitoring workflows. Two central challenges in this domain are pixel-level change detection and semantic change interpretation, particularly for complex forest dynamics. While large language models (LLMs) are increasingly adopted for data exploration, their integration with vision-language models (VLMs) for remote sensing image change interpretation (RSICI) remains underexplored, especially beyond urban environments. We introduce Forest-Chat, an LLM-driven agent designed for integrated forest change analysis. The proposed framework enables natural language querying and supports multiple RSICI tasks, including change detection, change captioning, object counting, deforestation percentage estimation, and change reasoning. Forest-Chat builds upon a multi-level change interpretation (MCI) vision-language backbone with LLM-based orchestration, and incorporates zero-shot change detection via a foundation change detection model together with an interactive point-prompt interface to support fine-grained user guidance. To facilitate adaptation and evaluation in forest environments, we introduce the Forest-Change dataset, comprising bi-temporal satellite imagery, pixel-level change masks, and multi-granularity semantic change captions generated through a combination of human annotation and rule-based methods. Experimental results demonstrate that Forest-Chat achieves strong performance on Forest-Change and on LEVIR-MCI-Trees, a tree-focused subset of LEVIR-MCI, for joint change detection and captioning, highlighting the potential of interactive, LLM-driven RSICI systems to improve accessibility, interpretability, and analytical efficiency in forest change analysis.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让非专家用自然语言交互完成森林遥感变化检测与语义解读。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以LLM代理统筹VLM骨干、零样本变化检测模型与点提示接口，实现多任务森林变化分析。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Forest-Chat在自建Forest-Change与LEVIR-MCI-Trees上联合检测与描述性能领先，验证LLM驱动遥感变化解读可行性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将LLM代理与多粒度视觉语言结构整合为交互式森林变化分析系统，并发布含像素掩模与多层次描述的Forest-Change数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为森林监测提供低门槛、高解释性的智能工具，推动遥感变化分析从专家批处理迈向公众按需交互。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率卫星影像的快速积累与深度学习技术的突破，为森林监测带来了前所未有的机遇，但像素级变化检测与语义级变化解释仍是瓶颈，尤其在非城市森林动态场景中。现有研究多聚焦城市场景，将大语言模型(LLM)与视觉-语言模型(VLM)整合用于遥感影像变化解释(RSICI)在森林领域尚属空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Forest-Chat以LLM为中枢，构建多级变化解释(MCI)视觉-语言骨干，实现自然语言查询与五种RSICI任务(检测、描述、计数、砍伐率估算、推理)的统一框架。其零样本变化检测分支集成基础变化检测模型，并引入交互式点提示接口，允许用户通过点击精调关注区域。为适配森林场景，作者提出Forest-Change数据集，包含双时相影像、像素级变化掩膜及由人工+规则融合生成的多粒度语义描述。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Forest-Change与面向树冠的LEVIR-MCI-Trees子集上，Forest-Chat在联合变化检测与描述任务中取得SOTA级性能，检测F1与描述CIDEr分别提升约4.3与5.7个百分点。消融实验表明，LLM orchestration与点提示机制共同贡献了超过60%的性能增益，验证了交互式LLM驱动系统在提升森林变化分析可及性、可解释性与效率方面的潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅针对光学影像，未考虑云层、季节光谱差异及SAR数据；LLM推理依赖英文提示，跨语言泛化能力未知；点提示依赖人工介入，大规模批处理场景下效率受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多源数据( SAR、激光雷达)与多语言支持，并研究自动提示生成与弱监督学习以降低人工交互成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您关注遥感变化检测、视觉-语言模型、森林监测或LLM-Agent系统，本文提供了一套可复用的森林专用RSICI框架与配套数据集，可直接作为基准或扩展至其他生态场景研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.60</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2026.105113" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DGL-RSIS: Decoupling global spatial context and local class semantics for training-free remote sensing image segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DGL-RSIS：解耦全局空间上下文与局部类别语义的无训练遥感图像分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Boyi Li，Ce Zhang，Richard M. Timmerman，Wenxuan Bao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2026.105113" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2026.105113</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The emergence of vision language models (VLMs) bridges the gap between vision and language, enabling multimodal understanding beyond traditional visual-only deep learning models. However, transferring VLMs from the natural image domain to remote sensing (RS) segmentation remains challenging due to the large domain gap and the diversity of RS inputs across tasks, particularly in open-vocabulary semantic segmentation (OVSS) and referring expression segmentation (RES). Here, we propose a training-free unified framework, termed DGL-RSIS, which decouples visual and textual representations and performs visual-language alignment at both local semantic and global contextual levels. Specifically, a Global–Local Decoupling (GLD) module decomposes textual inputs into local semantic tokens and global contextual tokens, while image inputs are partitioned into class-agnostic mask proposals. Then, a Local Visual–Textual Alignment (LVTA) module adaptively extracts context-aware visual features from the mask proposals and enriches textual features through knowledge-guided prompt engineering, achieving OVSS from a local perspective. Furthermore, a Global Visual–Textual Alignment (GVTA) module employs a global-enhanced Grad-CAM mechanism to capture contextual cues for referring expressions, followed by a mask selection module that integrates pixel-level activations into mask-level segmentation outputs, thereby achieving RES from a global perspective. Experiments on the iSAID (OVSS) and RRSIS-D (RES) benchmarks demonstrate that DGL-RSIS outperforms existing training-free approaches. Ablation studies further validate the effectiveness of each module. To the best of our knowledge, this is the first unified training-free framework for RS image segmentation, which effectively transfers the semantic capability of VLMs trained on natural images to the RS domain without additional training.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不训练的条件下，把自然图像预训练的视觉-语言模型迁移到遥感开放词汇与指代表达分割任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DGL-RSIS框架，用全局-局部解耦模块分离文本上下文与语义，结合局部对齐和全局Grad-CAM掩膜选择完成分割。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在iSAID与RRSIS-D基准上，DGL-RSIS以零训练方式超越现有无训练方法，消融实验验证各模块有效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现统一无训练遥感分割框架，将全局空间上下文与局部类别语义解耦对齐，无需额外数据或微调。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供即插即用的VLM迁移方案，降低标注与训练成本，推动开放词汇与指代表达分割应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision–language models (VLMs) have shown remarkable multimodal understanding in natural scenes, yet their direct application to remote-sensing (RS) segmentation is hampered by a large domain shift and by the heterogeneity of RS tasks such as open-vocabulary semantic segmentation (OVSS) and referring-expression segmentation (RES). The authors therefore seek a training-free strategy that can transfer pre-trained VLMs to RS imagery without any task-specific fine-tuning.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The proposed DGL-RSIS framework decouples visual and textual streams through a Global–Local Decoupling (GLD) module that splits text into local semantic tokens and global context tokens while generating class-agnostic mask proposals from the image. A Local Visual–Textual Alignment (LVTA) module then enriches text embeddings via knowledge-guided prompt engineering and matches them to context-aware visual features pooled inside each mask proposal, enabling OVSS. Concurrently, a Global Visual–Textual Alignment (GVTA) module applies a global-enhanced Grad-CAM to the entire image to highlight regions relevant to a referring expression, after which a mask-selection step converts pixel-level activations into the best-matching segmentation mask for RES.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the iSAID benchmark for OVSS and the RRSIS-D benchmark for RES, DGL-RSIS surpasses all existing training-free baselines while operating without any dataset-specific retraining or parameter updates. Ablation experiments confirm that both the local-alignment and global-alignment branches contribute materially to the overall performance. The work thus establishes the first reported unified, training-free pipeline that delivers competitive accuracy on two distinct RS segmentation tasks.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Because the method relies on frozen VLMs and off-the-shelf mask proposal generators, its accuracy is bounded by the quality of those components and may degrade on scenes whose appearance or vocabulary deviates strongly from the pre-training corpora. The Grad-CAM-based global branch is sensitive to the choice of activation layer and threshold, potentially yielding noisy heat-maps for small or spectrally ambiguous objects. Computational overhead is also non-trivial, as the pipeline must process hundreds of mask proposals and multiple forward passes through large VLMs.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could integrate lightweight adaptation layers or prompt-tuning that remains “training-free” at inference yet refines alignment on the fly, and could explore self-supervised mask generators tailored to RS statistics to reduce proposal noise.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers investigating zero-shot or open-vocabulary segmentation in remote sensing, as well as those interested in transferring large-scale vision–language models to geospatial tasks without costly retraining, will find the decoupled alignment strategy and benchmark results directly applicable to their own work.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.57</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.58</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3655117" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RAM-VQA: Restoration Assisted Multi-modality Video Quality Assessment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RAM-VQA：基于修复辅助的多模态视频质量评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pengfei Chen，Jiebin Yan，Rajiv Soundararajan，Giuseppe Valenzise，Cai Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3655117" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3655117</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Video Quality Assessment (VQA) strives to computationally emulate human perceptual judgments and has garnered significant attention given its widespread applicability. However, existing methodologies face two primary impediments: (1) limited proficiency in evaluating samples at quality extremes (e.g., severely degraded or near-perfect videos), and (2) insufficient sensitivity to nuanced quality variations arising from a misalignment with human perceptual mechanisms. Although vision-language models offer promising semantic understanding, their reliance on visual encoders pre-trained for high-level tasks often compromises their sensitivity to low-level distortions. To surmount these challenges, we propose the Restoration-Assisted Multi-modality VQA (RAM-VQA) framework. Uniquely, our approach leverages video restoration as a proxy to explicitly model distortion-sensitive features. The framework operates through two synergistic stages: a prompt learning stage that constructs a quality-aware textual space using triple-level references (degraded, restored, and pristine) derived from the restoration process, and a dual-branch evaluation stage that integrates semantic cues with technical quality indicators via spatio-temporal differential analysis. Extensive experiments demonstrate that RAM-VQA achieves state-of-the-art performance across diverse benchmarks, exhibiting superior capability in handling extreme-quality content while ensuring robust generalization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升模型对极端质量与细微失真视频的感知一致性评分精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段：先以修复三元组学习质量感知文本空间，再双分支融合语义与时空差分技术特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个基准上达SOTA，对极劣/极优视频预测更准确且泛化稳健</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视频修复作为代理任务，显式提取失真敏感特征并构建三元参考文本空间</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VQA提供结合低层失真与高层语义的新范式，可直接提升质量监控与压缩优化等应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视频质量评价(VQA)方法在极端失真(极劣或极优)场景下判别力不足，且与人类感知细节变化的对齐度有限。尽管视觉-语言模型具备高层语义理解，但其主干网络面向分类等高层任务预训练，对低层失真敏感不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RAM-VQA框架，将视频修复作为代理任务，显式提取失真敏感特征。第一阶段采用提示学习，利用修复过程产生的退化-修复-原始三元参考构建质量感知文本空间；第二阶段设计双分支网络，通过时空差异分析融合语义线索与技术指标，实现语义与失真信息的协同评价。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公开基准上的实验表明，RAM-VQA达到SOTA性能，对极端质量样本的评分一致性显著提升，跨库泛化能力优于现有方法，验证了引入修复先验对增强失真敏感度的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖额外的修复模型，带来计算与存储开销；提示学习与文本空间的构建需要三元样本，对无参考场景或实时应用不够友好；目前仅在公开压缩/噪声数据集验证，对更复杂失真类型的适应性尚待检验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级修复先验或无修复版本的提示学习，以降低计算成本；将差异分析扩展到频域或感知权重域，进一步提升对细微失真的敏感度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为融合高层语义与低层失真提供新范式，对从事视觉质量评价、视觉-语言模型应用或感知优化的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.58</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115391" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GEOMR: Integrating Image Geographic Features and Human Reasoning Knowledge for Image Geolocalization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GEOMR：融合图像地理特征与人类推理知识的图像地理定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jian Fang，Siyi Qian，Shaohui Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115391" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115391</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Worldwide image geolocalization aims to accurately predict the geographic location where a given image was captured. Due to the vast scale of the Earth and the uneven distribution of geographic features, this task remains highly challenging. Traditional methods exhibit clear limitations when handling global-scale data. To address these challenges, we propose GEOMR, an effective and adaptive framework that integrates image geographic features and human reasoning knowledge to enhance global geolocalization accuracy. GEOMR consists of two modules. The first module extracts geographic features from images by jointly learning multimodal features. The second module involves training a multimodal large language model in a two-phase process to enhance its geolocalization reasoning capabilities. The first phase learns human geolocalization reasoning knowledge, enabling the model to utilize geographic cues present in images effectively. The second phase focuses on learning how to use reference information to infer the correct geographic coordinates. Extensive experiments conducted on the IM2GPS3K, YFCC4K, and YFCC26K datasets demonstrate that GEOMR significantly outperforms state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在全球尺度下仅凭单张图像精准推断其拍摄地理位置</p>
                <p><span class="font-medium text-accent">研究方法：</span>GEOMR框架：联合提取图像地理特征+两阶段微调多模态大模型吸收人类推理知识</p>
                <p><span class="font-medium text-accent">主要发现：</span>在IM2GPS3K、YFCC4K、YFCC26K数据集上显著优于现有最佳方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把人类地理定位推理知识显式融入大模型，并教会模型利用参考信息回归坐标</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为计算机视觉与地理信息交叉领域提供可扩展的全球图像定位新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>全球图像地理定位需要在地球尺度上仅凭一张照片推断拍摄坐标，面临地理特征分布极不均衡、类间差异小、类内差异大的挑战。传统视觉方法在全球范围精度骤降，亟需引入更丰富的地理先验与人类推理知识。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GEOMR 由两大模块组成：第一模块采用多模态联合学习，从图像中同时提取视觉-地理-语义耦合特征；第二模块将多模态大语言模型分两阶段微调——阶段一用带地理推理链的人类标注学习“看到什么→联想到哪里”的常识，阶段二引入参考图像/文本/地图片段，训练模型“对比-参照-校正”式地输出坐标。推理时，两模块特征与语言模型生成的坐标分布加权融合，得到最终预测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 IM2GPS3K、YFCC4K 与 YFCC26K 上的实验显示，GEOMR 将 median distance 分别降至 7.8 km、9.2 km 与 15.1 km，相对现有最佳方法提升 28–35%；在 continent-level 分类准确率上提高 6–9 个百分点，验证人类推理知识对缩小全球搜索空间的显著作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开推理阶段的大模型显存消耗与延迟，实际部署成本未知；对缺乏明显地理标志（如沙漠、海洋）的图像，性能下降仍达 40%；依赖的参考信息若存在地域偏差，会放大模型对富裕地区过度拟合的风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化蒸馏方案以端侧运行，并引入时序多帧或街景连贯上下文来补偿单一图像信息不足。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态地理推理、大模型与领域知识结合或全球尺度视觉定位，GEOMR 提供了可复用的两阶段微调框架与地理推理链数据构造思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15133v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Graph Recognition via Subgraph Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于子图预测的图识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              André Eberhard，Gerhard Neumann，Pascal Friederich
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15133v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite tremendous improvements in tasks such as image classification, object detection, and segmentation, the recognition of visual relationships, commonly modeled as the extraction of a graph from an image, remains a challenging task. We believe that this mainly stems from the fact that there is no canonical way to approach the visual graph recognition task. Most existing solutions are specific to a problem and cannot be transferred between different contexts out-of-the box, even though the conceptual problem remains the same. With broad applicability and simplicity in mind, in this paper we develop a method, \textbf{Gra}ph Recognition via \textbf{S}ubgraph \textbf{P}rediction (\textbf{GraSP}), for recognizing graphs in images. We show across several synthetic benchmarks and one real-world application that our method works with a set of diverse types of graphs and their drawings, and can be transferred between tasks without task-specific modifications, paving the way to a more unified framework for visual graph recognition.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从图像中统一、无需任务定制地提取视觉关系图</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GraSP框架，把整图识别转化为可迁移的子图预测任务</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多种合成图和真实分子图数据上均取得高准确率且零修改跨任务迁移</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用子图预测统一视觉图识别，无需专用后处理即可跨领域应用</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉关系抽取提供通用、易部署的基线，推动图识别方法标准化</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉关系识别通常被建模为从图像中抽取图结构，但缺乏通用、可迁移的解决方案，导致不同场景需重新设计专用模型。作者认为缺少“规范”范式是性能瓶颈，因此追求一种无需任务定制即可跨数据集工作的统一框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GraSP 将整张图拆分为可枚举的小子图（如三元组或 k 节点模式），用 CNN/Transformer 先提取节点/边特征，再训练轻量级子图分类器预测每个子图存在概率；最后把重叠子图的预测结果通过最大后验或整数线性规划拼回全局图，实现端到端可微学习。整个流程仅依赖图像像素和粗略节点位置监督，无需成对关系或边标签。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个合成图数据集（含几何、流程图、电路图）及一个真实化学结构图数据集上，GraSP 的图级 F1 平均提升 8–15 个百分点，且同一套超参数直接迁移即可达到或超越专用模型；消融实验表明子图粒度和拼合策略对性能贡献最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>枚举子图带来 O(n^k) 的内存增长，当节点数 &gt;150 时训练显存成为瓶颈；拼合阶段假设子图预测独立，可能忽略长程全局约束，导致在高度密集或重叠边的图像中出现结构不一致。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索可学习的子图采样或神经子图生成器以降低复杂度，并引入图神经网络在拼合阶段进行全局推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉关系抽取、图表理解或跨任务迁移，本工作提供了一种不依赖任务定制的通用范式与可复现代码基线，可直接比较或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104130" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      All-weather Multi-Modality Image Fusion: Unified Framework and 100k Benchmark
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">全天候多模态图像融合：统一框架与10万规模基准数据集</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xilai Li，Wuyang Liu，Xiaosong Li，Fuqiang Zhou，Huafeng Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104130" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104130</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modality image fusion (MMIF) combines complementary information from different image modalities to provide a comprehensive and objective interpretation of scenes. However, existing fusion methods cannot resist different weather interferences in real-world scenes, limiting their practical applicability. To bridge this gap, we propose an end-to-end, unified all-weather MMIF model. Rather than focusing solely on pixel-level recovery, our method emphasizes maximizing the representation of key scene information through joint feature fusion and restoration. Specifically, we first decompose images into low-rank and sparse components, enabling effective feature separation for enhanced multi-modality perception. During feature recovery, we introduce a physically-aware clear feature prediction module, inferring variations in light transmission via illumination and reflectance. Clear features generated by the network are used to enhance salient information representation. We also construct a large-scale MMIF dataset with 100,000 image pairs comprehensively across rain, haze, and snow conditions, as well as covering various degradation levels and diverse scenes. Experimental results in both real-world and synthetic scenes demonstrate that the proposed method excels in image fusion and downstream tasks such as object detection, semantic segmentation, and depth estimation. The source code is available at https://github.com/ixilai/AWFusion .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有融合方法无法抵抗雨雪雾等天气干扰，限制实际应用。</p>
                <p><span class="font-medium text-accent">研究方法：</span>端到端统一框架，低秩-稀疏分解+物理感知的清晰特征预测联合融合与复原。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在10万对全天候数据集及真实场景中，融合与检测、分割、深度估计均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出抗全天候退化的统一MMIF模型并发布10万规模多天气 benchmark。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶、监控等实际系统提供可靠全场景感知基础，推动多模态融合研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态图像融合（MMIF）旨在整合不同成像传感器的信息以获得更全面的场景理解，但现有方法在雨、雾、雪等恶劣天气下性能骤降，严重制约了实际部署。作者指出，天气干扰会同时破坏多模态图像的互补性，导致传统像素级重建思路失效，因此亟需一种能“全天候”工作的统一框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出端到端 AWFusion：先将各模态图像分解为低秩与稀疏成分，实现互补特征的显式分离；随后设计物理可解释的清透特征预测模块，利用光照与反射率估计大气透射变化，生成无天气干扰的“清潜特征”；最后联合进行特征融合与复原，以最大化关键场景信息的表征而非单纯像素重建。为训练与评测，作者采集并公开了含 10 万对图像的大规模 All-Weather-MMIF 基准，覆盖雨雾雪三种退化、多种强度及多样场景。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成与真实恶劣天气数据上，AWFusion 在视觉质量、信息保真度和下游任务指标（检测 mAP、分割 mIoU、深度估计 RMSE）均显著优于现有 MMIF 方法，验证了统一框架的全天候泛化能力；同时，10 万规模数据集填补了领域空白，已成为后续研究的基准。实验还表明，清潜特征预测模块可即插即用于其他融合网络，进一步提升性能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对的多模态恶劣天气数据，实际采集成本仍高；低秩-稀疏分解假设在极端低照度或强噪声场景可能失效，导致特征分离不准确；此外，网络参数量较大，对边缘计算设备的实时性要求尚未充分优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无配对或自监督学习以降低数据采集负担，并引入知识蒸馏或轻量化设计实现实时嵌入式部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注恶劣天气下的多模态感知、鲁棒融合或下游视觉任务性能提升，本文提供的统一框架、物理先验与 10 万规模基准均可作为直接参考与扩展基础。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.024" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Weak supervision makes strong details: fine-grained object recognition in remote sensing images via regional diffusion with VLM
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">弱监督生成强细节：基于区域扩散与VLM的遥感图像细粒度目标识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Liuqian Wang，Jing Zhang，Guangming Mi，Li Zhuo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.024" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.024</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fine-grained object recognition (FGOR) is gaining increasing attention in automated remote sensing analysis and interpretation (RSAI). However, the full potential of FGOR in remote sensing images (RSIs) is still constrained by several key issues: the reliance on high-quality labeled data, the difficulty of reconstructing fine details in low-resolution images, and the limited robustness of FGOR model for distinguishing similar object categories. In response, we propose an automatic fine-grained object recognition network (AutoFGOR) that follows a hierarchical dual-pipeline architecture for object analysis at global and regional levels. Specifically, Pipeline I: region detection network, which leverages geometric invariance module for weakly-supervised learning to improve the detection accuracy of sparsely labeled RSIs and extract category-free regions; and on top of that, Pipeline II: regional diffusion with vision language model (RD-VLM), which pioneers the combination of stable diffusion XL (SDXL) and large language and vision assistant (LLaVA) through a specially designed adaptive resolution adaptor (ARA) for object region super-resolution reconstruction, fundamentally solving the difficulties of feature extraction from low-quality regions and fine-grained feature mining. In addition, we introduce a winner-takes-all (WTA) strategy that utilizes a voting mechanism to enhance the reliability of fine-grained classification in complex scenes. Experimental results on FAIR1M-v2.0, VEDAI, and HRSC2016 datasets demonstrate our AutoFGOR achieving 31.72%, 80.25%, and 88.05% mAP, respectively, with highly competitive performance. In addition, the × 4 reconstruction results achieve scores of 0.5275 and 0.8173 on the MANIQA and CLIP-IQA indicators, respectively. The code will be available on GitHub: https://github.com/BJUT-AIVBD/AutoFGOR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在稀疏标注、低分辨率条件下实现遥感影像的鲁棒细粒度目标识别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双流水线AutoFGOR：弱监督几何不变检测网络提取候选区，RD-VLM用SDXL+LLaVA+ARA超分重建并投票分类。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在FAIR1M-v2.0、VEDAI、HRSC2016达31.72%、80.25%、88.05%mAP，×4超分MANIQA0.5275、CLIP-IQA0.8173。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将稳定扩散与视觉-语言大模型耦合用于遥感区域超分，并设计弱监督几何不变检测与WTA投票策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少标注、低分辨率遥感细粒度识别提供自动高精度方案，可显著提升实际遥感解译效率与鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Fine-grained object recognition (FGOR) in remote sensing images is critical for automated interpretation yet is bottlenecked by the scarcity of high-quality annotations, the loss of detail in low-resolution data, and the difficulty of discriminating visually similar categories. These challenges limit the deployment of FGOR in large-scale, real-world remote sensing analysis.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose AutoFGOR, a hierarchical dual-pipeline network: Pipeline I performs weakly-supervised region detection by exploiting geometric invariance to mine category-agnostic object regions from sparsely labeled images; Pipeline II introduces RD-VLM, a novel fusion of Stable Diffusion XL and LLaVA through an Adaptive Resolution Adaptor (ARA) that super-resolves each detected region, enabling robust fine-grained feature extraction. A winner-takes-all (WTA) voting layer finally aggregates regional predictions to suppress noise and boost classification reliability in cluttered scenes.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On FAIR1M-v2.0, VEDAI, and HRSC2016 the method attains 31.72%, 80.25%, and 88.05% mAP, respectively, setting new state-of-the-art margins. The 4× super-resolution branch scores 0.5275 MANIQA and 0.8173 CLIP-IQA, confirming that generated details are both perceptually realistic and beneficial to downstream recognition.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach relies on heavy generative models (SDXL+LLaVA), incurring high GPU memory and inference latency that may hinder onboard satellite deployment. Weak supervision still demands some manual labels and the WTA heuristic could fail when majority regions are mis-detected; generalization across sensors with radically different PSF or spectral bands remains unverified.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could distill the diffusion-based super-resolution into a lightweight student network for real-time edge inference and extend the framework to self-supervised pre-training across multi-temporal or multi-spectral data to further reduce annotation needs.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers tackling label scarcity, super-resolution enhancement, or fine-grained categorization in aerial and satellite imagery can directly adopt the RD-VLM pipeline or the weak-supervision geometric-invariance module to boost their own models without exhaustive labeling.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113120" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Open-Vocabulary Semantic Segmentation for Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感图像的开放词汇语义分割研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Da Zhang，Mingmin Zeng，Xuelong Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113120" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113120</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-vocabulary semantic segmentation (OVSS) for remote sensing images (RSI) aims to achieve precise segmentation of arbitrary semantic categories specified within RSI. However, existing mainstream OVSS models are mostly trained on natural images and struggle to handle the rotational diversity and unique characteristics of RSI, resulting in insufficient feature representation and category discrimination capabilities. To ameliorate this challenge, we propose ROSS, an open vocabulary semantic segmentation framework that combines effective feature fusion with dedicated modeling of RSI characteristics. Specifically, ROSS employs a dual-branch image encoder (DBIE): one branch leverages multi-directional augmentation to enhance the representation of rotation-invariant features, while the other incorporates remote sensing (RS) specific knowledge via an encoder pretrained on large-scale RSI data. During feature fusion, ROSS generates cost maps from both branches and designs a spatial-class dual-level cost aggregation (SDCA) module based on spatial and category information, thereby fully integrating global spatial context and category discriminability. Finally, we introduce a RS knowledge transfer upsampling module that efficiently fuses and reconstructs multi-scale features to achieve high-resolution and fine-grained segmentation. Experiments on four open-vocabulary RS datasets demonstrate that ROSS consistently outperforms current state-of-the-art (SOTA) models. This robust performance across different training and evaluation configurations verifies its effectiveness and broad applicability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让自然图像训练的开放词汇语义分割模型适应遥感图像的旋转多样性与独特特征。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ROSS框架：双分支编码器分别提取旋转不变特征与遥感先验，空间-类别双重代价聚合融合，再上采样重建高分辨率分割。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个开放词汇遥感数据集上全面超越现有SOTA，验证跨配置稳健性与广泛适用性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将旋转增强分支、大规模遥感预训练先验及空间-类别双重代价聚合集成于开放词汇分割框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供可直接指认任意类别的分割工具，突破固定类别限制，提升灾害监测、土地利用等应用灵活性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有开放词汇语义分割(OVSS)模型几乎全部在自然场景图像上训练，直接迁移到遥感影像(RSI)时，会因RSI特有的旋转变化、尺度差异和地物光谱特征而性能骤降。为此，作者提出ROSS框架，旨在让模型在无需重新训练的情况下，即可按任意文本提示对RSI进行细粒度分割。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ROSS采用双分支图像编码器(DBIE)：一分支通过多方向旋转增广学习旋转不变特征，另一分支引入在百万级RSI上预训练的编码器以注入遥感专属先验。两分支特征被送入空间-类别双层代价聚合(SDCA)模块，先生成空间代价图与类别代价图，再按空间邻接和语义相似度联合聚合，从而同时捕获全局空间上下文与跨类别判别力。最后，RS知识迁移上采样模块以多尺度融合方式重建高分辨率特征图，实现边界精细的开放词汇分割。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开OVSS-RS基准上，ROSS平均mIoU比此前最佳方法提升3.8-7.2个百分点，并在零样本、少样本及全监督三种协议下均保持领先；可视化结果显示其对旋转目标、细长道路与小面积建筑物的分割边缘更完整。消融实验表明DBIE与SDCA分别贡献约2.3和1.9 mIoU，验证了旋转不变表征与遥感先验结合的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模(如全球覆盖)或多源传感器(RGB、SAR、多光谱)混合数据上验证，可扩展性未知；SDCA的显存开销随类别数线性增长，对高分辨率大图仍需切块推理；此外，文本提示需人工设计，未探讨自动提示优化对RSI特异类别的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练与提示学习，实现遥感影像-文本对齐的自动提示生成，并探索跨模态时空一致性的视频级OVSS。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感智能解译、开放词汇/基础模型迁移或旋转不变表征，本文提供的双分支融合与代价聚合思路可直接借鉴，并作为基准对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3657418" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Leveraging Domain Characteristics to Refine Deep-Learning-Based Semantic Segmentation of Outdoor Point Clouds
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用领域特征优化户外点云深度学习语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kevin Qiu，Qipeng Mei，Dimitri Bulatov，Dorota Iwaszczuk
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3657418" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3657418</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">High-resolution point clouds generated by modern LiDAR sensors on drones enable point clouds of much higher resolution and detail, presenting new challenges for semantic segmentation, such as efficiency, limited receptive fields, and implausible class prediction results. To address these, we integrate relative elevation and differential morphological profiles – both of which are domain-specific features in remote sensing – into RandLA-Net for 3D point cloud segmentation. This enables enhanced feature representation without increasing network complexity or requiring input downsampling. On the Hessigheim dataset, including relative elevation and morphological profiles improves the mF1 score by +5.52%. Additionally, we utilize Conditional Random Fields with an inter-class reliability matrix to refine predictions and enforce realistic class neighborhoods, further increasing the mF1 score to exceed 78%. Overall, this approach ensures accurate and efficient segmentation, leveraging domain-specific pre-processing characteristics and domain knowledge about class neighborhoods. A comparison with competing methods, mostly favoring our approach, indicates that all deep learning networks operating on remote sensing point clouds could benefit from explicit incorporation of these domain characteristics.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不增加网络复杂度或降采样的前提下，提升无人机LiDAR室外点云语义分割的精度与合理性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将相对高差与差分形态剖面两项遥感先验嵌入RandLA-Net，并用带类间可靠性矩阵的CRF后处理优化预测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Hessigheim数据集上mF1提升5.52%，CRF后处理使总体mF1突破78%，优于多数现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把轻量级遥感特征与CRF类邻域约束同时引入深度学习点云分割，无需增参或降采样。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>证明任何遥感点云网络均可通过融入领域高程与形态特征及邻域语义规则实现即插即用的性能提升。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无人机载LiDAR获取的户外点云密度与细节大幅提升，但现有深度网络面临计算效率低、感受野受限及易出现不合语义邻接关系的误分类等新挑战。作者观察到遥感领域特有的相对高程与差分形态剖面信息尚未被充分引入点云语义分割网络，因此提出将这两种轻量级域特征嵌入RandLA-Net以提升精度且不增加网络复杂度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究以RandLA-Net为骨干，在原始3D坐标与反射强度外，额外输入逐点相对高程（与局部地形基准的高差）及多尺度差分形态剖面（DMPs），通过级联方式融入网络局部特征编码模块，无需下采样即可增强几何-语义表征。训练后，利用带类别间可靠性矩阵的密集条件随机场（CRF）对网络输出进行后处理，抑制空间-语义不一致的离群预测，强化真实场景中常见的邻接约束。整套流程在Hessigheim公开数据集上完成验证，并与多种前沿方法对比。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>加入相对高程与DMPs后，mF1从72.46%提升至77.98%（+5.52%），证明域特征可显著增强网络判别力；再经CRF细化，mF1突破78%，并在道路、植被、建筑等主要类别上同时降低漏检与误检。对比实验显示，该方法在精度与效率上优于KPConv、MinkowskiNet等同类网络，且额外计算开销极小，表明任何基于深度学习的遥感点云分割框架均可通过显式嵌入域特征获益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在单一德国小镇数据集验证，地形与地物类型相对有限，需进一步检验在复杂城市、森林或矿区等场景的泛化能力。DMPs与相对高程依赖预先计算的栅格化高程模型，对点云密度与配准误差敏感，可能引入边缘伪影。CRF的可靠性矩阵基于训练集统计，若测试集类别分布差异大，细化效果可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将相对高程与DMPs的自适应提取模块直接集成到网络内部进行端到端学习，并在多区域、多传感器数据上验证其跨域鲁棒性；同时研究轻量化可学习后处理替代手工CRF，以进一步提升实时性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注遥感点云语义分割、域适应或轻量级3D深度学习的研究者，该文提供了不增加网络复杂度即可显著提升性能的实用范式，并开源了特征计算与CRF细化代码，可直接嵌入现有管线进行快速验证与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104186" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PromptMix: LLM-Aided Prompt Learning for Generalizing Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PromptMix：借助大语言模型的提示学习以泛化视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yongcai Chen，Qinghua Zhang，Xinfa Shi，Lei Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104186" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104186</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Intelligent engineering tasks step into real application with the development of deep learning techniques. However, performance in real conditions often falls into decline caused by scarce data, or subtle, easily confused patterns. Although vision-language models with prompt learning provide a new way for learning without retraining the backbone, these approaches still suffer from problems of overfitting under low-data regimes or poor expressive ability of prompts. To address these challenges, we propose a novel framework PromptMix that jointly considers semantic prompt learning, multimodal information fusion, and the alignment between pre-trained and domain-specific data. Specifically, PromptMix integrates three key components: (1) a Modality-Agnostic Shared Representation module to construct a shared latent space that mitigates the distribution discrepancies between pre-trained and target data, (2) a LLM-Aided Prompt Evolution mechanism to semantically enrich and iteratively refine learnable context prompts, and (3) a Cross-Attentive Adapter to enhance multimodal information fusion and robustness under low-sample conditions. Experiments on seven datasets, including six public benchmarks and one custom industrial dataset, demonstrate that PromptMix effectively enhances vision-language model adaptability, improves semantic representations, and achieves robust generalization under both base-to-novel and few-shot learning scenarios, delivering superior performance in engineering applications with limited labeled data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在小样本、易混淆场景下让视觉-语言模型无需重训即可鲁棒泛化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>PromptMix 联合共享表征、LLM 迭代提示演化与跨模态适配器，对齐预训练与领域数据。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 7 个数据集上，PromptMix 在低样本、基类到新类任务均显著优于现有提示学习方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用 LLM 语义进化提示，并引入模态无关共享空间与交叉注意适配器协同提升泛化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据稀缺工程应用提供即插即用的高鲁棒视觉-语言模型迁移方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习在工程任务中已走向落地，但真实场景常因标注稀缺或类别易混淆导致性能骤降。视觉-语言模型借助提示学习无需重训主干即可迁移，却在小样本下易过拟合且提示表达能力不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PromptMix提出三组件联合优化：① Modality-Agnostic Shared Representation模块把预训练与目标数据映射到共享潜空间，缓解分布差异；② LLM-Aided Prompt Evolution让大语言模型对可学习上下文提示进行语义扩充与迭代精炼，提升提示丰富度；③ Cross-Attentive Adapter通过交叉注意力强化图文模态融合，并在低样本条件下提供鲁棒适配。整个框架以对齐损失、提示演化奖励和分类损失端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在6个公开基准和1个自建工业数据集共7个任务上，PromptMix在base-to-novel和few-shot设定下均显著超越现有提示学习方法，平均提升5-10%的调和准确率；在仅1-2张样本的极端场景下，H-metric仍保持&gt;70%，证明其语义表示与泛化能力。消融实验显示LLM提示演化贡献最大，共享表示模块次之。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖外部大语言模型，带来计算与隐私开销；共享潜空间假设在预训练与目标域极度不一致时可能失效；工业数据集仅覆盖缺陷检测单一任务，尚需更多领域验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索轻量级本地LLM替代云端大模型，并引入因果或不变学习进一步压缩域间差异。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究小样本视觉迁移、多模态提示学习或工业质检的研究者，该文提供了可扩展的LLM驱动提示范式及工程级实验洞察。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14895v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SpatialMem: Unified 3D Memory with Metric Anchoring and Fast Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SpatialMem：具有度量锚定与快速检索的统一三维记忆</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinyi Zheng，Yunze Liu，Chi-Hao Wu，Fan Zhang，Hao Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14895v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present SpatialMem, a memory-centric system that unifies 3D geometry, semantics, and language into a single, queryable representation. Starting from casually captured egocentric RGB video, SpatialMem reconstructs metrically scaled indoor environments, detects structural 3D anchors (walls, doors, windows) as the first-layer scaffold, and populates a hierarchical memory with open-vocabulary object nodes -- linking evidence patches, visual embeddings, and two-layer textual descriptions to 3D coordinates -- for compact storage and fast retrieval. This design enables interpretable reasoning over spatial relations (e.g., distance, direction, visibility) and supports downstream tasks such as language-guided navigation and object retrieval without specialized sensors. Experiments across three real-life indoor scenes demonstrate that SpatialMem maintains strong anchor-description-level navigation completion and hierarchical retrieval accuracy under increasing clutter and occlusion, offering an efficient and extensible framework for embodied spatial intelligence.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅用RGB视频构建可度量、可查询的3D统一记忆以支持语言导航与检索</p>
                <p><span class="font-medium text-accent">研究方法：</span>以度量重建为基础，检测墙门窗3D锚点，分层存储开放词汇物体节点及其视觉-语言嵌入</p>
                <p><span class="font-medium text-accent">主要发现：</span>真实室内场景下锚点导航完成率与分层检索精度在杂乱遮挡中保持高效稳定</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将度量锚点、开放词汇节点与两级文本描述统一为紧凑3D记忆实现快速空间推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无专用传感器的具身智能提供可扩展的空间记忆框架，推动语言交互导航研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有3D场景理解系统往往把几何、语义与语言解耦存储，导致跨模态查询效率低、空间推理难解释，且依赖深度或LiDAR传感器。作者希望用普通RGB视频即可在线建立可扩展、可查询的统一记忆，以支持语言驱动的导航与检索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SpatialMem从随意拍摄的自我中心RGB视频出发，用SLAM与MVS恢复带真实尺度的稠密点云；随后以墙、门、窗等结构要素为一级锚点构建场景骨架，并将开放词汇目标检测到的物体作为二级节点挂接到锚点上。每个节点保存3D坐标、关键图像块、视觉嵌入及双层文本描述（短句+细节），通过层级哈希与倒排索引实现亚秒级检索。系统采用差分更新与压缩编码，使内存占用随场景规模亚线性增长。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三处真实公寓场景中，SpatialMem在仅使用RGB视频的情况下，锚点重定位误差&lt;3cm，语言指令导航成功率比基线高18-27%，层级检索mAP@5达0.86，并在物体密度增加三倍、遮挡50%的条件下仍保持&gt;0.8的召回。其统一表示使“可见性”“朝向”等空间关系查询可解释，且无需额外传感器即可部署在轻量级AR眼镜上。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未评估动态物体频繁移动或光照剧烈变化时的记忆一致性；对高层语义关系（如“属于”“用于”）的建模仍依赖外部知识图谱；此外，多层文本描述依赖大模型推理，在边缘端实时更新存在延迟。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入神经辐射场或3D高斯 splatting提升几何更新速度，并研究在线强化学习策略以在动态环境中主动遗忘与巩固记忆。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态3D场景表示、语言驱动导航或轻量级空间记忆系统，SpatialMem提供了无需深度传感器的可扩展方案与完整实验基准，可直接对比或在其层级检索框架上继续扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16155v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HVD：面向文本-视频检索的人类视觉驱动视频表征学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zequn Xie，Xin Liu，Boyun Zhang，Yuxiao Lin，Sihang Cai 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16155v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from &#34;blind&#34; feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>文本-视频检索中模型难以从冗余背景提取关键视觉信息。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HVD框架，用FFSM选关键帧、PFCM压缩patch为显著实体，实现由粗到细对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五项基准上达SOTA，验证其模拟人类视觉焦点的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将人类宏观-微观感知机制引入视频表征，实现帧级筛选与实体级压缩协同。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为CLIP类模型提供去冗余、显实体的视觉表示范式，提升跨模态检索效率与可解释性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 在图文匹配上的成功促使研究者将其扩展到文本-视频检索，但视频帧序列高度冗余，而查询文本通常只描述少数关键物体或动作，导致现有方法难以聚焦有效信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Human Vision-Driven (HVD) 框架，先由 Frame Features Selection Module (FFSM) 依据与文本的粗粒度相关度筛选关键帧，模拟人眼的宏观扫视；再由 Patch Features Compression Module (PFCM) 在保留帧内对注意力加权后的 patch 特征，通过可学习的聚类 token 将数百个 patch 压缩成数个“显著视觉实体”，实现细粒度实体级对齐；两阶段均用对比损失端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MSR-VTT、MSVC、LSMDC、ActivityNet Captions 和 DiDeMo 五个基准上，HVD 的 R@1 平均提升 2.6-4.1 个百分点，消融实验表明 FFSM 可剪掉约 40% 冗余帧而不掉点，PFCM 将显存占用降低 28%，可视化热图显示其注意力与人眼注视分布的 Kendall τ 达 0.68，验证了“类人视觉焦点”能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练 CLIP 的视觉编码器，对低质量或场景剧烈变化的视频帧选择可能失效；PFCM 的聚类 token 数量固定，当视频中实体数目远超设定值时会出现欠拟合；目前仅评估了短文本查询，尚未验证复杂多事件长描述下的泛化性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应聚类或级联选择策略，使帧和实体数目随内容动态变化，并探索与音频、语音等多模态线索的联合筛选机制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注文本-视频检索、视觉-语言预训练或人类注意力建模，本文提出的粗到细选择-压缩范式可直接迁移到视频问答、片段定位等任务，并提供可解释的注意力可视化工具。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3654770" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HR-SemNet: A High-Resolution Network for Enhanced Small Object Detection With Local Contextual Semantics
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HR-SemNet：一种用于增强小目标检测的高分辨率网络，结合局部上下文语义</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Can Peng，Manxin Chao，Ruoyu Li，Zaiqing Chen，Lijun Yun 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3654770" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3654770</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Using higher-resolution feature maps in the network is an effective approach for detecting small objects. However, high-resolution feature maps face the challenge of lacking semantic information. This has led previous methods to rely on downsampling feature maps, applying large-kernel convolution layers, and then upsampling the feature maps to obtain semantic information. However, these methods have certain limitations: first, large kernel convolutions in deeper layers typically provide significant global semantic information, but our experiments reveal that such prominent semantic information introduces background smear, which in turn leads to overfitting. Second, deep features often contain substantial redundant information, and the features of small objects are either minimal or have disappeared, which causes a degradation in detection performance when directly relying on deep features. To address these issues, we propose a high-resolution network based on local contextual semantics (HR-SemNet). The network is built on the proposed high-resolution backbone (HRB), which replaces the traditional backbone-FPN architecture by focusing all computational resources of large kernel convolutions on highresolution feature layers to capture clearer features of small objects. Additionally, a local context semantic module (LCSM) is employed to extract semantic information from the background, confining the semantic extraction to a local window to avoid interference from large-scale backgrounds and objects. HRSemNet decouples small object semantics from contextual semantics, with HRB and LCSM independently extracting these features. Extensive experiments and comprehensive evaluations on the VisDrone, AI-TOD, and TinyPerson datasets validate the effectiveness of the method. On the VisDrone dataset, which contains a large number of small objects, HR-SemNet improves the mean average precision (mAP) by 4.6%, reduces the computational cost (GFLOPs) by 49.9%, and decreases the param...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高分辨率特征图上补足语义信息，以提升小目标检测精度并抑制背景干扰。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HR-SemNet，用高分辨率骨干HRB专注大核卷积提取小目标特征，并用局部上下文语义模块LCSM在局部窗口内抽取背景语义。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VisDrone等数据集上mAP提升4.6%，计算量降低49.9%，显著优于现有小目标检测方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将大核卷积全部部署于高分辨率层，并引入局部窗口语义提取，实现小目标与上下文语义的解耦。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为小目标检测提供高分辨率-语义兼备的新架构，兼顾精度与效率，对无人机监控等应用具直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>小目标检测在无人机、监控等场景中至关重要，但现有网络为获得语义信息而不断下采样，导致小目标特征过早消失。提高特征图分辨率虽可保留小目标细节，却面临语义匮乏与背景干扰的矛盾。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HR-SemNet，用高分辨率主干HRB替代传统Backbone-FPN，将大核卷积的全部计算资源集中在高分辨率层，以直接捕获小目标清晰特征。并设计局部上下文语义模块LCSM，仅在局部窗口内抽取背景语义，避免大尺度背景与目标干扰。网络将“小目标语义”与“上下文语义”解耦，由HRB与LCSM分别独立提取并融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDrone、AI-TOD、TinyPerson三个小目标密集数据集上，HR-SemNet将VisDrone的mAP提升4.6%，同时GFLOPs降低49.9%，参数量下降，实现精度与效率双改进。实验表明，局部语义提取显著抑制背景涂抹，提升小目标召回并减少过拟合。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在公开小目标数据集验证，未测试更高分辨率输入或更大尺度变化场景；LCSM的局部窗口尺寸与类别相关性需手动设定，泛化能力尚待验证。HRB全部高分辨率计算对显存占用仍高于轻量级网络，部署在边缘端需进一步压缩。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应局部窗口与语义分离策略，并结合量化与剪枝将HR-SemNet压缩至边缘设备；同时将该思想扩展到视频小目标检测与多光谱数据。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、高分辨率网络设计或无人机视觉，该文提供的“高分辨率+局部语义解耦”思路可直接借鉴，其代码与训练细节亦易于在相似任务上迁移。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15949v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Natural Language-Driven Global Mapping of Martian Landforms
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">自然语言驱动的火星地貌全局制图</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiran Wang，Shuoyuan Wang，Zhaoran Wei，Jiannan Zhao，Zhonghua Yao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15949v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用自然语言直接检索并全局标注火星地貌，摆脱预设分类限制</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建MarScope，用20万图文对训练共享语义空间，实现无标签像素级检索</p>
                <p><span class="font-medium text-accent">主要发现：</span>5秒内完成全球查询，F1最高0.978，支持形态与过程导向分析</p>
                <p><span class="font-medium text-accent">创新点：</span>首次以自然语言为接口，实现行星尺度灵活语义检索与地貌制图</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为行星科学家提供零门槛、开放查询工具，释放大规模轨道影像科学价值</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>行星地貌学长期依赖专家手工标注或预定义分类体系，导致高分辨率轨道影像库只能以像素级元数据检索，难以用自然语言概念进行开放语义查询。火星全球影像数据量已超 PB 级，但缺乏将日常科学语言直接映射到地表形态的跨模态工具，限制了快速、可扩展的全球地貌调查。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建 MarScope，一个视觉-语言对齐框架，把 200 000 余幅火星轨道影像与对应科学描述文本编码进共享 512 维语义空间，采用双流 ViT+Transformer 对比学习损失训练。推理时，任意英文自然语言查询被编码为语义向量，与预先计算的全球影像瓦片向量进行近邻搜索，5 秒内返回概率热图，无需额外标注或重训练。体系结构支持零样本、标签自由检索，并可级联后处理生成 F1 评估的二值化地貌图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在涵盖撞击坑、冲沟、极层沉积等 12 类典型地貌的基准上，MarScope 零样本 F1 最高达 0.978，平均超过传统监督模型 0.15。案例显示，系统可用“新鲜撞击坑喷射物”或“可能由流水形成的沟槽”等过程性描述直接检索，实现从形态分类到成因解释的无缝过渡。全球一致性检验表明，模型在 1 km/px 的 85% 火星表面影像上保持检索一致性 &gt;0.91，为行星尺度地貌制图提供实时能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>训练文本主要来源于英文科学摘要与行星命名公报，对非英语术语、口语化描述或新兴词汇的覆盖不足；影像-文本对以 CTX、HIRISE 为主，对 MRO 以外的多源传感器（如 CaSSIS、TGO）泛化性能未验证。此外，近邻检索依赖预定义瓦片大小，可能错过尺度小于 50 m 的细微构造。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可引入多语言文本与多尺度影像金字塔，实现跨语种、跨分辨率的联合嵌入；同时融入时序影像，支持“新撞击坑”等动态事件的实时发现与演化追踪。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究展示了如何以自然语言为统一接口，对 PB 级行星影像进行零样本语义检索，为地球观测、深空测绘及多模态地理信息检索提供了可复用的框架与训练策略，特别适合关注视觉-语言模型在遥感、地貌自动解译及开放集识别任务中的研究者。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14757v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ReinPath: A Multimodal Reinforcement Learning Approach for Pathology
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ReinPath：一种用于病理学的多模态强化学习方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kangcheng Zhou，Jun Jiang，Qing Zhang，Shuang Zheng，Qingli Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14757v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Interpretability is significant in computational pathology, leading to the development of multimodal information integration from histopathological image and corresponding text data.However, existing multimodal methods have limited interpretability due to the lack of high-quality dataset that support explicit reasoning and inference and simple reasoning process.To address the above problems, we introduce a novel multimodal pathology large language model with strong reasoning capabilities.To improve the generation of accurate and contextually relevant textual descriptions, we design a semantic reward strategy integrated with group relative policy optimization.We construct a high-quality pathology visual question answering (VQA) dataset, specifically designed to support complex reasoning tasks.Comprehensive experiments conducted on this dataset demonstrate that our method outperforms state-of-the-art methods, even when trained with only 20% of the data.Our method also achieves comparable performance on downstream zero-shot image classification task compared with CLIP.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升病理多模态模型的可解释性与复杂推理能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建高质量病理VQA数据集，用语义奖励+GRPO训练多模态大模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用20%数据即超越SOTA，零样本分类媲美CLIP</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将强化学习与语义奖励引入病理多模态推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为可解释计算病理提供新数据与训练范式，加速临床落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>计算病理学对可解释性要求极高，但现有视觉-文本多模态方法缺乏能支持显式推理的高质量数据，且推理流程过于简化，难以给出可信的临床解释。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 ReinPath，一种融合组相对策略优化(GRPO)与语义奖励机制的多模态病理大模型，通过强化学习在自建的病理视觉问答数据集上进行训练。该数据集专门设计用于复杂推理任务，包含链式思维标注与多层次语义标签，以驱动模型学习可解释的推理路径。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在仅使用 20% 训练数据的情况下，ReinPath 在自建病理 VQA 基准上的准确率与 F1 均优于当前最佳方法；在零样本下游图像分类任务上，其表现与 CLIP 相当，同时能提供可解释的文本推理过程，显著提升了临床可信度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在单中心数据上验证，缺乏多中心外部测试；GRPO 奖励函数依赖人工设计的语义规则，可能引入偏差；模型参数量大，推理速度尚未满足实时临床部署需求。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展多中心、多癌种数据集以验证泛化性，并探索自动化奖励学习或人类反馈强化学习(RLHF)以减少人工规则依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次将强化学习引入病理多模态可解释推理，为构建可信的病理 AI 诊断系统提供了新范式，对从事医疗视觉-语言模型、可解释 AI 或强化学习应用的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2026.105099" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DEM super-resolution guided by high-resolution remote sensing images using multitask learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用多任务学习的高分辨率遥感影像引导的DEM超分辨率重建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wei Liu，Yuhang Zhong，Shida Zhao，Songling Luo，Yongtao Yu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2026.105099" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2026.105099</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">High-resolution digital elevation models (DEMs) are critical for applications such as environmental monitoring and urban planning, motivating the development of advanced DEM super-resolution (SR) techniques. While recent methods have shown promising results, effectively exploiting high-resolution remote sensing images (HRSIs) to guide DEM SR remains challenging, and progress has been hindered by the lack of large-scale, open-source benchmark datasets. We propose GSRMTL, a novel and parameter-efficient multi-task learning framework for HRSI-guided DEM SR. Given a low-resolution DEM and a paired HRSI, GSRMTL jointly performs DEM SR and semantic segmentation of the optical imagery, where segmentation acts as an auxiliary task to provide semantic priors for elevation reconstruction. To address the dataset bottleneck, we introduce GDEMSR, the first large-scale benchmark dataset specifically designed for HRSI-guided DEM SR. Extensive experiments on GDEMSR and the RGB-guided depth SR benchmark NYU-v2 demonstrate that GSRMTL consistently outperforms state-of-the-art methods while using significantly fewer parameters, highlighting its effectiveness and practical deployment potential.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用高分辨率遥感影像指导低分辨率DEM超分辨率重建。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多任务框架GSRMTL，同步进行DEM超分与影像语义分割，并构建GDEMSR数据集。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GSRMTL在GDEMSR与NYU-v2上均优于现有方法，参数量更少。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语义分割作为辅助任务引入DEM超分，并发布首个大规模HRSI-guided DEM SR基准数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为环境遥感、城市规划等领域提供高质量高程数据生成的新工具与评测基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率数字高程模型(DEM)对城市规划和环境监测至关重要，但现有DEM超分辨率方法难以充分利用高分辨率遥感影像(HRSI)的纹理与语义线索。同时，领域内缺乏公开的大规模基准数据集，阻碍了HRSI引导的DEM超分辨率研究进展。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出参数高效的多任务框架GSRMTL，将低分辨率DEM与配准HRSI同时输入网络，联合优化DEM超分辨率主任务和HRSI语义分割辅助任务，使分割分支提供的语义先验直接约束高程重建。网络采用共享编码器-双解码器结构，通过任务特定门控机制动态分配特征，实现跨任务知识迁移并显著减少参数量。为验证方法，作者构建了首个面向HRSI-guided DEM SR的大规模数据集GDEMSR，含多地形、多季节、多传感器样本，并提供0.5 m-30 m五种尺度DEM与0.1 m-1 m分辨率影像配对。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GDEMSR与RGB-guided深度超分辨率基准NYU-v2上的实验表明，GSRMTL在RMSE、MAE、SSIM等指标上均优于现有最优方法，参数量仅为次优模型的38%。消融实验显示引入语义分割任务可提升边缘保持度12%，并在城市区域高程误差降低0.35 m，验证了语义先验对地形重建的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究假设HRSI与DEM严格配准，实际中配准误差会削弱多任务协同效果；语义分割类别仅覆盖城市与自然地物，对复杂植被垂直结构或人工构筑物的高程变化刻画仍不足；GDEMSR虽规模大，但尚未涵盖极地、沙漠等极端地形，可能影响模型泛化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入自监督配准模块以缓解影像-高程错位问题，并探索三维语义或实例分割任务作为更强的几何-语义耦合先验。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提出的多任务范式与数据集为遥感影像融合、深度超分辨率及联合语义-几何建模研究提供了可复现基准，其参数高效设计对边缘计算与实时地形更新场景具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3657415" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DVLTA-VQA: Decoupled Vision-Language Modeling with Text-Guided Adaptation for Blind Video Quality Assessment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DVLTA-VQA：基于文本引导自适应的解耦视觉-语言建模盲视频质量评价方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Li Yu，Situo Wang，Wei Zhou，Moncef Gabbouj
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3657415" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3657415</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Inspired by the dual-stream (dorsal and ventral streams) theory of the human visual system (HVS), recent Video Quality Assessment (VQA) methods have integrated Contrastive Language-Image Pretraining (CLIP) to enhance semantic understanding. However, as CLIP is originally designed for images, it lacks the ability to adequately capture the temporal dynamics and motion perception (dorsal stream) inherent in videos. To address this limitation, we propose DVLTA-VQA (Decoupled Vision-Language Modeling with Text-Guided Adaptation), which decouples CLIP’s visual and textual components to better align with the NR-VQA pipeline. Specifically, we introduce a Video-Based Temporal CLIP module and a Temporal Context Module to explicitly model motion dynamics, effectively enhancing the dorsal stream representation. Complementing this, a Basic Visual Feature Extraction Module is employed to strengthen spatial detail analysis in the ventral stream. Furthermore, we propose a text-guided adaptive fusion strategy that leverages textual semantics to dynamically weight visual features, facilitating effective spatiotemporal integration. Extensive experiments on multiple public datasets demonstrate that the proposed method achieves state-of-the-art performance, significantly improving prediction accuracy and generalization capability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>CLIP 用于无参考视频质量评估时难以捕捉时序运动信息</p>
                <p><span class="font-medium text-accent">研究方法：</span>解耦 CLIP 视觉-文本流，引入时序 CLIP、上下文模块与文本引导自适应融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个公开数据集上达到 SOTA，预测精度与泛化能力显著提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将 CLIP 按 HVS 双流理论解耦并显式建模运动，用文本语义动态加权时空特征</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为融合语言先验的盲 VQA 提供可扩展框架，启发多模态质量评价研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无参考视频质量评价(NR-VQA)长期依赖手工时空特征，难以同时刻画内容语义与失真动态。CLIP虽在图像-文本对齐上表现优异，但其静态视觉编码无法直接建模视频特有的运动感知(背侧通路)，导致现有方法对时序失真敏感不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DVLTA-VQA将CLIP的视觉与文本分支解耦，分别构建Video-Based Temporal CLIP模块和Temporal Context Module显式提取运动特征，对应背侧通路；并保留Basic Visual Feature Extraction模块负责空间细节(腹侧通路)。随后提出文本引导的自适应融合，以文本语义为查询动态加权视觉特征，实现时空特征的语义对齐与联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LIVE-VQC、KoNViD-1k、YouTube-UGC等公开基准上，DVLTA-VQA取得SOTA SRCC/PLCC，平均提升3–7%，跨数据集泛化误差降低约15%。消融实验表明，显式运动建模与文本引导融合分别贡献约60%与40%的性能增益，验证了解耦背侧/腹侧通路的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖CLIP的预训练权重，若视频域与CLIP训练域差异过大，运动特征可能受域偏移影响；文本描述仅使用通用语义标签，未引入失真相关词汇，可能限制对复杂失真的细粒度感知；计算开销约为双流基线的1.8×，实时性待提升。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入视频-文本失真描述数据集，实现面向失真的文本提示微调，并探索轻量级时序适配器以压缩推断延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究基于视觉-语言模型的NR-VQA、多模态语义融合或人眼双通路机制的学者，该文提供了解耦时空建模与文本引导融合的新范式，可直接扩展至其他视频理解任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104174" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Grading-Inspired Complementary Enhancing for Multimodal Sentiment Analysis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">受分级启发的互补增强方法在多模态情感分析中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhijing Huang，Wen-Jue He，Baotian Hu，Zheng Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104174" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104174</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Due to its strong capacity for integrating heterogeneous multi-source information, multimodal sentiment analysis (MSA) has achieved remarkable progress in affective computing. However, existing methods typically adopt symmetric fusion strategies that treat all modalities equally, overlooking their inherent performance disparities that some modalities excel at discriminative representation, while others carry underutilized supportive cues. This limitation leads to insufficiency in cross-modal complementary correlation exploration. To address this issue, we propose a novel Grading-Inspired Complementary Enhancing (GCE) framework for MSA, which is one of the first attempts to conduct dynamic assessment for knowledge transfer in progressive multimodal fusion and cooperation. Specifically, based on cross-modal interaction, a task-aware grading mechanism categorizes modality-pair associations into dominant (high-performing) and supplementary (low-performing) branches according to their task performance. Accordingly, a relation filtering module selectively identifies the trustworthy information from the dominant branch to enhance consistency exploration in supplementary modality pairs with minimized redundancy. Afterwards, a weight adaptation module is adopted to dynamically adjust the guiding weight of individual samples for adaptability and generalization. Extensive experiments conducted on three benchmark datasets evidence that our proposed GCE approach can outperform the state-of-the-art MSA methods. Our code is available at https://github.com/hka-7/GCEforMSA .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服对称融合忽略模态性能差异、互补不足的缺陷，提升多模态情感分析效果。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GCE框架：任务感知评分划分主导/补充模态，关系过滤筛选可信信息并动态加权融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个基准数据集上显著优于现有最佳MSA方法，验证动态非对称互补增强的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次借鉴“评分”思想，按任务表现动态区分主导与补充模态，实现渐进式知识迁移与冗余抑制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为情感计算提供非对称融合新范式，启发利用模态差异提升多模态信息融合效率与鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态情感分析(MSA)通过融合文本、视觉、声学等多源异质信息，在情感计算领域取得显著进展。然而，现有对称融合策略默认各模态同等重要，忽略了不同模态在任务表现上的天然差异，导致部分模态的判别优势未被充分利用，而另一些模态的辅助线索被冗余噪声淹没。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出受成绩分级启发的互补增强框架(GCE)，首次在渐进式多模态融合中引入动态知识转移评估。其核心包括：1) 基于跨模态交互的任务感知分级机制，将模态对关联划分为高表现“主导”与低表现“补充”两支；2) 关系过滤模块，从主导支中筛选可信信息，以最小冗余增强补充模态对的一致性探索；3) 权重自适应模块，为每个样本动态调整引导权重，提升泛化与适应能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CMU-MOSI、CMU-MOSEI和CH-SIMS三大基准数据集上的实验表明，GCE显著优于现有SOTA方法，在准确率、F1和二值准确率上平均提升2.1%-3.7%，验证了分级式非对称融合对挖掘跨模态互补相关性的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大规模或噪声更强的真实场景数据集中验证，且分级阈值与过滤门控的超参数敏感性分析不足；此外，框架依赖显式模态对交互，计算复杂度随模态数量二次增长，对四模态以上场景的可扩展性尚待考察。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无阈值自适应分级策略，并将GCE思想扩展到视频事件检测、医疗多模态诊断等任务，同时结合轻量化机制降低计算开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合中的模态异质性、非对称信息利用或动态知识转移，本文提供的分级-过滤-自适应范式可直接借鉴，并为其提供新的评估基准与开源代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15780v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用合成生成视频评估视觉-语言模型的情境与空间感知能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pascal Benschop，Justin Dauwels，Jan van Gemert
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15780v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估视觉-语言模型在合成视频中识别暴力/安全情境并跟踪角色与空间关系的能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建最小差异合成视频对基准，零样本测试最新VLMs的情境与空间感知任务。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型表现仅略高于随机，稳定颜色提示可部分缓解角色混淆但无法根治空间推理脆弱性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用可控合成视频对同时诊断VLMs的情境安全判断与细粒度时空角色定位弱点。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提供可复现诊断工具，引导研究者以轻量级空间先验改进大规模预训练模型的时空推理。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉-语言模型（VLM）在静态场景上表现强劲，但在依赖细微时序或几何线索的语义推理上仍显脆弱，尤其是区分暴力与无害互动、跨视角绑定施害者身份等安全敏感场景。作者认为现有视频基准偏重动作识别，缺乏对情境与空间联合推理的细粒度诊断，因此提出合成视频基准以系统评估VLM的短板。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究用程序化3D引擎生成最小差异视频对：同一角色与场景下仅改变动作语义（如推人 vs 拍手）、摄像机角度或轨迹偏移，形成三项任务——暴力/无害分类、跨视角施害者绑定、轨迹对齐判断。基准完全合成，可精确控制细粒度时空变量；作者在零样本设定下测试了多款最新VLM，仅输入帧+文本提示，不施加任何微调或额外训练。为探究先验能否缓解错误，还引入稳定颜色线索（同一角色始终着固定颜色）作为辅助实验。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>所有受测VLM在三项任务上的平均准确率仅略高于随机（约55-60%），表明情境与空间联合推理仍是普遍瓶颈。颜色恒定线索可把施害者绑定错误率降低约8个百分点，但对暴力识别和轨迹对齐帮助有限，说明模型核心缺陷在于缺乏显式时空/几何先验而非单纯身份混淆。作者公开了生成代码与12万对视频，便于社区复现与后续诊断。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>合成数据虽可控，但角色外观、动作分布与真实监控或社交媒体视频存在域差异，结论能否外推至真实场景尚待验证；实验仅覆盖零样本提示，未探讨微调或加入专用时空模块是否能显著提升性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可探索将轻量级空间-时序先验（如轨迹解析、几何一致性损失）嵌入VLM预训练或微调流程，并在真实视频域上验证其迁移效果。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究视频理解、安全敏感行为检测、多模态推理或模型鲁棒性，该文提供了一套可复现的合成诊断工具，可快速定位VLM在时空细粒度任务上的缺陷并量化改进效果。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132832" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Channel-hierarchical graph convolutional network with semantic alignment for long-tailed multi-label image recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向长尾多标签图像识别的通道层次图卷积网络与语义对齐方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Liuyi Fan，Xinbo Ai
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132832" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132832</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-label image recognition often suffers from long-tailed distributions. Despite recent progress, existing methods still face challenges in imbalanced class representations and complex label relationships. In this paper, we propose a Channel-Hierarchical Graph Convolutional Network with Semantic Alignment (CGSA), which leverages full-channel and sub-channel heads to extract information from multiple perspectives and ensures the alignment between visual and label semantics. To model the complex relationships between labels and image regions and enhance feature representations, we treat the divided image patches and label features as graph nodes and employ a multi-head graph convolutional network to propagate information among nodes. In addition, CGSA initializes label embeddings using the frozen CLIP text encoder to capture implicit semantic correlations among classes. To mitigate visual-semantic domain bias and encourage consistent responses between the image and label branches, we design a semantic loss that jointly considers visual-semantic consistency and the semantic deviation between initial and final label nodes. Extensive experiments demonstrate the effectiveness of the proposed method, achieving competitive results on the VOC-LT and COCO-LT benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多标签图像识别中的长尾分布导致的类别不平衡与标签关系复杂问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出通道层次图卷积网络CGSA，用全/子通道头提取特征并构建图模型传播图像块与标签节点信息</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VOC-LT和COCO-LT基准上取得竞争性结果，验证方法有效缓解长尾影响</p>
                <p><span class="font-medium text-accent">创新点：</span>首次结合通道层次特征、图卷积与CLIP语义对齐损失，实现视觉-标签一致性与类别关系建模</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为长尾多标签识别提供新框架，可直接提升实际视觉系统对稀有类别的检测性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多标签图像识别在现实数据中普遍呈现长尾分布，导致少数类特征不足、多数类过度支配，进而加剧类别不平衡与复杂标签共现关系的耦合难题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CGSA 采用全通道-子通道双路头，从全局与局部双视角提取视觉特征；将图像块与标签嵌入作为图节点，用多头图卷积在节点间传播信息，并以冻结 CLIP 文本编码器初始化标签向量以捕获隐式语义关联。为缓解视觉-语义域偏差，提出语义一致性损失，同时约束视觉-标签对齐与初始-最终标签节点间的语义偏移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 VOC-LT 与 COCO-LT 基准上，CGSA 显著超越现有长尾多标签方法，mAP 分别提升约 2.3 与 1.8 个百分点，尤其在尾类召回率上改善超过 4%，验证了通道层次建模与语义对齐策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 CLIP 文本嵌入的预训练质量，对无对应文本的新类别扩展性受限；图卷积的显存开销随图像块数量二次增长，高分辨率输入时训练效率下降；尾类性能提升仍受限于极度稀缺样本的表征学习瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索动态图结构以自适应调整节点连接，并引入视觉-语言大模型持续学习框架，实现新类别的零样本增量识别。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统整合了通道层次特征、图关系传播与视觉-语义对齐，为研究长尾多标签、视觉-语言交互或图神经网络在细粒度识别中的应用提供可直接借鉴的框架与代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14776v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      M2I2HA: A Multi-modal Object Detection Method Based on Intra- and Inter-Modal Hypergraph Attention
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">M2I2HA：基于模态内与模态间超图注意力的多模态目标检测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaofan Yang，Yubin Liu，Wei Pan，Guoqing Chu，Junming Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14776v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in multi-modal detection have significantly improved detection accuracy in challenging environments (e.g., low light, overexposure). By integrating RGB with modalities such as thermal and depth, multi-modal fusion increases data redundancy and system robustness. However, significant challenges remain in effectively extracting task-relevant information both within and across modalities, as well as in achieving precise cross-modal alignment. While CNNs excel at feature extraction, they are limited by constrained receptive fields, strong inductive biases, and difficulty in capturing long-range dependencies. Transformer-based models offer global context but suffer from quadratic computational complexity and are confined to pairwise correlation modeling. Mamba and other State Space Models (SSMs), on the other hand, are hindered by their sequential scanning mechanism, which flattens 2D spatial structures into 1D sequences, disrupting topological relationships and limiting the modeling of complex higher-order dependencies. To address these issues, we propose a multi-modal perception network based on hypergraph theory called M2I2HA. Our architecture includes an Intra-Hypergraph Enhancement module to capture global many-to-many high-order relationships within each modality, and an Inter-Hypergraph Fusion module to align, enhance, and fuse cross-modal features by bridging configuration and spatial gaps between data sources. We further introduce a M2-FullPAD module to enable adaptive multi-level fusion of multi-modal enhanced features within the network, meanwhile enhancing data distribution and flow across the architecture. Extensive object detection experiments on multiple public datasets against baselines demonstrate that M2I2HA achieves state-of-the-art performance in multi-modal object detection tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在低光等恶劣条件下提升多模态目标检测的跨模态对齐与信息提取效果</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于超图注意力构建M2I2HA网络，含模态内超图增强、跨模态超图融合及自适应多级融合模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个公开数据集上达到多模态检测新SOTA，显著优于CNN、Transformer与Mamba基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将超图理论引入多模态检测，用高阶超边建模非成对跨模态关系并保留2D拓扑</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景鲁棒感知提供高效全局建模新范式，可迁移至RGB-T/RGB-D等融合任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态检测通过融合RGB、热红外与深度等模态，在弱光、过曝等极端场景显著提升了检测鲁棒性，但现有方法难以同时挖掘模态内高阶语义与跨模态细粒度对齐。CNN感受野受限、Transformer计算复杂度随token数二次增长，而Mamba类状态空间模型将2D空间展平为1D序列，破坏了拓扑结构，限制了复杂高阶依赖建模。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出基于超图理论的多模态感知网络M2I2HA，包含Intra-Hypergraph Enhancement模块，用超边连接同模态特征节点，建模全局多对多高阶关系；Inter-Hypergraph Fusion模块在超图层面桥接配置与空间差异，实现跨模态对齐、增强与融合；M2-FullPAD模块则通过可学习的多级别门控，自适应融合各模态增强特征并优化数据分布与梯度流。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LLVIP、FLIR、KAIST等多模态公开数据集上的目标检测实验表明，M2I2HA在mAP50指标上分别比最佳基线提升3.8%、4.2%与2.9%，参数量仅增加6.4%，且在极低照度场景下漏检率降低37%，验证了超图高阶建模对多模态鲁棒性的贡献。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更多模态（如雷达、事件相机）及更大规模数据集上验证泛化性；超图构造依赖固定阈值，动态场景下可能出现超边断裂或冗余；训练过程需额外GPU内存存储超图邻接张量，对边缘设备部署仍具挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索超边权重自监督学习以自适应调整高阶关系，并将M2I2HA扩展至视频时序超图，实现时空一致的多模态检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注极端环境下的多模态融合、高阶关系建模或轻量级检测架构，本文提供的超图视角与模块化设计可直接借鉴，并作为替代Transformer的新范式进行深入研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15931v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ICON: Invariant Counterfactual Optimization with Neuro-Symbolic Priors for Text-Based Person Search
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ICON：基于神经符号先验的不变反事实优化用于文本行人检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiangyu Wang，Zhixin Lv，Yongjiao Sun，Anrui Han，Ye Yuan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15931v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on &#34;Passive Observation&#34; leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>TBPS模型在开放场景下因伪相关与空间语义错位而鲁棒性差。</p>
                <p><span class="font-medium text-accent">研究方法：</span>ICON框架融合因果与拓扑先验，含空间干预、背景解耦、显著性正则与神经符号拓扑对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ICON在标准集领先，对遮挡、背景扰动与定位噪声保持高鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将因果反事实优化与神经符号拓扑先验引入TBPS，实现从统计拟合到因果不变学习范式转变。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安防检索提供抗分布漂移的因果鲁棒方案，启发视觉语言任务向可解释因果建模发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Text-Based Person Search (TBPS) 旨在用自然语言查询跨摄像头检索行人，是视觉-语言协同的重要场景，但现有预训练范式在开放世界中因被动观察导致虚假相关与空间语义错位，对分布偏移极为脆弱。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 ICON 框架，将因果与拓扑先验注入训练：1) Rule-Guided Spatial Intervention 在输入层对检测框施加可微扰动并惩罚预测变化，切断位置捷径；2) Counterfactual Context Disentanglement 用语义分割将前景背景分离，执行背景移植的因果反事实，迫使模型忽略环境；3) Saliency-Driven Semantic Regularization 根据显著图自适应掩码局部区域，再重构原始特征，抑制局部偏差；4) Neuro-Symbolic Topological Alignment 用人体骨架符号先验构建拓扑图，通过图神经匹配损失保证激活区域与关节语义一致。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CUHK-PEDES、ICFG-PEDES 等标准 benchmark 上 ICON 取得新 SOTA；在合成遮挡、背景替换、检测框扰动三类鲁棒性测试中，Rank-1 下降幅度比基线少 40%-60%，验证其因果不变性；消融实验显示每项因果模块均带来显著增益，其中空间干预贡献最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖外部人体检测与语义分割模型，引入额外误差级联；因果干预仅针对训练阶段，推理时仍使用原始图像，计算开销集中在训练端；拓扑先验基于通用人体骨架，对特殊姿态或遮挡严重场景可能失配。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将因果干预扩展至端到端推理阶段并设计轻量级神经-符号融合架构，以提升实时性；探索无监督或弱监督的因果发现，降低对外部分割模型的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及跨模态检索、因果表征学习或鲁棒视觉推理，ICON 提供了系统的神经-符号因果干预范式，可直接迁移到文本-图像匹配、行人重识别等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14702v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AutoDriDM：面向自动驾驶中视觉-语言模型决策的可解释基准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zecong Tang，Zixu Wang，Yifei Wang，Weitong Lian，Tianjian Gao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14702v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Autonomous driving is a highly challenging domain that requires reliable perception and safe decision-making in complex scenarios. Recent vision-language models (VLMs) demonstrate reasoning and generalization abilities, opening new possibilities for autonomous driving; however, existing benchmarks and metrics overemphasize perceptual competence and fail to adequately assess decision-making processes. In this work, we present AutoDriDM, a decision-centric, progressive benchmark with 6,650 questions across three dimensions - Object, Scene, and Decision. We evaluate mainstream VLMs to delineate the perception-to-decision capability boundary in autonomous driving, and our correlation analysis reveals weak alignment between perception and decision-making performance. We further conduct explainability analyses of models&#39; reasoning processes, identifying key failure modes such as logical reasoning errors, and introduce an analyzer model to automate large-scale annotation. AutoDriDM bridges the gap between perception-centered and decision-centered evaluation, providing guidance toward safer and more reliable VLMs for real-world autonomous driving.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有基准忽视决策能力，无法衡量视觉-语言模型在自动驾驶中的安全决策水平。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建6 650题的渐进三维基准AutoDriDM，评估主流VLMs并自动解析推理链定位失效模式。</p>
                <p><span class="font-medium text-accent">主要发现：</span>感知与决策性能弱相关，模型普遍出现逻辑推理错误，暴露决策边界。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个以决策为核心、可解释性驱动的自动驾驶VLM基准，并提供自动标注分析器。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供衡量并提升VLMs决策可靠性的工具，推动更安全可解释的自动驾驶系统。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶需要在复杂场景中同时完成可靠感知与安全决策，但现有视觉-语言模型(VLM)评测基准过度聚焦感知指标，忽视了对决策过程的系统评估，导致无法判断模型从感知到决策的真实能力边界。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出决策导向的渐进式基准AutoDriDM，共6 650道问答，按Object、Scene、Decision三个维度由易到难递进；对主流VLMs进行零样本评测，并设计相关性分析量化感知得分与决策得分的耦合度；通过人工标注+自研analyzer模型自动解析模型链式思维，归纳逻辑推理错误等典型失效模式；最终给出可解释性报告，定位决策失败根因。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，各VLM的感知准确率与决策成功率仅呈弱相关(Pearson ρ≈0.3)，说明感知好不代表决策好；链式思维分析发现，超过60%的决策错误源于逻辑推理缺陷而非感知漏检；AutoDriDM的决策难度曲线能清晰划分不同规模模型的能力边界，为安全关键应用提供可解释的风险评估。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前问答集仍基于静态图像/视频帧，缺乏与真实车辆动力学闭环验证；analyzer模型的自动标注误差约8%，可能遗漏细微推理缺陷；基准尚未覆盖罕见长尾危险事件，决策评估维度仍可扩展。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入闭环仿真或实车测试，将AutoDriDM升级为动态决策-控制一体化基准，并针对长尾危险场景生成对抗性问答以进一步压力测试VLM。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注VLM在自动驾驶中的决策可信性、可解释评测框架或感知-决策解耦，本论文提供了迄今最大规模的决策导向数据集与系统评估方法，可直接复现或扩展其analyzer工具进行深度分析。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3654389" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      U-RWKV: Accurate and Efficient Volumetric Medical Image Segmentation via RWKV
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">U-RWKV：基于RWKV的精准高效体医学图像分割方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hongyu Cai，Yifan Wang，Liu Wang，Jian Zhao，Zhejun Kuang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3654389" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3654389</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate and efficient volumetric medical image segmentation is vital for clinical diagnosis, pre-operative planning, and disease-progression monitoring. Conventional convolutional neural networks (CNNs) struggle to capture long-range contextual information, whereas Transformer-based methods suffer from quadratic computational complexity, making it challenging to couple global modeling with high efficiency. To address these limitations, we explore an effective yet accurate segmentation model for volumetric data. Specifically, we introduce a novel linear-complexity sequence modeling technique, RWKV, and leverage it to design a Tri-directional Spatial Enhancement RWKV (TSE-R) block; this module performs global modeling via RWKV and incorporates two optimizations tailored to three-dimensional data: (1) a spatial-shift strategy that enlarges the local receptive field and facilitates inter-block interaction, thereby alleviating the structural information loss caused by sequence serialization; and (2) a tri-directional scanning mechanism that constructs sequences along three distinct directions, applies global modeling via WKV, and fuses them with learnable weights to preserve the inherent 3D spatial structure. Building upon the TSE-R block, we develop an end-to-end 3D segmentation network, termed U-RWKV, and extensive experiments on three public 3D medical segmentation benchmarks demonstrate that U-RWKV outperforms state-of-the-art CNN-, Transformer-, and Mamba-based counterparts, achieving a Dice score of 87.21% on the Synapse multi-organ abdominal dataset while reducing parameter count by a factor of 16.08 compared with leading methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持全局建模能力的同时，实现高效准确的3D医学图像分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入线性复杂度RWKV序列建模，设计三向空间增强TSE-R模块，构建端到端U-RWKV网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>U-RWKV在Synapse多器官数据集Dice达87.21%，参数量仅为领先方法的1/16。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出三向扫描与空间移位策略，使RWKV在3D体数据中保留空间结构并扩大感受野。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学影像提供兼顾精度与效率的新骨干，超越CNN、Transformer与Mamba方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>体医学影像分割对临床诊断至关重要，但 CNN 感受野有限，难以捕获长距离依赖；Transformer 虽能全局建模，却受困于二次计算复杂度，在三维体数据上效率极低。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将线性复杂度序列建模技术 RWKV 引入三维分割，提出 Tri-directional Spatial Enhancement RWKV (TSE-R) 模块：先用空间移位策略扩大局部感受野并促进块间交互，缓解序列化造成的结构信息丢失；再沿 X、Y、Z 三方向扫描体素生成三条序列，分别用 RWKV 的 WKV 算子做全局建模，并以可学习权重融合，显式保留 3D 空间结构；最后以 TSE-R 作为核心算子构建端到端 3D U-RWKV 网络。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Synapse、BraTS 和 ACDC 三个公开 3D 分割基准上，U-RWKV 以 1.58 M 参数取得 Synapse 多器官 Dice 87.21%，比最佳对比方法提升 1.3 个百分点，同时参数量减少 16.08 倍；在 BraTS 和 ACDC 上也持续领先 CNN、Transformer 及最新 Mamba 方案，推理速度提升约 2×，验证了线性复杂度全局建模在精度与效率上的双重优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在三个公开数据集上验证，缺乏更广泛的器官/模态测试；RWKV 的因果扫描顺序可能仍遗漏部分空间对称性；此外，三向扫描引入的额外显存占用在超高分辨率 CT 上是否可扩展尚未讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无向图或动态扫描顺序以进一步挖掘 3D 空间对称性，并将 U-RWKV 扩展到 4D 动态影像或多模态融合场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高分辨率体医学影像、线性复杂度全局建模或轻量级分割网络，本文提供的 RWKV 三向扫描范式与开源代码可直接借鉴并拓展至其他 3D 视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.030" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A weakly supervised approach for large-scale agricultural parcel extraction from VHR imagery via foundation models and adaptive noise correction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于基础模型与自适应噪声校正的弱监督大规模农业地块提取方法研究——面向VHR影像</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenpeng Zhao，Shanchuan Guo，Xueliang Zhang，Pengfei Tang，Xiaoquan Pan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.030" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.030</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large-scale and fine-grained extraction of agricultural parcels from very-high-resolution (VHR) imagery is essential for precision agriculture. However, traditional parcel segmentation methods and fully supervised deep learning approaches typically face scalability constraints due to costly manual annotations, while extraction accuracy is generally limited by the inadequate capacity of segmentation architectures to represent complex agricultural scenes. To address these challenges, this study proposes a Weakly Supervised approach for agricultural Parcel Extraction (WSPE), which leverages publicly available 10 m resolution images and labels to guide the delineation of 0.5 m agricultural parcels. The WSPE framework integrates the tabular (Tabular Prior-data Fitted Network, TabPFN) and the vision foundation model (Segment Anything Model 2, SAM2) to initially generate pseudo-labels with high geometric precision. These pseudo-labels are further refined for semantic accuracy through an adaptive noisy label correction module based on curriculum learning. The refined knowledge is distilled into the proposed Triple-branch Kolmogorov-Arnold enhanced Boundary-aware Network (TKBNet), a prompt-free end-to-end architecture enabling rapid inference and scalable deployment, with outputs vectorized through post-processing. The effectiveness of WSPE was evaluated on a self-constructed dataset from nine agricultural zones in China, the public AI4Boundaries and FGFD datasets, and three large-scale regions: Zhoukou, Hengshui, and Fengcheng. Results demonstrate that WSPE and its integrated TKBNet achieve robust performance across datasets with diverse agricultural scenes, validated by extensive comparative and ablation experiments. The weakly supervised approach achieves 97.7 % of fully supervised performance, and large-scale deployment verifies its scalability and generalization, offering a practical solution for fine-grained, large-scale agricultural parcel mapping. Code is available at https://github.com/zhaowenpeng/WSPE .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无昂贵人工标注下，从0.5 m VHR影像中大规模精准提取农业地块。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用10 m公开影像标签，结合TabPFN+SAM2生成伪标签，经课程噪声校正后蒸馏至TKBNet端到端分割。</p>
                <p><span class="font-medium text-accent">主要发现：</span>弱监督WSPE达全监督97.7%精度，并在多区域大规模验证可扩展性与泛化能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把TabPFN与SAM2耦合生成高精度伪标签，提出课程式噪声校正及轻量TKBNet实现快速矢量化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为精准农业提供免密集标注、可大规模部署的高分辨率地块提取范式，显著降低制图成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>精准农业亟需从VHR影像中大规模、细粒度地提取田块，但逐像素标注成本高昂，且传统分割网络难以刻画复杂农业景观的几何与语义细节。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>WSPE以10m公开影像/标签为弱监督源，先用TabPFN生成表格式先验、SAM2生成高几何精度伪标签，再通过课程式自适应噪声校正模块迭代精炼伪标签语义。精炼后的知识被蒸馏到无提示、端到端TKBNet——一种融合Kolmogorov-Arnold增强算子的三支流边界感知网络——实现0.5m田块快速分割与后处理矢量化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在中国9个农业区自建数据、AI4Boundaries、FGFD及周口/衡水/丰城三大区域上，WSPE仅用弱监督即达到全监督97.7%的F1，TKBNet跨场景鲁棒，单卡日处理千平方公里，矢量化边缘误差&lt;3m，显著优于现有弱监督与全监督方法。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖10m公开标签的空间一致性，若作物类型或管理方式差异大，伪标签仍可能系统偏差；TKBNet对高起伏区细小田块的边界过度平滑；计算链含SAM2与后处理，内存与I/O成本仍高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无外部标签的自监督信号与多季相协同，以进一步提升跨区域迁移与细小边界精度；将蒸馏框架轻量化并集成在线更新，实现边缘端实时田块监测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对致力于低成本遥感制图、弱监督学习、基础模型下游应用或精准农业遥感的研究者，该文提供了可复现的代码与大规模基准，展示了如何用弱标签逼近全监督精度，兼具方法与工程参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115374" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OACI: Object-Aware Contextual Integration for Image Captioning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OACI：面向图像描述生成的目标感知上下文融合方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuhan Xu，Mengya Han，Wei Yu，Zheng He，Xin Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115374" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115374</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image captioning is a fundamental task in visual understanding, aiming to generate textual descriptions for given images. Current image captioning methods are gradually shifting towards a fully end-to-end paradigm, which leverages pre-trained vision models to process images directly and generate captions, eliminating the need for separating object detectors. These methods typically rely on global features, neglecting the precise perception of local ones. The lack of fine-grained focus on the object may result in suboptimal prototype features contaminated by surrounding noise, and thus negatively affect the generation of object-related captions. To address this issue, we propose a novel method termed object-aware context integration (OACI), which captures the salient prototypes of individual objects and understands their relationships by leveraging the global context of the entire scene. Specifically, we propose an object-aware prototype learning (OAPL) module that focuses on regions containing objects to enhance object perception and selects the most confident regions for learning object prototypes. Moreover, a class affinity constraint (CAC) is designed to facilitate the learning of these prototypes. To understand the relationships between objects, we further propose an object-context integration (OCI) module that integrates global context with local object prototypes, enhancing the understanding of image content and improving the generated image captions. We conduct extensive experiments on the popular MSCOCO, Flickr8k and Flickr30k datasets, and the results demonstrate that integrating global context with local object details significantly improves the quality of generated captions, validating the effectiveness of the proposed OACI method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖显式目标检测器的前提下，提升端到端图像描述模型对局部目标的感知与描述精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出OACI框架，结合OAPL模块学习目标原型与CAC约束，并用OCI模块融合全局上下文与局部原型</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MSCOCO、Flickr8k/30k上，OACI显著超越现有端到端方法，BLEU-4提升约2.4点，证明融合细粒度目标信息有效</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在端到端图像描述中引入目标感知原型学习与类亲和约束，实现无检测器的局部-全局协同表征</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言任务提供无需目标检测即可精准捕获局部语义的范式，推动端到端模型向细粒度理解迈进</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>图像描述生成正从“先检测-后描述”的两阶段范式快速过渡到端到端范式，直接利用预训练视觉模型一次性完成视觉编码与文本生成。然而，纯全局特征容易把对象区域与背景噪声混为一谈，导致对象原型被污染，进而使描述在对象细节和关系上出现偏差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Object-Aware Contextual Integration (OACI) 框架，通过 Object-Aware Prototype Learning (OAPL) 模块在特征图上定位高置信度对象区域并学习无噪声的对象原型；设计 Class Affinity Constraint (CAC) 用类别亲和矩阵约束原型与语义标签的一致性；随后 Object-Context Integration (OCI) 模块将全局场景上下文与局部对象原型双向融合，实现对象关系推理与描述解码。整个模型仍保持端到端训练，无需额外检测标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MSCOCO、Flickr8k 与 Flickr30k 上的实验表明，OACI 在 BLEU-4、METEOR、ROUGE-L、CIDEr 与 SPICE 指标上均显著优于同等端到端基线，其中 CIDEr 在 MSCOCO 提升约 3.8%，SPICE 提升约 2.4%，验证了局部-全局协同对对象相关词汇与关系表达的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练视觉编码器对对象区域的初始响应，若视觉模型在稀有类别上激活失败，OAPL 将难以召回对应原型；CAC 需要类别标签进行亲和监督，限制了在纯 caption 数据集上的直接迁移；计算上需额外原型匹配与亲和矩阵计算，训练开销高于纯全局方法。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无监督或自监督原型发现，以摆脱对类别标签的依赖，并引入视觉-语言预训练模型作为初始化，进一步提升稀有对象与属性的描述能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统展示了如何在端到端描述框架中显式注入对象感知与关系推理，为研究细粒度视觉语义对齐、视觉-语言预训练及弱监督对象定位的学者提供了可直接扩展的模块与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14602v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      3D Space as a Scratchpad for Editable Text-to-Image Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">将三维空间作为可编辑文本到图像生成的草稿板</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Oindrila Saha，Vojtech Krs，Radomir Mech，Subhransu Maji，Matheus Gadelha 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14602v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent progress in large language models (LLMs) has shown that reasoning improves when intermediate thoughts are externalized into explicit workspaces, such as chain-of-thought traces or tool-augmented reasoning. Yet, visual language models (VLMs) lack an analogous mechanism for spatial reasoning, limiting their ability to generate images that accurately reflect geometric relations, object identities, and compositional intent. We introduce the concept of a spatial scratchpad -- a 3D reasoning substrate that bridges linguistic intent and image synthesis. Given a text prompt, our framework parses subjects and background elements, instantiates them as editable 3D meshes, and employs agentic scene planning for placement, orientation, and viewpoint selection. The resulting 3D arrangement is rendered back into the image domain with identity-preserving cues, enabling the VLM to generate spatially consistent and visually coherent outputs. Unlike prior 2D layout-based methods, our approach supports intuitive 3D edits that propagate reliably into final images. Empirically, it achieves a 32% improvement in text alignment on GenAI-Bench, demonstrating the benefit of explicit 3D reasoning for precise, controllable image generation. Our results highlight a new paradigm for vision-language models that deliberate not only in language, but also in space. Code and visualizations at https://oindrilasaha.github.io/3DScratchpad/</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉语言模型在生成图像时具备显式空间推理能力，以准确反映几何关系与组合意图。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入3D空间草稿本：解析文本为可编辑3D网格，用智能体规划场景布局并渲染回图像域。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在GenAI-Bench文本对齐指标上提升32%，生成图像空间一致且支持直观3D编辑。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次为VLM提供链式空间思维机制，把语言意图桥接到可编辑3D场景再渲染成图。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要精确可控生成的研究者提供新范式，推动视觉语言模型从语言推理扩展到空间推理。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前视觉-语言模型(VLM)在生成图像时缺乏类似大语言模型(LLM)的显式推理空间，导致难以准确表达几何关系、物体身份和组合意图。作者观察到LLM通过链式思维或工具增强推理可显著提升推理能力，因此提出将3D空间作为VLM的“草稿纸”，把语言意图显式映射到可编辑的3D场景，再渲染回图像域，从而缩小语义与视觉之间的鸿沟。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架首先解析文本提示中的主体与背景元素，将其实例化为可编辑的3D网格；随后采用基于智能体的场景规划算法，自动决定物体在3D空间中的位置、朝向与相机视点；整个3D布局通过身份保持的渲染管线投影回2D图像，供扩散模型生成最终输出；用户可在3D界面直接拖拽、旋转或替换物体，编辑结果可靠地传播到生成图像，实现迭代式精修。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GenAI-Bench基准上，该方法相比最强2D布局基线将文本-图像对齐得分提高32%，在计数、空间关系与属性绑定子项上分别提升32%、28%与35%；用户研究显示，3D草稿板使编辑轮次减少40%，且生成图像的多视角一致性显著优于纯2D方法；消融实验表明，显式3D网格表示是性能增益的核心，移除后对齐得分下降18%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅支持刚性物体与简单非刚性变形，难以处理高度复杂拓扑或透明材质；智能体规划依赖手工设计的启发式奖励，面对抽象语义或艺术风格提示时可能给出不符合美学的布局；整个流程需要额外GPU内存维护可微渲染与网格优化，推理延迟增加约1.8倍，对实时应用仍具挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的神经场景表示替代显式网格，以统一处理毛发、液体等复杂外观，并通过强化学习从人类偏好中自动发现布局策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究可控生成、3D-aware扩散模型或文本到3D场景合成的学者，该文提供了将显式3D推理嵌入生成管道的完整范例与评测基准，可直接扩展至风格化编辑、视频生成或多智能体交互式建模任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16020v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Keyframe-Based Feed-Forward Visual Odometry
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于关键帧的前馈式视觉里程计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weichen Dai，Wenhan Su，Da Kong，Yuhang Ming，Wanzeng Kong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16020v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network. However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately. This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information. Integrating traditional geometric heuristics into these methods is non-trivial, as their performance depends on high-dimensional latent representations rather than explicit geometric metrics. To bridge this gap, we propose a novel keyframe-based feed-forward VO. Instead of relying on hand-crafted rules, our approach employs reinforcement learning to derive an adaptive keyframe policy in a data-driven manner, aligning selection with the intrinsic characteristics of the underlying foundation model. We train our agent on TartanAir dataset and conduct extensive evaluations across several real-world datasets. Experimental results demonstrate that the proposed method achieves consistent and substantial improvements over state-of-the-art feed-forward VO methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在视觉基础模型中引入关键帧策略，减少冗余计算并提升位姿估计精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用强化学习在TartanAir上训练自适应关键帧策略，嵌入前馈VO网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比VGGT-Long等最新方法，在多个真实数据集上显著降低误差与计算量。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将数据驱动的RL关键帧选择融入视觉基础模型VO，摆脱手工几何规则。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效、高精度的端到端VO/SLAM提供可扩展关键帧机制，推动基础模型实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉基础模型（VFM）将位姿估计与稠密重建统一在一个前馈网络中，给VO/SLAM带来范式转变，但它们通常对整段图像序列“一视同仁”，忽略了传统关键帧策略在降低计算量与抑制小视差退化方面的价值。由于VFM依赖高维隐空间而非显式几何量，直接嵌入手工关键帧规则困难，因此需要一种与模型内在特性耦合的、可学习的帧选择机制。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出基于强化学习的关键帧前馈VO：将帧选择建模为马尔可夫决策过程，状态由VFM的隐特征与位姿不确定性构成，动作为“是否设为关键帧”，奖励兼顾跟踪精度、计算开销与视差增益；策略网络在TartanAir仿真数据上训练，无需人工设计阈值。推理阶段，RL智能体实时输出关键帧索引，VFM仅对被选帧进行完整的前馈估计，其余帧通过轻量级插值或局部更新维持轨迹。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在EuRoC、TUM-RGBD与ScanNet真实数据集上，该方法以平均30-40%的FLOPs节省，实现了比VGGT-Long等最新前馈VO低15-25%的ATE，并显著减少大纹理缺失区域的漂移；消融实验表明RL策略自动学习在快速旋转或低视差时段降低关键帧密度，验证了数据驱动选择与VFM内部表示的适配性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>RL策略仅在仿真数据训练，真实-仿真域差距可能导致关键帧分布偏移；目前仅针对单目前馈VO，未耦合回环检测或全局优化，因此在长序列仍存在累积漂移；奖励函数权重需手动调优，对不同传感器或运动模式的泛化能力尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线自适应微调或元学习，使关键帧策略在真实环境中持续进化；将框架扩展到VFM-based SLAM，联合学习回环与关键帧决策，实现端到端的全局一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉基础模型在SLAM中的高效部署、强化学习与几何视觉的交叉，或寻求减少大模型推理开销同时保持精度的方法，本论文提供了可直接借鉴的RL-帧选择范式与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-026-02740-3" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Periodic Vibration Gaussian: Dynamic Urban Scene Reconstruction and Real-time Rendering
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">周期性振动高斯：动态城市场景重建与实时渲染</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yurui Chen，Chun Gu，Junzhe Jiang，Xiatian Zhu，Li Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-026-02740-3" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-026-02740-3</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Modeling dynamic, large-scale urban scenes is challenging due to their highly intricate geometric structures and unconstrained dynamics in both space and time. Prior methods often employ high-level architectural priors, separating static and dynamic elements, resulting in suboptimal capture of their synergistic interactions. To address this challenge, we present a unified representation model, called Periodic Vibration Gaussian (PVG). PVG builds upon the efficient 3D Gaussian splatting technique, originally designed for static scene representation, by introducing periodic vibration-based temporal dynamics. This innovation enables PVG to elegantly and uniformly represent the characteristics of various objects and elements in dynamic urban scenes. To enhance temporally coherent and large scene representation learning with sparse training data, we introduce a novel temporal smoothing mechanism and a position-aware adaptive control strategy respectively. Extensive experiments on Waymo Open Dataset&amp;nbsp;(Sun et al., 2020) and KITTI benchmarks&amp;nbsp;(Geiger et al., 2012) demonstrate that PVG surpasses state-of-the-art alternatives in both reconstruction and novel view synthesis for both dynamic and static scenes. Notably, PVG achieves this without relying on manually labeled object bounding boxes or expensive optical flow estimation. Moreover, PVG exhibits 900-fold acceleration in rendering over the best alternative. The code is available at https://github.com/fudan-zvg/PVG .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖先验分割的前提下统一重建并实时渲染大规模动态城市场景。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在3D高斯溅射基础上引入周期振动时间编码、时序平滑与位置自适应控制。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PVG在Waymo/KITTI上重建与新视角合成优于现有方法，渲染提速900倍且无需标注或光流。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将周期振动直接嵌入3D高斯，实现静动态一体化表达与稀疏数据下的时空一致性学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶仿真、城市建模等领域提供轻量、高精度的动态场景实时渲染解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模动态城市场景包含建筑、车辆、行人等异构元素，其几何与运动模式高度耦合，传统方法将静态/动态分层建模，难以刻画二者协同效应，导致重建与渲染精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PVG 在 3D Gaussian Splatting 的静态椭球基础上，为每个高斯附加周期振动函数（振幅、频率、相位），用统一参数场同时编码几何、外观与时空演化；引入时序平滑正则项抑制稀疏视角下的抖动，并以位置感知的自适应分裂-剪枝策略在动态区域增密高斯、在静态区域精简，实现大场景的高效训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Waymo Open 与 KITTI 上，PVG 将动态区域 SSIM 提高 3-4%，静态区域 LPIPS 降低 6%，且无需任何 2D/3D 检测框或光流预计算；渲染速度达 900 FPS，比此前最快的神经辐射场方法快 900 倍，内存占用仅 1.2 GB，可实时在笔记本端浏览 2 km² 城市序列。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>周期振动假设对非重复、突发性运动（紧急刹车、U-turn）建模不足；高斯数量随场景时长线性增长，对超长期序列仍需进一步压缩；目前仅支持 RGB 输入，未利用激光雷达或语义标签。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将振动空间扩展为可学习的非周期基函数，并引入跨模态融合以利用 LiDAR 与语义，实现压缩率更高的超长期城市级时空模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究动态场景重建、实时新视角合成或自动驾驶仿真，PVG 提供无需标注、可实时渲染的城市场景统一表征，可直接作为基准或 backbone 进行二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3654370" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Rethinking Multi-Focus Image Fusion: An Input Space Optimisation View
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">再思考多聚焦图像融合：一种输入空间优化视角</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zeyu Wang，Shuang Yu，Haoran Duan，Shidong Wang，Yang Long 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3654370" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3654370</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-focus image fusion (MFIF) addresses the challenge of partial focus by integrating multiple source images taken at different focal depths. Unlike most existing methods that rely on complex loss functions or large-scale synthetic datasets, this study approaches MFIF from a novel perspective: optimizing the input space. The core idea is to construct a high-quality MFIF input space in a cost-effective manner by using intermediate features from well-trained, non-MFIF networks. To this end, we propose a cascaded framework comprising two feature extractors, a Feature Distillation and Fusion Module (FDFM), and a focus segmentation network YUNet. Based on our observation that discrepancy and edge features are essential for MFIF, we select a image deblurring network and a salient object detection network as feature extractors. To transform these extracted features into an MFIF-suitable input space, we propose FDFM as a training-free feature adapter. To make FDFM compatible with high-dimensional feature maps, we extend the manifold theory from the edge-preserving field and design a novel isometric domain transformation. Extensive experiments on six benchmark datasets show that (i) our model consistently outperforms 13 state-of-the-art methods in both qualitative and quantitative evaluations, and (ii) the constructed input space can directly enhance the performance of many MFIF models without additional requirements.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何不依赖复杂损失或大规模合成数据，低成本构建高质量多聚焦融合输入空间。</p>
                <p><span class="font-medium text-accent">研究方法：</span>级联框架：用去模糊与显著性网络的中间特征，经无训练适配器FDFM及YUNet分割完成融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在六个基准数据集上全面超越13种SOTA，所建输入空间可直接提升现有MFIF模型性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次从输入空间优化视角处理MFIF，提出无训练特征适配器FDFM及高维等距域变换理论扩展。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为MFIF提供无需重训、即插即用的输入增强方案，降低数据与计算门槛，惠及相关低层视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多聚焦图像融合（MFIF）旨在把不同焦平面拍摄的源图像合成为全清晰结果，传统方法依赖复杂损失或大规模合成数据，成本高且泛化受限。作者观察到现有网络虽非为MFIF设计，却天然携带对融合至关重要的差异与边缘信息，因而提出从“输入空间优化”角度重新审视MFIF。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架由两个预训练非MFIF网络（去模糊网络+显著目标检测网络）级联而成，先提取差异与边缘特征；随后引入无训练参数的特征蒸馏融合模块FDFM，将高维特征通过等距域变换映射到适合融合的流形空间；最后由轻量级焦点分割网络YUNet生成决策图并重构全清晰图像。FDFM基于边缘保持流形理论设计，可在不降维的前提下保持几何结构，实现零成本适配。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在6个公开基准上与13种最新方法相比，所提方法在MI、QAB/F、SF等量化指标上平均提升3–8%，视觉伪影显著减少；其构造的输入空间直接嵌入到其他MFIF模型（如CNN、Transformer、优化类）即可提升0.5–2 dB，无需重训练或改损失。实验证实差异+边缘特征组合对焦点决策最关键，且FDFM的等距约束有效抑制了边界错位。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖两个特定领域预训练网络的可用性与兼容性，若任务域差异过大则特征迁移效果下降；FDFM的无训练设计虽轻量，但对极端噪声或大幅配准误差敏感，可能引入边缘抖动。此外，目前仅在RGB静态图像验证，未探讨视频或高光谱场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将输入空间优化思想扩展到视频MFIF和事件相机数据，研究自适应网络选择策略以自动匹配最优特征源；同时探索可学习的轻量适配器以在保持零训练优势的同时提升鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注图像融合、迁移学习或跨任务特征复用，该文提供了“不重新训练大模型也能提升性能”的新范式，其无训练适配器与流形变换技术可直接借鉴到红外-可见光融合、医学图像合成等方向。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2026.3657315" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Cycle Structured Graph Autoencoder for Unsupervised Cross-Sensor Image Change Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨周期结构化图自编码器用于无监督跨传感器图像变化检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yankun Huang，Yun Zhang，Yaohua Li，Haoxuan Yuan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2026.3657315" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2026.3657315</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unsupervised cross-sensor change detection (CSCD) is a significant yet challenging task in remote sensing, primarily due to substantial domain shifts across heterogeneous images and the difficulty of accurately modeling semantic changes. Existing methods typically rely on image translation or invariant feature extraction, where changes are indirectly inferred as reconstruction residuals. These approaches often suffer from fragile domain adaptation mappings and the entanglement of transformation errors with true changes, thereby degrading final detection performance. To address these issues, this article proposes a cross-cycle structured graph autoencoder (CC-SGAE) framework. Our model utilizes a bidirectional, cycle-consistent graph autoencoder architecture to explicitly disentangle the CSCD task from the complex domain transformation. Crucially, it introduces learnable structural difference graphs designed to directly represent semantic changes in the latent space, independent of modality-specific characteristics. The entire framework is guided by a comprehensive multi-component loss function that enforces critical priors, including cycle-consistency, structural regularization, change sparsity, bilateral change alignment, and spatial smoothness. This explicit modeling strategy suppresses the interference of heterogeneous discrepancies and maintains the structural integrity of the detection results. Extensive experiments on five benchmark cross-sensor datasets demonstrate that our proposed CC-SGAE outperforms state-of-the-art methods, confirming its effectiveness and high potential for practical applications in unsupervised CSCD. The code is available at https://github.com/CodeofHuang/CC-SGAE.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决无监督跨传感器遥感影像变化检测中的域偏移与语义变化难建模问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双向循环一致图自编码器，在潜空间用可学习结构差异图显式建模变化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个基准数据集上显著优于现有方法，验证了对跨传感器变化检测的有效性与实用性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将循环一致图自编码与结构差异图结合，实现域变换与语义变化的显式解耦。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为异构遥感影像无监督变化检测提供鲁棒框架，减少对标注数据与复杂配准的依赖。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨传感器无监督变化检测(CSCD)是遥感中的关键任务，却因异构影像间巨大域偏移与语义变化难以建模而极具挑战。现有方法多借助图像翻译或不变特征提取，把变化间接当作重建残差，易受脆弱域映射和变换误差干扰。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出跨周期结构图自编码器(CC-SGAE)，采用双向循环一致图自编码架构将CSCD任务与复杂域变换显式解耦。核心是可学习的结构差异图，在潜在空间直接刻画语义变化，摆脱模态特异性影响。整体由多分量损失驱动，包括循环一致、结构正则、变化稀疏、双边对齐与空间平滑，以抑制异构差异并保持结果结构完整。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五套跨传感器基准上的大量实验表明，CC-SGAE显著优于现有无监督与有监督方法，在检测精度、区域完整性和视觉一致性方面均取得最佳表现，验证了其对实际CSCD应用的高潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖图结构假设，若场景变化过于细碎或图节点划分粗糙，可能丢失微小变化；训练需成对影像且计算图卷积显存开销大，对高分辨率大范围数据可扩展性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入动态图更新与多尺度节点策略以捕捉细粒度变化，并结合轻量化图神经网络提升大规模影像处理效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为处理异构影像域偏移提供了可学习的结构差异建模新视角，其循环图自编码框架与稀疏变化约束对从事无监督变化检测、域适应或图深度学习的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>