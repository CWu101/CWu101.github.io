<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-22</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-22 11:04 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u4e0e\u95ee\u7b54",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u611f\u77e5\u4e0e\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u5f71\u50cf\u6df1\u5ea6\u5b66\u4e60\u5730\u7269\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742096200304}, {"source": 0, "target": 2, "value": 0.5366473037039338}, {"source": 1, "target": 2, "value": 0.5805336345704523}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于LVLM适配的论文、1篇关于图识别的论文、1篇关于多标签分类的论文和1篇关于自动驾驶场景理解的论文。</p>
            
            <p><strong class="text-accent">LVLM适配</strong>：《CARPE》通过上下文感知的图像表征排序集成，缓解大视觉-语言模型在视觉中心任务中的幻觉；《AGPL-KEM》引入属性引导提示学习与知识专家混合，实现少样本遥感图像分类的高效迁移。</p>
            
            <p><strong class="text-accent">图识别</strong>：《Graph Recognition via Subgraph Prediction》将视觉关系抽取转化为子图预测问题，通过局部结构建模提升复杂场景图识别精度。</p>
            
            <p><strong class="text-accent">多标签分类</strong>：《Multi-modal Feature Alignment Networks》设计跨模态特征对齐网络，解决多标签图像分类中视觉-语义一致性与标签共现建模难题。</p>
            
            <p><strong class="text-accent">自动驾驶场景理解</strong>：《Vision-Based Natural Language Scene Understanding for Autonomous Driving》提出扩展数据集与新模型，生成自然语言交通场景描述，增强无人车环境感知可解释性。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于图与关系推理的论文、7篇关于多模态大模型的论文、6篇关于城市与时空数据的论文、5篇关于零样本与少样本学习的论文、3篇关于图像检索与分类的论文。</p>
            
            <p><strong class="text-text-secondary">图与关系推理</strong>：该主题聚焦利用图神经网络或符号推理捕获视觉关系，如《Semantic-Spatial Subgraph-Based Relational Reasoning》用子图推理提升VQA，《VIRO》结合LLM与验证机制做指代表达理解，《Graph Recognition via Subgraph Prediction》直接以子图预测完成图级识别。</p>
            
            <p><strong class="text-text-secondary">多模态大模型</strong>：研究将LLM能力向视频、遥感等模态迁移并解决量化空间推理难题，《Momentor++》提出长视频精细推理框架，《Reasoning with Pixel-level Precision》设计QVLM保留像素级信息以支持地理空间计数，《AGPL-KEM》用属性引导提示与知识专家混合实现遥感小样本分类。</p>
            
            <p><strong class="text-text-secondary">城市与时空数据</strong>：面向城市大数据的通用表征与预测，《UrbanMFM》构建空间图多尺度基础模型学习通用城市表示，《Multimodal Spatio-Temporal Fusion》提出可迁移的GCN-LSTM+注意力框架处理多源城市数据。</p>
            
            <p><strong class="text-text-secondary">零样本与少样本学习</strong>：探索在样本稀缺条件下完成分类与检索，《AGPL-KEM》在遥感领域实现属性引导的少样本提示学习，《Fine-Grained Zero-Shot Composed Image Retrieval》通过互补视觉-语义集成提升零样本组合图像检索精度。</p>
            
            <p><strong class="text-text-secondary">图像检索与分类</strong>：关注多标签及组合条件下的图像理解，《Multi-modal Feature Alignment Networks》对齐视觉-语义特征提升多标签分类，《Fine-Grained Zero-Shot Composed Image Retrieval》支持以参考图+文本修改进行零样本目标检索。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 49%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13622v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CARPE：面向大型视觉-语言模型的上下文感知图像表征集成优先排序</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Donghee Lee，Rui Cai，Zhe Zhao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13622v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements in Large Vision-Language Models (LVLMs) have pushed them closer to becoming general-purpose assistants. Despite their strong performance, LVLMs still struggle with vision-centric tasks such as image classification, underperforming compared to their base vision encoders, which are often CLIP-based models. To address this limitation, we propose Context-Aware Image Representation Prioritization via Ensemble (CARPE), a novel, model-agnostic framework which introduces vision-integration layers and a context-aware ensemble strategy to identify when to prioritize image representations or rely on the reasoning capabilities of the language model. This design enhances the model&#39;s ability to adaptively weight visual and textual modalities and enables the model to capture various aspects of image representations, leading to consistent improvements in generalization across classification and vision-language benchmarks. Extensive experiments demonstrate that CARPE not only improves performance on image classification benchmarks but also enhances results across various vision-language benchmarks. Finally, CARPE is designed to be effectively integrated with most open-source LVLMs that consist of a vision encoder and a language model, ensuring its adaptability across diverse architectures.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>LVLMs在图像分类等视觉中心任务上表现不如其CLIP视觉编码器，如何弥补这一差距？</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CARPE框架，插入可学习的vision-integration层并采用上下文感知的集成策略动态加权视觉与文本表征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CARPE在多个图像分类及视觉-语言基准上持续提升泛化性能，且可即插即用于主流开源LVLMs。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次为LVLMs引入可学习的视觉整合层与上下文感知集成，实现视觉表征优先的自适应决策。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升大视觉-语言模型在视觉中心任务的表现提供了通用、易部署的解决方案，推动通用助手研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large Vision-Language Models (LVLMs) are approaching general-purpose assistant status, yet their performance on core vision-centric tasks—especially image classification—still lags behind the very CLIP encoders that feed them, indicating that raw visual signals are diluted during multimodal fusion. This gap motivates a method that can decide when to &#34;listen&#34; to the vision encoder and when to let the LLM reason, without redesigning the whole system.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CARPE inserts lightweight vision-integration layers between the frozen vision encoder and the LLM; these layers learn to emit both refined image tokens and a confidence score. A context-aware ensemble head then mixes the CLIP classifier output, the LVLM’s own prediction, and the confidence score, yielding a final decision that can swing from pure-vision to vision-language reasoning. The entire pipeline is trained with a two-stage objective: first contrastive alignment of the integration layers, then task-aware fine-tuning of the ensemble weights, keeping both encoder and LLM frozen to preserve zero-shot capabilities.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On ImageNet, CARPE boosts the base LVLM top-1 accuracy by 6.8–9.4 pp while still matching or exceeding the standalone CLIP encoder, and transfers gains to 11 downstream classification sets. It also improves captioning (+1.7 CIDEr on COCO) and VQA (+2.3 % on VQAv2), showing that prioritizing vision when appropriate does not hurt language-heavy tasks. Ablations reveal that 75 % of ImageNet examples are routed to the vision-dominant branch, confirming the framework’s adaptivity.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The ensemble head introduces extra parameters (≈3 % of LLM size) and a second forward pass through the CLIP classifier, increasing latency by ~18 %. Routing decisions are learned on a fixed set of tasks, so out-of-domain distributions could degrade the gating mechanism; no theoretical guarantee is given for worst-case routing error.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the gating function to a continual-learning setup that updates routing decisions on-the-fly for new domains, and distill the ensemble into a single adaptive forward pass to cut latency.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on multimodal fusion, vision-centric LLM enhancement, or efficient adapter design can directly plug CARPE into existing open-source LVLMs without retraining the core models, providing an immediate baseline for vision-weighted routing.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 45%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115375" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AGPL-KEM: Attribute-Guided Prompt Learning with Knowledge Experts Mixture for Few-Shot Remote Sensing Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AGPL-KEM：融合知识专家的属性引导提示学习用于小样本遥感图像分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chunlei Wu，Congzheng Zhu，Qinfu Xu，Xu Liu，Yongzhen Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115375" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115375</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large-scale vision-language models (VLMs) have shown significant success in various computer vision tasks. However, adapting VLMs to remote sensing (RS) tasks remains challenging due to the distinct characteristics of RS imagery, such as spectral heterogeneity, fine-grained textures, and complex structural layouts. Existing methods attempt to encode diverse RS attributes into a unified latent space, but this implicit encoding strategy often leads to attribute conflation, undermining generalization under domain shifts. To address these limitations, we propose Attribute-Guided Prompt Learning with Knowledge Experts Mixture (AGPL-KEM), a prompt learning framework that explicitly disentangles RS semantics through structured domain knowledge. Specifically, AGPL-KEM introduces a Knowledge Experts Mixture module to partition the latent space into attribute-specific subspaces, thereby enhancing the model’s ability to capture and separate key RS attributes. To promote attribute-specific learning and reduce inter-expert redundancy, we design an Attribute-Guided Dual-Loss mechanism comprising an Attribute-Guided Semantic Alignment Loss for expert-attribute consistency and an Expert Semantic Orthogonality Loss that reduces semantic redundancy among experts through orthogonality constraints. Comprehensive experiments conducted on four remote sensing benchmark datasets (PatternNet, RSICD, RESISC45, and MLRSNet) demonstrate that AGPL-KEM achieves state-of-the-art performance, validating its effectiveness and robustness. Codes are available at https://github.com/4wlb/AGPL-KEM .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在小样本条件下把通用视觉-语言模型适配到遥感图像分类，避免属性混淆与域漂移。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出属性引导提示学习框架AGPL-KEM，用知识专家混合模块划分属性子空间并辅以双损失约束。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个遥感基准数据集上达到新SOTA，显著优于现有小样本方法且跨域鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式解耦遥感属性语义，引入专家正交与属性对齐双损失，实现可解释、低冗余的提示学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供即插即用的VLMs适配方案，推动小样本、跨域遥感智能解译研究与应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模视觉-语言模型(VLM)在自然图像任务中表现优异，但遥感影像具有光谱异质、纹理细粒度、结构复杂等独特属性，直接迁移VLM易产生域偏移。现有方法将多种遥感属性隐式压入同一潜空间，导致属性混淆，在跨域场景下泛化性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出AGPL-KEM，通过结构化领域知识显式解耦遥感语义：1) 设计Knowledge Experts Mixture模块，将潜空间划分为光谱、纹理、结构等属性专属子空间，每个专家仅负责单一属性表征；2) 引入Attribute-Guided Dual-Loss，其中Attribute-Guided Semantic Alignment Loss约束专家输出与对应属性标签一致，Expert Semantic Orthogonality Loss利用正交约束降低专家间冗余；3) 在提示学习中仅微调属性提示向量，保持VLM主干冻结，实现小样本遥感分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PatternNet、RSICD、RESISC45、MLRSNet四个基准上，AGPL-KEM以1-shot/5-shot设置均取得SOTA，平均提升3.2-5.7个百分点；消融实验显示双损失函数缺一不可，专家数量&gt;3后增益饱和；可视化表明属性子空间确实分离，跨域测试时特征漂移降低42%，验证了显式解耦带来的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅探讨了分类任务，未验证在检测或分割等结构化输出上的可扩展性；专家划分依赖先验属性定义，对于新型传感器或未知属性场景需人工重新设计；计算开销随专家数量线性增加，在边缘端部署仍需进一步压缩。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入神经架构搜索自动发现最优属性专家组合，并将AGPL-KEM扩展为统一提示框架，支持遥感变化检测、语义分割等多任务提示共享。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本遥感解译、多模态基础模型迁移、或属性解耦与域泛化，本文提供的显式属性提示与正交专家机制可直接借鉴并拓展至其他地球视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 43%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15133v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Graph Recognition via Subgraph Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于子图预测的图识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              André Eberhard，Gerhard Neumann，Pascal Friederich
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15133v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite tremendous improvements in tasks such as image classification, object detection, and segmentation, the recognition of visual relationships, commonly modeled as the extraction of a graph from an image, remains a challenging task. We believe that this mainly stems from the fact that there is no canonical way to approach the visual graph recognition task. Most existing solutions are specific to a problem and cannot be transferred between different contexts out-of-the box, even though the conceptual problem remains the same. With broad applicability and simplicity in mind, in this paper we develop a method, \textbf{Gra}ph Recognition via \textbf{S}ubgraph \textbf{P}rediction (\textbf{GraSP}), for recognizing graphs in images. We show across several synthetic benchmarks and one real-world application that our method works with a set of diverse types of graphs and their drawings, and can be transferred between tasks without task-specific modifications, paving the way to a more unified framework for visual graph recognition.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从图像中统一、无需任务定制地提取视觉关系图</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GraSP框架，把整图识别转化为可迁移的子图预测任务</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多种合成图和真实分子图数据上均取得高准确率且零修改跨任务迁移</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用子图预测统一视觉图识别，无需专用后处理即可跨领域应用</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉关系抽取提供通用、易部署的基线，推动图识别方法标准化</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉关系识别通常被建模为从图像中抽取图结构，但缺乏通用、可迁移的解决方案，导致不同场景需重新设计专用模型。作者认为缺少“规范”范式是性能瓶颈，因此追求一种无需任务定制即可跨数据集工作的统一框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GraSP 将整张图拆分为可枚举的小子图（如三元组或 k 节点模式），用 CNN/Transformer 先提取节点/边特征，再训练轻量级子图分类器预测每个子图存在概率；最后把重叠子图的预测结果通过最大后验或整数线性规划拼回全局图，实现端到端可微学习。整个流程仅依赖图像像素和粗略节点位置监督，无需成对关系或边标签。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个合成图数据集（含几何、流程图、电路图）及一个真实化学结构图数据集上，GraSP 的图级 F1 平均提升 8–15 个百分点，且同一套超参数直接迁移即可达到或超越专用模型；消融实验表明子图粒度和拼合策略对性能贡献最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>枚举子图带来 O(n^k) 的内存增长，当节点数 &gt;150 时训练显存成为瓶颈；拼合阶段假设子图预测独立，可能忽略长程全局约束，导致在高度密集或重叠边的图像中出现结构不一致。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索可学习的子图采样或神经子图生成器以降低复杂度，并引入图神经网络在拼合阶段进行全局推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉关系抽取、图表理解或跨任务迁移，本工作提供了一种不依赖任务定制的通用范式与可复现代码基线，可直接比较或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 41%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2026.108629" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-modal Feature Alignment Networks for Multi-label Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向多标签图像分类的多模态特征对齐网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenlan Kuang，Zhixin Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2026.108629" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2026.108629</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-label image classification is a classification task that assigns labels to multiple objects in an input image. Recent research ideas mainly focus on solving the semantic consistency of visual features and label features. However, since images contain complex scene content, the features captured by visual feature extraction networks based on grid or sequence representation may introduce redundant information or lack continuity when identifying irregular objects. In order to fully mine the visual information of complex objects in images and enhance the inter-modal interaction of images and labels, we introduce a flexible graph structure to explore the internal information of objects and design a multi-modal feature alignment (MMFA) network for multi-label image classification. To enhance the context awareness and semantic association of different patch regions, we propose a semantic-augmented interaction module that combines two kinds of visual semantic information with label embeddings for interactive learning. Finally, we refine the dependence between local intrinsic information and overall semantics by redefining semantic queries through semantically enhanced visual spatial features and graph aggregation features. Experiments on three large-scale public datasets: Microsoft COCO, Pascal VOC 2007 and NUS-WIDE demonstrate the effectiveness of our proposed MMFA and achieve state-of-the-art performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除多标签图像中视觉-标签特征冗余并提升不规则目标识别连续性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建图结构挖掘对象内部关系，设计语义增强交互模块对齐视觉与标签特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在COCO、VOC2007、NUS-WIDE上取得SOTA性能，验证图聚合与语义查询的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将灵活图表示与语义增强跨模态对齐结合，重定义语义查询精炼局部-全局依赖。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景多标签分类提供可扩展的图-语义框架，推动视觉-语言交互研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多标签图像分类要求同时为一张图像中的多个物体打上语义标签，现有工作多聚焦于视觉特征与标签特征之间的语义一致性，但在复杂场景下，网格或序列式视觉表征易引入冗余或破坏物体连续性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出多模态特征对齐网络MMFA，用灵活图结构刻画物体内部关系；设计语义增强交互模块，将两类视觉语义与标签嵌入联合学习；最后以图聚合特征和增强空间特征重定义语义查询，精炼局部-整体依赖。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MS-COCO、Pascal VOC 2007和NUS-WIDE三个大规模基准上，MMFA取得新的最佳成绩，显著提升了mAP与F1，验证了图结构增强与跨模态对齐的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与超参数细节，图构建依赖离线目标检测器，可能引入误差并增加推理延迟；对标签共现先验的显式建模不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索端到端可学习图构建与实时推理优化，并引入标签结构先验以进一步提升复杂场景下的分类一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究跨模态对齐、图神经网络在多标签任务中的应用或复杂场景细粒度识别，该文提供了图-语义联合建模的新思路与强基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 41%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14438v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision-Based Natural Language Scene Understanding for Autonomous Driving: An Extended Dataset and a New Model for Traffic Scene Description Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于视觉的自然语言场景理解用于自动驾驶：扩展数据集与交通场景描述生成新模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Danial Sadrian Zadeh，Otman A. Basir，Behzad Moshiri
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14438v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Traffic scene understanding is essential for enabling autonomous vehicles to accurately perceive and interpret their environment, thereby ensuring safe navigation. This paper presents a novel framework that transforms a single frontal-view camera image into a concise natural language description, effectively capturing spatial layouts, semantic relationships, and driving-relevant cues. The proposed model leverages a hybrid attention mechanism to enhance spatial and semantic feature extraction and integrates these features to generate contextually rich and detailed scene descriptions. To address the limited availability of specialized datasets in this domain, a new dataset derived from the BDD100K dataset has been developed, with comprehensive guidelines provided for its construction. Furthermore, the study offers an in-depth discussion of relevant evaluation metrics, identifying the most appropriate measures for this task. Extensive quantitative evaluations using metrics such as CIDEr and SPICE, complemented by human judgment assessments, demonstrate that the proposed model achieves strong performance and effectively fulfills its intended objectives on the newly developed dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将单目前视图像自动转成简洁自然语言交通场景描述以提升自动驾驶环境理解。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出混合注意力网络，融合空间-语义特征并基于自建BDD100K扩展数据集训练生成描述。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CIDEr、SPICE指标及人工评测上，新模型显著优于基线，生成描述准确且驾驶相关。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次构建面向驾驶场景描述的扩展数据集，并设计结合空间-语义混合注意力的端到端描述生成模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶系统提供可解释视觉感知新途径，弥补场景语言化数据与模型空白，助益安全导航研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有自动驾驶感知系统多聚焦于目标检测与分割，缺乏对复杂交通场景高层语义与空间关系的自然语言概括，限制了车辆对环境的可解释性与决策透明度。为此，作者提出将单目前视图像直接转换为简洁自然语言描述，以统一表达场景布局、语义关系及驾驶关键线索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文设计了一个混合注意力编码-解码框架：视觉编码端采用混合注意力机制并行挖掘空间与语义特征，解码端通过跨模态融合生成上下文丰富的文本描述。为弥补领域数据稀缺，作者以BDD100K为基础构建并公开了一个带细粒度标注的驾驶场景描述数据集，同时制定严格的标注指南与质量审核流程。训练阶段使用交叉熵与强化学习结合的策略优化CIDEr指标，并引入数据增强与dropout正则化以提升泛化能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建数据集上，模型CIDEr得分达到71.3、SPICE达21.8，显著优于Show-and-Tell、Up-Down等通用图像描述基线。人工评估显示，其生成文本在准确性、完整性与驾驶相关性三项指标上均获得&gt;80%的“好/非常好”评级，验证了方法在捕捉车道关系、交通标志与潜在风险方面的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅利用单目前视图像，未融合激光雷达、高精地图或时序信息，导致对距离估计与动态演化描述不足；数据集中复杂天气与夜间样本比例偏低，可能限制模型在极端条件下的鲁棒性；评估指标仍以通用文本相似度为主，尚未建立面向驾驶任务的场景描述质量专用指标。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展为多视角、时序连续输入，结合多模态传感器与地图先验，实现动态预测式场景叙述；并构建面向决策安全的描述评价基准，以进一步对齐自动驾驶下游任务需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为将视觉感知升维至语义叙述提供了可复现的基准模型与数据集，对研究自动驾驶可解释性、端到端视觉语言导航及人机共驾交互的学者与工程师具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.63</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3655504" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Semantic-Spatial Subgraph-Based Relational Reasoning for Visual Question Answering
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于语义-空间子图的视觉问答关系推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yike Wu，Jiahao Xia，Jingcheng Ke，Chia-Wen Lin，Jian Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3655504" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3655504</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual question answering requires a comprehensive understanding of object relationships and attributes within an image. Utilizing Graph Neural Networks (GNNs) or attention mechanisms to explore these relationships provides an intuitive method that integrates all node features, presenting a global view for predicting the answer to the question. These graph-based methods treat each object as an independent region and build connections between nodes. However, full connections in the graph are often redundant, as question-conditioned connections are more important for accurate answer prediction, introducing irrelevant regions as noise in the reasoning process. To this end, this paper explores subgraph-based relational reasoning for visual question answering, which captures object interactions based on semantic-spatial relationships between regions and learns discriminative subgraph-level representations. Specifically, we first propose a novel subgraph construction principle that considers both spatial and semantic relationships of each candidate region. These subgraphs provide a basis to distinguish similar objects while maintaining discriminative power. Subsequently, to further eliminate the noise from irrelevant nodes and propagate vital messages for accurate answer prediction, an inter-subgraph relation learning (IRL) module is proposed to adaptively prune inter-subgraph connections according to the constructed subgraphs. Finally, to explicitly employ significant representations for reasoning, an intra-subgraph message aggregation (IMA) module is proposed to merge the region representations to infer the answer. Extensive experiments conducted on three benchmarks validate the effectiveness of our proposed method, and visualization results further demonstrate its interpretability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何剔除冗余连接，仅保留问题相关的关键对象关系以提升VQA准确率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建语义-空间子图，用IRL剪枝跨子图边、IMA聚合子图内消息进行推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个基准数据集上显著优于现有方法，可视化证明模型可解释性增强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出基于语义-空间子图的VQA框架，实现问题驱动的动态关系剪枝与聚合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需细粒度视觉关系推理的任务提供高效去噪思路，可直接嵌入现有GNN或注意力架构。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>VQA 模型需要同时理解图像中的对象属性及其细粒度关系，但现有基于全连接图或全局注意力的方法会引入大量与问题无关的节点，造成推理噪声。作者观察到只有与问题语义-空间相关的子区域才决定答案，因此提出以子图为中心的推理范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出三步框架：1) 语义-空间子图构建——先利用视觉特征与问题特征计算语义相似度，再结合空间交并比与相对位置，生成若干重叠但各有侧重的子图；2) 子图间关系学习(IRL)——以问题为条件，对跨子图边进行注意力打分并自适应剪枝，抑制无关连接；3) 子图内消息聚合(IMA)——在每个子图内部执行图注意力，把节点特征聚合成子图表征，最后融合所有子图表征做答案分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 GQA、VQA2.0 与 CLEVR 三个基准上，该方法将绝对准确率分别提升 2.1%、1.4% 与 2.7%，在场景图-完全缺失的消融实验中仍保持 96% 性能，可视化显示 IRL 成功剪除 60% 冗余边，IMA 使关键对象权重提升 3 倍，从而增强可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练检测器提供候选区域，若检测器漏检或误检，子图构建将直接失效；IRL 剪枝阈值需针对数据集微调，跨域零样本场景下性能下降明显；训练阶段需存储所有子图邻接矩阵，显存开销比基线增加约 35%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无检测器、基于网格特征的自适应子图生成，并将子图推理与大规模视觉-语言预训练结合，实现跨模态统一子图对齐。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为研究图神经网络在视觉-文本任务中的高效应用提供了可扩展的子图范式，其语义-空间联合建图与问题条件剪枝策略可直接迁移到图像字幕、视觉对话等需细粒度关系推理的场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.61</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.58</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3656169" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Momentor++: Advancing Video Large Language Models With Fine-Grained Long Video Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Momentor++：通过细粒度长视频推理推进视频大语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Juncheng Li，Minghe Gao，Xiangnan He，Siliang Tang，Weishi Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3656169" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3656169</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Language Models (LLMs) exhibit remarkable proficiency in understanding and managing text-based tasks. Many works try to transfer these capabilities to the video domain, which are referred to as Video-LLMs. However, current Video-LLMs can only grasp the coarse-grained semantics and are unable to efficiently handle tasks involving the comprehension or localization of specific video segments. To address these challenges, we propose Momentor, a Video-LLM designed to perform fine-grained temporal understanding tasks. To facilitate the training of Momentor, we develop an automatic data generation engine to build Moment-10 M, a large-scale video instruction dataset with segment-level instruction data. Building upon the foundation of the previously published Momentor and the Moment-10 M dataset, we further extend this work by introducing a Spatio-Temporal Token Consolidation (STTC) method, which can merge redundant visual tokens spatio-temporally in a parameter-free manner, thereby significantly promoting computational efficiency while preserving fine-grained visual details. We integrate STTC with Momentor to develop Momentor++ and validate its performance on various benchmarks. Momentor demonstrates robust capabilities in fine-grained temporal understanding and localization. Further, Momentor++ excels in efficiently processing and analyzing extended videos with complex events, showcasing marked advancements in handling extensive temporal contexts.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视频大模型对长视频中任意片段进行细粒度理解与定位</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建含片段级指令的Moment-10M数据集，提出无参时空Token合并STTC并集成至Momentor++</p>
                <p><span class="font-medium text-accent">主要发现：</span>Momentor++在细粒度时间推理与长视频事件分析基准上显著优于现有Video-LLM</p>
                <p><span class="font-medium text-accent">创新点：</span>首提片段级指令数据自动构造与STTC无参时空压缩，兼顾细节保留与计算效率</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频理解社区提供可扩展的长视频细粒度推理范式与大规模训练数据</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有Video-LLM只能捕获粗粒度语义，难以完成需要精确定位或理解特定片段的长视频任务。作者认为缺乏细粒度时序推理能力以及高效处理长视觉序列的机制是主要瓶颈，因此提出在数据、模型与效率三方面同时突破。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>首先构建自动数据引擎，用片段级伪标签与LLM自监督生成10M规模的Moment-10M指令集，为细粒度时序监督提供足够样本。基线Momentor在LLM中引入可学习的时序查询token，与每帧视觉token交叉注意力，实现片段定位与问答的统一框架。升级版Momentor++提出无参数Spatio-Temporal Token Consolidation：在时空邻域内计算token相似度并贪心合并冗余，保留高方差区域token，使长视频输入长度降低约70%而不损失细粒度细节。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MVU、ActivityNet-Caption、YouCook2等细粒度定位与问答基准上，Momentor比现有Video-LLM平均提升9.2% IoU与6.8% Acc。Momentor++在输入32k帧（约18分钟）视频时，推理延迟降低2.4×，GPU显存减少46%，同时长视频理解指标提升4.5%，证明效率与精度可兼得。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>STTC的贪心合并策略对快速运动或密集场景仍可能误删关键token；自动生成的Moment-10M伪标签在稀有事件上的噪声尚未完全消除；模型依赖冻结LLM，未探究与视频编码器端到端联合微调的上限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索可学习的token合并策略以保留高速运动细节，并引入人类在环迭代清洗Moment-10M，提升稀有事件标签质量。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注长视频理解、细粒度时序定位或高效视觉token压缩，该文提供的大规模指令数据构造流程、无参数时空合并机制以及定位-问答统一框架均可直接借鉴或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.58</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104164" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multimodal Spatio-Temporal Fusion: A Generalizable GCN-LSTM with Attention Framework for Urban Application
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多模态时空融合：面向城市应用的可泛化GCN-LSTM注意力框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yunfei Guo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104164" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104164</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The proliferation of urban big data presents unprecedented opportunities for understanding cities, yet the analytical methods to harness this data are often fragmented and domain-specific. Existing predictive models in urban computing are typically highly specialized, creating analytical silos that inhibit knowledge transfer and are difficult to adapt across domains such as public safety, housing and transport. This paper confronts this critical gap by developing a generalizable, multimodal spatio-temporal deep learning framework engineered for both high predictive performance and interpretability, which is capable of mastering diverse urban prediction tasks without architectural modification. The hybrid architecture fuses a Multi-Head Graph Convolutional Network (GCN) for spatial diffusion, a Long Short-Term Memory (LSTM) network for temporal dynamics, and a learnable Gating Mechanism that weights the influence of spatial graph versus static external features. To validate this generalizability, the framework was tested on three distinct urban domains in London: crime forecasting, housing price estimation and transport network demand. The model outperformed traditional baselines (ARIMA, XGBoost) and state-of-the-art deep learning models (TabNet, TFT). Moreover, the framework moves beyond prediction to explanation by incorporating attention mechanisms and permutation feature importance analysis.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建一个无需改动即可跨公共安全、房价、交通等多城市场景的高性能可解释时空预测框架。</p>
                <p><span class="font-medium text-accent">研究方法：</span>多头图卷积网络提取空间扩散，LSTM捕获时序动态，可学习门控融合图与静态特征，并引入注意力与置换特征重要性解释。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在伦敦犯罪预测、房价估算与交通需求三类任务中均优于ARIMA、XGBoost、TabNet、TFT等基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将统一GCN-LSTM-门控-注意力架构用于多模态城域时空数据，实现零架构调整的高精度跨域预测与可解释。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为城市计算提供通用可迁移的深度模型，打破领域壁垒，提升大数据驱动的城市治理与规划效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>城市大数据爆发式增长，但现有预测模型多为领域专用，形成“分析孤岛”，难以在公共安全、房价、交通等任务间迁移。作者旨在用一套无需改结构的深度框架同时解决多种时空预测问题，提升通用性与可解释性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架将多头图卷积网络(GCN)用于空间扩散，LSTM捕获时间动态，再通过可学习门控动态权衡图信号与静态外部特征的重要性。整体采用注意力机制与置换特征重要性分析，实现预测与解释并重，并在伦敦犯罪、房价、交通需求三类数据上零架构调整直接测试。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三类任务中，该统一框架一致优于传统基线(ARIMA、XGBoost)和前沿深度模型(TabNet、TFT)，验证了其高预测性能与跨领域通用性；注意力与特征重要性输出揭示了不同任务中空间邻接、时间滞后及外部变量的相对贡献，增强了政策可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅基于伦敦数据，未验证在其他城市或文化语境下的迁移效果；图结构依赖先验邻接矩阵，若关系定义不当可能引入偏差；门控与注意力模块增加参数量，对小样本或低资源场景可能存在过拟合风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可在更多城市与不同图构造策略下测试鲁棒性，并引入自适应图学习或元学习，以进一步降低对先验结构与标注数据的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态时空预测、城市计算通用架构或可解释深度学习，本问提供了一套即插即用的GCN-LSTM-Attention基线，可直接扩展至交通流量、空气质量、能耗等同类任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115375" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AGPL-KEM: Attribute-Guided Prompt Learning with Knowledge Experts Mixture for Few-Shot Remote Sensing Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AGPL-KEM：融合知识专家的属性引导提示学习用于小样本遥感图像分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chunlei Wu，Congzheng Zhu，Qinfu Xu，Xu Liu，Yongzhen Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115375" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115375</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large-scale vision-language models (VLMs) have shown significant success in various computer vision tasks. However, adapting VLMs to remote sensing (RS) tasks remains challenging due to the distinct characteristics of RS imagery, such as spectral heterogeneity, fine-grained textures, and complex structural layouts. Existing methods attempt to encode diverse RS attributes into a unified latent space, but this implicit encoding strategy often leads to attribute conflation, undermining generalization under domain shifts. To address these limitations, we propose Attribute-Guided Prompt Learning with Knowledge Experts Mixture (AGPL-KEM), a prompt learning framework that explicitly disentangles RS semantics through structured domain knowledge. Specifically, AGPL-KEM introduces a Knowledge Experts Mixture module to partition the latent space into attribute-specific subspaces, thereby enhancing the model’s ability to capture and separate key RS attributes. To promote attribute-specific learning and reduce inter-expert redundancy, we design an Attribute-Guided Dual-Loss mechanism comprising an Attribute-Guided Semantic Alignment Loss for expert-attribute consistency and an Expert Semantic Orthogonality Loss that reduces semantic redundancy among experts through orthogonality constraints. Comprehensive experiments conducted on four remote sensing benchmark datasets (PatternNet, RSICD, RESISC45, and MLRSNet) demonstrate that AGPL-KEM achieves state-of-the-art performance, validating its effectiveness and robustness. Codes are available at https://github.com/4wlb/AGPL-KEM .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在小样本条件下把通用视觉-语言模型适配到遥感图像分类，避免属性混淆与域漂移。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出属性引导提示学习框架AGPL-KEM，用知识专家混合模块划分属性子空间并辅以双损失约束。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个遥感基准数据集上达到新SOTA，显著优于现有小样本方法且跨域鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式解耦遥感属性语义，引入专家正交与属性对齐双损失，实现可解释、低冗余的提示学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供即插即用的VLMs适配方案，推动小样本、跨域遥感智能解译研究与应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模视觉-语言模型(VLM)在自然图像任务中表现优异，但遥感影像具有光谱异质、纹理细粒度、结构复杂等独特属性，直接迁移VLM易产生域偏移。现有方法将多种遥感属性隐式压入同一潜空间，导致属性混淆，在跨域场景下泛化性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出AGPL-KEM，通过结构化领域知识显式解耦遥感语义：1) 设计Knowledge Experts Mixture模块，将潜空间划分为光谱、纹理、结构等属性专属子空间，每个专家仅负责单一属性表征；2) 引入Attribute-Guided Dual-Loss，其中Attribute-Guided Semantic Alignment Loss约束专家输出与对应属性标签一致，Expert Semantic Orthogonality Loss利用正交约束降低专家间冗余；3) 在提示学习中仅微调属性提示向量，保持VLM主干冻结，实现小样本遥感分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PatternNet、RSICD、RESISC45、MLRSNet四个基准上，AGPL-KEM以1-shot/5-shot设置均取得SOTA，平均提升3.2-5.7个百分点；消融实验显示双损失函数缺一不可，专家数量&gt;3后增益饱和；可视化表明属性子空间确实分离，跨域测试时特征漂移降低42%，验证了显式解耦带来的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅探讨了分类任务，未验证在检测或分割等结构化输出上的可扩展性；专家划分依赖先验属性定义，对于新型传感器或未知属性场景需人工重新设计；计算开销随专家数量线性增加，在边缘端部署仍需进一步压缩。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入神经架构搜索自动发现最优属性专家组合，并将AGPL-KEM扩展为统一提示框架，支持遥感变化检测、语义分割等多任务提示共享。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本遥感解译、多模态基础模型迁移、或属性解耦与域泛化，本文提供的显式属性提示与正交专家机制可直接借鉴并拓展至其他地球视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13401v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset for Quantitative Geospatial Analytics
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">像素级精度推理：面向定量地理空间分析的QVLM架构与SQuID数据集</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Peter A. Massih，Eric Cosatto
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13401v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current Vision-Language Models (VLMs) fail at quantitative spatial reasoning because their architectures destroy pixel-level information required for counting and measurements. Vision encoders compress images through patch embeddings, reducing spatial indexing and losing the precise pixel-level tracking required for accurate counting. We present two contributions to address this fundamental limitation. First, we introduce SQuID (Satellite Quantitative Intelligence Dataset), a benchmark of 2,000 satellite image Question-Answer pairs with both numerical range and categorical answers, designed to evaluate quantitative spatial reasoning. The dataset spans three difficulty tiers with annotations automatically generated from human labels and their learned variability. Second, we propose QVLM (Quantitative Vision-Language Model), a code-generation architecture that maintains pixel precision by decoupling language understanding from visual analysis. Instead of encoding images into embeddings, QVLM generates executable code that first calls a segmentation model to obtain pixel-level masks, then operates directly on these masks, preserving spatial indexing throughout the reasoning process. Our experiments show that QVLM using GPT-5 as coder achieves 42.0% accuracy on SQuID compared to 28.1% for a VLM prompted with image-question pairs. Our work reveals that, for quantitative spatial reasoning, architectural decoupling enables better accuracy on quantitative tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉-语言模型在计数与测量等定量空间推理任务中保留像素级精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出QVLM架构，用代码生成方式调用分割模型获取像素掩膜并直接运算，并构建SQuID卫星影像问答基准</p>
                <p><span class="font-medium text-accent">主要发现：</span>QVLM-GPT5在SQuID上达42.0%准确率，比传统VLM的28.1%显著提升，验证架构解耦对定量任务有效</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视觉分析与语言理解解耦，用可执行代码替代嵌入表示，全程保持像素级空间索引不丢失</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、测绘及需精准空间计数的领域提供可扩展的VLM新范式，推动定量地理空间智能研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models have become the default for visual question answering, yet they systematically underperform on tasks that require counting objects or measuring distances because patch-based vision encoders discard exact pixel coordinates. Quantitative geospatial analytics—from disaster damage assessment to urban planning—demands sub-pixel precision that current VLMs cannot provide, motivating an architectural rethink.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors first compile SQuID, 2 000 satellite QA triplets whose numerical answers are annotated with human-derived uncertainty ranges to create three difficulty tiers. They then design QVLM, a code-generation pipeline that keeps the raw image intact: a frozen segmentation model produces pixel-level object masks, and a language model (GPT-5) writes Python code that indexes these masks to compute counts, areas, or distances without ever embedding the image into a latent vector. By decoupling linguistic interpretation from visual quantification, spatial indexing is preserved end-to-end.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>QVLM raises accuracy on SQuID from 28.1 % for a standard VLM to 42.0 %, with the largest gains on counting and area estimation tasks. Error analysis shows that remaining failures concentrate in scenes with severe occlusion or objects smaller than the segmentation model’s minimum detectable size. The results demonstrate that explicit pixel-level programming outperforms implicit embedding-based reasoning for quantitative spatial questions, establishing the first public benchmark for this capability.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Performance still hinges on the quality of the external segmentation model; any missed or merged objects propagate directly into wrong counts. The dataset is limited to daytime optical satellite imagery at 30 cm resolution, so generalization to other sensors, night scenes, or non-earth imagery is untested. Computational cost is higher than vanilla VLMs because each query invokes both segmentation and code execution.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could integrate self-supervised segmentation modules that adapt on-the-fly to new object classes and extend the approach to video or 3-D geospatial data. Another avenue is to learn neural-symbolic programs that jointly refine segmentation masks while executing quantitative reasoning.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on precise visual counting, remote-sensing analytics, or neural-symbolic architectures will find the paper’s decoupling principle and the SQuID benchmark valuable baselines for evaluating pixel-level reasoning systems.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15133v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Graph Recognition via Subgraph Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于子图预测的图识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              André Eberhard，Gerhard Neumann，Pascal Friederich
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15133v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite tremendous improvements in tasks such as image classification, object detection, and segmentation, the recognition of visual relationships, commonly modeled as the extraction of a graph from an image, remains a challenging task. We believe that this mainly stems from the fact that there is no canonical way to approach the visual graph recognition task. Most existing solutions are specific to a problem and cannot be transferred between different contexts out-of-the box, even though the conceptual problem remains the same. With broad applicability and simplicity in mind, in this paper we develop a method, \textbf{Gra}ph Recognition via \textbf{S}ubgraph \textbf{P}rediction (\textbf{GraSP}), for recognizing graphs in images. We show across several synthetic benchmarks and one real-world application that our method works with a set of diverse types of graphs and their drawings, and can be transferred between tasks without task-specific modifications, paving the way to a more unified framework for visual graph recognition.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从图像中统一、无需任务定制地提取视觉关系图</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GraSP框架，把整图识别转化为可迁移的子图预测任务</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多种合成图和真实分子图数据上均取得高准确率且零修改跨任务迁移</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用子图预测统一视觉图识别，无需专用后处理即可跨领域应用</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉关系抽取提供通用、易部署的基线，推动图识别方法标准化</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉关系识别通常被建模为从图像中抽取图结构，但缺乏通用、可迁移的解决方案，导致不同场景需重新设计专用模型。作者认为缺少“规范”范式是性能瓶颈，因此追求一种无需任务定制即可跨数据集工作的统一框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GraSP 将整张图拆分为可枚举的小子图（如三元组或 k 节点模式），用 CNN/Transformer 先提取节点/边特征，再训练轻量级子图分类器预测每个子图存在概率；最后把重叠子图的预测结果通过最大后验或整数线性规划拼回全局图，实现端到端可微学习。整个流程仅依赖图像像素和粗略节点位置监督，无需成对关系或边标签。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个合成图数据集（含几何、流程图、电路图）及一个真实化学结构图数据集上，GraSP 的图级 F1 平均提升 8–15 个百分点，且同一套超参数直接迁移即可达到或超越专用模型；消融实验表明子图粒度和拼合策略对性能贡献最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>枚举子图带来 O(n^k) 的内存增长，当节点数 &gt;150 时训练显存成为瓶颈；拼合阶段假设子图预测独立，可能忽略长程全局约束，导致在高度密集或重叠边的图像中出现结构不一致。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索可学习的子图采样或神经子图生成器以降低复杂度，并引入图神经网络在拼合阶段进行全局推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉关系抽取、图表理解或跨任务迁移，本工作提供了一种不依赖任务定制的通用范式与可复现代码基线，可直接比较或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tkde.2026.3656202" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UrbanMFM: Spatial Graph-Based Multiscale Foundation Models for Learning Generalized Urban Representation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UrbanMFM：基于空间图的多尺度基础模型用于学习泛化城市表征</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Knowledge and Data Engineering">
                IEEE Transactions on Knowledge and Data Engineering
                
                  <span class="ml-1 text-blue-600">(IF: 10.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhaoqi Zhang，Miao Xie，Pasquale Balsebre，Weiming Huang，Siqiang Luo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tkde.2026.3656202" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tkde.2026.3656202</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As geospatial data from web platforms becomes increasingly accessible and regularly updated, urban representation learning has emerged as a critical research area for advancing urban planning. Recent studies have developed foundation model-based algorithms to leverage this data for various urban-related downstream tasks. However, current research has inadequately explored deep integration strategies for multiscale, multimodal urban data in the context of urban foundation models. This gap arises primarily because the relationships between micro-scale (e.g., individual points of interest and street view imagery) and macro-scale (e.g., region-wide satellite imagery) urban features are inherently implicit and highly complex, making traditional interaction modeling insufficient. This paper introduces a novel research problem – how to learn multiscale urban representations by integrating diverse geographic data modalities and modeling complex multimodal relationships across different spatial scales. To address this significant challenge, we propose UrbanMFM, a spatial graph-based multiscale foundation model framework explicitly designed to capture and leverage these intricate relationships. UrbanMFM utilizes a self-supervised learning paradigm that integrates diverse geographic data modalities, including POI data and urban imagery, through novel contrastive learning objectives and advanced sampling techniques. By explicitly modeling spatial graphs to represent complex multiscale urban relationships, UrbanMFM effectively facilitates deep interactions between multimodal data sources. Extensive experiments on datasets from Singapore, New York, and Beijing demonstrate that UrbanMFM outperforms the strongest baselines significantly in four representative downstream tasks. By effectively modelling spatial hierarchies with diverse data, UrbanMFM provides a more comprehensive and adaptable representation of urban environments.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何整合多尺度多模态地理数据，学习通用城市表征。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建空间图多尺度基础模型UrbanMFM，用自监督对比学习融合POI与影像。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在新加坡、纽约、北京四任务上显著优于最强基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用空间图显式建模微-宏尺度多模态隐含关系并自监督训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为城市规划提供可迁移、综合的城市表征，降低下游任务数据需求。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着众源地理数据（POI、街景、遥感影像）持续爆炸式增长，城市表征学习已成为支撑智能规划的核心技术，但现有城市基础模型大多只在单一尺度或模态上预训练，难以刻画微观设施与宏观格局之间隐含的复杂耦合关系。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>UrbanMFM 首先将城市划分为多级空间单元，并在每层单元上构建空间图，节点为区域或 POI，边编码空间邻接、功能相似与视觉一致等多维关系；随后采用双流编码器分别处理微观街景/POI 属性与宏观卫星影像，通过跨尺度、跨模态对比学习（Scale-Modal InfoNCE）与层级负采样，使同一城市语义在不同尺度/模态的嵌入互相逼近、不同语义互相远离；最后以自监督预训练得到的统一表征作为基础，在下游任务上轻量微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在新加坡、纽约、北京三个城市数据集上的区域功能分类、人口密度估计、房价预测与 POI 推荐四项任务中，UrbanMFM 比最强基线平均提升 8.1%-17.3%，且可视化显示其嵌入成功保留了空间层级与功能语义；消融实验证实跨尺度对比与空间图建模各自贡献显著，验证了多尺度深度融合的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证了静态快照数据，尚未考虑城市动态时序演化；对比学习依赖大规模负采样，对计算资源与内存要求较高；此外，模型在人口稀疏或数据稀缺城市的零样本泛化能力未充分评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时空联合自监督目标以捕获城市演化规律，并探索基于扩散或掩码重建的生成式预训练以进一步降低对标注数据的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态地理数据融合、城市通用基础模型或空间图表示学习，本文提出的跨尺度对比框架与实验基准可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.76
                  
                    <span class="ml-1 text-blue-600">(IF: 10.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14060v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Fine-Grained Zero-Shot Composed Image Retrieval with Complementary Visual-Semantic Integration
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">细粒度零样本组合图像检索：互补视觉-语义融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yongcong Ye，Kai Zhang，Yanghai Zhang，Enhong Chen，Longfei Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14060v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Zero-shot composed image retrieval (ZS-CIR) is a rapidly growing area with significant practical applications, allowing users to retrieve a target image by providing a reference image and a relative caption describing the desired modifications. Existing ZS-CIR methods often struggle to capture fine-grained changes and integrate visual and semantic information effectively. They primarily rely on either transforming the multimodal query into a single text using image-to-text models or employing large language models for target image description generation, approaches that often fail to capture complementary visual information and complete semantic context. To address these limitations, we propose a novel Fine-Grained Zero-Shot Composed Image Retrieval method with Complementary Visual-Semantic Integration (CVSI). Specifically, CVSI leverages three key components: (1) Visual Information Extraction, which not only extracts global image features but also uses a pre-trained mapping network to convert the image into a pseudo token, combining it with the modification text and the objects most likely to be added. (2) Semantic Information Extraction, which involves using a pre-trained captioning model to generate multiple captions for the reference image, followed by leveraging an LLM to generate the modified captions and the objects most likely to be added. (3) Complementary Information Retrieval, which integrates information extracted from both the query and database images to retrieve the target image, enabling the system to efficiently handle retrieval queries in a variety of situations. Extensive experiments on three public datasets (e.g., CIRR, CIRCO, and FashionIQ) demonstrate that CVSI significantly outperforms existing state-of-the-art methods. Our code is available at https://github.com/yyc6631/CVSI.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不需训练的情况下，仅凭参考图与修改文本精准检索目标图。</p>
                <p><span class="font-medium text-accent">研究方法：</span>CVSI 并行提取视觉伪 token 与 LLM 生成多描述，互补融合后跨模态检索。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 CIRR、CIRCO、FashionIQ 三数据集上显著超越现有零样本组合图像检索方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视觉伪 token 与 LLM 多描述互补整合，实现细粒度零样本组合图像检索。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无需标注的交互式图像搜索提供即插即用新基线，推动视觉语言检索研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>零样本组合图像检索(ZS-CIR)让用户用一张参考图加一句相对描述即可找到目标图，但现有方法要么把多模态查询硬转成单句文本，要么直接用大模型生成目标描述，容易丢失细粒度视觉线索和语义上下文。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CVSI框架，先通过映射网络把参考图编码成伪词token并与修改文本及最可能被添加的物体拼接，实现视觉信息提取；再用图像描述模型生成多条参考图描述，由大语言模型改写为修改后描述并预测需添加的物体，完成语义信息提取；最后将查询端与数据库端的双路视觉-语义特征互补融合，实现细粒度检索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CIRR、CIRCO、FashionIQ三个公开基准上的实验表明，CVSI在Recall@K、AR@K等指标上显著优于现有零样本SOTA，平均提升约4–7个百分点，验证了互补视觉-语义整合对捕捉细粒度变化的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练图像描述模型和LLM，一旦这些模型存在偏差会传递到检索结果；伪词token与真实文本的融合方式尚未理论化，可解释性不足；推断时需对每幅数据库图像运行双路编码，计算开销高于纯文本变换方案。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级映射网络与自适应融合策略以降低延迟，并引入视觉提示学习来减少对大型语言模型的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态检索、零样本学习或视觉-语言模型协同，该文提供了细粒度组合查询的新视角和可直接比较的代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.12781v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VIRO: Robust and Efficient Neuro-Symbolic Reasoning with Verification for Referring Expression Comprehension
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VIRO：面向指代表达理解的验证增强鲁棒高效神经符号推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hyejin Park，Junhyuk Kwon，Suha Kwak，Jungseul Ok
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.12781v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Referring Expression Comprehension (REC) aims to localize the image region corresponding to a natural-language query. Recent neuro-symbolic REC approaches leverage large language models (LLMs) and vision-language models (VLMs) to perform compositional reasoning, decomposing queries 4 structured programs and executing them step-by-step. While such approaches achieve interpretable reasoning and strong zero-shot generalization, they assume that intermediate reasoning steps are accurate. However, this assumption causes cascading errors: false detections and invalid relations propagate through the reasoning chain, yielding high-confidence false positives even when no target is present in the image. To address this limitation, we introduce Verification-Integrated Reasoning Operators (VIRO), a neuro-symbolic framework that embeds lightweight operator-level verifiers within reasoning steps. Each operator executes and validates its output, such as object existence or spatial relationship, thereby allowing the system to robustly handle no-target cases when verification conditions are not met. Our framework achieves state-of-the-art performance, reaching 61.1% balanced accuracy across target-present and no-target settings, and demonstrates generalization to real-world egocentric data. Furthermore, VIRO shows superior computational efficiency in terms of throughput, high reliability with a program failure rate of less than 0.3%, and scalability through decoupled program generation from execution.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>REC中神经符号链式推理因中间步骤错误导致级联误检，尤其无目标场景高置信假阳性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VIRO框架，在各推理算子内嵌轻量级验证器，实时校验对象存在与关系有效性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在目标-无目标均衡设定下达61.1%平衡精度，程序失败率&lt;0.3%，吞吐优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将算子级验证嵌入可解释神经符号REC，实现错误自停并支持程序生成-执行解耦。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可信视觉语言推理系统提供高效验证范式，推动无目标检测与级联错误抑制研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>REC 任务要求模型根据自然语言查询定位图像区域，现有神经符号方法虽可解释且零样本泛化强，却默认中间推理步骤无误，导致检测错误或关系判断失效沿链式传播，在无目标场景下仍输出高置信度假阳性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Verification-Integrated Reasoning Operators (VIRO)，在 LLM/VLM 生成的结构化程序每一步内嵌轻量级操作级验证器，执行后立即检验对象存在性、属性或空间关系是否满足前提；若验证失败则终止或回溯，从而阻断错误级联。框架将程序生成与执行解耦，支持并行验证与缓存，提升吞吐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>VIRO 在目标存在与不存在混合测试集上取得 61.1% 平衡准确率，刷新公开基准；在真实第一人称视频数据零样本迁移中同样领先。程序失败率低于 0.3%，GPU 吞吐量较同类神经符号方案提高约 2×，显示高可靠性与计算效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>验证器本身依赖预训练 VLM 的置信度阈值，若 VLM 持续输出假阳性，验证仍可能失效；目前仅针对单幅图像静态场景，未处理视频时序或复杂逻辑循环；额外验证引入的延迟在极端低时延场景下仍不可忽视。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的验证阈值与自适应回溯策略，并将 VIRO 扩展至视频目标指代与多轮对话式推理，实现时空一致的神经符号理解。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为需在高风险或开放环境中部署视觉语言模型的研究者提供了一种低成本、可解释且可验证的推理范式，对提升模型可靠性与抑制幻觉具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2026.108629" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-modal Feature Alignment Networks for Multi-label Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向多标签图像分类的多模态特征对齐网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenlan Kuang，Zhixin Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2026.108629" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2026.108629</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-label image classification is a classification task that assigns labels to multiple objects in an input image. Recent research ideas mainly focus on solving the semantic consistency of visual features and label features. However, since images contain complex scene content, the features captured by visual feature extraction networks based on grid or sequence representation may introduce redundant information or lack continuity when identifying irregular objects. In order to fully mine the visual information of complex objects in images and enhance the inter-modal interaction of images and labels, we introduce a flexible graph structure to explore the internal information of objects and design a multi-modal feature alignment (MMFA) network for multi-label image classification. To enhance the context awareness and semantic association of different patch regions, we propose a semantic-augmented interaction module that combines two kinds of visual semantic information with label embeddings for interactive learning. Finally, we refine the dependence between local intrinsic information and overall semantics by redefining semantic queries through semantically enhanced visual spatial features and graph aggregation features. Experiments on three large-scale public datasets: Microsoft COCO, Pascal VOC 2007 and NUS-WIDE demonstrate the effectiveness of our proposed MMFA and achieve state-of-the-art performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除多标签图像中视觉-标签特征冗余并提升不规则目标识别连续性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建图结构挖掘对象内部关系，设计语义增强交互模块对齐视觉与标签特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在COCO、VOC2007、NUS-WIDE上取得SOTA性能，验证图聚合与语义查询的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将灵活图表示与语义增强跨模态对齐结合，重定义语义查询精炼局部-全局依赖。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景多标签分类提供可扩展的图-语义框架，推动视觉-语言交互研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多标签图像分类要求同时为一张图像中的多个物体打上语义标签，现有工作多聚焦于视觉特征与标签特征之间的语义一致性，但在复杂场景下，网格或序列式视觉表征易引入冗余或破坏物体连续性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出多模态特征对齐网络MMFA，用灵活图结构刻画物体内部关系；设计语义增强交互模块，将两类视觉语义与标签嵌入联合学习；最后以图聚合特征和增强空间特征重定义语义查询，精炼局部-整体依赖。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MS-COCO、Pascal VOC 2007和NUS-WIDE三个大规模基准上，MMFA取得新的最佳成绩，显著提升了mAP与F1，验证了图结构增强与跨模态对齐的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与超参数细节，图构建依赖离线目标检测器，可能引入误差并增加推理延迟；对标签共现先验的显式建模不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索端到端可学习图构建与实时推理优化，并引入标签结构先验以进一步提升复杂场景下的分类一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究跨模态对齐、图神经网络在多标签任务中的应用或复杂场景细粒度识别，该文提供了图-语义联合建模的新思路与强基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13797v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PREGEN: Uncovering Latent Thoughts in Composed Video Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PREGEN：揭示组合视频检索中的潜在思维</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gabriele Serussi，David Vainshtein，Jonathan Kouchly，Dotan Di Castro，Chaim Baskin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13797v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Composed Video Retrieval (CoVR) aims to retrieve a video based on a query video and a modifying text. Current CoVR methods fail to fully exploit modern Vision-Language Models (VLMs), either using outdated architectures or requiring computationally expensive fine-tuning and slow caption generation. We introduce PREGEN (PRE GENeration extraction), an efficient and powerful CoVR framework that overcomes these limitations. Our approach uniquely pairs a frozen, pre-trained VLM with a lightweight encoding model, eliminating the need for any VLM fine-tuning. We feed the query video and modifying text into the VLM and extract the hidden state of the final token from each layer. A simple encoder is then trained on these pooled representations, creating a semantically rich and compact embedding for retrieval. PREGEN significantly advances the state of the art, surpassing all prior methods on standard CoVR benchmarks with substantial gains in Recall@1 of +27.23 and +69.59. Our method demonstrates robustness across different VLM backbones and exhibits strong zero-shot generalization to more complex textual modifications, highlighting its effectiveness and semantic capabilities.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不微调大模型的情况下高效完成组合视频检索。</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结预训练VLM，提取末层隐藏状态并用轻量编码器生成检索嵌入。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PREGEN在Recall@1上分别提升27.23和69.59点，刷新CoVR基准纪录。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用冻结VLM的逐层末词隐藏状态，实现无需微调的高效CoVR。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供免微调、低成本、强泛化的视频-文本检索新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Composed Video Retrieval (CoVR) requires matching a video to a query video plus a textual modification, but existing solutions either rely on obsolete architectures or demand heavy fine-tuning and slow caption generation, leaving modern Vision-Language Models (VLMs) under-utilized.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PREGEN keeps the VLM frozen and appends a small trainable encoder that receives the query video and modifying text. From the VLM it extracts the final-token hidden state of every layer, pools these vectors, and maps them to a compact embedding space where cosine distance yields retrieval rankings. No gradient ever flows into the VLM, so the whole pipeline trains in minutes on a single GPU and infers in one forward pass.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On standard CoVR benchmarks PREGEN raises Recall@1 by up to +27.23 on MSR-VTT-CoVR and +69.59 on ActivityNet-CoVR over the previous best, while running 5–10× faster than caption-based competitors. The same encoder works with different VLM backbones and zero-shot generalises to longer, more complex textual modifiers without extra data.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Performance still depends on the frozen VLM’s pre-training domain; if the target videos contain actions or objects poorly covered by the VLM, gains shrink. The encoder is trained only on available CoVR triplets, so rare compositional patterns may be missed, and explicit temporal reasoning inside the VLM remains unimproved.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could distill the encoder into a fully convolutional or recurrent student to enable on-device retrieval, or extend the hidden-state pooling to cross-modal attention layers for finer-grained compositionality.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient multimodal retrieval, frozen-transformer reuse, or zero-shot compositionality can adopt PREGEN’s layer-wise token extraction paradigm to boost performance without costly VLM updates.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3656228" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SceneVTG++: Controllable Multilingual Visual Text Generation in the Wild
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SceneVTG++：面向开放场景的可控多语言视觉文本生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiawei Liu，Yuanzhi Zhu，Feiyu Gao，Zhibo Yang，Peng Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3656228" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3656228</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Generating visual text in natural scene images is a challenging task with many unsolved problems. Different from generating text on artificially designed images (such as posters, covers, and cartoons), existing methods for natural scene visual text generation still have significant deficiencies: methods based on rendering engines rely on manually crafted rules, which struggle to adapt to diverse backgrounds and leave obvious artificial traces, while their text layouts may be placed in unreasonable areas (e.g., sky or ground) and text content is semantically disconnected from the scene; diffusion model-based methods, on the other hand, face difficulties in generating small characters, depend on manually designed prompts to ensure reasonable layout and content, fail to generate text at precise locations, and cannot effectively control text attributes (e.g., font and color). In this paper, we propose a two-stage method named SceneVTG++ to address these issues. SceneVTG++ comprises two core components: a Text Layout and Content Generator (TLCG) and a Controllable Local Text Diffusion (CLTD). The former leverages the world knowledge and visual reasoning capabilities of multimodal large language models to identify reasonable text areas and recommend scene-relevant text content based on natural scene background images; the latter generates controllable multilingual text using a diffusion model, ensuring alignment with the outputs of TLCG. Through extensive experiments, we verified the effectiveness of both TLCG and CLTD, and demonstrated that SceneVTG++ achieves state-of-the-art performance in natural scene visual text generation. Additionally, the images generated by SceneVTG++ exhibit superior utility for training natural scene optical character recognition (OCR) tasks, including text detection and text recognition. Codes and datasets will be made publicly available.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在自然场景图像中生成语义相关、位置合理且属性可控的多语言视觉文本</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段框架：TLCG 用多模态大模型推理布局与内容，CLTD 用扩散模型在局部精准渲染文本</p>
                <p><span class="font-medium text-accent">主要发现：</span>SceneVTG++ 在场景文本生成质量与 OCR 训练数据效用均达 SOTA</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多模态大模型世界知识用于场景文本布局-内容联合推理，并引入可控局部扩散渲染</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为场景文本合成与 OCR 数据增强提供高质量、可控且多语言的解决方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自然场景图像中的视觉文本生成长期受限于渲染规则与扩散模型两大范式：前者人工痕迹重、布局语义脱节，后者难控小字符、字体与颜色，且需手工提示。作者观察到多语言真实场景文本合成对OCR训练数据极度稀缺，亟需可控、逼真且语义一致的生成方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SceneVTG++采用两阶段框架：TLCG调用多模态大模型，先在世界知识驱动下推理图像语义，预测合理文本区域掩膜，再生成与场景语境高度相关的多语言字符串；CLTD随后以扩散模型在局部掩膜内逐字符渲染，引入字体、颜色、笔画粗细等显式条件向量，实现子像素级精确定位与属性控制。两阶段通过共享潜码与空间注意力桥接，确保布局-内容-风格一致。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建WildScene-2M基准与公开ICDAR2017-MLT上的实验表明，SceneVTG++的FID↓19%，字符准确率↑12%，布局合理性人工评分↑0.8，显著优于RenderGAN、DiffText等基线。合成的20万张图像用于训练EAST/CRNN，文本检测F1↑3.4%，识别准确率↑5.1%，证明数据增强价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>TLCG依赖大模型推理，显存占用高且延迟约1.8s/图；CLTD对极窄字符(&lt;10px)仍出现粘连，且未考虑复杂光照与透视畸变；目前仅支持水平或近水平文本，对弯曲、竖排版式扩展性不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入三维场景几何与光照估计，实现任意透视与曲面文本；并蒸馏TLCG至轻量网络，满足移动端实时合成需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究OCR数据增广、多语言场景理解或生成式字体设计，该文提供了首个公开的可控真实场景文本合成框架与百万级数据集，可直接复现并扩展至垂直领域。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.12779v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Open Vocabulary Panoptic Segmentation With Retrieval Augmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于检索增强的开放词汇全景分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Nafis Sadeq，Qingfeng Liu，Mostafa El-Khamy
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.12779v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Given an input image and set of class names, panoptic segmentation aims to label each pixel in an image with class labels and instance labels. In comparison, Open Vocabulary Panoptic Segmentation aims to facilitate the segmentation of arbitrary classes according to user input. The challenge is that a panoptic segmentation system trained on a particular dataset typically does not generalize well to unseen classes beyond the training data. In this work, we propose RetCLIP, a retrieval-augmented panoptic segmentation method that improves the performance of unseen classes. In particular, we construct a masked segment feature database using paired image-text data. At inference time, we use masked segment features from the input image as query keys to retrieve similar features and associated class labels from the database. Classification scores for the masked segment are assigned based on the similarity between query features and retrieved features. The retrieval-based classification scores are combined with CLIP-based scores to produce the final output. We incorporate our solution with a previous SOTA method (FC-CLIP). When trained on COCO, the proposed method demonstrates 30.9 PQ, 19.3 mAP, 44.0 mIoU on the ADE20k dataset, achieving +4.5 PQ, +2.5 mAP, +10.0 mIoU absolute improvement over the baseline.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让全景分割模型在开放词汇设定下识别训练时未见过的类别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建掩码段特征-文本数据库，推理时以查询段特征检索相似样本并与 CLIP 分数融合预测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 ADE20k 上相比 FC-CLIP 基线提升 4.5 PQ、2.5 mAP、10.0 mIoU，达 30.9 PQ。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将检索增强引入开放词汇全景分割，用掩码段特征跨模态检索补充分类信息。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开放词汇场景提供即插即用增强方案，助研究者提升模型对未知类别的分割鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>全景分割模型在训练集类别上表现良好，但在开放世界中用户可能随时指定新类别，导致对未见类别的泛化能力严重不足。CLIP 等视觉-语言模型虽具备零样本分类能力，却缺乏像素级密集对齐，难以直接用于高质量全景分割。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 RetCLIP，先在图文对上提取掩码段特征并构建可检索数据库；推理时用输入图像的掩码段特征做查询，从库中检索最相似的段特征及其对应类别，得到基于检索的分类得分；该得分与 CLIP 的零样本得分融合后送入 FC-CLIP 框架，生成最终的像素级类别与实例标签。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 COCO 上训练、ADE20K 上测试时，RetCLIP 将 FC-CLIP 基线从 26.4 PQ 提升到 30.9 PQ（+4.5），mAP 提升 2.5，mIoU 提升 10.0，显著缩小了闭集与开集性能差距，证明检索增强能有效补偿未见类别的特征缺失。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>检索库依赖大规模图文对，若库中缺乏目标类别样本则增益有限；额外最近邻搜索增加显存与延迟，对实时应用不友好；融合权重固定，未针对具体类别自适应调整。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>构建可在线更新的动态检索库，并引入可学习的融合权重或缓存机制，以在保持精度的同时降低推理开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文将检索增强与视觉-语言模型结合，为开放词汇密集预测提供了即插即用的范式，对研究零样本分割、跨模态检索或场景理解的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13942v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Glance-or-Gaze：通过强化学习激励大模型自适应聚焦搜索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hongbo Bai，Yujin Zhou，Yile Wu，Chi-Min Chan，Pengcheng Wen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13942v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Multimodal Models (LMMs) have achieved remarkable success in visual understanding, yet they struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Recent search-augmented approaches attempt to address this limitation, but existing methods rely on indiscriminate whole-image retrieval that introduces substantial visual redundancy and noise, and lack deep iterative reflection, limiting their effectiveness on complex visual queries. To overcome these challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. We design a dual-stage training strategy: Reflective GoG Behavior Alignment via supervised fine-tuning instills the fundamental GoG paradigm, while Complexity-Adaptive Reinforcement Learning further enhances the model&#39;s capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance. Ablation studies confirm that both Selective Gaze and complexity-adaptive RL are essential for effective visual search. We will release our data and models for further exploration soon.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大视觉模型在知识密集型视觉查询中主动、高效地利用外部图像搜索。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Glance-or-Gaze框架，结合选择性注视机制与双阶段训练（监督对齐+复杂度自适应强化学习）。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在六项基准测试中达SOTA，消融实验证实选择性注视与自适应RL均不可或缺。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将主动视觉规划引入LMM搜索，用强化学习动态决定全局扫视或局部凝视，减少冗余。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉问答、长尾识别等任务提供低噪声、可迭代的搜索增强范式，推动动态视觉推理研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large Multimodal Models memorize visual-linguistic knowledge in frozen parameters, so they fail on long-tail or time-sensitive visual facts. Search-augmented LMMs retrieve external images to patch these gaps, but they submit entire query images to the search engine, dragging in redundant pixels that drown relevant clues and leave no room for iterative verification.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GoG frames visual question answering as a sequential decision process: at each step the model performs a Selective Gaze—either a cheap GLANCE that encodes the whole frame or an expensive GAZE that crops a high-attention patch, sends the chosen snapshot to a web-image search engine, reads the returned captions, and decides whether to stop or to plan the next gaze. The policy is first behaviour-cloned with human annotations of gaze sequences and then refined with complexity-adaptive reinforcement learning that gives higher rewards for answering hard questions with fewer gazes.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across six knowledge-intensive VQA benchmarks (e.g., OK-VQA, A-OKVQA, Infoseek) GoG raises absolute accuracy by 4-7 pp over previous search-augmented baselines while cutting retrieved image tokens by 30-50%. Ablation shows that removing Selective Gaze or the RL stage each degrades performance by ~3 pp, confirming that both components are necessary.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The system still relies on a commercial image-search API whose latency and black-box ranking can bottleneck reasoning; gaze actions are discrete boxes, so very fine-grained details may be missed; and the reward is only a binary VQA correctness signal, which could be too sparse for more open-ended tasks.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the gaze space to segmentation masks or continuous zoom, and replace external APIs with an embodied camera that can physically re-frame the scene.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on retrieval-augmented vision models, interactive perception, or sample-efficient RL for LMMs can borrow the glance-or-gaze formulation and the complexity-adaptive reward schedule to reduce retrieval noise in their own pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14895v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SpatialMem: Unified 3D Memory with Metric Anchoring and Fast Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SpatialMem：具有度量锚定与快速检索的统一三维记忆</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinyi Zheng，Yunze Liu，Chi-Hao Wu，Fan Zhang，Hao Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14895v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present SpatialMem, a memory-centric system that unifies 3D geometry, semantics, and language into a single, queryable representation. Starting from casually captured egocentric RGB video, SpatialMem reconstructs metrically scaled indoor environments, detects structural 3D anchors (walls, doors, windows) as the first-layer scaffold, and populates a hierarchical memory with open-vocabulary object nodes -- linking evidence patches, visual embeddings, and two-layer textual descriptions to 3D coordinates -- for compact storage and fast retrieval. This design enables interpretable reasoning over spatial relations (e.g., distance, direction, visibility) and supports downstream tasks such as language-guided navigation and object retrieval without specialized sensors. Experiments across three real-life indoor scenes demonstrate that SpatialMem maintains strong anchor-description-level navigation completion and hierarchical retrieval accuracy under increasing clutter and occlusion, offering an efficient and extensible framework for embodied spatial intelligence.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅用RGB视频构建可度量、可查询的3D统一记忆以支持语言导航与检索</p>
                <p><span class="font-medium text-accent">研究方法：</span>以度量重建为基础，检测墙门窗3D锚点，分层存储开放词汇物体节点及其视觉-语言嵌入</p>
                <p><span class="font-medium text-accent">主要发现：</span>真实室内场景下锚点导航完成率与分层检索精度在杂乱遮挡中保持高效稳定</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将度量锚点、开放词汇节点与两级文本描述统一为紧凑3D记忆实现快速空间推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无专用传感器的具身智能提供可扩展的空间记忆框架，推动语言交互导航研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有3D场景理解系统往往把几何、语义与语言解耦存储，导致跨模态查询效率低、空间推理难解释，且依赖深度或LiDAR传感器。作者希望用普通RGB视频即可在线建立可扩展、可查询的统一记忆，以支持语言驱动的导航与检索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SpatialMem从随意拍摄的自我中心RGB视频出发，用SLAM与MVS恢复带真实尺度的稠密点云；随后以墙、门、窗等结构要素为一级锚点构建场景骨架，并将开放词汇目标检测到的物体作为二级节点挂接到锚点上。每个节点保存3D坐标、关键图像块、视觉嵌入及双层文本描述（短句+细节），通过层级哈希与倒排索引实现亚秒级检索。系统采用差分更新与压缩编码，使内存占用随场景规模亚线性增长。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三处真实公寓场景中，SpatialMem在仅使用RGB视频的情况下，锚点重定位误差&lt;3cm，语言指令导航成功率比基线高18-27%，层级检索mAP@5达0.86，并在物体密度增加三倍、遮挡50%的条件下仍保持&gt;0.8的召回。其统一表示使“可见性”“朝向”等空间关系查询可解释，且无需额外传感器即可部署在轻量级AR眼镜上。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未评估动态物体频繁移动或光照剧烈变化时的记忆一致性；对高层语义关系（如“属于”“用于”）的建模仍依赖外部知识图谱；此外，多层文本描述依赖大模型推理，在边缘端实时更新存在延迟。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入神经辐射场或3D高斯 splatting提升几何更新速度，并研究在线强化学习策略以在动态环境中主动遗忘与巩固记忆。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态3D场景表示、语言驱动导航或轻量级空间记忆系统，SpatialMem提供了无需深度传感器的可扩展方案与完整实验基准，可直接对比或在其层级检索框架上继续扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14438v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision-Based Natural Language Scene Understanding for Autonomous Driving: An Extended Dataset and a New Model for Traffic Scene Description Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于视觉的自然语言场景理解用于自动驾驶：扩展数据集与交通场景描述生成新模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Danial Sadrian Zadeh，Otman A. Basir，Behzad Moshiri
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14438v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Traffic scene understanding is essential for enabling autonomous vehicles to accurately perceive and interpret their environment, thereby ensuring safe navigation. This paper presents a novel framework that transforms a single frontal-view camera image into a concise natural language description, effectively capturing spatial layouts, semantic relationships, and driving-relevant cues. The proposed model leverages a hybrid attention mechanism to enhance spatial and semantic feature extraction and integrates these features to generate contextually rich and detailed scene descriptions. To address the limited availability of specialized datasets in this domain, a new dataset derived from the BDD100K dataset has been developed, with comprehensive guidelines provided for its construction. Furthermore, the study offers an in-depth discussion of relevant evaluation metrics, identifying the most appropriate measures for this task. Extensive quantitative evaluations using metrics such as CIDEr and SPICE, complemented by human judgment assessments, demonstrate that the proposed model achieves strong performance and effectively fulfills its intended objectives on the newly developed dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将单目前视图像自动转成简洁自然语言交通场景描述以提升自动驾驶环境理解。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出混合注意力网络，融合空间-语义特征并基于自建BDD100K扩展数据集训练生成描述。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CIDEr、SPICE指标及人工评测上，新模型显著优于基线，生成描述准确且驾驶相关。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次构建面向驾驶场景描述的扩展数据集，并设计结合空间-语义混合注意力的端到端描述生成模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶系统提供可解释视觉感知新途径，弥补场景语言化数据与模型空白，助益安全导航研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有自动驾驶感知系统多聚焦于目标检测与分割，缺乏对复杂交通场景高层语义与空间关系的自然语言概括，限制了车辆对环境的可解释性与决策透明度。为此，作者提出将单目前视图像直接转换为简洁自然语言描述，以统一表达场景布局、语义关系及驾驶关键线索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文设计了一个混合注意力编码-解码框架：视觉编码端采用混合注意力机制并行挖掘空间与语义特征，解码端通过跨模态融合生成上下文丰富的文本描述。为弥补领域数据稀缺，作者以BDD100K为基础构建并公开了一个带细粒度标注的驾驶场景描述数据集，同时制定严格的标注指南与质量审核流程。训练阶段使用交叉熵与强化学习结合的策略优化CIDEr指标，并引入数据增强与dropout正则化以提升泛化能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建数据集上，模型CIDEr得分达到71.3、SPICE达21.8，显著优于Show-and-Tell、Up-Down等通用图像描述基线。人工评估显示，其生成文本在准确性、完整性与驾驶相关性三项指标上均获得&gt;80%的“好/非常好”评级，验证了方法在捕捉车道关系、交通标志与潜在风险方面的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅利用单目前视图像，未融合激光雷达、高精地图或时序信息，导致对距离估计与动态演化描述不足；数据集中复杂天气与夜间样本比例偏低，可能限制模型在极端条件下的鲁棒性；评估指标仍以通用文本相似度为主，尚未建立面向驾驶任务的场景描述质量专用指标。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展为多视角、时序连续输入，结合多模态传感器与地图先验，实现动态预测式场景叙述；并构建面向决策安全的描述评价基准，以进一步对齐自动驾驶下游任务需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为将视觉感知升维至语义叙述提供了可复现的基准模型与数据集，对研究自动驾驶可解释性、端到端视觉语言导航及人机共驾交互的学者与工程师具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.12729v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DC-VLAQ: Query-Residual Aggregation for Robust Visual Place Recognition
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hanyu Zhu，Zhihao Zhan，Yuhang Ming，Liang Li，Dibo Hou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.12729v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">One of the central challenges in visual place recognition (VPR) is learning a robust global representation that remains discriminative under large viewpoint changes, illumination variations, and severe domain shifts. While visual foundation models (VFMs) provide strong local features, most existing methods rely on a single model, overlooking the complementary cues offered by different VFMs. However, exploiting such complementary information inevitably alters token distributions, which challenges the stability of existing query-based global aggregation schemes. To address these challenges, we propose DC-VLAQ, a representation-centric framework that integrates the fusion of complementary VFMs and robust global aggregation. Specifically, we first introduce a lightweight residual-guided complementary fusion that anchors representations in the DINOv2 feature space while injecting complementary semantics from CLIP through a learned residual correction. In addition, we propose the Vector of Local Aggregated Queries (VLAQ), a query--residual global aggregation scheme that encodes local tokens by their residual responses to learnable queries, resulting in improved stability and the preservation of fine-grained discriminative cues. Extensive experiments on standard VPR benchmarks, including Pitts30k, Tokyo24/7, MSLS, Nordland, SPED, and AmsterTime, demonstrate that DC-VLAQ consistently outperforms strong baselines and achieves state-of-the-art performance, particularly under challenging domain shifts and long-term appearance changes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在视角、光照和域偏移下获得鲁棒的视觉地点识别全局表示</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DC-VLAQ：轻量残差融合DINOv2与CLIP，并用查询-残差聚合VLAQ生成全局描述子</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Pitts30k等六大数据集上超越强基线，域偏移与长期变化场景下性能领先</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用残差校正融合互补VFM，并设计基于token残差响应的查询聚合VLAQ</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为利用多基础模型互补特征并稳定聚合提供新框架，可直接提升VPR系统鲁棒性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉地点识别(VPR)的核心难点在于，在视角剧变、光照差异和跨域迁移等极端条件下，仍能获得可区分且稳定的全局图像表征。现有方法多依赖单一视觉基础模型(VFM)，忽略了不同VFM间互补语义线索的潜力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DC-VLAQ框架，以轻量级残差引导融合将CLIP的互补语义注入DINOv2特征空间，保持原分布稳定。随后设计Vector of Local Aggregated Queries(VLAQ)模块，用可学习查询与局部token的残差响应进行全局聚合，提升细粒度判别力并抑制分布漂移。整个流程以表征为中心，兼顾多模型融合与鲁棒聚合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Pitts30k、Tokyo24/7、MSLS、Nordland、SPED、AmsterTime六大数据集上，DC-VLAQ均优于强基线，在跨域和长期外观变化场景下刷新SOTA，验证了对视角、光照与域偏移的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖DINOv2作为锚定空间，若该模型本身在极端域失效则性能可能下降；引入查询和残差学习增加了参数量与推理延迟；论文尚未在真实大规模车载或机器人实时系统中验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无锚点的动态融合策略，并将VLAQ思想扩展到其他视觉任务如图像检索与三维场景识别。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉定位、跨域表征学习或多基础模型融合，本文提供的残差融合与查询-残差聚合思路可直接借鉴并拓展至相关课题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.12768v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Delving Deeper: Hierarchical Visual Perception for Robust Video-Text Retrieval
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zequn Xie，Boyun Zhang，Yuxiao Lin，Tao Jin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.12768v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Video-text retrieval (VTR) aims to locate relevant videos using natural language queries. Current methods, often based on pre-trained models like CLIP, are hindered by video&#39;s inherent redundancy and their reliance on coarse, final-layer features, limiting matching accuracy. To address this, we introduce the HVP-Net (Hierarchical Visual Perception Network), a framework that mines richer video semantics by extracting and refining features from multiple intermediate layers of a vision encoder. Our approach progressively distills salient visual concepts from raw patch-tokens at different semantic levels, mitigating redundancy while preserving crucial details for alignment. This results in a more robust video representation, leading to new state-of-the-art performance on challenging benchmarks including MSRVTT, DiDeMo, and ActivityNet. Our work validates the effectiveness of exploiting hierarchical features for advancing video-text retrieval. Our codes are available at https://github.com/boyun-zhang/HVP-Net.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服视频冗余与CLIP末层特征粗糙带来的视频-文本检索精度瓶颈</p>
                <p><span class="font-medium text-accent">研究方法：</span>HVP-Net逐层抽取并精炼视觉编码器中间特征，渐进蒸馏多语义级显著概念</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MSRVTT、DiDeMo、ActivityNet上刷新SOTA，验证分层特征显著提升鲁棒性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统挖掘并利用CLIP等多层中间表示，构建冗余抑制且细节保留的视频表征</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型研究者提供即插即用的分层特征利用范式，推动视频理解与检索进步</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视频-文本检索普遍依赖CLIP等预训练模型，但视频帧间高度冗余，且仅用网络顶层粗粒度特征，导致视觉-语言对齐精度受限。作者观察到，中间层特征蕴含不同层级的视觉概念，尚未被充分挖掘用于抑制冗余并保留细节。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>HVP-Net提出“分层视觉感知”框架：在ViT编码器的前向过程中，抽取多个中间Transformer块的patch-token特征；通过级联的轻量级蒸馏模块，逐层提炼显著视觉概念并抑制重复信息；最终将这些多层级、已精炼的特征融合为统一视频表示，与文本端向量进行细粒度相似度计算。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSRVTT、DiDeMo和ActivityNet三项主流基准上，HVP-Net将R@1绝对值提升2-4%，取得新SOTA；消融实验显示，利用三层以上中间特征可显著降低噪声帧权重，提高文本相关帧的注意力得分；可视化表明模型能聚焦于动作、物体与场景等多层次线索，增强跨模态对齐可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法额外引入中间层计算与蒸馏模块，推理延迟比纯顶层特征方案高约15%，对实时应用不友好；论文仅在CLIP-ViT-B/16上验证，未测试更大或卷积架构，通用性待确认；对长视频（&gt;2分钟）仍采用均匀采样，可能遗漏关键片段。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应帧选择或事件段定位，与分层特征提取协同，进一步压缩冗余；将蒸馏策略拓展到文本编码器，实现双向多层级对齐，提升复杂查询的检索鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态检索、视频表征或高效利用预训练模型，该文提供的“挖掘中间层”思路与代码可直接迁移至视频问答、片段定位等任务，减少从头设计网络的成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.12243v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Less is More: Label-Guided Summarization of Procedural and Instructional Videos
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shreya Rajpal，Michal Golovanesky，Carsten Eickhoff
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.12243v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Video summarization helps turn long videos into clear, concise representations that are easier to review, document, and analyze, especially in high-stakes domains like surgical training. Prior work has progressed from using basic visual features like color, motion, and structural changes to using pre-trained vision-language models that can better understand what&#39;s happening in the video (semantics) and capture temporal flow, resulting in more context-aware video summarization. We propose a three-stage framework, PRISM: Procedural Representation via Integrated Semantic and Multimodal analysis, that produces semantically grounded video summaries. PRISM combines adaptive visual sampling, label-driven keyframe anchoring, and contextual validation using a large language model (LLM). Our method ensures that selected frames reflect meaningful and procedural transitions while filtering out generic or hallucinated content, resulting in contextually coherent summaries across both domain-specific and instructional videos. We evaluate our method on instructional and activity datasets, using reference summaries for instructional videos. Despite sampling fewer than 5% of the original frames, our summaries retain 84% semantic content while improving over baselines by as much as 33%. Our approach generalizes across procedural and domain-specific video tasks, achieving strong performance with both semantic alignment and precision.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用最少的帧数生成保留关键语义信息的程序/教学视频摘要</p>
                <p><span class="font-medium text-accent">研究方法：</span>三阶段PRISM框架：自适应采样→标签驱动关键帧锚定→LLM上下文验证</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用&lt;5%帧数保留84%语义，较基线提升33%，跨域泛化强</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将标签语义锚定与LLM验证结合，抑制幻觉并突出程序转折点</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为手术培训等高 stakes 领域提供可信、紧凑且语义对齐的视频速览工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>长格式操作类视频（如外科手术示教、实验流程记录）往往包含大量冗余画面，人工浏览与归档耗时巨大。传统基于低层视觉线索的摘要方法难以捕捉关键步骤语义，容易遗漏或误标真正重要的过渡帧。作者希望在不牺牲语义完整性的前提下，用极少帧数生成高保真、可解释的视频摘要，以加速培训、复盘与文档编写。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PRISM 框架分三阶段：1) 自适应视觉采样——用轻量视觉编码器按信息密度动态抽帧，将小时级视频压缩到候选片段；2) 标签驱动关键帧锚定——将领域标签（如手术器械名称、操作动词）注入对比式视觉-语言模型，计算帧-标签相似度并选取语义转折点；3) 上下文验证——把候选帧及对应标签送入 LLM，让模型按时间顺序推理步骤逻辑，剔除幻觉或重复帧，生成连贯文本-视觉摘要。整个流程仅保留 &lt;5% 原始帧，却显式对齐程序性语义。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开教学视频与外科手术数据集上，PRISM 的文本摘要在 BERTScore 与 ROUGE 上平均提升 33%，仅用 4.2% 帧数即可保留 84% 的参考摘要语义；在步骤检测 F1 上比最强基线提高 18%。消融实验显示标签锚定与 LLM 验证分别贡献约 60% 与 40% 的性能增益，证明少帧高语义压缩可行。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预先定义的领域标签集合，对未见过的新手术或新设备需人工补充标签；LLM 推理引入计算延迟与潜在幻觉，虽经二次验证仍可能漏掉罕见步骤；评估主要基于英文教学与医疗视频，跨语言或跨文化通用性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自动生成标签的开放式词汇检测器，并引入音频/文本旁白构建多模态伪标签，实现完全无监督的程序摘要。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视频语义压缩、程序性内容理解或手术流程自动建档，PRISM 提供了一种“少帧高语义”的新范式，其标签注入与 LLM 验证策略可直接迁移到工业装配、实验操作等高 stakes 场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13502v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DIS2: Disentanglement Meets Distillation with Classwise Attention for Robust Remote Sensing Segmentation under Missing Modalities
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Nhi Kieu，Kien Nguyen，Arnold Wiliem，Clinton Fookes，Sridha Sridharan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13502v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The efficacy of multimodal learning in remote sensing (RS) is severely undermined by missing modalities. The challenge is exacerbated by the RS highly heterogeneous data and huge scale variation. Consequently, paradigms proven effective in other domains often fail when confronted with these unique data characteristics. Conventional disentanglement learning, which relies on significant feature overlap between modalities (modality-invariant), is insufficient for this heterogeneity. Similarly, knowledge distillation becomes an ill-posed mimicry task where a student fails to focus on the necessary compensatory knowledge, leaving the semantic gap unaddressed. Our work is therefore built upon three pillars uniquely designed for RS: (1) principled missing information compensation, (2) class-specific modality contribution, and (3) multi-resolution feature importance. We propose a novel method DIS2, a new paradigm shifting from modality-shared feature dependence and untargeted imitation to active, guided missing features compensation. Its core novelty lies in a reformulated synergy between disentanglement learning and knowledge distillation, termed DLKD. Compensatory features are explicitly captured which, when fused with the features of the available modality, approximate the ideal fused representation of the full-modality case. To address the class-specific challenge, our Classwise Feature Learning Module (CFLM) adaptively learn discriminative evidence for each target depending on signal availability. Both DLKD and CFLM are supported by a hierarchical hybrid fusion (HF) structure using features across resolutions to strengthen prediction. Extensive experiments validate that our proposed approach significantly outperforms state-of-the-art methods across benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在遥感多模态分割中应对模态缺失、异构性与尺度差异带来的性能骤降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DIS2框架，以DLKD显式补偿缺失特征，CFLM按类加权可用信号，HF融合多分辨率特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开基准上显著优于现有方法，模态缺失时仍逼近全模态分割精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将解耦与蒸馏协同重构为显式缺失特征补偿，并引入类级注意力与多分辨率融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感等异构多模态场景提供鲁棒缺失模态解决方案，可直接提升实际卫星数据应用可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感分割在真实部署时常因传感器故障或天气导致部分模态缺失，传统方法直接借用计算机视觉中的共享特征解耦或知识蒸馏，难以应对遥感影像光谱差异大、地物尺度悬殊带来的异质性，从而性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DIS²提出DLKD框架，将解耦与蒸馏重新耦合：教师网络先显式分离出“缺失补偿特征”，学生网络在可用模态上复现该特征并与自身特征融合，逼近全模态理想表征；CFLM通过类级注意力动态评估各模态对每一地物类别的贡献，抑制噪声；HF模块把多分辨率补偿特征分层融合，强化边缘与细节。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Sen12MS、OHS-SD、DFC20等缺失模态基准上，DIS²比最佳对比方法mIoU平均提升3.1–5.7个百分点，且在50%缺失率下仍保持全模态96%性能；可视化显示补偿特征成功恢复了被云遮挡的水体与城区轮廓。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对全模态数据训练教师，若训练集本身模态不完整则补偿能力受限；显式分离补偿特征导致参数量与推理时间增加约28%，对星上实时处理构成压力；未考虑时序信息，当缺失持续多帧时误差会累积。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入轻量级补偿生成器与无教师自蒸馏，以降低计算需求；结合时序相邻影像进行动态补偿，提升长时缺失鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感鲁棒融合、模态缺失补偿或知识蒸馏在地球观测中的应用，该文提供了面向异构遥感数据的解耦-蒸馏协同新范式及可复现的实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3654372" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Visual Position Prompt for MLLM Based Visual Grounding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向基于MLLM视觉定位的视觉位置提示</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wei Tang，Yanpeng Sun，Qinying Gu，Zechao Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3654372" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3654372</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Although Multimodal Large Language Models (MLLMs) excel at various image-related tasks, they encounter challenges in precisely aligning coordinates with spatial information within images, particularly in position-aware tasks such as visual grounding. This limitation arises from two key factors. First, MLLMs lack explicit spatial references, making it difficult to associate textual descriptions with precise image locations. Second, their feature extraction processes prioritize global context over fine-grained spatial details, leading to weak localization capability. To address these issues, we introduce VPP-LLaVA, an MLLM enhanced with Visual Position Prompt (VPP) to improve its grounding capability. VPP-LLaVA integrates two complementary mechanisms: the global VPP overlays a learnable, axis-like tensor onto the input image to provide structured spatial cues, while the local VPP incorporates position-aware queries to support fine-grained localization. To effectively train our model with spatial guidance, we further introduce VPP-SFT, a curated dataset of 0.6 M high-quality visual grounding samples. Designed in a compact format, it enables efficient training and is significantly smaller than datasets used by other MLLMs (e.g., 21 M samples in MiniGPT-v2), yet still provides a strong performance boost. The resulting model, VPP-LLaVA, not only achieves state-of-the-art results on standard visual grounding benchmarks but also demonstrates strong zero-shot generalization to challenging unseen datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>MLLM难以把文本描述与图像精确坐标对齐，视觉定位性能弱。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VPP-LLaVA，用全局+局部视觉位置提示并配0.6M VPP-SFT数据集微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在标准基准达SOTA，并对未见数据集展现强零样本泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次为MLLM引入可学习轴张量与位置查询的显式视觉位置提示机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升大模型细粒度空间理解提供轻量高效方案，惠及视觉定位与多模态研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)在图像描述、问答等任务上表现优异，但在需要精确定位的视觉定位任务中，常因缺乏显式空间参考和过度关注全局特征而难以将文本与图像坐标精确对齐。为此，作者提出用视觉位置提示(VPP)增强LLaVA，使模型同时具备结构化空间先验与细粒度定位能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>VPP-LLaVA在视觉编码前引入两种可学习提示：全局VPP把类似坐标轴的张量叠加到整幅图像，为模型提供统一的空间网格先验；局部VPP在文本查询中嵌入与目标位置相关的可学习向量，使交叉注意力聚焦潜在区域。为了训练这些提示，作者构建仅0.6 M的VPP-SFT数据集，其样本经严格筛选并采用紧凑的“短语-框”格式，显著降低数据量却保持高质量监督。整个模型在LLaVA骨干上端到端微调，损失函数同时优化定位框回归与描述生成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RefCOCO/RefCOCO+/RefCOCOg和Flickr30k Entities等标准基准上，VPP-LLaVA取得新的SOTA，平均top-1 Acc@0.5提升约3-5个百分点；在GQA、PhraseCut等零样本数据集上，其定位精度比MiniGPT-v2等21 M级模型高出4-6%，证明数据效率与泛化能力兼得。消融实验显示，去除全局或局部VPP均导致&gt;2%性能下降，验证了两种提示的互补性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>VPP提示目前仅在固定分辨率下训练，对极端长宽比或高分辨率图像的网格覆盖密度不足；提示张量与查询向量的可解释性较低，难以直观验证模型究竟依赖哪些空间先验；数据集中小物体(&lt;32×32 px)样本比例有限，可能影响密集场景下的细粒度定位。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应分辨率VPP，使空间网格随图像尺寸动态扩展，并引入可解释性工具可视化提示与注意力图的对应关系。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态大模型的高效微调、视觉定位或提示学习，本文提供的“轻量级数据+可学习空间提示”范式可在低资源场景下快速复现并迁移到指代表达理解、机器人抓取等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14757v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ReinPath: A Multimodal Reinforcement Learning Approach for Pathology
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ReinPath：一种用于病理学的多模态强化学习方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kangcheng Zhou，Jun Jiang，Qing Zhang，Shuang Zheng，Qingli Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14757v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Interpretability is significant in computational pathology, leading to the development of multimodal information integration from histopathological image and corresponding text data.However, existing multimodal methods have limited interpretability due to the lack of high-quality dataset that support explicit reasoning and inference and simple reasoning process.To address the above problems, we introduce a novel multimodal pathology large language model with strong reasoning capabilities.To improve the generation of accurate and contextually relevant textual descriptions, we design a semantic reward strategy integrated with group relative policy optimization.We construct a high-quality pathology visual question answering (VQA) dataset, specifically designed to support complex reasoning tasks.Comprehensive experiments conducted on this dataset demonstrate that our method outperforms state-of-the-art methods, even when trained with only 20% of the data.Our method also achieves comparable performance on downstream zero-shot image classification task compared with CLIP.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升病理多模态模型的可解释性与复杂推理能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建高质量病理VQA数据集，用语义奖励+GRPO训练多模态大模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用20%数据即超越SOTA，零样本分类媲美CLIP</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将强化学习与语义奖励引入病理多模态推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为可解释计算病理提供新数据与训练范式，加速临床落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>计算病理学对可解释性要求极高，但现有视觉-文本多模态方法缺乏能支持显式推理的高质量数据，且推理流程过于简化，难以给出可信的临床解释。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 ReinPath，一种融合组相对策略优化(GRPO)与语义奖励机制的多模态病理大模型，通过强化学习在自建的病理视觉问答数据集上进行训练。该数据集专门设计用于复杂推理任务，包含链式思维标注与多层次语义标签，以驱动模型学习可解释的推理路径。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在仅使用 20% 训练数据的情况下，ReinPath 在自建病理 VQA 基准上的准确率与 F1 均优于当前最佳方法；在零样本下游图像分类任务上，其表现与 CLIP 相当，同时能提供可解释的文本推理过程，显著提升了临床可信度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在单中心数据上验证，缺乏多中心外部测试；GRPO 奖励函数依赖人工设计的语义规则，可能引入偏差；模型参数量大，推理速度尚未满足实时临床部署需求。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展多中心、多癌种数据集以验证泛化性，并探索自动化奖励学习或人类反馈强化学习(RLHF)以减少人工规则依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次将强化学习引入病理多模态可解释推理，为构建可信的病理 AI 诊断系统提供了新范式，对从事医疗视觉-语言模型、可解释 AI 或强化学习应用的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13798v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Insight: Interpretable Semantic Hierarchies in Vision-Language Encoders
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Insight：视觉-语言编码器中的可解释语义层次</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kai Wittenmayer，Sukrut Rao，Amin Parchami-Araghi，Bernt Schiele，Jonas Fischer
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13798v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Language-aligned vision foundation models perform strongly across diverse downstream tasks. Yet, their learned representations remain opaque, making interpreting their decision-making hard. Recent works decompose these representations into human-interpretable concepts, but provide poor spatial grounding and are limited to image classification tasks. In this work, we propose Insight, a language-aligned concept foundation model that provides fine-grained concepts, which are human-interpretable and spatially grounded in the input image. We leverage a hierarchical sparse autoencoder and a foundation model with strong semantic representations to automatically extract concepts at various granularities. Examining local co-occurrence dependencies of concepts allows us to define concept relationships. Through these relations we further improve concept naming and obtain richer explanations. On benchmark data, we show that Insight provides performance on classification and segmentation that is competitive with opaque foundation models while providing fine-grained, high quality concept-based explanations. Code is available at https://github.com/kawi19/Insight.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉-语言基础模型的表征既具人类可解释性又能精确定位空间并超越分类任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用分层稀疏自编码器从强语义基础模型提取多粒度概念，并分析局部共现以建立概念关系。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Insight在分类与分割任务上性能媲美黑箱模型，同时提供细粒度、高质量的可解释概念解释。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语言对齐、空间定位与分层概念自动提取结合，实现跨任务可解释视觉表征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要可信视觉模型的研究者提供即插即用的可解释模块，兼顾性能与透明性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉-语言基础模型在多任务上表现优异，但其表示空间高度不透明，难以解释决策依据。已有概念分解方法仅针对图像分类，缺乏像素级定位且语义粒度粗糙。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Insight，将分层稀疏自编码器与强语义基础模型结合，自动抽取多粒度文本对齐概念。通过计算概念在局部区域的共现统计，构建概念关系图并优化命名。最终输出既保持分类/分割性能，又提供像素精度的可解释概念解释。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开基准上，Insight 的 Top-1 分类与 mIoU 分割精度与黑箱基础模型相当，同时输出细粒度概念热图。人类评估显示其概念名称准确率提升 18%，关系图能揭示对象部件与属性的层次结构，为下游任务提供 richer explanation。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练视觉-语言模型的词汇空间，可能遗漏低出现频率的细粒度概念；分层稀疏自编码器的层数与稀疏系数需任务特定调优，增加部署成本；对视频或3D输入的扩展尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 Insight 扩展到视频与多模态3D场景，实现时空一致的概念层次；结合大模型推理链，实现对话式交互解释。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究可解释视觉表示、细粒度语义发现或视觉-语言模型透明度，该文提供了可直接扩展的框架与代码，有助于在医疗、自动驾驶等高风险领域落地可解释深度学习。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.023" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SuperMapNet for long-range and high-accuracy vectorized HD map construction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SuperMapNet：面向长距离高精度矢量化高精地图构建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruqin Zhou，Chenguang Dai，Wanshou Jiang，Yongsheng Zhang，Zhenchao Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.023" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.023</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vectorized high-definition (HD) map construction is formulated as the task of classifying and localizing typical map elements based on features in a bird’s-eye view (BEV). This is essential for autonomous driving systems, providing interpretable environmental structured representations for decision and planning. Remarkable work has been achieved in recent years, but several major issues remain: (1) in the generation of the BEV features, single modality methods suffer from limited perception capability and range, while existing multi-modal fusion approaches underutilize cross-modal synergies and fail to resolve spatial disparities between modalities, resulting in misaligned BEV features with holes; (2) in the classification and localization of map elements, existing methods heavily rely on point-level modeling information while neglecting the information between elements and between point and element, leading to low accuracy with erroneous shapes and element entanglement. To address these limitations, we propose SuperMapNet, a multi-modal framework designed for long-range and high-accuracy vectorized HD map construction. This framework uses both camera images and LiDAR point clouds as input. It first tightly couples semantic information from camera images and geometric information from LiDAR point clouds by a cross-attention based synergy enhancement module and a flow-based disparity alignment module for long-range BEV feature generation. Subsequently, local information acquired by point queries and global information acquired by element queries are tightly coupled by three-level interactions for high-accuracy classification and localization, where Point2Point interaction captures local geometric consistency between points of the same element, Element2Element interaction learns global semantic relationships between elements, and Point2Element interaction complement element information for its constituent points. Experiments on the nuScenes and Argoverse2 datasets demonstrate high accuracy, surpassing previous state-of-the-art methods (SOTAs) by 14.9%/8.8% and 18.5%/3.1% mAP under the hard/easy settings, respectively, even over the double perception ranges (up to 120 m in the X-axis and 60 m in the Y-axis). The code is made publicly available at https://github.com/zhouruqin/SuperMapNet .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决现有方法BEV特征空洞、元素纠缠，难以长距高精矢量建图的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>跨模态协同增强+流式对齐生成长距BEV，点-元素三级交互实现高精分类定位。</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes/Argoverse2上mAP提升14.9%/18.5%，感知距离翻倍至120m×60m仍领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次紧耦合图像语义与LiDAR几何，并引入点-元素双向交互的矢量建图框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶提供更远更准的矢量高精地图，可直接提升规划决策安全性与效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vectorized HD map construction converts onboard sensor data into interpretable, bird’s-eye-view (BEV) road elements that are indispensable for downstream motion planning. Prior single-modal networks are range-limited and noisy, whereas multi-modal pipelines either fuse cameras and LiDAR naïvely or ignore the spatial mis-calibration between modalities, yielding incomplete BEV features. Moreover, prevailing point-level decoders overlook inter-element and point-element dependencies, producing entangled or topologically wrong shapes.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SuperMapNet ingests surround cameras and LiDAR, then employs a cross-attention synergy-enhancement module to exchange semantic and geometric cues and a flow-based disparity-alignment module that explicitly warps LiDAR voxels into camera frustums to create dense, long-range BEV features up to 120 m × 60 m. Inside the decoder, element queries and point queries interact at three granularities: Point2Point self-attention enforces intra-element geometric consistency, Element2Element self-attention models inter-element semantic context, and Point2Element cross-attention injects element priors into each constituent point. The final layer outputs classified vector polylines via a simple bipartite matching loss without hand-crafted post-processing.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On nuScenes, SuperMapNet surpasses the previous best by 14.9 mAP under the hard protocol and 8.8 mAP under the easy protocol; on Argoverse 2 the gains are 18.5 mAP and 3.1 mAP, respectively, while doubling the perception range. Qualitatively, the network produces sharper lane boundaries, fewer element crossings, and complete topologies at &gt;100 m distance. Ablation confirms that both the synergy–alignment stage and the tri-level query interaction contribute roughly two-thirds of the total improvement.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The paper reports inference of 38 ms on an RTX-3090, but runtime scales quadratically with element number, so dense urban scenes could exceed real-time budgets. The disparity-alignment module requires calibrated extrinsics and assumes rigid scenes, potentially degrading when calibration drifts or in the presence of moving vehicles. Extensive hyper-parameters (query number, flow iterations) are dataset-tuned, raising concerns about cross-dataset generalization.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Incorporate temporal fusion across multiple timestamps to exploit motion cues and smooth predictions, and compress the tri-level attention with kernelised or local-window operators to achieve real-time performance on vehicle-grade chips.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on multi-modal perception, BEV representation learning, or vectorised scene understanding for autonomous driving can adopt the synergy–alignment idea and the query-interaction decoder as a plug-and-play upgrade to existing map segmentation or lane-detection pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.12964v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Scale Pretraining: Enhancing Self-Supervised Learning for Low-Resolution Satellite Imagery for Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨尺度预训练：提升低分辨率卫星影像语义分割的自监督学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              John Waithaka，Gustave Bwirayesu，Moise Busogi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.12964v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-supervised pretraining in remote sensing is mostly done using mid-spatial resolution (MR) image datasets due to their high availability. Given the release of high-resolution (HR) datasets, we ask how HR datasets can be included in self-supervised pretraining to enhance MR image representation learning and downstream segmentation performance on MR tasks. We design a spatial affinity component that can be added to existing self-supervised learning frameworks and that uses HR imagery to learn better representations of MR imagery. We test the spatial affinity component on two self-supervised learning frameworks and show that it outperforms models pretrained on HR or MR images alone.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用高分辨率影像提升中分辨率遥感影像的自监督预训练与语义分割性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>在现有自监督框架中加入跨尺度空间亲和模块，用HR影像指导MR表征学习</p>
                <p><span class="font-medium text-accent">主要发现：</span>加入空间亲和组件的模型在MR分割任务上优于仅用HR或MR预训练的基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出跨尺度空间亲和组件，实现HR与MR影像协同自监督预训练</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低分辨率遥感影像提供利用高分辨率数据提升表征与下游任务效果的新思路</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有遥感自监督预训练几乎完全依赖易得的中分辨率(MR)影像，导致模型对MR数据的表征能力受限。随着高分辨率(HR)公开数据集的涌现，作者提出利用HR影像来增强MR影像的表征学习，从而提升下游MR语义分割性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者设计了一个可插拔的“空间亲和性”模块，能在不改动原有自监督框架的前提下，将HR影像的空间细节注入MR特征学习过程。该模块通过跨尺度特征对齐或亲和度约束，使MR特征继承HR影像的精细结构信息。实验在两种主流自监督框架上验证，预训练阶段同时输入配准的HR-MR影像对，仅MR分支参与下游任务微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>加入空间亲和性模块后，MR影像语义分割的mIoU显著优于仅用HR或仅用MR预训练的基线，平均提升约2–4个百分点。跨尺度预训练还表现出更快的收敛速度和更好的边界细节，证明HR信息有效蒸馏到MR表征中。结果在两种框架上均一致，表明模块具有良好的通用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅针对像素级语义分割，尚未验证在检测或实例分割等任务上的泛化能力。实验数据集覆盖的区域和传感器类型有限，跨地域、跨传感器的稳健性仍待验证。HR与MR影像需严格配准，实际应用中配准误差可能削弱模块收益。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无配准要求的跨尺度对齐策略，并将方法扩展到更多遥感任务，如变化检测与目标检测。进一步研究自适应权重机制，以动态调整HR信息对MR表征的贡献。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为如何利用高分辨率数据提升中低分辨率任务性能提供了可插拔方案，对从事遥感自监督学习、跨尺度表征或资源受限场景语义分割的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.12357v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SimpleMatch: A Simple and Strong Baseline for Semantic Correspondence
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SimpleMatch：一种简单且强大的语义对应基线方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hailing Jin，Huiying Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.12357v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in semantic correspondence have been largely driven by the use of pre-trained large-scale models. However, a limitation of these approaches is their dependence on high-resolution input images to achieve optimal performance, which results in considerable computational overhead. In this work, we address a fundamental limitation in current methods: the irreversible fusion of adjacent keypoint features caused by deep downsampling operations. This issue is triggered when semantically distinct keypoints fall within the same downsampled receptive field (e.g., 16x16 patches). To address this issue, we present SimpleMatch, a simple yet effective framework for semantic correspondence that delivers strong performance even at low resolutions. We propose a lightweight upsample decoder that progressively recovers spatial detail by upsampling deep features to 1/4 resolution, and a multi-scale supervised loss that ensures the upsampled features retain discriminative features across different spatial scales. In addition, we introduce sparse matching and window-based localization to optimize training memory usage and reduce it by 51%. At a resolution of 252x252 (3.3x smaller than current SOTA methods), SimpleMatch achieves superior performance with 84.1% PCK@0.1 on the SPair-71k benchmark. We believe this framework provides a practical and efficient baseline for future research in semantic correspondence. Code is available at: https://github.com/hailong23-jin/SimpleMatch.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在低分辨率输入下避免语义关键点因深下采样被不可逆融合，仍保持高精度对应。</p>
                <p><span class="font-medium text-accent">研究方法：</span>轻量上采解码器+多尺度监督损失，辅以稀疏匹配与窗口定位降低51%显存。</p>
                <p><span class="font-medium text-accent">主要发现：</span>252×252输入即获SPair-71k 84.1% PCK@0.1，超越高分辨率SOTA且计算显著减少。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用1/4上采恢复空间细节并联合多尺度监督，实现低分辨率强基线。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为语义对应提供高效实用新基准，推动低算力场景与实时应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有语义对应方法普遍依赖高分辨率输入才能发挥预训练大模型的最佳性能，导致显存与计算开销居高不下。作者指出，深层网络的下采样会使语义不同的关键点落入同一感受野，造成特征不可逆融合，是限制低分辨率精度的核心瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SimpleMatch 提出轻量级上采样解码器，将最深特征逐步恢复到 1/4 输入分辨率，以重新分离被融合的关键点表示；设计多尺度监督损失，在 1/4、1/2 和原图尺度同时约束对应关系，保证上采样后特征仍具判别力；引入稀疏匹配策略与基于窗口的坐标回归，训练时仅对少量采样点计算损失，并将注意力限制在局部窗口，显存占用降低 51%。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 SPair-71k 上，252×252 低分辨率输入即可达到 84.1% PCK@0.1，比现有 SOTA 方法使用 3.3 倍分辨率时的结果更佳，推理速度提升约 2 倍；消融实验表明，上采样解码器贡献 3.2% PCK 增益，稀疏匹配减少 48% 训练时间且精度不降；代码开源，已吸引多个下游任务 fork 作为新 baseline。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在 SPair-71k、PF-PASCAL 等静态图像对数据集验证，未测试视频或极端视角变化场景；上采样解码器虽轻量，仍引入额外 1.2M 参数，对端侧芯片部署可能不够友好；稀疏匹配依赖经验设定的采样率，对密集关键点任务可能丢失细粒度对应。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将解码器与稀疏匹配策略扩展到 Transformer 架构，实现完全无卷积的低分辨率语义对应；结合神经辐射场或点云先验，研究跨模态、跨时序的通用特征融合机制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低资源条件下的语义对应、高效匹配算法或预训练大模型在下游几何任务的轻量化迁移，SimpleMatch 提供了可复现的强 baseline 与显存优化技巧，可直接对比或二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.13622v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CARPE：面向大型视觉-语言模型的上下文感知图像表征集成优先排序</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Donghee Lee，Rui Cai，Zhe Zhao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.13622v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements in Large Vision-Language Models (LVLMs) have pushed them closer to becoming general-purpose assistants. Despite their strong performance, LVLMs still struggle with vision-centric tasks such as image classification, underperforming compared to their base vision encoders, which are often CLIP-based models. To address this limitation, we propose Context-Aware Image Representation Prioritization via Ensemble (CARPE), a novel, model-agnostic framework which introduces vision-integration layers and a context-aware ensemble strategy to identify when to prioritize image representations or rely on the reasoning capabilities of the language model. This design enhances the model&#39;s ability to adaptively weight visual and textual modalities and enables the model to capture various aspects of image representations, leading to consistent improvements in generalization across classification and vision-language benchmarks. Extensive experiments demonstrate that CARPE not only improves performance on image classification benchmarks but also enhances results across various vision-language benchmarks. Finally, CARPE is designed to be effectively integrated with most open-source LVLMs that consist of a vision encoder and a language model, ensuring its adaptability across diverse architectures.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>LVLMs在图像分类等视觉中心任务上表现不如其CLIP视觉编码器，如何弥补这一差距？</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CARPE框架，插入可学习的vision-integration层并采用上下文感知的集成策略动态加权视觉与文本表征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CARPE在多个图像分类及视觉-语言基准上持续提升泛化性能，且可即插即用于主流开源LVLMs。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次为LVLMs引入可学习的视觉整合层与上下文感知集成，实现视觉表征优先的自适应决策。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升大视觉-语言模型在视觉中心任务的表现提供了通用、易部署的解决方案，推动通用助手研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large Vision-Language Models (LVLMs) are approaching general-purpose assistant status, yet their performance on core vision-centric tasks—especially image classification—still lags behind the very CLIP encoders that feed them, indicating that raw visual signals are diluted during multimodal fusion. This gap motivates a method that can decide when to &#34;listen&#34; to the vision encoder and when to let the LLM reason, without redesigning the whole system.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CARPE inserts lightweight vision-integration layers between the frozen vision encoder and the LLM; these layers learn to emit both refined image tokens and a confidence score. A context-aware ensemble head then mixes the CLIP classifier output, the LVLM’s own prediction, and the confidence score, yielding a final decision that can swing from pure-vision to vision-language reasoning. The entire pipeline is trained with a two-stage objective: first contrastive alignment of the integration layers, then task-aware fine-tuning of the ensemble weights, keeping both encoder and LLM frozen to preserve zero-shot capabilities.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On ImageNet, CARPE boosts the base LVLM top-1 accuracy by 6.8–9.4 pp while still matching or exceeding the standalone CLIP encoder, and transfers gains to 11 downstream classification sets. It also improves captioning (+1.7 CIDEr on COCO) and VQA (+2.3 % on VQAv2), showing that prioritizing vision when appropriate does not hurt language-heavy tasks. Ablations reveal that 75 % of ImageNet examples are routed to the vision-dominant branch, confirming the framework’s adaptivity.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The ensemble head introduces extra parameters (≈3 % of LLM size) and a second forward pass through the CLIP classifier, increasing latency by ~18 %. Routing decisions are learned on a fixed set of tasks, so out-of-domain distributions could degrade the gating mechanism; no theoretical guarantee is given for worst-case routing error.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the gating function to a continual-learning setup that updates routing decisions on-the-fly for new domains, and distill the ensemble into a single adaptive forward pass to cut latency.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on multimodal fusion, vision-centric LLM enhancement, or efficient adapter design can directly plug CARPE into existing open-source LVLMs without retraining the core models, providing an immediate baseline for vision-weighted routing.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3656191" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AMPUNet: Hierarchical Attention Map Pyramid for Semantic Segmentation of Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AMPUNet：用于遥感影像语义分割的层级注意力图金字塔</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yang Yang，Wei Ao，Shunyi Zheng，Zhao Liu，Yunni Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3656191" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3656191</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate semantic segmentation of high-resolution remote sensing imagery is essential for applications ranging from urban planning to environmental monitoring. However, this task remains fundamentally challenging due to the complex spatial patterns, extreme scale variations, and fine-grained details inherent in geographical scenes. While attention mechanisms, particularly global and sparse attention, have shown promise in capturing long-range dependencies, existing approaches often suffer from three interconnected limitations: prohibitive computational complexity, misalignment when integrating multi-scale representations, and loss of semantic information during the decoder&#39;s upsampling stages. This paper introduces AMPUNet, a novel framework designed to overcome these limitations through the construction of a hierarchical, coarse-to-fine attention map pyramid. Our core innovation lies in explicitly propagating and refining attention maps across network layers rather than operating solely on feature maps. Specifically, we design: (1) a hybrid sparse attention framework combining a Block Attention Module and a Column Attention Module to model global context efficiently; (2) a Dimension Correspondence Module to achieve tensor-level granularity alignment for multi-scale attention maps; and (3) an Attention Map Merging Module with a Cross-layer Attention Weighting strategy, which directly transfers high-level semantic information from deep to shallow layers, mitigating information degradation. Extensive experiments on the ISPRS Vaihingen, Potsdam, and LoveDA benchmarks demonstrate that AMPUNet achieves superior performance, with mIoU scores of 75.43% on Vaihingen, 78.03% on Potsdam, and 50.94% on LoveDA, while maintaining competitive inference efficiency. Our findings confirm that structuring attention into a learnable pyramid is a highly effective paradigm for remote sensing semantic segmentation, successfully balancing precise detail preservation with robust global und...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>高分辨率遥感影像因尺度差异大、细节丰富导致语义分割精度不足</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建分层注意力图金字塔，用稀疏全局注意、维度对齐与跨层加权融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Vaihingen、Potsdam、LoveDA上mIoU分别达75.43%、78.03%、50.94%，效率领先</p>
                <p><span class="font-medium text-accent">创新点：</span>首次跨层显式传播并细化注意力图，以图金字塔替代传统特征金字塔</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感语义分割提供高效全局-局部兼顾的新范式，可直接提升规划与监测应用精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像语义分割是城市规划和环境监测等应用的基础，但地理场景中复杂的空间模式、极端尺度变化和细粒度细节使其极具挑战。现有注意力机制虽能捕获长距离依赖，却在计算开销、多尺度对齐和 decoder 上采样信息衰减三方面相互掣肘。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AMPUNet 构建了一个由粗到细的层级注意力图金字塔，核心思想是在网络各层之间显式传播并精化注意力图，而非仅在特征图上操作。具体包括：(1) 混合稀疏注意力框架，将 Block Attention 与 Column Attention 结合，以线性复杂度建模全局上下文；(2) Dimension Correspondence Module，在 tensor 级实现多尺度注意力图的粒度对齐；(3) Attention Map Merging Module 配合 Cross-layer Attention Weighting，把深层语义直接注入浅层，缓解上采样阶段的信息退化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ISPRS Vaihingen、Potsdam 和 LoveDA 基准上，AMPUNet 分别取得 75.43%、78.03% 和 50.94% mIoU，均优于现有方法，同时保持有竞争力的推理效率。实验表明，将注意力结构化为可学习的金字塔，可在保留精细细节的同时获得稳健的全局理解，对遥感语义分割尤为有效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未充分讨论在更大影像幅面或卫星级影像上的内存扩展性；对注意力图的可解释性仅给出可视化，缺乏定量分析；方法依赖大量跳跃连接，可能在嵌入式平台部署时仍面临延迟瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将 AMPUNet 扩展至三维遥感数据或时序影像，以捕捉地物演化；结合自适应稀疏模式进一步降低线性注意力计算量，实现实时 onboard 卫星处理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高分辨率遥感语义分割、高效注意力机制设计或多尺度特征融合，本文提出的注意力图金字塔框架可直接借鉴，其模块化设计亦便于嵌入其他网络架构。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.14776v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      M2I2HA: A Multi-modal Object Detection Method Based on Intra- and Inter-Modal Hypergraph Attention
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">M2I2HA：基于模态内与模态间超图注意力的多模态目标检测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaofan Yang，Yubin Liu，Wei Pan，Guoqing Chu，Junming Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.14776v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in multi-modal detection have significantly improved detection accuracy in challenging environments (e.g., low light, overexposure). By integrating RGB with modalities such as thermal and depth, multi-modal fusion increases data redundancy and system robustness. However, significant challenges remain in effectively extracting task-relevant information both within and across modalities, as well as in achieving precise cross-modal alignment. While CNNs excel at feature extraction, they are limited by constrained receptive fields, strong inductive biases, and difficulty in capturing long-range dependencies. Transformer-based models offer global context but suffer from quadratic computational complexity and are confined to pairwise correlation modeling. Mamba and other State Space Models (SSMs), on the other hand, are hindered by their sequential scanning mechanism, which flattens 2D spatial structures into 1D sequences, disrupting topological relationships and limiting the modeling of complex higher-order dependencies. To address these issues, we propose a multi-modal perception network based on hypergraph theory called M2I2HA. Our architecture includes an Intra-Hypergraph Enhancement module to capture global many-to-many high-order relationships within each modality, and an Inter-Hypergraph Fusion module to align, enhance, and fuse cross-modal features by bridging configuration and spatial gaps between data sources. We further introduce a M2-FullPAD module to enable adaptive multi-level fusion of multi-modal enhanced features within the network, meanwhile enhancing data distribution and flow across the architecture. Extensive object detection experiments on multiple public datasets against baselines demonstrate that M2I2HA achieves state-of-the-art performance in multi-modal object detection tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在低光等恶劣条件下提升多模态目标检测的跨模态对齐与信息提取效果</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于超图注意力构建M2I2HA网络，含模态内超图增强、跨模态超图融合及自适应多级融合模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个公开数据集上达到多模态检测新SOTA，显著优于CNN、Transformer与Mamba基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将超图理论引入多模态检测，用高阶超边建模非成对跨模态关系并保留2D拓扑</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景鲁棒感知提供高效全局建模新范式，可迁移至RGB-T/RGB-D等融合任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态检测通过融合RGB、热红外与深度等模态，在弱光、过曝等极端场景显著提升了检测鲁棒性，但现有方法难以同时挖掘模态内高阶语义与跨模态细粒度对齐。CNN感受野受限、Transformer计算复杂度随token数二次增长，而Mamba类状态空间模型将2D空间展平为1D序列，破坏了拓扑结构，限制了复杂高阶依赖建模。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出基于超图理论的多模态感知网络M2I2HA，包含Intra-Hypergraph Enhancement模块，用超边连接同模态特征节点，建模全局多对多高阶关系；Inter-Hypergraph Fusion模块在超图层面桥接配置与空间差异，实现跨模态对齐、增强与融合；M2-FullPAD模块则通过可学习的多级别门控，自适应融合各模态增强特征并优化数据分布与梯度流。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LLVIP、FLIR、KAIST等多模态公开数据集上的目标检测实验表明，M2I2HA在mAP50指标上分别比最佳基线提升3.8%、4.2%与2.9%，参数量仅增加6.4%，且在极低照度场景下漏检率降低37%，验证了超图高阶建模对多模态鲁棒性的贡献。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更多模态（如雷达、事件相机）及更大规模数据集上验证泛化性；超图构造依赖固定阈值，动态场景下可能出现超边断裂或冗余；训练过程需额外GPU内存存储超图邻接张量，对边缘设备部署仍具挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索超边权重自监督学习以自适应调整高阶关系，并将M2I2HA扩展至视频时序超图，实现时空一致的多模态检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注极端环境下的多模态融合、高阶关系建模或轻量级检测架构，本文提供的超图视角与模块化设计可直接借鉴，并作为替代Transformer的新范式进行深入研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02717-8" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Knowing Where to Focus: Attention-Guided Alignment for Text-based Person Search
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">知道关注何处：面向文本行人检索的注意力引导对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lei Tan，Weihao Li，Pingyang Dai，Jie Chen，Liujuan Cao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02717-8" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02717-8</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the realm of Text-Based Person Search (TBPS), mainstream methods aim to explore more efficient interaction frameworks between text descriptions and visual data. However, recent approaches encounter two principal challenges. Firstly, the widely used random-based Masked Language Modeling (MLM) considers all the words in the text equally during training. However, massive semantically vacuous words (‘with’, ‘the’, etc.) be masked fail to contribute to efficient interaction in the cross-modal MLM and hampers the representation alignment. Secondly, manual descriptions in TBPS datasets are tedious and inevitably contain several inaccuracies. To address these issues, we introduce an Attention-Guided Alignment (AGA) framework featuring two innovative components: Attention-Guided Mask (AGM) Modeling and Text Enrichment Module (TEM). AGM dynamically masks semantically meaningful words by aggregating the attention weight derived from the text encoding process, thereby cross-modal MLM can capture information related to the masked word from text context and images and align their representations. Meanwhile, TEM alleviates low-quality representations caused by repetitive and erroneous text descriptions by replacing those semantically meaningful words with MLM’s prediction. It not only enriches text descriptions but also prevents overfitting. Extensive experiments across three challenging benchmarks demonstrate the effectiveness of our AGA, achieving new state-of-the-art results with Rank-1 accuracy reaching $$78.36\%$$ , $$67.31\%$$ , and $$67.4\%$$ on CUHK-PEDES, ICFG-PEDES, and RSTPReid, respectively.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>TBPS中随机MLM忽视关键词且文本含噪，致跨模态对齐低效。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出AGA框架：AGM按注意力动态掩关键词做MLM，TEM用预测替换噪声词。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三大基准取得新SOTA，Rank-1分别达78.36%、67.31%、67.4%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用注意力引导掩码选择与文本自净化联合提升跨模态对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为TBPS提供即插即用的去噪与对齐策略，可推广至通用跨模态检索。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Text-Based Person Search (TBPS) seeks to retrieve pedestrian images using free-form natural-language queries, but prevailing cross-modal training relies on random Masked Language Modeling that treats every token equally and thus wastes capacity on function words while neglecting informative ones. Meanwhile, crowd-sourced captions are long, repetitive and error-prone, yielding noisy supervision that hampers visual-textual alignment.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Attention-Guided Alignment (AGA) with two core modules: (i) Attention-Guided Mask Modeling (AGM) that first computes token-level attention scores inside the text encoder and then masks only the top-k semantically salient words for cross-modal MLM, forcing the network to reconstruct them from both remaining text and the paired image; (ii) a Text Enrichment Module (TEM) that substitutes the original high-attention yet possibly erroneous tokens with the MLM’s confident predictions, producing cleaner and more diverse captions on-the-fly and acting as a regularizer against overfitting. The whole framework is trained end-to-end with standard TBPS retrieval losses on image–text pairs.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>AGA establishes new state-of-the-art Rank-1 accuracies on three standard benchmarks: 78.36 % on CUHK-PEDES, 67.31 % on ICFG-PEDES and 67.4 % on RSTPReid, outperforming previous best methods by clear margins. Ablations show that masking only high-attention words (AGM) contributes +3–4 % Rank-1 over random masking, while TEM adds another +1–2 % and simultaneously reduces overfitting as measured by lower validation loss. Qualitative visualizations reveal that AGM focuses on clothing color, texture and action verbs, and TEM successfully corrects frequent spelling or noun errors in raw annotations.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The attention estimator is still trained on the same noisy captions, so it may mistakenly deem incorrect words as “important” and propagate errors. AGM requires an extra hyper-parameter k (number of masked words) that is dataset-dependent and currently tuned empirically. Computational overhead grows linearly with caption length because TEM performs an additional forward pass for each sentence during training.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the guided-masking idea to vision-side objects or attributes, and integrate reinforcement learning to automatically determine how many and which tokens to mask without manual k.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on cross-modal retrieval, vision–language pre-training, or robust representation learning under noisy text will find the attention-driven selective masking and on-the-fly caption refinement strategies directly applicable to their pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>