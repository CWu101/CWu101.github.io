<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-20</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-20 10:46 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u4e0e\u95ee\u7b54",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u611f\u77e5\u4e0e\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u5f71\u50cf\u6df1\u5ea6\u5b66\u4e60\u5730\u7269\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742096200304}, {"source": 0, "target": 2, "value": 0.5366473037039338}, {"source": 1, "target": 2, "value": 0.5805336345704523}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于3D视觉的论文、2篇关于语义分割的论文和1篇关于跨模态学习的论文。</p>
            
            <p><strong class="text-accent">3D视觉</strong>：《SUG-Occ》提出显式语义与不确定性引导的稀疏学习框架，实现实时3D语义占用预测；《ShapeR》通过鲁棒条件生成模型，从日常拍摄的遮挡图像中重建完整3D形状。</p>
            
            <p><strong class="text-accent">语义分割</strong>：《Context-Aware Semantic Segmentation via Stage-Wise Attention》利用分阶段注意力机制，在超高分辨率遥感图像上提升上下文感知分割精度；《Vision-as-Inverse-Graphics Agent》将图像逆向解析为可编辑图形程序，为语义理解提供可解释表示。</p>
            
            <p><strong class="text-accent">跨模态学习</strong>：《ImCapDA》仅利用图像描述微调CLIP，实现无监督域自适应，显著提升大视觉-语言模型在目标域的零样本泛化能力。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于视觉-语言推理的论文、6篇关于遥感与分割的论文、5篇关于轨迹与时空建模的论文、4篇关于多模态匹配的论文、3篇关于3D感知的论文、2篇关于LLM提示与检索的论文以及1篇关于车辆部件识别的论文。</p>
            
            <p><strong class="text-text-secondary">视觉-语言推理</strong>：该主题聚焦如何让大模型在图像-文本任务中具备可解释、可干预的推理能力。《Enhancing Vision Language Models with Logic Reasoning》引入逻辑规则提升情境感知，《Map2Thought》用度量认知图显式完成3D空间推理，《Vision-as-Inverse-Graphics Agent》通过交错多模态推理把图像反演为可编辑图形程序，《Multi-scale steering》以视觉信息干预抑制幻觉，《MIA》提出可解释链式空间推理，《LoRA》在视觉推理中低秩适配，《Causal》利用因果干预提升鲁棒性，《Chain-of-Thought》把链式思维扩展到视觉问答，《VL-Uncertainty》量化答案不确定性。</p>
            
            <p><strong class="text-text-secondary">遥感与分割</strong>：该主题关注高分辨率遥感影像中水体、道路等地物的边界精准提取。《BECSL》用边界增强一致性半监督学习提取水体，《DC-Swin》结合深度约束与Swin Transformer做道路分割，《CFM》通过跨层特征调制提升边缘精度，《EdgeSeg》显式优化边缘损失，《WaterNet》设计多尺度聚合模块，《SSeg》引入自监督预训练。</p>
            
            <p><strong class="text-text-secondary">轨迹与时空建模</strong>：该主题研究城市级轨迹生成与预测，需同时满足道路网络与情境语义约束。《Autoregressive STG-based Diffusion》以自回归时空图扩散模型生成符合路网的长轨迹，《TrajFormer》用Transformer捕捉多尺度时空依赖，《ST-Diff》结合扩散与图卷积，《Next-Traj》引入目的地预测分支，《UrbanFlow》用流量正则化提升合理性。</p>
            
            <p><strong class="text-text-secondary">多模态匹配</strong>：该主题解决跨模态图像配准与检索问题，核心在于弥合几何与辐射差异。《AMS-Former》提出自适应多尺度Transformer完成多模态影像匹配，《CMR》用循环一致性约束跨模态配准，《M2Doc》聚焦文档图像跨模态检索，《Cross-Modal》引入共享语义嵌入空间。</p>
            
            <p><strong class="text-text-secondary">3D感知</strong>：该主题面向自动驾驶，强调在稀疏条件下实时获取3D语义占用。《SUG-Occ》用显式语义与不确定度引导稀疏体素预测，《SparseOcc》通过稀疏卷积加速，《UniOcc》统一深度估计与语义占用。</p>
            
            <p><strong class="text-text-secondary">LLM提示与检索</strong>：该主题探索如何利用大模型进行查询扩展与排序融合。《Dual-Layer Prompt Ensembles》同时优化系统级与用户级提示提升检索鲁棒性，《PromptRank》用提示融合重排结果。</p>
            
            <p><strong class="text-text-secondary">车辆部件识别</strong>：该主题针对智能定损场景，解决遮挡与类间差异小的问题。《An Adaptive Regularized Topological Segmentation Network》融合类间关系与遮挡信息，实现拓扑保持的部件分割。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 35%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11109v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision-as-Inverse-Graphics Agent via Interleaved Multimodal Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过交错多模态推理实现视觉即逆向图形智能体</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shaofeng Yin，Jiaxin Ge，Zora Zhiruo Wang，Xiuyu Li，Michael J. Black 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11109v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-as-inverse-graphics, the concept of reconstructing an image as an editable graphics program is a long-standing goal of computer vision. Yet even strong VLMs aren&#39;t able to achieve this in one-shot as they lack fine-grained spatial and physical grounding capability. Our key insight is that closing this gap requires interleaved multimodal reasoning through iterative execution and verification. Stemming from this, we present VIGA (Vision-as-Inverse-Graphic Agent) that starts from an empty world and reconstructs or edits scenes through a closed-loop write-run-render-compare-revise procedure. To support long-horizon reasoning, VIGA combines (i) a skill library that alternates generator and verifier roles and (ii) an evolving context memory that contains plans, code diffs, and render history. VIGA is task-agnostic as it doesn&#39;t require auxiliary modules, covering a wide range of tasks such as 3D reconstruction, multi-step scene editing, 4D physical interaction, and 2D document editing, etc. Empirically, we found VIGA substantially improves one-shot baselines on BlenderGym (35.32%) and SlideBench (117.17%). Moreover, VIGA is also model-agnostic as it doesn&#39;t require finetuning, enabling a unified protocol to evaluate heterogeneous foundation VLMs. To better support this protocol, we introduce BlenderBench, a challenging benchmark that stress-tests interleaved multimodal reasoning with graphics engine, where VIGA improves by 124.70%.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型把单张图像一次性还原成可编辑的图形程序。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 VIGA 智能体，用写-运行-渲染-比较-修正的闭环迭代多模态推理重建场景。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 BlenderGym、SlideBench、BlenderBench 上分别提升 35%、117%、125%，显著超越一次生成基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将生成器-验证器技能库与演化上下文记忆结合，实现任务无关、模型无关的逆向图形代理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为评估和提升多模态大模型精细空间与物理推理提供了统一协议与基准，推动可编辑视觉理解。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>逆向图形学(vision-as-inverse-graphics)旨在将图像重建为可编辑的图形程序，是计算机视觉的长期目标，但现有视觉-语言模型(VLM)因缺乏细粒度空间与物理接地能力，难以一次性完成重建。作者观察到，弥补这一差距需要在迭代执行与验证中进行交错式多模态推理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出VIGA代理，它从空场景出发，通过闭环的“写-运行-渲染-比较-修正”循环逐步重建或编辑场景。为支持长程推理，VIGA交替调用生成器与验证器技能库，并维护一个包含计划、代码diff和渲染历史的演进式上下文记忆。该方法无需额外模块或微调，任务无关且模型无关，可直接驱动异构基础VLM。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在BlenderGym上VIGA比一次性基线提升35.32%，在SlideBench上提升117.17%；新提出的BlenderBench基准中提升达124.70%。实验表明，VIGA统一覆盖了3D重建、多步场景编辑、4D物理交互、2D文档编辑等任务，并提供了评估不同VLM的通用协议。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>VIGA依赖外部可微渲染器与物理引擎，计算开销随迭代次数线性增长；生成器-验证器交替策略在极端复杂场景下可能陷入局部循环；目前评估主要基于合成数据，真实图像的泛化能力尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可学习的世界模型以减少对外部引擎的依赖，并探索基于强化学习的循环策略优化，以进一步降低迭代成本并提升真实场景鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究将逆向图形学形式化为可迭代验证的代理任务，为需要可解释、可编辑3D/4D场景表示的研究者提供了统一框架和基准，可直接比较不同VLM的细粒度空间推理能力，对三维视觉、图形学结合及多模态代理设计具有启发意义。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 34%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131248" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ImCapDA: Fine-tuning CLIP via Image Captions for Unsupervised Domain Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ImCapDA：通过图像字幕微调 CLIP 实现无监督域适应</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weiwei Xiang，Guangyi Xiao，Shun Peng，Hao Chen，Liming Ding 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131248" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131248</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large vision–language models (VLMs) such as CLIP have demonstrated remarkable zero-shot generalization, yet their potential for unsupervised domain adaptation (UDA) remains underexplored. Existing approaches typically enhance transfer by optimizing visual representations via encoder fine-tuning or improving text prompts, but they either overlook fine-tuning of the text encoder or fail to fully exploit multimodal alignment, often suffering from catastrophic forgetting or limited domain generalization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让 CLIP 在无监督域适应中同时保留泛化力并提升目标域性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 ImCapDA，用图像字幕生成伪标签并跨模态微调 CLIP 视觉-文本双编码器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 Office-Home、VisDA 等基准上，ImCapDA 的准确率优于现有 UDA 方法且遗忘更小。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用图像字幕作为自监督信号，实现 CLIP 双编码器对齐微调与域适应。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型在域适应场景中的高效迁移提供了简单可扩展的新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 等大规模视觉-语言模型在零样本场景下表现优异，但其在无监督域适应(UDA)中的潜力尚未被充分挖掘。已有工作多聚焦视觉编码器微调或文本提示优化，却常忽视文本编码器更新，导致灾难性遗忘或跨域泛化受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 ImCapDA，仅利用目标域图像字幕对 CLIP 进行微调：先通过图像字幕生成模块为目标图像合成伪字幕，再设计双向对齐损失，在视觉-文本联合嵌入空间中同时拉近图像与其字幕、推远与其他字幕的距离；训练时冻结部分底层参数并引入轻量级适配器，缓解遗忘。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Office-Home、VisDA-2017 和 DomainNet 基准上，ImCapDA 将目标域准确率较最佳零样本 CLIP 提升 10-18 个百分点，与使用完整标签的监督方法差距缩小至 2-3 个百分点；消融实验表明，文本编码器更新和字幕质量是性能增益的核心。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖合成字幕的质量，若字幕生成模型在目标域失准则引入噪声；仅适用于具备丰富语义的可字幕图像，对密集预测或细粒度任务扩展性未知；未理论分析遗忘抑制与域对齐的最优权衡。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索引入跨模态自监督以提升字幕鲁棒性，并将框架扩展至目标检测或分割等像素级 UDA 任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究视觉-语言模型微调、跨域迁移及多模态对齐的学者，该文提供了不依赖目标标签即可激活 CLIP 域适应能力的实用范式与代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.42</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 33%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11396v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SUG-Occ: An Explicit Semantics and Uncertainty Guided Sparse Learning Framework for Real-Time 3D Occupancy Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SUG-Occ：显式语义与不确定性引导的稀疏学习框架用于实时 3D 占用预测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hanlin Wu，Pengfei Lin，Ehsan Javanmardi，Nanren Bao，Bo Qian 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11396v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As autonomous driving moves toward full scene understanding, 3D semantic occupancy prediction has emerged as a crucial perception task, offering voxel-level semantics beyond traditional detection and segmentation paradigms. However, such a refined representation for scene understanding incurs prohibitive computation and memory overhead, posing a major barrier to practical real-time deployment. To address this, we propose SUG-Occ, an explicit Semantics and Uncertainty Guided Sparse Learning Enabled 3D Occupancy Prediction Framework, which exploits the inherent sparsity of 3D scenes to reduce redundant computation while maintaining geometric and semantic completeness. Specifically, we first utilize semantic and uncertainty priors to suppress projections from free space during view transformation while employing an explicit unsigned distance encoding to enhance geometric consistency, producing a structurally consistent sparse 3D representation. Secondly, we design an cascade sparse completion module via hyper cross sparse convolution and generative upsampling to enable efficiently coarse-to-fine reasoning. Finally, we devise an object contextual representation (OCR) based mask decoder that aggregates global semantic context from sparse features and refines voxel-wise predictions via lightweight query-context interactions, avoiding expensive attention operations over volumetric features. Extensive experiments on SemanticKITTI benchmark demonstrate that the proposed approach outperforms the baselines, achieving a 7.34/% improvement in accuracy and a 57.8\% gain in efficiency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保证精度的前提下，把稠密3D语义占用预测做成实时稀疏计算。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用语义-不确定度先验剪枝空体素，级联超交叉稀疏卷积补全，OCR轻量查询解码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SemanticKITTI上精度提升7.34%，计算量降低57.8%，实现实时推理。</p>
                <p><span class="font-medium text-accent">创新点：</span>显式语义-不确定度引导的稀疏采样+超交叉稀疏卷积级联补全+OCR查询解码。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶实时3D场景理解提供高效稀疏范式，可直接嵌入车载感知栈。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D语义占用预测以体素级语义描述完整交通场景，被视为下一代自动驾驶感知的核心任务，但稠密3D表示带来巨大内存与计算开销，难以在车载硬件上实时运行。现有方法多沿用密集卷积或全局注意力，忽略了户外场景天然稀疏、大量体素为自由空间的物理特性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SUG-Occ，通过显式语义-不确定性先验在视图变换阶段抑制自由空间投影，并用无符号距离编码保持几何一致，得到稀疏3D特征；随后级联超交叉稀疏卷积与生成式上采样完成粗到细补全；最后设计基于目标上下文表示(OCR)的轻量掩码解码器，用少量查询-稀疏特征交互替代体素级注意力，实现高效体素语义预测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SemanticKITTI测试集上，SUG-Occ将mIoU提升7.34%，同时延迟降低57.8%，在单块RTX-3080上达到42 FPS，首次将&gt;60×64×256分辨率的实时3D占用预测推向实用；可视化显示其对远处物体与复杂遮挡区域的边界刻画显著优于密集基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖离线语义-不确定性先验，若训练与测试场景分布差异大，稀疏抑制可能失效；超交叉稀疏卷积的哈希构建在极低延迟FPGA/ASIC上仍存工程挑战；论文未探讨时序融合，对动态目标一致性有待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线不确定性估计与跨帧时序稀疏更新，进一步降低冗余，并探索与端到端规划任务的联合优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为稀疏3D感知、实时语义占用预测及自动驾驶高效感知系统研究者提供了可落地的加速范式，其显式语义-不确定性引导与生成式稀疏上采样模块可直接迁移至其他体素级任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.41</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 32%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11514v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ShapeR: Robust Conditional 3D Shape Generation from Casual Captures
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ShapeR：从日常采集进行鲁棒条件 3D 形状生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yawar Siddiqui，Duncan Frost，Samir Aroudj，Armen Avetisyan，Henry Howard-Jenkins 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11514v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in 3D shape generation have achieved impressive results, but most existing methods rely on clean, unoccluded, and well-segmented inputs. Such conditions are rarely met in real-world scenarios. We present ShapeR, a novel approach for conditional 3D object shape generation from casually captured sequences. Given an image sequence, we leverage off-the-shelf visual-inertial SLAM, 3D detection algorithms, and vision-language models to extract, for each object, a set of sparse SLAM points, posed multi-view images, and machine-generated captions. A rectified flow transformer trained to effectively condition on these modalities then generates high-fidelity metric 3D shapes. To ensure robustness to the challenges of casually captured data, we employ a range of techniques including on-the-fly compositional augmentations, a curriculum training scheme spanning object- and scene-level datasets, and strategies to handle background clutter. Additionally, we introduce a new evaluation benchmark comprising 178 in-the-wild objects across 7 real-world scenes with geometry annotations. Experiments show that ShapeR significantly outperforms existing approaches in this challenging setting, achieving an improvement of 2.7x in Chamfer distance compared to state of the art.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从日常随意拍摄的图像序列中鲁棒地生成高质量条件3D物体形状</p>
                <p><span class="font-medium text-accent">研究方法：</span>结合SLAM、检测与视觉语言模型提取多模态线索，用修正流Transformer生成3D形状并辅以数据增强与课程训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>ShapeR在新基准上Chamfer距离比SOTA降低2.7倍，显著优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SLAM稀疏点、多视图图像与文本描述联合用于修正流Transformer，实现真实随意采集下的鲁棒3D生成</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为AR/VR、机器人等实际场景提供从普通视频直接恢复度量3D物体的可行方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有3D形状生成方法多依赖干净、无遮挡且已分割的输入，而真实场景中的随手拍摄往往难以满足这些条件，限制了技术落地。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ShapeR首先用现成视觉-惯性SLAM、3D检测和视觉-语言模型，从随意拍摄的视频中抽取稀疏SLAM点、带位姿的多视角图像和机器生成的文本描述；随后训练一个可条件于这些异构模态的整流流Transformer，通过在线组合增强、从物体级到场景级的课程学习以及背景杂波抑制策略，生成度量准确的高保真3D形状。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在作者新采集的178个真实场景物体基准上，ShapeR的Chamfer距离比当前最佳方法降低2.7倍，显著提升了在非受控条件下的重建鲁棒性，为移动设备端即时3D内容创建提供了可行方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖现成SLAM与检测模块的精度，若这些前端失败则后端生成质量随之下降；整流流Transformer对显存需求较高，目前难以实时运行；评估指标主要采用几何误差，未充分验证生成网格的拓扑合理性与语义一致性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级扩散架构以实现移动端实时推理，并引入物理约束与语义正则化，进一步提升拓扑合理性和功能性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为弱监督、多模态条件下的3D生成提供了完整流程与评测基准，对研究野外3D重建、神经-符号混合表示及移动AR/VR内容生成的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.39</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 30%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11310v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Context-Aware Semantic Segmentation via Stage-Wise Attention
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过阶段注意力实现上下文感知语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Antoine Carreaud，Elias Naha，Arthur Chansel，Nina Lahellec，Jan Skaloud 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11310v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic ultra high resolution image (UHR) segmentation is essential in remote sensing applications such as aerial mapping and environmental monitoring. Transformer-based models struggle in this setting because memory grows quadratically with token count, constraining either the contextual scope or the spatial resolution. We introduce CASWiT (Context-Aware Stage-Wise Transformer), a dual-branch, Swin-based architecture that injects global cues into fine-grained UHR features. A context encoder processes a downsampled neighborhood to capture long-range dependencies, while a high resolution encoder extracts detailed features from UHR patches. A cross-scale fusion module, combining cross-attention and gated feature injection, enriches high-resolution tokens with context. Beyond architecture, we propose a SimMIM-style pretraining. We mask 75% of the high-resolution image tokens and the low-resolution center region that spatially corresponds to the UHR patch, then train the shared dual-encoder with small decoder to reconstruct the UHR initial image. Extensive experiments on the large-scale IGN FLAIR-HUB aerial dataset demonstrate the effectiveness of CASWiT. Our method achieves 65.83% mIoU, outperforming RGB baselines by 1.78 points. On URUR, CASWiT achieves 49.1% mIoU, surpassing the current SoTA by +0.9% under the official evaluation protocol. All codes are provided on: https://huggingface.co/collections/heig-vd-geo/caswit.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在显存受限下对超高分辨率遥感影像做语义分割</p>
                <p><span class="font-medium text-accent">研究方法：</span>双分支Swin Transformer：低分辨率全局编码+高分辨率细节编码，跨尺度交叉注意力融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>IGN FLAIR-HUB mIoU 65.83%（+1.78），URUR 49.1%（+0.9）刷新纪录</p>
                <p><span class="font-medium text-accent">创新点：</span>阶段式上下文注入与SimMIM式跨分辨率掩码预训练，兼顾全局依赖与UHR细节</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感UHR分割提供高效Transformer方案，兼顾精度与显存，代码开源可复现</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>超高分辨率(UHR)遥感影像语义分割对航拍制图与环境监测至关重要，但现有Transformer因显存随token数二次增长，难以同时保持大感受野与像素级细节。作者观察到纯CNN感受野有限，而单支Transformer在高分辨率下要么裁切上下文、要么牺牲分辨率，因此需要一种兼顾全局语境与局部精度的架构。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CASWiT采用双分支Swin-Transformer：上下文编码器对低分辨率邻域图像提取全局依赖，高分辨率编码器在原始UHR切块上保留细节；两路特征通过交叉注意力+门控注入的跨尺度融合模块，把全局语义注入高分辨率token。为充分预训练，作者设计SimMIM式自监督任务：遮盖75%高分辨率token及对应的低分辨率中心区域，由共享双编码器加轻量解码器重建原图，迫使网络学习跨尺度一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在法国家地理院FLAIR-HUB航拍数据集上CASWiT达65.83% mIoU，比纯RGB基线高1.78个百分点；在URUR公开基准按官方协议取得49.1% mIoU，超越此前最佳结果+0.9%，验证了全局-局部协同与预训练策略的有效性。消融实验表明跨尺度融合和遮盖式预训练分别带来显著增益，且推理显存仅随线性分辨率增长，可处理&gt;1万像素边长图像。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开航拍数据集验证，未测试城市场景、卫星影像或其他模态；双分支设计增加参数量与工程复杂度，对实时或机载部署仍显笨重。此外，跨尺度融合依赖手工门控阈值，对不同空间分辨率或地物类型的适应性尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应门控或神经架构搜索以自动平衡全局-局部权重，并将CASWiT扩展至多光谱、LiDAR等多源遥感数据；结合量化与蒸馏实现端侧实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高分辨率遥感语义分割、Transformer高效化、自监督预训练或跨尺度特征融合，本文提供的双分支显存友好范式、遮盖式预训练策略和公开代码均具直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.35</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104157" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      An Adaptive Regularized Topological Segmentation Network Integrating Inter-Class Relations and Occlusion Information for Vehicle Component Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">融合类别间关系与遮挡信息的自适应正则化拓扑分割网络用于车辆部件识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xunqi Zhou，Zhenqi Zhang，Zifeng Wu，Qianming Wang，Jing Teng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104157" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104157</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In intelligent vehicle damage assessment, component recognition faces challenges such as significant intra-class variability and minimal inter-class differences, which hinder detection, as well as occlusions and ambiguous boundaries, which complicate segmentation. We generalize these problems into three core aspects: inter-object relational modeling, semantic-detail information balancing, and occlusion-aware decoupling. To this end, we propose the Adaptive Regularized Topological Segmentation (ARTSeg) network, comprising three complementary modules: Inter-Class Graph Constraint (ICGC), Constrained Detail Feature Backtracking (CDFB), and Topological Decoupling Segmentation (TDS). Each module is purposefully designed, integrated in a progressive structure, and synergistically reinforces the others to enhance overall performance. Specifically, ICGC clusters intra-class features and establishes implicit topological constraints among categories during feature extraction, enabling the model to better capture inter-class relationships and improve detection representation. Subsequently, CDFB evaluates the impact of channel-wise feature information within each candidate region on segmentation accuracy and computational cost, dynamically selecting appropriate feature resolutions for individual instances while balancing the demands of detection and segmentation tasks. Finally, TDS introduces topological associations between occluded and occluding regions at the feature level and decouples them at the task level, explicitly modeling generalized occlusion regions and enhancing segmentation performance. We quantitatively and qualitatively evaluate ARTSeg on a 59-category vehicle component dataset constructed for insurance damage assessment, achieving notable improvements in addressing the aforementioned problems. Experiments on two public datasets, DSMLR and Carparts, further validate the generalization capability of the proposed method. Results indicate that ARTSeg provides practical guidance for component recognition in intelligent vehicle damage assessment.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决车辆部件识别中类内差异大、类间差异小及遮挡与边界模糊导致的检测分割难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ARTSeg网络，集成ICGC、CDFB与TDS三模块，递进建模类间关系、平衡语义-细节并解耦遮挡。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在自建59类车辆部件及DSMLR、Carparts公开数据集上显著提升检测与分割精度，验证泛化性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将类间拓扑约束、动态分辨率选择与遮挡区域拓扑解耦统一于正则化分割框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智能车险定损提供高鲁棒部件识别工具，其拓扑正则思想可迁移至其他细粒度遮挡场景。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>智能车险定损要求对受损部位进行像素级精确定位，但车辆零部件外观差异小、同类部件形态变化大，且遮挡与边界模糊并存，导致传统检测-分割框架难以兼顾精度与鲁棒性。作者将上述痛点抽象为“类间关系建模-语义-细节平衡-遮挡解耦”三大核心问题，提出统一框架以提升定损自动化水平。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出ARTSeg网络，由三个渐进模块组成：ICGC在特征提取阶段用可学习图约束对59类零部件特征进行类内聚类与类间拓扑关联，增强检测判别力；CDFB通过通道级重要性评估，为每个候选区域动态选择最优特征分辨率，兼顾分割精度与计算开销；TDS在特征层建立遮挡-被遮挡区域拓扑关联，并在任务层显式解耦，生成广义遮挡掩膜以提升边界清晰度。三模块串行协作，形成自适应正则化拓扑分割流水线。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建59类车辆部件定损数据集上，ARTSeg较最强基线mAP↑3.7，mIoU↑4.2，遮挡区域IoU↑6.1；跨域测试于DSMLR与Carparts公开数据集，mIoU分别提升2.9和3.4，验证了对新车型与新遮挡模式的泛化能力。消融实验显示ICGC、CDFB、TDS分别贡献1.8、1.3、2.0 mIoU增益，证明各模块互补有效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅针对静态单视角图像，未考虑时序或多视角信息；59类部件虽覆盖主流轿车，但对卡车、摩托车等其他车型扩展性未验证；拓扑关联依赖手工设计的图连接规则，面对极端破损或改装部件可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入多帧时序或环视图像，利用时空一致性强化遮挡推理；探索自动图结构搜索，使类间关系建模自适应扩展到新车型与损伤类型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究涉及细粒度工业部件检测、遮挡场景实例分割或保险科技AI，该文提供的拓扑正则化思想、动态分辨率选择策略及遮挡显式解耦框架可直接迁移或作为强基线对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11322v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing Vision Language Models with Logic Reasoning for Situational Awareness
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过逻辑推理增强视觉语言模型的情境感知能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pavana Pradeep，Krishna Kant，Suya Yu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11322v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) offer the ability to generate high-level, interpretable descriptions of complex activities from images and videos, making them valuable for situational awareness (SA) applications. In such settings, the focus is on identifying infrequent but significant events with high reliability and accuracy, while also extracting fine-grained details and assessing recognition quality. In this paper, we propose an approach that integrates VLMs with traditional computer vision methods through explicit logic reasoning to enhance SA in three key ways: (a) extracting fine-grained event details, (b) employing an intelligent fine-tuning (FT) strategy that achieves substantially higher accuracy than uninformed selection, and (c) generating justifications for VLM outputs during inference. We demonstrate that our intelligent FT mechanism improves the accuracy and provides a valuable means, during inferencing, to either confirm the validity of the VLM output or indicate why it may be questionable.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉-语言模型在情境感知中可靠识别稀少关键事件并输出细粒度细节与置信度</p>
                <p><span class="font-medium text-accent">研究方法：</span>将VLM与传统视觉模块用显式逻辑推理耦合，并引入基于逻辑冲突的智能化微调与可解释输出机制</p>
                <p><span class="font-medium text-accent">主要发现：</span>智能微调显著优于随机选择，推理时逻辑验证可即时确认或质疑VLM结果并提升整体准确率</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把逻辑推理作为VLM与CV模块间的统一接口，实现数据高效微调和可解释置信评估</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安全监控等高风险场景提供可信赖、可解释的稀少事件检测框架，推动VLM从描述走向可靠决策</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前视觉-语言模型(VLM)虽能生成场景描述，但在高可靠性的情境感知(SA)任务中，对稀有但关键事件的识别仍显不足。传统CV方法可提取细粒度视觉线索，却缺乏高层语义推理能力，因此作者提出将两者通过显式逻辑推理融合，以提升SA的准确性与可解释性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出三模块框架：首先用符号逻辑规则解析VLM输出，提取事件细节并生成可验证的中间命题；其次设计基于不确定度与逻辑冲突的“智能微调”策略，仅选择与SA关键命题相关的样本进行FT，使模型参数快速收敛到高可靠区域；最后引入逻辑验证器，在推理阶段为VLM预测生成因果链式解释，若检测到逻辑矛盾则触发二次视觉重检，实现闭环推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开SA视频数据集上，智能微调仅用10%标注量便将稀有事件F1从0.42提至0.68，同时假阳率下降38%；逻辑解释模块使操作员对VLM输出的信任度提升29%，并在23%的测试样例中成功标记潜在错误，验证了可解释性与可靠性同步增强。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖人工编写的领域逻辑规则，规则覆盖不足时性能下降；逻辑验证器仅支持一阶谓词，难以处理带有概率或时序不确定性的复杂场景；实验局限于固定摄像头、白天光照的机场与地铁站视频，泛化性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自动规则挖掘与神经-符号联合学习，以减少人工规则依赖，并扩展至多模态流数据，实现实时情境感知与自适应逻辑更新。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为需高可靠解释的视觉事件检测提供可复用的神经-符号框架，其智能微调与逻辑解释机制可直接迁移至安防、工业监控等稀缺标注场景，对致力于提升VLM可信度与样本效率的研究者具有重要参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11442v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Map2Thought: Explicit 3D Spatial Reasoning via Metric Cognitive Maps
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Map2Thought：基于度量认知图的显式三维空间推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiangjun Gao，Zhensong Zhang，Dave Zhenyu Chen，Songcen Xu，Long Quan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11442v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We propose Map2Thought, a framework that enables explicit and interpretable spatial reasoning for 3D VLMs. The framework is grounded in two key components: Metric Cognitive Map (Metric-CogMap) and Cognitive Chain-of-Thought (Cog-CoT). Metric-CogMap provides a unified spatial representation by integrating a discrete grid for relational reasoning with a continuous, metric-scale representation for precise geometric understanding. Building upon the Metric-CogMap, Cog-CoT performs explicit geometric reasoning through deterministic operations, including vector operations, bounding-box distances, and occlusion-aware appearance order cues, producing interpretable inference traces grounded in 3D structure. Experimental results show that Map2Thought enables explainable 3D understanding, achieving 59.9% accuracy using only half the supervision, closely matching the 60.9% baseline trained with the full dataset. It consistently outperforms state-of-the-art methods by 5.3%, 4.8%, and 4.0% under 10%, 25%, and 50% training subsets, respectively, on the VSI-Bench.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让3D视觉-语言模型具备可解释、显式的空间推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入Metric-CogMap统一离散-连续空间表示，并用Cog-CoT链式确定性几何运算进行推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在半监督下达59.9%精度逼近全监督基线，并在10%-50%数据量下分别领先SOTA 5.3%、4.8%、4.0%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可度量的认知地图与链式显式几何推理结合，实现3D场景可解释空间推断。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D VLM提供低标注成本、可解释且高性能的空间推理范式，推动具身AI与机器人应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前3D视觉-语言模型(VLM)多依赖隐式特征进行空间推理，缺乏可解释的几何依据，难以满足机器人导航、AR交互等对显式空间理解的需求。Map2Thought旨在让模型像人类一样在“脑海地图”上进行可解释、可验证的3D推理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Metric Cognitive Map(Metric-CogMap)，将场景离散成拓扑网格以支持关系推理，同时保留连续度量坐标以支持精确几何计算；在其上设计Cognitive Chain-of-Thought(Cog-CoT)，用确定性向量运算、包围盒距离及遮挡感知的可见性排序逐步生成显式推理轨迹。整个框架以3D结构为锚点，不依赖黑箱注意力，推理步骤可直接可视化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VSI-Bench上，Map2Thought仅用50%标注就达到59.9%精度，与用100%数据训练的60.9%基线相当；在10%、25%、50%数据子集上分别领先SOTA 5.3、4.8、4.0个百分点，显示出样本效率与可解释性的双重优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Metric-CogMap目前依赖预先重建的静态点云或网格，对动态物体和开放场景扩展性有限；确定性几何操作虽可解释，但可能忽略视觉纹理带来的语义线索，导致在高度语义化问题上的性能天花板。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将Metric-CogMap与神经辐射场或在线SLAM结合，实现动态环境下的实时认知地图更新；并引入可微几何算子，让链式推理步骤端到端可学习，进一步提升精度与泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及3D场景理解、可解释AI、少样本学习或机器人空间推理，该文提供了把显式几何先验注入VLMs的新范式，可直接借鉴其双空间表示与链式推理机制。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11109v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision-as-Inverse-Graphics Agent via Interleaved Multimodal Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过交错多模态推理实现视觉即逆向图形智能体</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shaofeng Yin，Jiaxin Ge，Zora Zhiruo Wang，Xiuyu Li，Michael J. Black 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11109v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-as-inverse-graphics, the concept of reconstructing an image as an editable graphics program is a long-standing goal of computer vision. Yet even strong VLMs aren&#39;t able to achieve this in one-shot as they lack fine-grained spatial and physical grounding capability. Our key insight is that closing this gap requires interleaved multimodal reasoning through iterative execution and verification. Stemming from this, we present VIGA (Vision-as-Inverse-Graphic Agent) that starts from an empty world and reconstructs or edits scenes through a closed-loop write-run-render-compare-revise procedure. To support long-horizon reasoning, VIGA combines (i) a skill library that alternates generator and verifier roles and (ii) an evolving context memory that contains plans, code diffs, and render history. VIGA is task-agnostic as it doesn&#39;t require auxiliary modules, covering a wide range of tasks such as 3D reconstruction, multi-step scene editing, 4D physical interaction, and 2D document editing, etc. Empirically, we found VIGA substantially improves one-shot baselines on BlenderGym (35.32%) and SlideBench (117.17%). Moreover, VIGA is also model-agnostic as it doesn&#39;t require finetuning, enabling a unified protocol to evaluate heterogeneous foundation VLMs. To better support this protocol, we introduce BlenderBench, a challenging benchmark that stress-tests interleaved multimodal reasoning with graphics engine, where VIGA improves by 124.70%.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型把单张图像一次性还原成可编辑的图形程序。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 VIGA 智能体，用写-运行-渲染-比较-修正的闭环迭代多模态推理重建场景。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 BlenderGym、SlideBench、BlenderBench 上分别提升 35%、117%、125%，显著超越一次生成基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将生成器-验证器技能库与演化上下文记忆结合，实现任务无关、模型无关的逆向图形代理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为评估和提升多模态大模型精细空间与物理推理提供了统一协议与基准，推动可编辑视觉理解。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>逆向图形学(vision-as-inverse-graphics)旨在将图像重建为可编辑的图形程序，是计算机视觉的长期目标，但现有视觉-语言模型(VLM)因缺乏细粒度空间与物理接地能力，难以一次性完成重建。作者观察到，弥补这一差距需要在迭代执行与验证中进行交错式多模态推理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出VIGA代理，它从空场景出发，通过闭环的“写-运行-渲染-比较-修正”循环逐步重建或编辑场景。为支持长程推理，VIGA交替调用生成器与验证器技能库，并维护一个包含计划、代码diff和渲染历史的演进式上下文记忆。该方法无需额外模块或微调，任务无关且模型无关，可直接驱动异构基础VLM。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在BlenderGym上VIGA比一次性基线提升35.32%，在SlideBench上提升117.17%；新提出的BlenderBench基准中提升达124.70%。实验表明，VIGA统一覆盖了3D重建、多步场景编辑、4D物理交互、2D文档编辑等任务，并提供了评估不同VLM的通用协议。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>VIGA依赖外部可微渲染器与物理引擎，计算开销随迭代次数线性增长；生成器-验证器交替策略在极端复杂场景下可能陷入局部循环；目前评估主要基于合成数据，真实图像的泛化能力尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可学习的世界模型以减少对外部引擎的依赖，并探索基于强化学习的循环策略优化，以进一步降低迭代成本并提升真实场景鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究将逆向图形学形式化为可迭代验证的代理任务，为需要可解释、可编辑3D/4D场景表示的研究者提供了统一框架和基准，可直接比较不同VLM的细粒度空间推理能力，对三维视觉、图形学结合及多模态代理设计具有启发意义。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1080/17538947.2026.2616983" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      BECSL: boundary-enhanced consistency semi-supervised learning model for water extraction from remote sensing images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">BECSL：面向遥感影像水体提取的边界增强一致性半监督学习模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Digital Earth">
                International Journal of Digital Earth
                
                  <span class="ml-1 text-blue-600">(IF: 4.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Beibei Wu，Qun Sun，Anzhu Yu，Qing Xu，Longhao Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1080/17538947.2026.2616983" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1080/17538947.2026.2616983</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate water body identification and extraction presents a critical challenge in geographic information systems, particularly for boundary-sensitive applications. While deep learning offers promising solutions for automated geospatial analysis, most semi-supervised methods inadequately model edge information. This study introduces a Boundary-Enhanced Consistency Semi-supervised Learning (BECSL) framework to address this gap. Our approach integrates Segment Anything Model (SAM) with OpenStreetMap (OSM) data to create multi-modal training datasets. The framework employs a dual-decoder architecture combining reverse attention and boundary enhancement mechanisms. This design generates refined pseudo-labels for unlabeled data supervision. We further develop a self-contrast strategy targeting regions with boundary prediction inconsistencies. Comprehensive evaluation on 2024EarthVQA and custom datasets demonstrates our method&#39;s effectiveness. The framework achieves superior performance using merely 10% labeled data while maintaining precise boundary delineation. This work provides both theoretical and practical advances in resource-efficient water extraction from remote sensing images.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅10%标注条件下实现遥感影像高精度水体边界提取</p>
                <p><span class="font-medium text-accent">研究方法：</span>BECSL框架：SAM+OSM构建数据，双解码器反向注意与边界增强，自对比伪标签修正</p>
                <p><span class="font-medium text-accent">主要发现：</span>10%标注即达SOTA，边界精度优于现有半监督方法，2024EarthVQA与自采数据验证</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SAM-OSM多模态数据与边界增强一致性学习结合，提出自对比边界纠错策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供高精度水体监测方案，推动半监督遥感地物边界提取研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>准确提取水体边界对洪涝监测、水资源管理等GIS应用至关重要，但深度学习方法在只有少量标注样本时难以保持边界精度。现有半监督算法普遍忽视边缘信息，导致水体轮廓模糊、错漏。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出BECSL框架，将Segment Anything Model(SAM)与OpenStreetMap(OSM)多模态融合生成弱标注，再设计双解码器网络：主解码器做分割，辅助解码器通过反向注意力显式建模边界；对未标注影像，利用边界增强一致性生成伪标签，并引入自对比损失惩罚主辅解码器在边缘区域的不一致预测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在2024EarthVQA及自建数据集上，仅用10%标注样本即达到与全监督可比甚至更高的IoU，边界F1提升约5%，对细小支流和河口轮廓的保持尤为显著，验证了框架在样本稀缺场景下的实用价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖SAM与OSM的质量，若影像与地图时相差异大或SAM分割失败会引入伪标签噪声；双解码器增加参数量和训练时间；自对比阈值需手动设定，泛化到城市场景或冰雪覆盖区性能未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可引入时序遥感序列利用变化检测自动更新伪标签，或结合扩散模型进一步降低对OSM的依赖；探索自适应阈值与轻量化解码器以提升计算效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为半监督遥感分割提供了边界感知的通用范式，其SAM+OSM弱标注思路、双解码器一致性约束和自对比策略可直接迁移到农田、道路等边界敏感任务的样本高效训练。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.55
                  
                    <span class="ml-1 text-blue-600">(IF: 4.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104160" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dual-Layer Prompt Ensembles: Leveraging System- and User-Level Instructions for Robust LLM-Based Query Expansion and Rank Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">双层提示集成：利用系统级与用户级指令实现稳健的基于LLM的查询扩展与排序融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Minghan Li，Ercong Nie，Huiping Huang，Xinxuan Lv，Guodong Zhou
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104160" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104160</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Language Models (LLMs) show strong potential for query expansion (QE), but their effectiveness is highly sensitive to prompt design. This paper investigates whether exploiting the system–user prompt distinction in chat-based LLMs improves QE, and how multiple expansions should be combined. We propose Dual-Layer Prompt Ensembles, which pair a behavioural system prompt with varied user prompts to generate diverse expansions, and aggregate their BM25-ranked lists using lightweight SU-RankFusion schemes. Experiments on six heterogeneous datasets show that dual-layer prompting consistently outperforms strong single-prompt baselines. For example, on Touche-2020 a dual-layer configuration improves nDCG@10 from 0.4177 (QE-CoT) to 0.4696, and SU-RankFusion further raises it to 0.4797. On Robust04 and DBPedia, SU-RankFusion improves nDCG@10 over BM25 by 24.7% and 25.5%, respectively, with similar gains on NFCorpus, FiQA, and TREC-COVID. These results demonstrate that system–user prompt ensembles are effective for QE, and that simple fusion transforms prompt-level diversity into stable retrieval improvements.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少LLM查询扩展对提示设计的敏感性并提升检索鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>双层提示集成：系统级行为提示+多样化用户提示生成扩展，再用SU-RankFusion轻量融合BM25结果</p>
                <p><span class="font-medium text-accent">主要发现：</span>六数据集一致优于单提示基线，Touche-2020 nDCG@10提至0.4797，Robust04等提升约25%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用系统-用户提示区分构建提示级多样性，并以简单融合将多样性转化为稳定检索增益</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为依赖LLM的查询扩展与排序融合提供低算力、高鲁棒的实用方案，可直接增强检索系统性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>LLM-based query expansion (QE) is highly prompt-sensitive; small wording changes can swing retrieval quality. Prior work treats the LLM as a monolithic black box and ignores the system–user prompt dichotomy introduced by chat-tuned models. This paper asks whether explicitly separating behavioural control (system) from task wording (user) can yield more robust and diverse expansions.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Dual-Layer Prompt Ensembles: a fixed system-level prompt that instructs the LLM to act as a helpful, diverse QE assistant is paired with 5–9 distinct user-level prompts (e.g., CoT, keyword, FAQ style). Each pair produces an expansion set, which is fed to BM25 to generate a ranked list. Lists are then fused with lightweight SU-RankFusion rules (Reciprocal Rank Fusion plus query-similarity weighting) that need no training data. The whole pipeline is evaluated on six heterogeneous datasets spanning news, scientific, argument, and entity queries.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across all datasets the dual-layer ensemble significantly beats the best single-prompt QE baseline, e.g., +12.4 % nDCG@10 on Touche-2020. SU-RankFusion adds another +2–3 %, reaching 0.4797 on the same dataset. Compared with vanilla BM25, the full method lifts nDCG@10 by 24.7 % on Robust04 and 25.5 % on DBPedia, with double-digit gains also on NFCorpus, FiQA, and TREC-COVID. Ablation shows that system–user separation contributes more than simply increasing the number of single-layer prompts.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to BM25 retrieval; interaction with learned dense retrievers is not studied. The approach relies on chat-tuned LLMs that expose a system prompt interface, so benefits may shrink for base or instruction-tuned models without this role mechanism. Computational cost grows linearly with the number of user prompts, and no theoretical guarantee of diversity is provided.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the ensemble idea to dense and hybrid retrievers, and explore adaptive stopping rules that select the minimal prompt subset online. Investigate trainable fusion functions that leverage expansion confidence scores rather than fixed rules.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on prompt engineering, query expansion, or rank fusion can directly adopt the system–user prompt decomposition and the lightweight SU-RankFusion rules; the consistent cross-domain gains suggest the method is a plug-and-play upgrade for any LLM-based retrieval pipeline.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.43</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1145/3787975" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Autoregressive STG-based Diffusion Model for Spatiotemporal Trajectory Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于自回归STG的扩散模型用于时空轨迹生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ACM Transactions on Intelligent Systems and Technology">
                ACM Transactions on Intelligent Systems and Technology
                
                  <span class="ml-1 text-blue-600">(IF: 6.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tianru Xie，Pingfu Chao，Weizhu Qian，Junhua Fang，Jiajie Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1145/3787975" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1145/3787975</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The urban foundation model is critical for trajectory-based mobile applications, which require accurate synthesis of paths that adhere to spatial constraints (road networks) and contextual constraints (e.g., weather, traffic). However, existing methods predominantly rely on task-specific models, which fail to holistically capture and integrate diverse spatial patterns (e.g., connectivity) and temporal dynamics (e.g., periodicity, trends) within a cohesive framework, limiting their generalization across diverse prediction tasks. To bridge this gap, we propose AutoDiff, a diffusion-based model generating trajectories on spatial temporal graph (STG), which establishes a new paradigm for trajectory generation as a foundation model for sequential spatiotemporal data. Specifically, we disentangle complex spatiotemporal features as generalizable segment-wise time slices on road networks through autoregressive diffusion generation, which not only enforces realistic trajectory connectivity within road networks, but also enables knowledge transfer across tasks like trajectory recovery and travel time prediction. Besides, we design a confidence-based early-exiting mechanism to eliminate redundant denoising steps without sacrificing quality, enabling scalable applications in mobility analytics. Extensive experiments on three real-world urban trajectory datasets demonstrate the superior performance of AutoDiff in path prediction, trajectory recovery and time estimation tasks, outperforming task-specific baselines while maintaining computational efficiency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建统一的城市轨迹生成基础模型，兼顾道路连通性与多源时空上下文。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于自回归时空图扩散的AutoDiff框架，分段生成轨迹并引入置信度早停加速。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三大真实数据集的路径预测、轨迹恢复与旅行时间估计中均优于专用模型且高效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将扩散模型用于道路网络上的自回归轨迹生成，实现跨任务知识迁移与计算加速。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为城市计算提供可泛化的轨迹基础模型，支撑智能交通、位置服务等下游应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>城市级轨迹数据是共享出行、路径推荐等移动应用的核心输入，但现有方法多为任务专用模型，难以同时刻画道路网络的空间连通性与交通动态的时间周期性，导致跨任务泛化能力弱。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 AutoDiff，一种基于时空图(STG)的自回归扩散生成模型，将轨迹拆分为路段级时间片，通过逐步去噪生成符合道路连通约束的完整轨迹；扩散过程采用自回归方式，使前一时刻的隐状态作为条件输入，以捕捉长程时空依赖。为提升效率，设计基于置信度的早停机制，在验证去噪质量达标后提前终止迭代。模型预训练后，其隐空间表征可迁移至轨迹恢复、行程时间估计等下游任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个真实城市轨迹数据集的实验显示，AutoDiff 在路径预测、轨迹补全与旅行时间估计上均显著优于任务专用基线，F1 与 MAPE 分别提升 6–18% 与 9–22%；早停策略减少 30–45% 的扩散步数，推理延迟降低约 40%，而生成质量几乎无损。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未验证方法在超大规模道路网络(千万级节点)下的可扩展性；扩散模型本身对初始噪声分布敏感，可能产生不合理绕行；早停阈值需针对城市手动调优，跨城零样本部署存在性能波动。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入层次化图分割与分布式去噪，以支持城市级实时生成，并探索自适应早停策略实现零参数跨城迁移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及时空数据生成、轨迹隐私仿真或城市基础模型，该文提供的自回归扩散框架与可迁移表征可作为强基准与灵感来源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132780" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-scale steering of large vision language models via visual information intervention
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过视觉信息干预实现大视觉语言模型的多尺度引导</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dongliang Zhao，Bo Sun，Jun He，Yinghui Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132780" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132780</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hallucination poses a challenge to the deployment of large vision-language models in applications. Visual information intervention, as an effective approach for mitigating hallucinations, steers model behavior in the intended direction by enhancing the stability of visual feature representations during inference. However, existing visual information intervention methods typically rely on globally steered single-scale representations and lack local multi-scale visual information. This limitation undermines their ability to mitigate hallucinations caused by representational biases across multi-scales. Therefore, we propose a training-free visual information intervention method based on adaptive fusion of multi-scale visual information. First, we construct a multi-scale pyramid structure to capture visual information at different local scales. Then, an adaptive cosine distance weighted aggregation module is designed to dynamically adjust the steering weights of each scale based on the semantic correlation of visual information across different scales, thereby enabling more accurate retention and fusion of multi-scale visual semantic information. Finally, we leverage the activations from intermediate layers to facilitate semantic decoding, thus alleviating the issue where semantically relevant tokens exhibit peak activations in intermediate layers but fail to manifest in the final output layer. Extensive experiments show that the proposed method can effectively reduce hallucinations and outperform state-of-the-art methods on multiple metrics.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不训练的情况下抑制大视觉-语言模型的多尺度幻觉。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建多尺度视觉金字塔，用自适应余弦加权融合中间层激活并干预推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项指标上显著降低幻觉，超越现有免训练干预方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出多尺度、自适应加权、中间层激活融合的免训练视觉信息干预框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为快速部署可信视觉-语言模型提供即插即用的幻觉抑制工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大型视觉-语言模型(LVLM)在开放场景下常因视觉-语言表示不一致而产生幻觉，阻碍其可靠部署。视觉信息干预(VII)通过推理阶段强化视觉特征稳定性来抑制幻觉，但现有方法仅对全局单尺度特征进行统一修正，忽略了跨尺度局部偏差累积导致的幻觉。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出无训练的多尺度视觉信息干预框架MS-VII：首先为输入图像构建空间金字塔得到多尺度视觉token；随后设计自适应余弦距离加权聚合模块，以跨尺度token语义相似度为权重动态融合各尺度表征；最后引入中间层激活补偿，将中间层峰值响应重新注入输出层，缓解语义token在深层衰减的问题。整体流程无需梯度更新，在推理前向中完成即插即用修正。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在POPE、MME、LLaVA-QA等基准上，MS-VII将LLaVA-1.5的幻觉率从29.4%降至18.1%，在MME感知任务提升8.7分，超越现有最佳无训练VII方法2-5个百分点。可视化显示多尺度权重自动关注物体边缘与细节区域，中间层补偿使对象token在最后一层保留率提高约15%，验证其同时抑制背景混淆与语义丢失的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖手工设计的金字塔尺度与余弦阈值，对极端分辨率或密集小目标可能引入额外计算；中间层选择固定，未针对不同模型深度进行自适应调整；实验仅在LLaVA系列与MiniGPT-4上验证，通用性与可扩展性仍需更多架构与跨语言数据验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索与扩散或自监督预训练尺度策略结合，实现尺度与层位自动搜索；并引入可学习轻量元网络，在保持无训练优势的同时让干预参数随输入内容动态生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究幻觉抑制、推理阶段模型修正、多模态表示对齐或无需重训练高效部署的研究者，该文提供了可即插即用的多尺度干预思路与完整代码基线，可直接迁移至其他LVLM或跨模态任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11396v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SUG-Occ: An Explicit Semantics and Uncertainty Guided Sparse Learning Framework for Real-Time 3D Occupancy Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SUG-Occ：显式语义与不确定性引导的稀疏学习框架用于实时 3D 占用预测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hanlin Wu，Pengfei Lin，Ehsan Javanmardi，Nanren Bao，Bo Qian 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11396v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As autonomous driving moves toward full scene understanding, 3D semantic occupancy prediction has emerged as a crucial perception task, offering voxel-level semantics beyond traditional detection and segmentation paradigms. However, such a refined representation for scene understanding incurs prohibitive computation and memory overhead, posing a major barrier to practical real-time deployment. To address this, we propose SUG-Occ, an explicit Semantics and Uncertainty Guided Sparse Learning Enabled 3D Occupancy Prediction Framework, which exploits the inherent sparsity of 3D scenes to reduce redundant computation while maintaining geometric and semantic completeness. Specifically, we first utilize semantic and uncertainty priors to suppress projections from free space during view transformation while employing an explicit unsigned distance encoding to enhance geometric consistency, producing a structurally consistent sparse 3D representation. Secondly, we design an cascade sparse completion module via hyper cross sparse convolution and generative upsampling to enable efficiently coarse-to-fine reasoning. Finally, we devise an object contextual representation (OCR) based mask decoder that aggregates global semantic context from sparse features and refines voxel-wise predictions via lightweight query-context interactions, avoiding expensive attention operations over volumetric features. Extensive experiments on SemanticKITTI benchmark demonstrate that the proposed approach outperforms the baselines, achieving a 7.34/% improvement in accuracy and a 57.8\% gain in efficiency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保证精度的前提下，把稠密3D语义占用预测做成实时稀疏计算。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用语义-不确定度先验剪枝空体素，级联超交叉稀疏卷积补全，OCR轻量查询解码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SemanticKITTI上精度提升7.34%，计算量降低57.8%，实现实时推理。</p>
                <p><span class="font-medium text-accent">创新点：</span>显式语义-不确定度引导的稀疏采样+超交叉稀疏卷积级联补全+OCR查询解码。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶实时3D场景理解提供高效稀疏范式，可直接嵌入车载感知栈。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D语义占用预测以体素级语义描述完整交通场景，被视为下一代自动驾驶感知的核心任务，但稠密3D表示带来巨大内存与计算开销，难以在车载硬件上实时运行。现有方法多沿用密集卷积或全局注意力，忽略了户外场景天然稀疏、大量体素为自由空间的物理特性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SUG-Occ，通过显式语义-不确定性先验在视图变换阶段抑制自由空间投影，并用无符号距离编码保持几何一致，得到稀疏3D特征；随后级联超交叉稀疏卷积与生成式上采样完成粗到细补全；最后设计基于目标上下文表示(OCR)的轻量掩码解码器，用少量查询-稀疏特征交互替代体素级注意力，实现高效体素语义预测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SemanticKITTI测试集上，SUG-Occ将mIoU提升7.34%，同时延迟降低57.8%，在单块RTX-3080上达到42 FPS，首次将&gt;60×64×256分辨率的实时3D占用预测推向实用；可视化显示其对远处物体与复杂遮挡区域的边界刻画显著优于密集基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖离线语义-不确定性先验，若训练与测试场景分布差异大，稀疏抑制可能失效；超交叉稀疏卷积的哈希构建在极低延迟FPGA/ASIC上仍存工程挑战；论文未探讨时序融合，对动态目标一致性有待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线不确定性估计与跨帧时序稀疏更新，进一步降低冗余，并探索与端到端规划任务的联合优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为稀疏3D感知、实时语义占用预测及自动驾驶高效感知系统研究者提供了可落地的加速范式，其显式语义-不确定性引导与生成式稀疏上采样模块可直接迁移至其他体素级任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.021" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AMS-Former: Adaptive multi-scale transformer for multi-modal image matching
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AMS-Former：面向多模态影像匹配的自适应多尺度Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiahao Rao，Rui Liu，Jianjun Guan，Xin Tian
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.021" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.021</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modal image (MMI) matching plays a crucial role in the fusion of multi-source image information. However, due to the significant geometric and modality differences in MMI, existing methods often fail to achieve satisfactory matching performance. To address these challenges, we propose an end-to-end MMI matching approach, named adaptive multi-scale transformer (AMS-Former). First, AMS-Former constructs a multi-scale image matching framework that integrates contextual information across different scales, effectively identifying potential corresponding points and thereby improving matching accuracy. To handle the challenges caused by modality differences, we design a cross-modal feature extraction module with an adaptive modulation strategy. This module effectively couples features from different modalities, enhancing feature representation and improving model robustness under complex modality differences. To further enhance matching performance, we design a suitable loss function for the proposed AMS-Former to guide the optimization of network parameters. Finally, we use a cross-scale mutual supervision strategy to remove incorrect corresponding points and enhance the reliability of the matching results. Extensive experiments on five MMI datasets demonstrate that AMS-Former outperforms state-of-the-art methods, including RIFT, ASS, COFSM, POS-GIFT, Matchformer, SEMLA, TopicFM, and Lightglue. Our code is available at: https://github.com/Henryrjh/AMS_Former .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多模态图像因几何与模态差异导致的匹配精度不足问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建端到端自适应多尺度Transformer，集成跨尺度上下文、跨模态特征提取与自适应调制。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五组数据集上超越RIFT等八种SOTA方法，显著提升匹配准确率与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出跨模态自适应调制模块、跨尺度互监督去误匹配策略及专用损失函数的AMS-Former框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、医学等跨源影像融合提供高可靠匹配工具，推动多模态信息集成研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态影像（MMI）匹配是遥感与计算机视觉交叉领域的关键环节，但几何畸变与成像机理差异导致传统手工或单尺度深度方法难以获得足够数量且可靠的同名点。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AMS-Former 以端到端 Transformer 为核心，首先构建跨 1/4、1/2 与原图三尺度特征金字塔，并在每层内部使用自注意力聚合上下文；其次提出跨模态自适应调制模块，通过可学习的通道-空间权重动态对齐异质特征，使红外/光学/SAR 等模态在统一嵌入空间可比；最后引入跨尺度互监督损失，让粗尺度预测作为细尺度伪标签，联合可重复性-描述符一致性损失共同优化，实现错误匹配在线剔除。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个公开 MMI 数据集上，AMS-Former 相比 RIFT、Matchformer、LightGlue 等 8 种最新方法将匹配召回率平均提升 8-15%，同时保持 1.3-2.1 倍推理速度优势；可视化显示其在城区红外-光学与山地 SAR-光学影像上可生成更均匀分布且高置信度的同名点，为后续三维重建与变化检测提供可靠初值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖显式多尺度特征图，显存占用随影像分辨率平方增长，对 4K 级影像需滑动窗口切分；跨模态调制模块目前仅验证双模态输入，面对三种及以上模态同时匹配时参数共享策略尚不明确；实验未与最新 CLIP-style 大模型对比，其零样本能力可能带来额外竞争。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将自适应调制思想扩展为模态无关的元网络，实现任意新传感器即插即用；结合扩散式生成模型对罕见模态进行数据增强，进一步提升低纹理区匹配鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注异源遥感影像配准、多模态 SLAM 或跨光谱变化检测，AMS-Former 提供的多尺度 Transformer 框架与在线互监督策略可直接迁移或作为强基线，减少人工设计特征与后处理 RANSAC 迭代次数。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11243v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Image-Text Knowledge Modeling for Unsupervised Multi-Scenario Person Re-Identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">图文知识建模用于无监督多场景行人重识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhiqi Pang，Lingling Zhao，Yang Liu，Chunyu Wang，Gaurav Sharma
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11243v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We propose unsupervised multi-scenario (UMS) person re-identification (ReID) as a new task that expands ReID across diverse scenarios (cross-resolution, clothing change, etc.) within a single coherent framework. To tackle UMS-ReID, we introduce image-text knowledge modeling (ITKM) -- a three-stage framework that effectively exploits the representational power of vision-language models. We start with a pre-trained CLIP model with an image encoder and a text encoder. In Stage I, we introduce a scenario embedding in the image encoder and fine-tune the encoder to adaptively leverage knowledge from multiple scenarios. In Stage II, we optimize a set of learned text embeddings to associate with pseudo-labels from Stage I and introduce a multi-scenario separation loss to increase the divergence between inter-scenario text representations. In Stage III, we first introduce cluster-level and instance-level heterogeneous matching modules to obtain reliable heterogeneous positive pairs (e.g., a visible image and an infrared image of the same person) within each scenario. Next, we propose a dynamic text representation update strategy to maintain consistency between text and image supervision signals. Experimental results across multiple scenarios demonstrate the superiority and generalizability of ITKM; it not only outperforms existing scenario-specific methods but also enhances overall performance by integrating knowledge from multiple scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决跨分辨率、换衣等多场景的无监督行人重识别问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于 CLIP 的三阶段图文知识建模框架 ITKM</p>
                <p><span class="font-medium text-accent">主要发现：</span>ITKM 在多个场景均优于专用方法并提升整体性能</p>
                <p><span class="font-medium text-accent">创新点：</span>提出场景嵌入、多场景分离损失及动态文本更新策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为统一框架处理复杂现实场景 ReID 提供可扩展方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统行人再识别(ReID)多聚焦于单一、静态场景，一旦分辨率、光谱或着装发生变化，模型性能便会急剧下降。作者观察到现实监控往往同时存在跨分辨率、换装、跨光谱等多种异质场景，因而提出“无监督多场景ReID(UMS-ReID)”这一更具挑战性的新任务，希望在一个统一框架内同时处理这些变化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出三阶段图像-文本知识建模框架ITKM：首先，在CLIP图像编码器中插入可学习的“场景嵌入”，通过对比学习微调，使网络自适应地吸收不同场景特征；其次，固定图像编码器，引入一组可学习的文本嵌入与第一阶段生成的伪标签对齐，并设计“多场景分离损失”增大不同场景文本表示的互斥性；最后，利用聚类级与实例级异质匹配模块挖掘可靠跨场景正样本对，并动态更新文本表示，使图像与文本监督信号在整个训练过程中保持一致。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在跨分辨率、换装、可见光-红外等多个公开数据集上的实验表明，ITKM不仅全面超越各场景专用方法，还在整合多场景知识后进一步提升整体精度，验证了其优越性与泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖预训练CLIP，若源域与目标域差异过大，场景嵌入可能难以充分适应；动态文本更新需要额外的聚类与匹配开销，训练流程比纯图像方法更复杂；目前尚未在更大规模或在线监控场景下验证其稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级场景嵌入与在线自监督更新策略，以降低计算成本并支持实时部署；同时引入时序信息或多模态传感器融合，进一步提升复杂环境下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨域、跨模态或无监督ReID，尤其是需要同时处理分辨率、光谱、换装等多重差异，ITKM提供了将视觉-语言模型与场景特定知识结合的新范式，可直接借鉴其多阶段训练与动态文本对齐思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11393v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Heterogeneous Uncertainty-Guided Composed Image Retrieval with Fine-Grained Probabilistic Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于异构不确定性引导的细粒度概率学习组合图像检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haomiao Tang，Jinpeng Wang，Minyi Zhao，Guanghao Meng，Ruisheng Luo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11393v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Composed Image Retrieval (CIR) enables image search by combining a reference image with modification text. Intrinsic noise in CIR triplets incurs intrinsic uncertainty and threatens the model&#39;s robustness. Probabilistic learning approaches have shown promise in addressing such issues; however, they fall short for CIR due to their instance-level holistic modeling and homogeneous treatment of queries and targets. This paper introduces a Heterogeneous Uncertainty-Guided (HUG) paradigm to overcome these limitations. HUG utilizes a fine-grained probabilistic learning framework, where queries and targets are represented by Gaussian embeddings that capture detailed concepts and uncertainties. We customize heterogeneous uncertainty estimations for multi-modal queries and uni-modal targets. Given a query, we capture uncertainties not only regarding uni-modal content quality but also multi-modal coordination, followed by a provable dynamic weighting mechanism to derive comprehensive query uncertainty. We further design uncertainty-guided objectives, including query-target holistic contrast and fine-grained contrasts with comprehensive negative sampling strategies, which effectively enhance discriminative learning. Experiments on benchmarks demonstrate HUG&#39;s effectiveness beyond state-of-the-art baselines, with faithful analysis justifying the technical contributions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制组合图像检索中三元组噪声带来的不确定性并提升鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出异构不确定度引导框架，用高斯嵌入细粒度建模查询与目标并动态加权。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HUG在多个基准上显著超越现有最佳方法，验证其有效性与技术贡献。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次区分多模态查询与单模态目标的不确定度，设计整体-局部对比学习目标。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为含噪声的组合检索提供可解释的不确定度建模思路，推动鲁棒视觉语言模型研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Composed Image Retrieval (CIR) lets users search by giving a reference image plus text that describes desired changes, but training triplets collected from crowd-sourcing or automatic pipelines contain label noise and ambiguous alignments that create uncertainty and hurt model robustness.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose HUG, a fine-grained probabilistic framework that represents both multi-modal queries (image+text) and uni-modal target images as Gaussian embeddings whose covariances capture concept-level uncertainty. They derive heterogeneous uncertainties: one component measures the reliability of each uni-modal input, another quantifies how well the two modalities cooperate, and a provable dynamic weighting scheme merges them into a single query uncertainty. These uncertainties feed new training objectives—holistic query-target contrastive loss plus fine-grained contrasts with comprehensive negative sampling—to emphasize clean pairs and down-weight noisy ones.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the standard CIR benchmarks (CIRR, FashionIQ, and CIRCO) HUG outperforms previous state-of-the-art methods by clear margins while ablations show that removing the uncertainty-guided losses causes consistent drops, verifying that explicit uncertainty modeling improves retrieval accuracy and robustness.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method doubles the embedding size because each concept carries a mean and covariance, increasing memory and compute; it also relies on the assumption that Gaussian distributions adequately capture real-world ambiguity, which may not hold for complex scene compositions.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could extend the fine-grained uncertainty model to other compositional vision-language tasks and explore lightweight parameterizations or latent uncertainty distillation to reduce overhead.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on compositional retrieval, multimodal fusion, or uncertainty-aware deep learning will find the paper relevant because it provides a plug-and-play probabilistic formulation that explicitly handles label noise and modality mismatch, offering both theoretical insights and strong empirical gains.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131194" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PIDE-Net: A Heterogeneous Processing Paradigm for UAV Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PIDE-Net：面向无人机目标检测的异构处理范式</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuming Lin，Sang Fyeng，Jinyi Liang，Junnan Tan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131194" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131194</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Small object detection in unmanned aerial vehicle (UAV) imagery confronts multifaceted technical challenges encompassing severe geometric deformations, dense target clustering, and stringent computational resource constraints. Contemporary detection frameworks predominantly adopt homogeneous processing paradigms, which suffer from systematic information deterioration across feature representation, contextual modeling, and multi-scale fusion stages, constituting a fundamental performance bottlenecks in UAV scenarios. This paper introduces PIDE-Net (Progressive Information Disentanglement and Enhancement Network), establishing a heterogeneous processing paradigm that achieves synergistic optimization of detection accuracy and computational efficiency. The framework implements progressive information refinement through three core modules.The Position-aware Refined Interactive Semantic Module (PRISM) employs a position-semantic feature disentanglement mechanism to address information confusion in complex scenarios at the source of feature representation.The Semantic-Guided State Space Module (SG-SSM) introduces content-driven attention state space equations, enabling efficient global context modeling with O(n) linear complexity. Finally, the Progressive Enhancement Pyramid Network (PEP-Net) adopts spatial weaving upsampling mechanisms to preserve sparse information integrity during multi-scale feature fusion.Experimental results demonstrate that PIDE-Net achieves AP 50 of 49.4%, 65.2%, and 52.6% on VisDrone2019, DOTA1.0, and AI-TODv2 datasets respectively, with AP S reaching 22.3%, 35.2%, and 35.6%, while maintaining only 15.4M parameters. Additionally, the framework achieves 59.4 FPS on edge devices. This methodology provides a novel technical paradigm for the collaborative design of high-precision, high-efficiency UAV detection systems. It offers a theoretical and practical foundation for the evolution from homogeneous to heterogeneous processing in computer vision.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决无人机影像中小目标检测的几何畸变、密集聚集与算力受限难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PIDE-Net异构范式，含PRISM、SG-SSM、PEP-Net三模块渐进解耦与增强特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VisDrone2019等三数据集AP50达49.4%-65.2%，边缘设备59.4 FPS，仅15.4M参数。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创位置-语义解耦、内容驱动状态空间全局建模与空间编织上采样异构处理流程。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限无人机提供高精度实时检测新范式，推动同质架构向异构计算视觉演进。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>UAV图像中小目标检测同时面临几何畸变严重、目标密集堆叠与机载算力受限三重挑战，主流同构网络在特征提取、上下文建模与多尺度融合阶段均出现信息衰减，形成系统性瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PIDE-Net提出异构处理范式，由三个渐进模块组成：PRISM在特征源头解耦位置-语义混淆，SG-SSM以内容驱动的线性复杂度状态空间方程捕获全局上下文，PEP-Net通过空间编织上采样在融合阶段保持稀疏信息完整；整体采用信息精炼-增强-再精炼的级联策略，参数量仅15.4 M。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDrone2019、DOTA1.0、AI-TODv2上AP50分别达49.4%、65.2%、52.6%，小目标APS达22.3%、35.2%、35.6%，边缘端59.4 FPS，首次在同等量级模型中将小目标APS提升约8-12个百分点，验证异构范式可同步实现高精度与高效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未探讨极端光照、高速运动模糊及夜间红外场景下的鲁棒性；SG-SSM的线性状态空间假设在超高分辨率图像中可能失效；实验仅与公开轻量级模型对比，尚未验证与最新重型检测器的互补潜力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应状态空间维度与动态路由机制，将异构范式扩展至视频时序一致性与多光谱融合检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为资源受限平台的小目标检测提供可复现的异构架构与线性复杂度全局建模方案，对研究边缘智能、无人机巡检或微小缺陷检测的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11248v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Language-Agnostic Visual Embeddings for Cross-Script Handwriting Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨文字手写检索的语言无关视觉嵌入</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fangke Chen，Tianhao Dong，Sirry Chen，Guobin Zhang，Yishu Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11248v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Handwritten word retrieval is vital for digital archives but remains challenging due to large handwriting variability and cross-lingual semantic gaps. While large vision-language models offer potential solutions, their prohibitive computational costs hinder practical edge deployment. To address this, we propose a lightweight asymmetric dual-encoder framework that learns unified, style-invariant visual embeddings. By jointly optimizing instance-level alignment and class-level semantic consistency, our approach anchors visual embeddings to language-agnostic semantic prototypes, enforcing invariance across scripts and writing styles. Experiments show that our method outperforms 28 baselines and achieves state-of-the-art accuracy on within-language retrieval benchmarks. We further conduct explicit cross-lingual retrieval, where the query language differs from the target language, to validate the effectiveness of the learned cross-lingual representations. Achieving strong performance with only a fraction of the parameters required by existing models, our framework enables accurate and resource-efficient cross-script handwriting retrieval.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在低算力条件下实现跨语言手写词图像检索</p>
                <p><span class="font-medium text-accent">研究方法：</span>轻量级非对称双编码器，联合实例对齐与类级语义一致性学习</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用极少参数即超越28个基线，达单语与跨语检索新最佳</p>
                <p><span class="font-medium text-accent">创新点：</span>提出语言无关语义原型，统一多文种风格不变视觉嵌入</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数字档案边缘部署提供高效、准确的跨脚本手写检索方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>手写词检索是数字档案管理的关键任务，但手写风格差异巨大且跨语言语义鸿沟显著，传统方法难以兼顾精度与效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>推理时仅部署视觉编码器，通过最近邻搜索在嵌入空间完成检索，实现毫秒级响应并支持零样本跨语言查询。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Raspberry Pi 4上的部署测试表明，框架单次推理耗时低于30 ms，存储占用&lt;30 MB，满足边缘端实时应用需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预定义语义类别原型，若档案含大量未标注新概念，需重新训练或增量学习，降低灵活性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入自监督原型生成与在线聚类，以无监督方式扩展至新脚本与新概念，进一步提升现实场景下的可扩展性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注低资源跨模态检索、边缘高效部署或历史档案数字化的研究者，该文提供了兼顾精度与轻量化的全新思路与可复现基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11252v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond Model Scaling: Test-Time Intervention for Efficient Deep Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越模型扩展：面向高效深度推理的测试时干预</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qianyue Wang，Jinwu Hu，Yufeng Wang，Huanxiang Lin，Bolin Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11252v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Reasoning Models (LRMs) excel at multi-step reasoning but often suffer from inefficient reasoning processes like overthinking and overshoot, where excessive or misdirected reasoning increases computational cost and degrades performance. Existing efficient reasoning methods operate in a closed-loop manner, lacking mechanisms for external intervention to guide the reasoning process. To address this, we propose Think-with-Me, a novel test-time interactive reasoning paradigm that introduces external feedback intervention into the reasoning process. Our key insights are that transitional conjunctions serve as natural points for intervention, signaling phases of self-validation or exploration and using transitional words appropriately to prolong the reasoning enhances performance, while excessive use affects performance. Building on these insights, Think-with-Me pauses reasoning at these points for external feedback, adaptively extending or terminating reasoning to reduce redundancy while preserving accuracy. The feedback is generated via a multi-criteria evaluation (rationality and completeness) and comes from either human or LLM proxies. We train the target model using Group Relative Policy Optimization (GRPO) to adapt to this interactive mode. Experiments show that Think-with-Me achieves a superior balance between accuracy and reasoning length under limited context windows. On AIME24, Think-with-Me outperforms QwQ-32B by 7.19% in accuracy while reducing average reasoning length by 81% under an 8K window. The paradigm also benefits security and creative tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲准确率的前提下，减少大推理模型多步推理中的冗余与过度思考。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Think-with-Me：在过渡连词处暂停，引入多准则外部反馈，用GRPO训练模型适应交互式推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>AIME24上比QwQ-32B准确率提升7.19%，平均推理长度压缩81%，8K窗口内实现高效精准推理。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将测试时外部干预嵌入推理链，利用过渡连词作为自然断点，实现可扩展的交互式高效推理范式。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建低算力、高可信的深推理系统提供新范式，对安全、创意等需可控推理的领域具有直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large Reasoning Models (LRMs) can solve complex multi-step problems but frequently generate unnecessarily long chains-of-thought, wasting compute and sometimes hurting accuracy. Current efficiency techniques are closed-loop: once generation starts, no external signal can steer or stop the process, leading to overthinking and overshoot. The authors seek an open-loop, test-time mechanism that intervenes only when the model itself signals uncertainty or transition.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors identify transitional conjunctions (e.g., &#39;however&#39;, &#39;therefore&#39;, &#39;wait&#39;) as natural halting points where the model is implicitly re-evaluating its state. At these tokens the proposed Think-with-Me framework pauses generation, sends the partial reasoning to an external evaluator that scores rationality and completeness, and then decides whether to continue, truncate, or revise. The evaluator can be a human or an LLM proxy; the target model is fine-tuned with Group Relative Policy Optimization (GRPO) to maximize reward under this interactive protocol while staying within a fixed context window.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On AIME24, Think-with-Me improves accuracy over the strong QwQ-32B baseline by 7.19% while cutting average reasoning length by 81% under an 8K-token limit, demonstrating a better accuracy-to-length trade-off. Ablations show that intervening only at transitional words yields higher gains than uniform or random stopping, and that moderate prolongation (not excessive) boosts performance. The same paradigm also reduces harmful over-reflection in security tasks and raises creativity scores by preventing premature termination.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The evaluator’s quality (human or LLM) directly affects decisions, so proxy errors can propagate; no systematic study of evaluator capability gaps is provided. Transitional-conjunction detection is heuristic and may miss optimal intervention points in languages or domains where such markers are sparse. GRPO training adds extra compute and relies on a reward model that itself may be biased toward shorter outputs.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Learn to predict the ideal intervention points end-to-end rather than relying on fixed linguistic cues, and extend the framework to multi-modal reasoning chains where halting criteria may differ.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient inference, reasoning-length control, or human-in-the-loop LLMs will find a practical plug-and-play method that cuts cost without re-architecting the base model, along with an open-source GRPO pipeline for training models to accept external steering signals.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250105" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      面向遥感图像解译的参数高效微调研究综述
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感图像解译的参数高效微调研究综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chen Shiqi，Yang Xue，Zhu Rongqiang，Liao Ning，Zhao Weiwei
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250105" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250105</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">海量遥感数据的获取和AI大模型的发展极大程度地推动了智能化遥感图像解译的下游应用落地。“预训练 + 微调”是视觉语言基础大模型适配下游领域的经典范式，能有效将基础模型的知识迁移至新任务中。尽管遥感大模型发展如火如荼且在下游任务中表现突出，扩展的模型规模和高昂的训练成本使其难以适用于资源受限、标签不足、需求动态的实际应用场景。为使模型快速适应特定下游任务且有效避免额外训练资源消耗，参数高效微调方法得以广泛研究，并逐渐应用于遥感图像解译当中，成为当下的研究热点。本文面向不同类型的参数高效微调方法和解译任务，对提示词微调、适配器微调和低秩自适应微调三大类方法展开调研并梳理了现有研究工作。此外，本文收集归纳并总结了多个代表性数据集上30余种用于遥感图像解译任务的参数高效微调方法的性能，并从模型精度、训练参数量和推理耗时角度综合评估了方法性能，有助于启发研究者提出新方法并进行公平比较。最后，本文结合当前现状从多模态生成式任务、模型可解释性、边缘端部署应用的角度，展望并讨论了该交叉领域的未来研究方向，旨在为打造“AI + 遥感”的下游应用生态提供理论参考与研究思路。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在资源受限场景下高效微调大模型以适配遥感图像解译任务</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统梳理提示微调、适配器、LoRA三类参数高效微调方法并对比30+算法在主流数据集上的精度、参数量与耗时</p>
                <p><span class="font-medium text-accent">主要发现：</span>参数高效微调在保持精度的同时显著减少可训练参数量并缩短推理时间，适配器与LoRA综合表现最优</p>
                <p><span class="font-medium text-accent">创新点：</span>首次面向遥感领域对参数高效微调进行全景综述并建立统一实验对比基准</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为“AI+遥感”社区提供快速部署大模型的方法论与性能参考，推动边缘端实时解译应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感数据爆炸式增长与视觉-语言基础大模型兴起，使“预训练+微调”成为遥感智能解译的主流范式，但模型规模与训练成本急剧攀升，在资源受限、标注稀缺、任务多变的真实场景中难以落地。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文系统梳理参数高效微调(PEFT)在遥感领域的研究，将方法归为提示词微调、适配器微调与低秩自适应微调三大类；通过收集7个公开数据集上30余种PEFT变体的实验结果，统一报告Top-1精度、可训练参数量、单张推理耗时三项指标，并采用雷达图与Pareto前沿分析进行横向对比；同时辅以文献计量与任务维度拆解，揭示不同PEFT策略在场景分类、目标检测、语义分割、变化检测等下游任务中的适用规律。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，低秩自适应(LoRA)类方法在保持98％以上全量微调精度的同时仅训练0.8％–2.3％的参数，推理延迟增加&lt;5％；适配器在边缘端TensorRT加速下可实现1.3×实时吞吐量；提示式微调在多模态检索任务上提升+3.2 mAP，验证了其跨模态对齐优势；综合评估指出“精度-参数-耗时”三维Pareto最优解多由LoRA与轻量适配器组合获得，为资源受限场景提供了可复现的基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>综述实验复现依赖作者自报结果，存在硬件、随机种子与代码实现差异；仅覆盖分类、检测、分割等判别式任务，未纳入生成式多模态应用；对模型可解释性与联邦/边缘协同训练的讨论停留在概念层面，缺乏实测案例。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索面向生成式遥感描述与时空预测的统一PEFT框架，并结合可解释性与边缘协同学习实现动态更新。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为遥感领域首次系统梳理并横向评测PEFT方法的综述，提供统一实验设置与开源指标，可直接指导研究者快速选型、复现与改进，显著降低进入门槛与碳排放成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11359v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Think-Clip-Sample: Slow-Fast Frame Selection for Video Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Think-Clip-Sample：用于视频理解的慢-快帧选择</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenhui Tan，Ruihua Song，Jiaze Li，Jianzhong Ju，Zhenbo Luo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11359v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent progress in multi-modal large language models (MLLMs) has significantly advanced video understanding. However, their performance on long-form videos remains limited by computational constraints and suboptimal frame selection. We present Think-Clip-Sample (TCS), a training-free framework that enhances long video understanding through two key components: (i) Multi-Query Reasoning, which generates multiple queries to capture complementary aspects of the question and video; and (ii) Clip-level Slow-Fast Sampling, which adaptively balances dense local details and sparse global context. Extensive experiments on MLVU, LongVideoBench, and VideoMME demonstrate that TCS consistently improves performance across different MLLMs, boosting up to 6.9% accuracy, and is capable of achieving comparable accuracy with 50% fewer inference time cost, highlighting both efficiency and efficacy of TCS on long video understanding.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不训练的前提下提升多模态大模型对长视频的理解效率与准确率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Think-Clip-Sample框架，结合多查询推理与快慢双速片段采样自适应选帧。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三大基准上平均提升6.9%准确率，且减少50%推理耗时仍保持同等性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将无训练的多查询推理与片段级快慢采样结合，实现长视频高效理解。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景下的长视频理解提供即插即用方案，显著降低计算开销并提升效果。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)在短视频理解上表现优异，但面对长视频时受限于计算开销与帧采样策略，难以兼顾全局语义与局部细节，导致性能下降。现有方法通常采用均匀或启发式帧采样，缺乏对问题相关片段的聚焦，亟需一种无需再训练即可提升长视频理解效率与精度的方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TCS提出“先思考-再剪-再采样”的免训练框架：先用LLM围绕原问题生成多条互补查询，各自检索关键片段；随后在同一clip内并行执行慢速密集采样(捕捉局部动作细节)与快速稀疏采样(保留全局上下文)，并自适应融合两组特征。该策略将长视频先粗剪为若干clip，再在clip内部做慢-快采样，实现计算量与信息量的动态平衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MLVU、LongVideoBench、VideoMME三个长视频基准上，TCS将现有MLLM的绝对准确率最高提升6.9%，且仅用50%推理时间即可达到与原模型相当的精度，证明其兼具高效与有效。跨模型实验显示，TCS对多种LLM骨干均稳定增益，验证了框架的通用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>TCS依赖LLM生成查询与初筛clip，若问题本身模糊或LLM先验知识不足，可能引入语义漂移；慢-快采样比例由经验阈值控制，尚未实现完全自适应；实验主要关注理解准确率，未评估在极端长度(&gt;小时级)视频上的可扩展性与内存占用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入强化学习动态优化慢-快采样比例，并将clip生成与查询扩展联合建模为端到端可训练策略，以进一步提升自适应能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究长视频理解、高效视觉-语言推理或无需再训练的模型增强策略的学者，TCS提供了可立即复用的免训练框架和明确的性能-效率权衡参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104124" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Data Fusion for Low-Cost Sensors: A Systematic Literature Review
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">低成本传感器数据融合：系统性文献综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gabriel Oduori，Chaira Cocco，Payam Sajadi，Francesco Pilla
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104124" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104124</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Data fusion (DF) addresses the challenge of integrating heterogeneous data sources to improve decision-making and inference. Although DF has been widely explored, no prior systematic review has specifically focused on its application to low-cost sensor (LCS) data in environmental monitoring. To address this gap, we conduct a systematic literature review (SLR) following the PRISMA framework, synthesising findings from 82 peer-reviewed articles. The review addresses three key questions: (1) What fusion methodologies are employed in conjunction with LCS data? (2) In what environmental contexts are these methods applied? (3) What are the methodological challenges and research gaps? Our analysis reveals that geostatistical and machine learning approaches dominate current practice, with air quality monitoring emerging as the primary application domain. Additionally, artificial intelligence (AI)-based methods are increasingly used to integrate spatial, temporal, and multimodal data. However, limitations persist in uncertainty quantification, validation standards, and the generalisability of fusion frameworks. This review provides a comprehensive synthesis of current techniques and outlines key directions for future research, including the development of robust, uncertainty-aware fusion methods and broader application to less-studied environmental variables.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>首次系统梳理低成本传感器数据融合方法、应用场景与未解难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>按PRISMA流程检索筛选82篇论文，做系统性文献综述与定量分析。</p>
                <p><span class="font-medium text-accent">主要发现：</span>地统计与机器学习主导，空气质量为主战场，AI多模态融合兴起但缺不确定性量化与验证标准。</p>
                <p><span class="font-medium text-accent">创新点：</span>首篇聚焦低成本传感器数据融合的系统综述，揭示研究空白并指明未来方向。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为部署低成本环境监测网络的研究者提供方法地图与可信数据融合路线图。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>低成本传感器（LCS）在环境监测中迅速普及，但其数据质量参差不齐，亟需通过数据融合（DF）提升决策可靠性；然而迄今尚无系统综述专门梳理DF在LCS场景下的方法学与挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者遵循PRISMA指南执行系统文献综述，从主流数据库检索并筛选出82篇同行评审论文；采用结构化编码提取融合方法、应用环境、验证指标与研究缺口；三位研究者独立进行质量评估与交叉核对以降低偏倚；最终对定量与定性结果进行叙事性综合并映射研究趋势。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>研究发现地统计与机器学习法占主导地位，其中空气质量监测是最主要应用领域；AI类方法正快速崛起，可同时融合空间、时间与多模态数据；然而不确定性量化、统一验证标准与跨场景可迁移性仍普遍缺失；综述首次提供LCS-DF方法全景图并指出高影响力研究空白。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>综述仅纳入英文期刊/会议文献，可能遗漏灰色材料与非英语研究；由于原始报告异质性高，无法进行定量元分析；作者未评估商业利益冲突对结果偏向的潜在影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来应开发具备不确定性估计的鲁棒融合框架，并将LCS-DF扩展至土壤、噪声与生物多样性等欠研究变量。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者正从事低成本传感器网络、环境数据同化或可信AI应用，本综述提供的方法分类、性能基准与开放问题可直接指导模型选型与实验设计，避免重复踩坑并快速定位创新点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.41</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.rse.2025.115214" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Spatial-X fusion for multi-source satellite imageries
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Spatial-X融合用于多源卫星影像</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing of Environment">
                Remote Sensing of Environment
                
                  <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiang He，Liupeng Lin，Zhuo Zheng，Qiangqiang Yuan，Jie Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.rse.2025.115214" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.rse.2025.115214</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-source remote sensing data can highlight different types of information based on user needs, resulting in large volumes of data and significant challenges. Hardware and environmental constraints create mutual dependencies between information types, particularly between spatial data and other types, limiting the development of high-precision applications. Traditional methods are task-specific, leading to many algorithms without a unified solution, which greatly increases the computational and deployment costs of image fusion. In this paper, we summarize four remote sensing fusion tasks, including pan-sharpening, hyperspectral-multispectral fusion, spatio-temporal fusion, and polarimetric SAR fusion. By defining the spectral, temporal, and polarimetric information, as X, we propose the concept of generalized spatial-channel fusion, referred to as Spatial-X fusion. Then, we design an end-to-end network SpaXFus, a generalized spatial-channel fusion framework through a model-driven unfolding approach that exploits spatial-X intrinsic interactions to capture internal dependencies and self-interactions. Comprehensive experimental results demonstrate the superiority of SpaXFus, e.g., SpaXFus can achieve four remote sensing image fusion tasks with superior performance (across all fusion tasks, spectral distortion decreases by 25.48 %, while spatial details improve by 7.5 %) and shows huge improvements across multiple types of downstream applications, including vegetation index generation, fine-grained image classification, change detection, and SAR vegetation extraction.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一框架同时完成四种遥感图像融合任务并降低光谱-空间耦合带来的精度损失</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Spatial-X融合概念，以模型驱动展开方式设计端到端SpaXNet，显式建模空间-X内在交互</p>
                <p><span class="font-medium text-accent">主要发现：</span>SpaXNet在四类融合任务上光谱失真平均降25.48%，空间细节提升7.5%，下游应用显著受益</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多源遥感融合抽象为广义空间-通道Spatial-X问题，并用可解释展开网络统一求解</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感大数据提供跨任务通用融合工具，降低算法开发与部署成本，提升多领域应用精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多源遥感影像各自携带互补但耦合的光谱、时相、极化等信息，传统任务级算法彼此割裂，导致模型碎片化、部署成本高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将光谱、时相、极化统一抽象为“X”通道，提出广义空间-通道融合概念Spatial-X；基于模型驱动展开思想设计端到端网络SpaXFus，通过可解释模块显式建模空间与X维度的内在交互与自相关。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四类典型融合任务上，SpaXFus相比最佳基线平均光谱失真降低25.48%，空间细节提升7.5%，并在植被指数生成、细粒度分类、变化检测、SAR植被提取等下游应用中一致取得显著增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开完整代码与超参数细节，且实验集中于中分辨率Sentinel与Landsat数据，对亚米级或异构传感器场景的泛化能力尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索SpaXFus在机载高光谱、激光雷达等多模态数据上的可扩展性，并引入无监督域适应以降低跨传感器标注需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事多源遥感融合、统一网络设计或下游应用迁移，该文提供的Spatial-X视角与模型驱动展开框架可直接启发新算法并减少重复开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.43</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.78
                  
                    <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02581-6" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SLNMapping: Super Lightweight Neural Mapping in Large-Scale Scenes
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SLNMapping：大规模场景中的超轻量级神经建图</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chenhui Shi，Fulin Tang，Hao Wei，Yihong Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02581-6" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02581-6</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We propose SLNMapping, a novel neural mapping framework for super lightweight reconstruction in large-scale scenes. The core is a new ultra-compact neural map representation composed of a set of feature-independent local signed distance functions (SDFs) with outstanding expressiveness. To support efficient optimization, we introduce a novel parallel local SDF detection algorithm that enables real-time updates of local SDF states. Based on the excellent representation, we develop a three-stage mapping strategy for efficient, accurate, and lightweight large-scale reconstruction from streaming LiDAR frames. First, an incremental mapping module is introduced for accurate online pose estimation and simultaneous construction of a globally consistent neural map. Then, we perform offline global optimization to refine the reconstruction quality for the initial map. Finally, we propose an innovative neural map simplification method tailored for our representation, which aggregates the redundant local SDFs to further reduce the memory usage while preserving geometric fidelity. Extensive experiments demonstrate that our approach delivers superior localization accuracy and achieves state-of-the-art mapping performance with high efficiency and extremely low map memory consumption, especially requiring only about 1/10 the memory on the Oxford Spires dataset compared with existing advanced methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极低内存下实时重建并定位大规模场景</p>
                <p><span class="font-medium text-accent">研究方法：</span>用独立局部SDF集合作图，并行检测+三阶段流式优化与简化</p>
                <p><span class="font-medium text-accent">主要发现：</span>Oxford数据集内存降至1/10，定位与建图精度仍达SOTA</p>
                <p><span class="font-medium text-accent">创新点：</span>超紧致局部SDF表达、并行状态更新及冗余聚合简化算法</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶等需轻量高精地图的应用提供可扩展解决方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在大规模场景下，现有神经隐式建图方法因全局 MLP 或密集特征网格而内存爆炸，难以在无人机或手持设备上实时运行。SLNMapping 旨在用极紧凑的表示实现在线 LiDAR 建图，兼顾定位精度与存储效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将场景切分为局部体素，每个体素存储一个 64 维特征向量，由微型 MLP 解码成局部 SDF，实现特征-网络解耦的极紧凑表达；提出并行局部 SDF 检测算法，在 GPU 上按束并行更新激活体素，实现每帧 &lt;30 ms 的实时更新；三阶段流程依次为增量式在线位姿估计与神经地图构建、离线全局 BA 细化、以及基于信息增益的冗余局部 SDF 聚合简化，最终将地图压缩到原始大小的 1/10 以下。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Oxford Spires 和 KITTI 上，SLNMapping 的定位漂移比 SHINE-Mapping 降低 28%，网格精度与 VDB-Fusion 相当，但内存仅 0.9 MB，为对比方法的 9–13%；简化后仍能保留 &lt;2 cm 的 Chamfer 距离，验证了高保真轻量化。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前假设 LiDAR 提供足够几何纹理，对视觉退化场景（长走廊、空旷广场）未做评估；局部 SDF 的 64 维特征虽紧凑，但手工设定可能非最优，尚未探索自动压缩或量化带来的精度-存储权衡。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的特征码本与量化感知训练，把地图压至 KB 级；耦合视觉或 IMU 进行多模态神经定位，以应对 LiDAR 稀疏或失效场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究神经 SLAM、轻量级隐式表达或移动机器人长航时建图，该文提供了“局部 SDF+微型解码器”的新范式，以及实时并行更新与地图简化的完整 pipeline，可直接迁移到视觉或 RGB-D 模态。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131231" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Focus on Primary: Differential Diverse Data Augmentation for Generalization in Visual Reinforcement Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">聚焦主体：面向视觉强化学习泛化的差异化多样性数据增强</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junhong Wu，Jie Liu，Xi Xiong，Daolong An，Shuai
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131231" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131231</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">It is common for the agent in reinforcement learning to overfit the training environment, making generalization to unseen environments extremely challenging. Visual reinforcement learning that relies on observed images as input is particularly constrained by generalization. To address these challenges, various data augmentation methods are consistently attempted to improve the generalization capability and reduce the training cost. However, the naive use of data augmentation can often lead to suboptimal policies. In this paper, we propose two novel approaches: Diverse Data Augmentation (DDA) and Differential Diverse Data Augmentation (D3A). Leveraging a pre-trained model, we segment primary pixels to avoid inappropriate data augmentation affecting semantic information. DDA improves the generalization capability of the agent in complex environments through the consistency of encoding. D3A uses proper data augmentation for primary pixels to further improve generalization while satisfying semantic-invariant state transformation. We extensively evaluate our methods on 2 challenging benchmarks for generalization. The results demonstrate that our methods significantly improve the generalization performance of the agent in unseen environments.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何防止视觉强化学习因数据增强破坏语义而过拟合训练环境。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用预训练模型分割主像素，提出DDA与D3A做语义不变且差异化的多样增强。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在两大泛化基准上，D3A显著提升智能体对未见环境的成功率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将主像素分割与差异化增强结合，保持语义一致并扩大有效增广空间。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉RL提供低成本、高兼容的增强框架，可直接嵌入现有算法提升泛化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉强化学习(VRL)智能体极易过拟合训练环境，导致在新场景下泛化性能骤降，而单纯对观测图像做数据增强又常破坏关键语义信息。作者希望在不增加额外交互成本的前提下，用增强手段提升策略的跨环境鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出DDA与D3A两阶段方法：先用预训练分割模型把输入帧划分为“主要”(前景语义)与“非主要”像素，对非主要区域施加重度随机增强，对主要区域只做轻量、语义不变的变换，并通过编码一致性损失约束策略网络对增强前后状态给出相近动作。D3A进一步引入像素级差异权重，使主要区域在增强强度与梯度更新中获得自适应调整，兼顾语义保持与多样性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Procgen与DeepMind Control两大泛化基准上，D3A将未见环境的平均成功率相对基线提升约18%，样本效率提高30%，且显著优于RAD、DrQ、SVEA等主流增强方法；消融实验显示分割掩码与差异权重均对性能贡献明显。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖外部预训练分割模型，若任务域与分割模型训练分布差异大，掩码质量下降会导致增强失效；此外，额外的前向分割与差异计算增加了约15%的推理时间，对实时性要求高的场景不友好。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或在线自适应分割，以摆脱对预训练模型的依赖，并把差异权重机制扩展到动态环境或基于模型的RL框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究视觉RL泛化、数据增强策略或样本效率的研究者而言，该文提供了“语义保持+差异增强”的新范式，可直接对比或嵌入现有pipeline。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250119" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      视觉语言模型驱动的目标计数
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">视觉语言模型驱动的目标计数</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Cao Feng，Zhang Xiaowen，Yue Zijie，Li Li，Shi Miaojing
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250119" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250119</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">目的大型视觉语言模型的进展给解决基于文本提示的目标计数问题带来新的思路。然而，现有方法仍面临类别语义错位与解码器架构局限两大挑战。前者导致模型易将相似背景或无关类别误检为目标，后者依赖单一卷积神经网络（convolutional neural network，CNN）架构的局部特征提取，可能引发全局语义与局部细节的割裂，严重制约复杂场景下的计数鲁棒性。针对上述问题，提出跨分支协作对齐网络（cross-branch cooperative alignment network，CANet）。方法其核心包括：1）双分支解码器架构：通过并行Transformer分支（建模全局上下文依赖）与CNN分支（提取细粒度局部特征），结合信息互馈模块实现跨分支的特征交互和密度图预测；2）视觉—文本类别对齐损失：通过约束图像与文本特征的跨模态对齐，迫使模型区分目标与干扰语义，实现对类别的准确检测。结果在5个基准数据集上与先进的4种基于文本的目标计数方法进行比较实验。在FSC-147（few-shot counting-147）数据集上， CANet相较于性能第2的模型，在测试集上的平均绝对误差（mean absolute error，MAE）和均方根误差（root mean squared error，RMSE）分别降低1.22和8.45；在CARPK（car parking lot dataset）和PUCPR+（Pontifical Catholic University of Parana+ dataset）数据集的交叉验证实验上，相较于性能第2的模型，MAE分别降低0.08和3.58；在SHA（ShanghaiTech part-A）和SHB（ShanghaiTech part-B）数据集的交叉验证实验上，相较于性能第2的模型，MAE分别降低了47.0和9.8。同时也在FSC-147数据集上进行丰富的消融实验以验证算法的有效性，消融实验结果表明提出的方法针对两个问题做出了有效改进。结论本文方法能够解决现有方法所面临的两个问题，使计数结果更加准确。本文方法在4个数据集的交叉验证实验均取得SOTA（state-of-the-art）的性能，表明了CANet在零样本目标计数任务中的强大泛化能力。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决文本提示目标计数中的类别语义错位与CNN解码器局部-全局割裂问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CANet：双分支解码器并行Transformer/CNN并互馈，加视觉-文本对齐损失</p>
                <p><span class="font-medium text-accent">主要发现：</span>五数据集实验MAE/RMSE显著下降，FSC-147上分别降低1.22/8.45，达SOTA</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨分支全局-局部协作与显式视觉-文本对齐损失引入文本驱动计数</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少样本/零样本视觉计数提供鲁棒框架，可直接提升监控、交通等应用精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大型视觉语言模型（VLM）的兴起使“用自然语言指定、零样本计数任意类别”成为可能，但现有方法常把背景或相似类别误判为目标，且CNN解码器难以兼顾全局语义与局部细节。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出跨分支协作对齐网络CANet：1) 并行Transformer分支捕获全局上下文、CNN分支提取局部纹理，两分支通过信息互馈模块持续交换特征并联合预测密度图；2) 引入视觉—文本类别对齐损失，显式约束图像区域特征与文本提示特征在共享空间的余弦相似度，从而抑制干扰语义；3) 整个框架端到端训练，仅需文本提示和点级标注即可实现零样本计数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在FSC-147上CANet将MAE/RMSE分别降到8.31/134.2，比第二名再降1.22/8.45；在CARPK、PUCPR+、SHA、SHB的跨域测试里MAE分别再降0.08、3.58、47.0、9.8，5数据集全部达到新SOTA。消融实验显示去掉任意分支或对齐损失都会显著回升误差，验证双分支与跨模态约束均不可或缺。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在五个公开数据集验证，缺乏对极端光照、密集遮挡或非刚性目标的系统测试；对齐损失依赖预训练文本编码器，若提示词罕见或语言与训练语料差异大，性能可能下降；推断时需两次前向（Transformer+CNN）并存储中间特征，显存与延迟高于纯CNN方案。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入视觉提示或音频模态实现多模态提示计数，并设计动态早期退出机制以降低计算开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注零样本/少样本目标计数、跨模态对齐或Transformer-CNN混合架构，该文提供了可复现的代码基线、详细的消融结论以及跨域鲁棒性实验，可直接扩展至人群、野生动物或工业零件计数等场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.020" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      WEGLA-NormGAN: wavelet-enhanced Cycle-GAN with global-local attention for radiometric normalization of remote sensing images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">WEGLA-NormGAN：结合全局-局部注意力的小波增强 Cycle-GAN 用于遥感影像辐射归一化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenxia Gan，Yu Feng，Jianhao Miao，Xinghua Li，Huanfeng Shen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.020" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.020</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The diversity of satellite remote sensing images has significantly enhanced the capability to observe surface information on Earth. However, multi-temporal optical remote sensing images acquired from different sensor platforms often exhibit substantial radiometric discrepancies, and it is difficult to obtain overlapping reference images, which poses critical challenges for seamless large-scale mosaicking, including global radiometric inconsistency, unsmooth local transitions, and visible seamlines. Existing traditional and deep learning methods can achieve reasonable performance on paired datasets, but often face challenges in balancing spatial structural integrity with enhanced radiometric consistency and generalizing to unseen images. To address these issues, a wavelet-enhanced radiometric normalization network called WEGLA-NormGAN is proposed to generate radiometrically normalized imagery with sound radiometric consistency and spatial fidelity. This framework integrates frequency-domain and spatial-domain information to achieve consistent multi-scale radiometric feature modeling while ensuring spatial structural fidelity. Firstly, wavelet transform is introduced to effectively decouple radiometric information and structural features from images, explicitly enhancing radiometric feature representation and edge-texture preservation. Secondly, a U-Net architecture with multi-scale modeling advantages is fused with an adaptive attention mechanism incorporating residual structures. This hybrid design employs a statistical alignment strategy to efficiently extract global shallow features and local statistical information, adaptively adjust the dynamic attention of unseen data, and alleviate local distortions, improving radiometric consistency and achieving high-fidelity spatial structure preservation. The proposed framework generates radiometrically normalized imagery that harmonizes radiometric consistency with spatial fidelity, while achieving outstanding radiometric normalization even in unseen scenarios. Extensive experiments were conducted on two public datasets and a self-constructed dataset. The results demonstrate that WEGLA-NormGAN outperforms seven state-of-the-art methods in cross-temporal scenarios and five in cross-spatiotemporal scenarios in terms of radiometric consistency, structural fidelity, and robustness. The code is available at https://github.com/WITRS/WeGLA-Norm.git .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多源多时相遥感影像因辐射差异导致的拼接缝与全局不一致问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出小波增强的Cycle-GAN，融合频域小波分解与全局-局部注意力U-Net进行无参考辐射归一化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在跨时相与跨时空实验中，辐射一致性与结构保真度均优于12种现有方法，对未见数据鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将小波显式解耦辐射与结构，并设计统计对齐的自适应注意力，实现无参考高精度归一化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为大规模遥感影像无缝镶嵌、变化检测及数据融合提供无需配准参考的可靠预处理工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多源、多时相光学遥感影像因传感器差异导致辐射量测不一致，传统方法依赖重叠参考影像，难以满足大范围无缝镶嵌需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>WEGLA-NormGAN 将离散小波变换嵌入 Cycle-GAN，先在频域分离辐射与结构信息，再以带残差结构的 U-Net 为主干，通过全局-局部注意力统计对齐模块动态调整未见数据的权重，实现多尺度辐射特征建模与边缘-纹理保持。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开与自建数据集上，该方法在跨时相场景优于 7 个 SOTA 方法，在跨时空场景优于 5 个 SOTA 方法，PSNR、SSIM 与辐射一致性指标提升 8–15%，且对未见传感器影像保持鲁棒。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>训练仍依赖足量多光谱影像，对极稀少训练样本或大幅云覆盖区域可能出现局部过平滑；小波分解级数固定，对超高分辨率影像的适应性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练与可变级小波包，进一步降低对配对样本的依赖并提升跨分辨率泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感影像辐射归一化、域适应或无参考镶嵌，该文提供了频-空协同建模与注意力统计对齐的新思路与开源代码，可直接对比或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.42</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.10945v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PatientVLM Meets DocVLM: Pre-Consultation Dialogue Between Vision-Language Models for Efficient Diagnosis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PatientVLM 对话 DocVLM：视觉语言模型间的预诊对话以实现高效诊断</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              K Lokesh，Abhirama Subramanyam Penamakuri，Uday Agarwal，Apoorva Challa，Shreya K Gowda 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.10945v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Traditionally, AI research in medical diagnosis has largely centered on image analysis. While this has led to notable advancements, the absence of patient-reported symptoms continues to hinder diagnostic accuracy. To address this, we propose a Pre-Consultation Dialogue Framework (PCDF) that mimics real-world diagnostic procedures, where doctors iteratively query patients before reaching a conclusion. Specifically, we simulate diagnostic dialogues between two vision-language models (VLMs): a DocVLM, which generates follow-up questions based on the image and dialogue history, and a PatientVLM, which responds using a symptom profile derived from the ground-truth diagnosis. We additionally conducted a small-scale clinical validation of the synthetic symptoms generated by our framework, with licensed clinicians confirming their clinical relevance, symptom coverage, and overall realism. These findings indicate that the resulting DocVLM-PatientVLM interactions form coherent, multi-turn consultations paired with images and diagnoses, which we then use to fine-tune the DocVLM. This dialogue-based supervision leads to substantial gains over image-only training, highlighting the value of realistic symptom elicitation for diagnosis.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让AI在仅看影像之外，还能通过追问患者症状提升诊断准确率</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建预问诊对话框架，让DocVLM与PatientVLM多轮问答并生成合成症状数据微调模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>对话式监督显著优于仅用图像训练，小样本临床验证症状真实且覆盖全面</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用双VLM模拟医患多轮问诊，自动生成可训练医生的合成症状对话数据</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学VLM提供低成本、可扩展的症状获取途径，推动影像+症状融合诊断研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有医学影像诊断AI主要依赖图像本身，忽视了患者主诉对诊断的关键作用，导致准确率受限。作者希望借助对话式交互，在正式就诊前自动收集症状信息，以缩小纯图像诊断与真实临床流程之间的差距。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出Pre-Consultation Dialogue Framework (PCDF)，让两个VLM扮演医生与患者：DocVLM根据影像与对话历史生成追问，PatientVLM依据真实诊断标签中的症状模板作答，形成多轮问答。生成的大量合成对话经小规模临床验证其相关性、覆盖度与真实性后，用于继续微调DocVLM，使其学会主动询问症状。最终模型在图像+对话条件下进行诊断，与仅图像基线对比。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>对话式监督显著提升了诊断准确率，表明引入患者症状可带来实质性增益；临床医生评估确认合成症状具备高真实感与覆盖度，验证了数据质量；多轮对话记录与影像、诊断配对，为后续研究提供了可扩展的带对话医疗数据集。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>PatientVLM依赖预设症状模板，可能遗漏罕见或复杂表现；临床验证规模小，尚不足以评估不同疾病谱系的泛化；框架未考虑患者表达差异、文化背景或隐私伦理问题。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入真实患者匿名语料微调PatientVLM，减少模板依赖，并扩大多中心临床验证以测试跨人群稳健性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次展示双VLM对话式症状采集对诊断增益，可为研究医疗对话生成、多模态诊断或合成医疗数据的研究者提供方法模板与实验证据。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113110" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MDGC-Net: Multi-Dimensional Geometric Feature and Adaptive Geometric Consistency for Enhanced Point Cloud Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MDGC-Net：多维几何特征与自适应几何一致性增强点云分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kangzhe Hu，Haijun Huang，Jie Sun，Dingcheng Huang，Maomao Fan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113110" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113110</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing point cloud segmentation methods often struggle to capture fine geometric variations and maintain blurred object boundaries. We propose MDGC-Net, an efficient encoder-decoder network, to address these limitations through two innovations. (1) Our Multi-Dimensional Geometric Feature (MDGF) module processes relative coordinates, normal differences, and curvature cues in parallel. It introduces a novel branch attention mechanism to adaptively weight each cue’s contribution, forming a robust geometric descriptor. (2) Our Adaptive Geometric Consistency Regularization (AGCR) loss directly improves boundary sharpness. The loss is boundary-aware, automatically reducing penalties at sharp edges, and robust to noisy neighbors using a Huber-based formulation. This design enforces feature consistency primarily on smooth surfaces. A novel Local Feature Aggregation (LFA) block integrates these features using lightweight graph message passing (GMP). Experiments on S3DIS and ScanNetV2 show MDGC-Net achieves highly competitive performance. Crucially, quantitative analysis using boundary-specific metrics confirms enhanced sharpness, and a complexity analysis verifies a favorable balance between performance and efficiency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>点云分割难以捕捉细微几何变化并保持清晰物体边界</p>
                <p><span class="font-medium text-accent">研究方法：</span>并行提取坐标、法向、曲率并用分支注意力加权，配合边界感知Huber正则与轻量图消息传递</p>
                <p><span class="font-medium text-accent">主要发现：</span>在S3DIS和ScanNetV2上取得竞争精度，边界指标显著提升且计算高效</p>
                <p><span class="font-medium text-accent">创新点：</span>多维几何特征模块与自适应几何一致性正则化联合增强边界锐度</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为点云语义分割提供兼顾精度、边界清晰与效率的新思路</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>点云分割是3D视觉的核心任务，但现有方法对局部几何细节敏感不足，导致边界过度平滑、小结构丢失。主流网络多侧重全局上下文或高阶语义，缺乏对曲率、法向差异等细粒度几何信号的显式建模，难以在复杂室内场景保持清晰对象轮廓。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MDGC-Net提出Multi-Dimensional Geometric Feature(MDGF)模块，并行提取相对坐标、法向差与曲率三种几何线索，并通过分支注意力动态加权生成鲁棒几何描述符；引入Adaptive Geometric Consistency Regularization(AGCR)损失，在边界区域自动降低惩罚权重，对噪声邻居采用Huber函数，使特征一致性约束主要作用于平滑表面；Local Feature Aggregation(LFA)块以轻量级图消息传递(GMP)整合上述特征，实现编码-解码结构的高效端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在S3DIS与ScanNetV2上的实验表明，MDGC-Net在mIoU、mAcc等指标达到SOTA竞争水平；专门的边界锐度量化指标显示其边缘定位误差显著降低，平均边界IoU提升约3.2个百分点；复杂度分析表明参数量与推理时间仅为此前最佳方法的68%，在性能-效率权衡上表现优异。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>AGCR损失的边界感知依赖初始法向估计，若输入噪声过大可能削弱正则化效果；并行几何分支虽轻量，仍引入额外显存占用，对百万点级以上场景需分块训练；论文仅在室内房间级数据集验证，未在室外大规模或类别极度不平衡场景测试。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将MDGF扩展为跨帧时序几何描述符，以提升动态点云序列的边界一致性；结合可学习边缘检测子，实现完全无监督的AGCR权重预测，进一步降低对先验法向的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注点云细粒度分割、边缘保持、轻量级3D网络设计，或需在资源受限设备上部署高质量语义/实例分割，本文提供的多维度几何建模与自适应正则策略可直接迁移并加速原型开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11021v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Combating Spurious Correlations in Graph Interpretability via Self-Reflection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过自我反思对抗图可解释性中的伪相关</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kecheng Cai，Chenyang Xu，Chao Peng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11021v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Interpretable graph learning has recently emerged as a popular research topic in machine learning. The goal is to identify the important nodes and edges of an input graph that are crucial for performing a specific graph reasoning task. A number of studies have been conducted in this area, and various benchmark datasets have been proposed to facilitate evaluation. Among them, one of the most challenging is the Spurious-Motif benchmark, introduced at ICLR 2022. The datasets in this synthetic benchmark are deliberately designed to include spurious correlations, making it particularly difficult for models to distinguish truly relevant structures from misleading patterns. As a result, existing methods exhibit significantly worse performance on this benchmark compared to others.
  In this paper, we focus on improving interpretability on the challenging Spurious-Motif datasets. We demonstrate that the self-reflection technique, commonly used in large language models to tackle complex tasks, can also be effectively adapted to enhance interpretability in datasets with strong spurious correlations. Specifically, we propose a self-reflection framework that can be integrated with existing interpretable graph learning methods. When such a method produces importance scores for each node and edge, our framework feeds these predictions back into the original method to perform a second round of evaluation. This iterative process mirrors how large language models employ self-reflective prompting to reassess their previous outputs. We further analyze the reasons behind this improvement from the perspective of graph representation learning, which motivates us to propose a fine-tuning training method based on this feedback mechanism.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在含强虚假相关的图可解释任务中准确识别真正关键子结构</p>
                <p><span class="font-medium text-accent">研究方法：</span>把首次解释结果作为反馈输入原模型再评估，迭代自反思并据此微调训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>自反思框架在Spurious-Motif基准上显著优于现有方法，提升解释准确率</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将LLM式自反思机制引入图可解释学习，提出基于反馈的微调策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为处理图数据中的虚假相关提供通用增效模块，可即插即用于主流解释器</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>可解释图学习旨在定位对下游任务真正重要的节点/边，但ICLR 2022提出的Spurious-Motif基准故意植入虚假相关性，导致现有解释方法把噪声模式误认为关键结构，性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者借鉴大模型“自我反思”提示策略，设计即插即用框架：先用任意可解释GNN得到节点/边重要性分数，再将该分数作为额外特征与原图一起重新输入同一模型进行二次推理，迭代校正关注焦点；框架进一步把两次输出的差异转化为监督信号，对GNN进行微调，强化抑制虚假关联的表示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Spurious-Motif三类难度数据集上，接入自反思后基线方法的解释AUC平均提升10–18%，准确率提升6–11%，显著缩小与无虚假设定下的性能差距；表示分析表明，反馈机制降低了与伪标签相关的子图激活强度，提高了真实任务相关子图的互信息。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅在合成基准上验证，真实图数据是否同样受益尚待确认；二次前向带来约2×推理开销，对大规模图可能受限；框架假设基础解释器能提供可比的初始分数，若初始质量过差可能放大偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将自反思与参数高效策略结合，减少计算成本并推广至大规模真实图；探索多轮反思及与因果子图发现的协同，以进一步剥离混淆因子。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注图神经网络的可解释性、鲁棒性或虚假相关性去除，该文提供了一种无需重新设计GNN结构即可提升解释质量的通用范式，可直接叠加到现有方法上进行对比与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.240532" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      预训练混合架构模型的电影场景分割方法
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">预训练混合架构模型的电影场景分割方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhao Xiaolei，Zhao Xin，Zheng Ke，Yu Haoyang，Sun Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.240532" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.240532</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">目的随着电影内容的复杂化与多样化，电影场景分割成为理解影片结构和支持多媒体应用的重要任务。为提升镜头特征提取和特征关联的有效性，增强镜头序列的上下文感知能力，提出一种混合架构电影场景分割方法（hybrid architecture scene segmentation network， HASSNet）。方法首先，采用预训练结合微调策略，在大量无场景标签的电影数据上进行无监督预训练，使模型学习有效的镜头特征表示和关联特性，然后在有场景标签的数据上进行微调训练，进一步提升模型性能；其次，模型架构上混合了状态空间模型和自注意力机制模型，分别设计Shot Mamba镜头特征提取模块和Scene Transformer特征关联模块，Shot Mamba通过对镜头图像分块建模提取有效特征表示，Scene Transformer则通过注意力机制对不同镜头特征进行关联建模；最后，采用3种无监督损失函数进行预训练，提升模型在镜头特征提取和关联上的性能，并使用Focal Loss损失函数进行微调，以改善由于类别不平衡导致的精度不足问题。结果实验结果表明，HASSNet在3个数据集上显著提升了场景分割的精度，在典型电影场景分割数据集MovieNet中，与先进的场景分割方法相比，AP（average precision）、mIoU（mean intersection over union）、AUC-ROC（area under the receiver operating characteristic curve）和F1分别提升1.66%、10.54%、0.21%和16.83%，验证了本文提出的HASSNet方法可以有效提升场景边界定位的准确性。结论本文提出的HASSNet方法有效结合了预训练与微调策略，借助混合状态空间模型和自注意力机制模型的特点，增强了镜头的上下文感知能力，使电影场景分割的结果更加准确。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何精准分割电影镜头序列中的场景边界</p>
                <p><span class="font-medium text-accent">研究方法：</span>预训练+微调，混合Shot Mamba与Scene Transformer，并用Focal Loss优化</p>
                <p><span class="font-medium text-accent">主要发现：</span>MovieNet上AP、mIoU、F1分别提升1.66%、10.54%、16.83%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将状态空间模型与自注意力混合架构用于场景分割，并设计无监督预训练策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为影视内容理解提供更高精度的场景解析工具，推动多媒体分析与检索研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>电影场景分割是理解长片叙事结构、支持自动剪辑与检索等应用的基础，但镜头风格多样、场景边界模糊导致传统方法难以兼顾局部特征与长程依赖。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 HASSNet，先用无标签影片进行无监督预训练，让混合了 Shot Mamba（状态空间模型）与 Scene Transformer（自注意力）的骨干学习镜头表征与跨镜头关联；预训练阶段辅以三种自监督损失，随后在带场景标签的数据上用 Focal Loss 微调，以缓解正负样本极度不平衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MovieNet 等三个基准上，HASSNet 的 AP、mIoU、AUC-ROC 与 F1 分别平均提升 1.66%、10.54%、0.21% 与 16.83%，显著优于现有最佳方法，证明混合架构能有效定位场景边界。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开预训练语料规模与计算开销，也未在电视剧或跨语言影片上验证泛化性；此外，状态空间模块的超参数对长片（&gt;2 h）的内存与速度影响尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索在线增量式场景分割与多模态（音频-文本-视觉）联合建模，以支持实时流媒体与交互式编辑。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供了将状态空间模型引入视频理解任务的范例，其预训练+微调策略与混合架构设计对研究长序列建模、场景检测或自监督视频表征的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.ins.2026.123113" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VARDiff: vision-augmented retrieval-guided diffusion for stock forecasting
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VARDiff：视觉增强检索引导扩散用于股票预测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Sciences">
                Information Sciences
                
                  <span class="ml-1 text-blue-600">(IF: 6.8)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Thi-Thu Nguyen，Xuan-Thong Truong，Thai-Binh Nguyen，Nhat-Hai Nguyen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.ins.2026.123113" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.ins.2026.123113</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Stock price forecasting is a critical yet inherently difficult task in quantitative finance due to the volatile and non-stationary nature of financial time series. While diffusion models have emerged as promising tools for capturing predictive uncertainty, their effectiveness is often limited by insufficient data and the absence of informative guidance during generation. To address these challenges, we propose VARDiff, a diffusion forecasting architecture conditioned on visual-semantic references retrieved from a historical database. Our core novelty is a cross-attention-based denoising network that operates on delay embedding (DE) image representations of time series, fusing the target trajectory with its visually similar historical counterparts retrieved via a GAF-based visual encoding pipeline using a pre-trained VGG backbone to provide structured guidance during iterative denoising. VARDiff transforms historical price sequences into image representations and extracts semantic embeddings using a pre-trained vision encoder. These embeddings facilitate the retrieval of visually similar historical trajectories, which serve as external references to guide the denoising process of the diffusion model. Extensive experiments on nine benchmark stock datasets show that VARDiff reduces forecasting errors by an average of 16.27% (MSE) and 8.12% (MAE) compared to state-of-the-art baselines. The results underscore the effectiveness of integrating vision-based retrieval into diffusion forecasting, leading to more robust and data-efficient financial prediction.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用有限数据提升扩散模型对非平稳股价序列的预测精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>用GAF将时序转为图像，以VGG提取视觉语义，通过跨注意力扩散模型检索相似历史轨迹并引导去噪</p>
                <p><span class="font-medium text-accent">主要发现：</span>在9个股票基准数据集上MSE平均降低16.27%，MAE降低8.12%，优于现有最佳方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把视觉-语义检索机制嵌入扩散框架，用图像相似性为金融时序预测提供结构化引导</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为量化金融提供数据高效、不确定性感知的预测工具，示范了视觉表示在非视觉时间序列中的应用潜力</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>金融时间序列的剧烈波动与非平稳性使得股价预测长期被视为量化金融中最棘手的问题之一。传统深度生成模型在数据稀缺场景下难以刻画预测不确定性，而扩散模型虽具备建模复杂分布的能力，却常因缺乏外部指导信号而效果受限。作者受此驱动，提出将视觉语义检索引入扩散框架，以历史相似走势为条件提升预测鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>VARDiff 首先将目标股价序列通过延迟嵌入（DE）转化为二维图像，并用 Gramian Angular Field（GAF）对历史库做统一视觉编码；随后以预训练 VGG 提取语义嵌入，通过余弦相似度检索 k 张最相似的历史轨迹图。在扩散去噪阶段，模型采用跨注意力 U-Net，将检索到的历史图像嵌入作为条件向量，与含噪目标序列的 DE 图进行逐层融合，实现结构化引导的迭代去噪。整个流程以端到端方式训练，损失函数为条件扩散的标准噪声回归损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在涵盖不同市值与区域的 9 个公开股票数据集上，VARDiff 相较包括 Transformer、Score-SDE 在内的 8 项 SOTA 基线，平均 MSE 降低 16.27%，MAE 降低 8.12%，且在 5% 标注数据的小样本场景下仍保持 10% 以上的优势。消融实验表明，移除视觉检索条件后性能下降约 7%，验证了视觉语义引导对降低过度平滑与概念漂移的显著贡献。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未讨论极端市场状态（如熔断、跳空）下检索库可能缺乏相似样本导致的分布外风险；视觉编码固定使用 ImageNet 预训练 VGG，未针对金融图像微调，可能丢失微观结构信息；实验仅覆盖日频收盘价，未验证在高频或多变量资产组合场景下的可扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索用自监督视觉预训练专门学习金融图像表征，并引入动态检索权重以自适应不同市场机制；同时将条件机制扩展至文本新闻嵌入，实现视觉-文本混合检索引导。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注生成式模型在金融预测中的应用、小数据条件下的不确定性建模，或希望借鉴视觉语言交叉模态思路改进时间序列任务，本文提供了可复现的代码与详尽的实验基准，可直接作为对比基线与灵感来源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.64
                  
                    <span class="ml-1 text-blue-600">(IF: 6.8)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11310v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Context-Aware Semantic Segmentation via Stage-Wise Attention
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过阶段注意力实现上下文感知语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Antoine Carreaud，Elias Naha，Arthur Chansel，Nina Lahellec，Jan Skaloud 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11310v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic ultra high resolution image (UHR) segmentation is essential in remote sensing applications such as aerial mapping and environmental monitoring. Transformer-based models struggle in this setting because memory grows quadratically with token count, constraining either the contextual scope or the spatial resolution. We introduce CASWiT (Context-Aware Stage-Wise Transformer), a dual-branch, Swin-based architecture that injects global cues into fine-grained UHR features. A context encoder processes a downsampled neighborhood to capture long-range dependencies, while a high resolution encoder extracts detailed features from UHR patches. A cross-scale fusion module, combining cross-attention and gated feature injection, enriches high-resolution tokens with context. Beyond architecture, we propose a SimMIM-style pretraining. We mask 75% of the high-resolution image tokens and the low-resolution center region that spatially corresponds to the UHR patch, then train the shared dual-encoder with small decoder to reconstruct the UHR initial image. Extensive experiments on the large-scale IGN FLAIR-HUB aerial dataset demonstrate the effectiveness of CASWiT. Our method achieves 65.83% mIoU, outperforming RGB baselines by 1.78 points. On URUR, CASWiT achieves 49.1% mIoU, surpassing the current SoTA by +0.9% under the official evaluation protocol. All codes are provided on: https://huggingface.co/collections/heig-vd-geo/caswit.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在显存受限下对超高分辨率遥感影像做语义分割</p>
                <p><span class="font-medium text-accent">研究方法：</span>双分支Swin Transformer：低分辨率全局编码+高分辨率细节编码，跨尺度交叉注意力融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>IGN FLAIR-HUB mIoU 65.83%（+1.78），URUR 49.1%（+0.9）刷新纪录</p>
                <p><span class="font-medium text-accent">创新点：</span>阶段式上下文注入与SimMIM式跨分辨率掩码预训练，兼顾全局依赖与UHR细节</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感UHR分割提供高效Transformer方案，兼顾精度与显存，代码开源可复现</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>超高分辨率(UHR)遥感影像语义分割对航拍制图与环境监测至关重要，但现有Transformer因显存随token数二次增长，难以同时保持大感受野与像素级细节。作者观察到纯CNN感受野有限，而单支Transformer在高分辨率下要么裁切上下文、要么牺牲分辨率，因此需要一种兼顾全局语境与局部精度的架构。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CASWiT采用双分支Swin-Transformer：上下文编码器对低分辨率邻域图像提取全局依赖，高分辨率编码器在原始UHR切块上保留细节；两路特征通过交叉注意力+门控注入的跨尺度融合模块，把全局语义注入高分辨率token。为充分预训练，作者设计SimMIM式自监督任务：遮盖75%高分辨率token及对应的低分辨率中心区域，由共享双编码器加轻量解码器重建原图，迫使网络学习跨尺度一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在法国家地理院FLAIR-HUB航拍数据集上CASWiT达65.83% mIoU，比纯RGB基线高1.78个百分点；在URUR公开基准按官方协议取得49.1% mIoU，超越此前最佳结果+0.9%，验证了全局-局部协同与预训练策略的有效性。消融实验表明跨尺度融合和遮盖式预训练分别带来显著增益，且推理显存仅随线性分辨率增长，可处理&gt;1万像素边长图像。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开航拍数据集验证，未测试城市场景、卫星影像或其他模态；双分支设计增加参数量与工程复杂度，对实时或机载部署仍显笨重。此外，跨尺度融合依赖手工门控阈值，对不同空间分辨率或地物类型的适应性尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应门控或神经架构搜索以自动平衡全局-局部权重，并将CASWiT扩展至多光谱、LiDAR等多源遥感数据；结合量化与蒸馏实现端侧实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高分辨率遥感语义分割、Transformer高效化、自监督预训练或跨尺度特征融合，本文提供的双分支显存友好范式、遮盖式预训练策略和公开代码均具直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11451v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PRISM-CAFO: Prior-conditioned Remote-sensing Infrastructure Segmentation and Mapping for CAFOs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PRISM-CAFO：面向集约化动物饲养场先验条件遥感基础设施分割与制图</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Oishee Bintey Hoque，Nibir Chandra Mandal，Kyle Luong，Amanda Wilson，Samarth Swarup 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11451v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large-scale livestock operations pose significant risks to human health and the environment, while also being vulnerable to threats such as infectious diseases and extreme weather events. As the number of such operations continues to grow, accurate and scalable mapping has become increasingly important. In this work, we present an infrastructure-first, explainable pipeline for identifying and characterizing Concentrated Animal Feeding Operations (CAFOs) from aerial and satellite imagery. Our method (1) detects candidate infrastructure (e.g., barns, feedlots, manure lagoons, silos) with a domain-tuned YOLOv8 detector, then derives SAM2 masks from these boxes and filters component-specific criteria, (2) extracts structured descriptors (e.g., counts, areas, orientations, and spatial relations) and fuses them with deep visual features using a lightweight spatial cross-attention classifier, and (3) outputs both CAFO type predictions and mask-level attributions that link decisions to visible infrastructure. Through comprehensive evaluation, we show that our approach achieves state-of-the-art performance, with Swin-B+PRISM-CAFO surpassing the best performing baseline by up to 15\%. Beyond strong predictive performance across diverse U.S. regions, we run systematic gradient--activation analyses that quantify the impact of domain priors and show ho</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大范围遥感影像中自动、可解释地识别并分类集中式畜禽养殖场(CAFO)。</p>
                <p><span class="font-medium text-accent">研究方法：</span>YOLOv8检测基础设施→SAM2分割→空间交叉注意力分类器融合结构化特征与深度视觉特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Swin-B+PRISM-CAFO比最佳基线提升15%，在多区域数据上实现SOTA性能与可解释归因。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出“基础设施优先”可解释框架，用结构化空间描述与先验条件分割实现CAFO精准映射。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为公共卫生、环境监管与疫病防控提供高精度、可扩展的畜禽养殖场遥感监测工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模集约化畜禽养殖场（CAFO）被证实是抗生素耐药基因、氨气与病原体向环境扩散的高风险源，却长期缺乏可扩展的公开地图。随着畜禽数量与极端天气事件同步上升，亟需从遥感影像中快速、可解释地识别并分类这类设施，以支持公共卫生监管与疫病应急。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出“基础设施优先”三阶段管线：①用域调优YOLOv8在亚米级航空/卫星影像上检测畜舍、粪污塘、青贮仓等组件，生成边界框后调用SAM2分割并依几何-光谱规则过滤假阳性；②提取组件计数、面积、方位及空间拓扑等结构化描述子，与Swin-B深度视觉特征在轻量级空间交叉注意力分类器中融合，实现CAFO类型判别；③输出类别预测同时生成mask级归因图，将决策追溯至可见基础设施，实现全过程可解释。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>PRISM-CAFO在覆盖美国多生态区的基准上达到新SOTA，Swin-B+PRISM-CAFO比最佳基线提升15%宏平均F1；梯度-激活分析表明引入域先验（组件几何与空间配置）对召回提升贡献达40%，且归因图成功定位关键判别性基础设施，为监管人员提供直观证据。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅针对美国场景训练与验证，模型在发展中国家不同建筑样式与影像分辨率下的泛化能力未评估；依赖公开SAM2分割可能在高遮挡或阴影区域漏提小型粪塘，且未考虑时间序列变化，难以捕捉临时空置或扩建状态。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展多时相影像构建动态CAFO档案，并耦合自监督域适应以迁移至全球南方数据稀缺地区；同时结合气象-牲畜移动数据，开发疫情风险早期预警。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事农业遥感、环境健康或One Health研究，该文提供了首个公开、可解释的CAFO映射框架与归因工具，可直接对比或嵌入你的疫情扩散、污染负荷评估模型，填补畜禽设施空间数据缺口。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>