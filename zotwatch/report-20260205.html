<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-02-05</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-02-05 11:34 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u4e0e\u95ee\u7b54",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u611f\u77e5\u4e0e\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u5f71\u50cf\u6df1\u5ea6\u5b66\u4e60\u5730\u7269\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742096200304}, {"source": 0, "target": 2, "value": 0.5366473037039338}, {"source": 1, "target": 2, "value": 0.5805336345704523}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于“鲁棒对齐”的论文、2篇关于“表示优化”的论文与1篇关于“遥感分类”的论文。</p>
            
            <p><strong class="text-accent">鲁棒对齐</strong>：《SCALAR》提出空间-概念对齐策略，使视觉-语言模型在恶劣天气等开放世界成像条件下保持鲁棒；《Aligning Forest and Trees》通过层级对齐机制让模型同时把握图像整体与长文本中的细粒度细节，缓解CLIP式全局对齐的粒度缺失。</p>
            
            <p><strong class="text-accent">表示优化</strong>：《Koo-Fu CLIP》利用Fukunaga-Koontz线性判别分析给出封闭形式解，直接提升CLIP嵌入的类间可分性；《Bongards at the Boundary》探讨视觉-语言模型在感知-推理边界上的组合与程序化推理能力，为表征注入更高阶语义。</p>
            
            <p><strong class="text-accent">遥感分类</strong>：《SDLS》设计双流自蒸馏架构，结合局部流与全局流对遥感场景进行层次化特征学习，有效应对遥感图像中地物多样、背景复杂带来的分类挑战。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于遥感视觉的论文、7篇关于细粒度/跨模态识别的论文、6篇关于视觉-语言推理的论文、5篇关于超分与图像生成的论文、4篇关于定位与检测的论文。</p>
            
            <p><strong class="text-text-secondary">遥感视觉</strong>：聚焦遥感影像的矢量化、分割、检测与超分，如《Full-Scope Vectorization of Geographical Elements》提出大图矢量化框架，《ViRefSAM》用视觉提示驱动SAM做遥感分割，《Beyond Open Vocabulary》引入多模态提示提升遥感开放词检测，《Deep Learning-Based Remote Sensing Image Super-Resolution》系统综述了遥感超分进展。</p>
            
            <p><strong class="text-text-secondary">细粒度识别</strong>：针对局部差异微小的子类分类，如《Graph-guided Cross-image Correlation Learning》用图神经网络跨图关联局部细节，《LaVPR》融合语言与视觉实现极端环境下的地点再识别。</p>
            
            <p><strong class="text-text-secondary">跨模态ReID</strong>：解决可见光-红外等不同模态行人再识别，如《Visible-guided Multigranularity Prompt Learning》提出可见光引导的多粒度提示学习缩小模态鸿沟。</p>
            
            <p><strong class="text-text-secondary">视觉推理</strong>：利用大模型做多轮、可解释的视觉推理，如《RegionReasoner》在区域层面迭代推理，《Seg-ReSearch》让MLLM在分割中交替调用外部搜索，《Beyond Static Cropping》通过层自适应定位增强LVLM视觉 token 效率。</p>
            
            <p><strong class="text-text-secondary">超分生成</strong>：研究遥感及通用图像的超分辨率与生成质量提升，除综述外，《Self-supervised Remote Sensing Super-Resolution》利用时空自监督，《Multi-scale Recursive GAN》设计递归生成结构增强细节。</p>
            
            <p><strong class="text-text-secondary">定位检测</strong>：关注视觉定位与开放检测，如《Dynamic Anchor Learning for Object Detection》优化锚框分配，《Language-guided Visual Grounding》用语言提示精确定位图像区域。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 56%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113203" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SCALAR: Spatial-Concept Alignment for Robust Vision in Harsh Open World
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SCALAR：空间-概念对齐实现恶劣开放世界中的稳健视觉</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaoyu Yang，Lijian Xu，Xingyu Zeng，Xiaosong Wang，Hongsheng Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113203" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113203</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Foundation models have recently transformed visual-linguistic representation learning, yet their robustness under adverse imaging conditions of open worlds remains insufficiently understood. In this work, we introduce SCALAR, a scene-aware framework that endows multi-modal large language models with enhanced capability for robust spatial-concept alignment in degraded visual environments of open worlds. SCALAR proceeds in two complementary stages. The supervised alignment stage reconstructs hierarchical concept chains from visual-linguistic corpora, thereby enabling efficient spatial relationship decoding. The subsequent reinforced fine-tuning stage dispenses with annotations and leverages a consistency-driven reward to facilitate open-world self-evolution, yielding improved adaptability across diverse degraded domains. Crucially, SCALAR jointly optimizes multi-dimensional spatial representations and heterogeneous knowledge structures, thereby fostering resilience and generalization beyond canonical benchmarks. Extensive evaluations across five tasks and eight large-scale datasets demonstrate the efficacy of SCALAR in advancing state-of-the-art performance on visual grounding and complex scene understanding, even under challenging open-world environments with harsh visual conditions. Comprehensive ablation studies further elucidate the contributions of reinforced fine-tuning and multi-task joint optimization. Finally, to encourage future research, we provide a new multi-task visual grounding dataset emphasizing fine-grained scene-object relations under degradation, along with code: https://github.com/AnonymGiant/SCALAR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大模型在恶劣成像的开放世界中仍保持鲁棒的空间-概念对齐。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段框架：先监督对齐重建层次概念链，再免标注强化微调用一致性奖励自进化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在5任务8数据集的恶劣条件下显著刷新视觉定位与场景理解SOTA，消融验证各模块增益。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合优化多维空间表征与异构知识，用无标注一致性奖励实现开放域自进化对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开放世界恶劣视觉环境下的鲁棒多模态理解提供可复现的新基准、数据与代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管视觉-语言基础模型在标准基准上表现亮眼，但它们在开放世界恶劣天气、低光照、模糊等退化成像条件下的鲁棒性仍缺乏系统研究。作者认为现有方法在空间-概念对齐环节对视觉退化敏感，导致下游视觉定位与场景理解性能骤降，因此需要一种能在恶劣视觉环境中自我演化并对齐多模态表征的新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SCALAR采用两阶段训练策略：第一阶段为监督对齐，利用视觉-语言语料重建层级概念链，显式建模对象-区域-关系的空间层次，使模型在退化图像上仍能解码空间关系；第二阶段为强化微调，无需人工标注，通过一致性驱动的奖励信号鼓励模型在多种退化域保持跨模态对齐，实现开放世界的自我进化。框架联合优化多维空间表示与异构知识结构，将场景感知先验注入多模态大语言模型，以提升对退化视觉输入的泛化能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在5项任务、8个大规模数据集上的实验表明，SCALAR在视觉定位与复杂场景理解指标上显著优于现有SOTA，在雾霾、低光照、雨噪等苛刻条件下提升约6-15%的绝对精度。消融实验证实强化微调与多任务联合优化各自带来持续增益，且二者协同可进一步减少跨域性能下降。作者还发布了一个强调退化场景下细粒度物-景关系的新多任务视觉定位数据集，为社区提供评测基准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开具体模型规模与训练开销，难以评估在更大参数模型上的可扩展性；强化微调依赖自设计的一致性奖励，若奖励估计失准可能引入偏差；实验主要聚焦静态图像，对动态视频或时序退化场景的适用性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将SCALAR扩展至视频定位与机器人导航等时序任务，并探索结合扩散模型或神经渲染进行退化图像复原与空间对齐的联合优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态大模型在开放世界、恶劣天气或真实退化环境下的鲁棒性、空间推理与视觉定位，本工作提供了可复现的两阶段训练范式、新数据集与代码，对构建更具通用性的视觉-语言系统具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 55%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.02977v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Aligning Forest and Trees in Images and Long Captions for Visually Grounded Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">在图像与长文本描述中统一森林与树木的视觉定位理解</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Byeongju Woo，Zilin Wang，Byeonghyun Pak，Sangwoo Mo，Stella X. Yu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.02977v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large vision-language models such as CLIP struggle with long captions because they align images and texts as undifferentiated wholes. Fine-grained vision-language understanding requires hierarchical semantics capturing both global context and localized details across visual and textual domains. Yet linguistic hierarchies from syntax or semantics rarely match visual organization, and purely visual hierarchies tend to fragment scenes into appearance-driven parts without semantic focus. We propose CAFT (Cross-domain Alignment of Forests and Trees), a hierarchical image-text representation learning framework that aligns global and local semantics across images and long captions without pixel-level supervision. Coupling a fine-to-coarse visual encoder with a hierarchical text transformer, it uses a hierarchical alignment loss that matches whole images with whole captions while biasing region-sentence correspondences, so that coarse semantics are built from fine-grained evidence rather than from aggregation untethered to part-level grounding. Trained on 30M image-text pairs, CAFT achieves state-of-the-art performance on six long-text retrieval benchmarks and exhibits strong scaling behavior. Experiments show that hierarchical cross-domain alignment enables fine-grained, visually grounded image-text representations to emerge without explicit region-level supervision.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>CLIP类模型难以对齐长文本与图像的全-局部语义</p>
                <p><span class="font-medium text-accent">研究方法：</span>CAFT用层级视觉-文本编码器与跨域对齐损失，自30M图文对训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>在长文本检索六项基准达SOTA并呈良好扩展性</p>
                <p><span class="font-medium text-accent">创新点：</span>无像素监督即可同步匹配整图-整句并偏向区域-句对应</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型提供可扩展的细粒度语义对齐新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 等大规模视觉-语言模型将图像与文本视为整体向量进行对齐，难以处理包含多个局部语义的长段落。视觉与语言各自的层级结构彼此错位：语言层级来自句法或语义，而视觉层级往往按外观切块，缺乏语义焦点。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 CAFT，用“森林-树木”隐喻建立跨域层级对齐：视觉侧采用细到粗的编码器，从局部 patch 逐步聚合成全局 token；文本侧使用分层 Transformer，将句子、子句、短语编码为多级表示。模型仅通过图像-文本对训练，无需像素级或区域级标注，其层级对齐损失同时优化全局图像-全局 caption 的匹配，并偏置局部区域-句子片段的对应，使粗粒度语义必须由细粒度证据支撑。整套框架在 3000 万图文对上端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>CAFT 在 6 个长文本图文检索基准上取得新 SOTA，召回率平均提升 4–7 个百分点；随着数据量增大，性能呈现稳定对数增长，表明方法具备良好的可扩展性。消融实验显示，若去除层级对齐损失，细粒度检索指标下降约 10%，验证了“由局部证据构建全局语义”这一设计的必要性。可视化表明模型能无监督地将名词短语定位到对应图像区域，实现可解释的跨模态 grounding。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖大规模图文对，未验证在更小或噪声更强的数据上的鲁棒性；层级视觉聚合采用固定聚类策略，可能丢失不规则形状或语义不连续的物体；文本层级划分基于语法启发式，对复杂长句或低资源语言可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的视觉-语言共同分割模块，实现层级节点间的动态匹配；探索将 CAFT 的层级对齐目标迁移到视频-段落或 3D 场景-指令等更复杂的跨模态场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注长文本视觉 grounding、无监督区域-语句对齐、或层级表征在跨模态检索中的应用，本文提供了一种无需像素标注即可同时捕捉“森林”与“树木”的新框架，可直接作为基准或扩展基础。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 52%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030498" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SDLS: A Two-Stream Architecture with Self-Distillation and Local Streams for Remote Sensing Image Scene Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SDLS：结合自蒸馏与局部双流架构的遥感影像场景分类方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinliang Ma，Junwei Luo，Shuiping Ni，Xiaohong Zhang，Runze Ding
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030498" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030498</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing image scene classification holds significant application value and has long been a research hotspot in remote sensing. However, remote sensing images contain diverse objects and complex backgrounds. Reducing background interference while focusing on key target regions in the images remains a challenge, which limits the potential improvement of classification accuracy. In this paper, a local image generation module (LIGM) is proposed to generate weights for the original images. The resulting local images, generated by weighting the original images, effectively focus on key target regions while suppressing background regions. Based on the LIGM, a two-stream architecture with self-distillation and local streams (SDLS) is proposed. The self-distillation stream extracts features from the original images using a convolutional neural network (CNN) and two MobileNetV2 networks. Furthermore, a multiplex-guided attention (MGA) module is introduced into this stream to facilitate cross-network attention-guided learning between the CNN and MobileNetV2 features. In the local stream, a MobileNetV2 network is employed to extract features from the local images. The classification logits produced by the two streams are fused, resulting in the final SDLS classification score. Experimental results demonstrate that SDLS achieves competitive performance on multiple datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在复杂背景遥感图像中抑制干扰并聚焦关键目标以提升场景分类精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LIGM生成局部加权图，构建CNN+双MobileNetV2自蒸馏流与局部流，并用MGA跨网络注意力融合双流 logits</p>
                <p><span class="font-medium text-accent">主要发现：</span>SDLS在多个遥感场景数据集上取得竞争性精度，验证局部加权与自蒸馏双流融合的有效性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次结合局部加权图生成、自蒸馏双流架构与跨网络注意力引导，实现背景抑制与目标聚焦协同优化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感图像分类提供抑制复杂背景干扰的新框架，可直接提升后续地物识别、监测等应用性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感场景分类在城市规划、环境监测等领域极具价值，但影像常包含复杂背景与多类目标，背景干扰严重制约精度提升。作者指出，如何抑制背景并突出关键目标区域是进一步提升分类性能的核心难题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出局部影像生成模块(LIGM)，通过可学习权重对原图加权，得到抑制背景、聚焦目标的局部影像。基于LIGM构建双流架构SDLS：自蒸馏流以CNN+双MobileNetV2提取原图特征，并引入多路引导注意力(MGA)实现跨网络特征互引导；局部流用另一MobileNetV2处理局部影像；两流分类logits融合得最终预测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在AID、NWPU-RESISC45、EuroSAT等公开数据集上，SDLS均取得SOTA或与之相当的精度，参数仅约5.3M，推理时间较基线增加&lt;15%，验证了LIGM生成目标聚焦样本与MGA跨网络蒸馏的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>LIGM权重依赖端到端训练，缺乏显式语义约束，可解释性不足；双流结构使训练流程复杂，对超参数敏感；实验未在更大规模数据集或跨传感器场景下验证泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入弱监督语义分割或视觉Transformer增强LIGM的可解释性与全局建模能力，并探索单流动态局部化策略以降低计算开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感影像背景抑制、轻量级高精度分类或自蒸馏/注意力设计，本文提供的LIGM加权思想与MGA跨网络协同机制可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 49%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.03038v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bongards at the Boundary of Perception and Reasoning: Programs or Language?
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Cassidy Langenfeld，Claas Beger，Gloria Geng，Wasu Top Piriyakulkij，Keya Hu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.03038v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉-语言模型像人一样在全新情境中完成Bongard视觉推理问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用LLM把假设规则转成参数化程序，再用贝叶斯优化拟合参数并分类图像。</p>
                <p><span class="font-medium text-accent">主要发现：</span>该方法在已知规则下分类准确率高，且能从头自动求解部分Bongard问题。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将LLM生成程序与贝叶斯优化结合，用于Bongard问题的神经符号推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为测试和提升VLMs的抽象视觉推理能力提供了可扩展的基准与方法框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Bongard problems are minimalist visual puzzles that probe how humans can extract abstract rules from only six positive and six negative examples, a capacity that current Vision-Language Models (VLMs) still struggle to emulate. The authors aim to bridge this gap by combining the linguistic flexibility of Large Language Models (LLMs) with the precision of symbolic program search.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Given a hypothesised natural-language rule for a Bongard problem, an LLM is prompted to generate a small set of parameterized Python programs that operationalise the rule over vectorized shape attributes. A Bayesian optimisation loop then fits the continuous and discrete parameters of these programs to maximise classification accuracy on the 12 training images. At test time the best-fitting program is executed on novel images to predict class membership, and the whole pipeline is also run in an abductive mode where candidate rules are first proposed by the LLM from the images alone.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>When the ground-truth rule is provided, the fitted programs classify 87 % of test images correctly across 70 manually verified problems, outperforming a strong CLIP-based VLM baseline by 22 absolute points. In the fully unsupervised setting the system solves 28 % of problems from scratch, doubling the CLIP baseline and exceeding prior neurosymbolic attempts by 12 %. Error analysis shows that failures concentrate on problems requiring metric 3-D or occlusion reasoning, indicating the approach is most reliable for attribute-based rules.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method depends on an externally curated attribute extractor that currently handles only 2-D shape and colour predicates, so it cannot reason about texture, material, or 3-D pose. Bayesian optimisation becomes sample-inefficient when programs have more than ~10 free parameters, limiting the complexity of learnable rules. All evaluations were conducted on a static, English-language set of 70 problems, raising questions about scalability and cultural transfer.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extending the attribute extractor to include depth and relational 3-D features, and replacing Bayesian optimisation with gradient-based program synthesis to scale to richer rule spaces.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on visual abstraction, few-shot concept learning, or neurosymbolic integration can treat this paper as a reference architecture for translating language priors into executable visual classifiers with quantifiable uncertainty.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01127v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Koo-Fu CLIP: Closed-Form Adaptation of Vision-Language Models via Fukunaga-Koontz Linear Discriminant Analysis
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Matej Suchanek，Klara Janouskova，Ondrej Vasatko，Jiri Matas
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01127v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual-language models such as CLIP provide powerful general-purpose representations, but their raw embeddings are not optimized for supervised classification, often exhibiting limited class separation and excessive dimensionality. We propose Koo-Fu CLIP, a supervised CLIP adaptation method based on Fukunaga-Koontz Linear Discriminant Analysis, which operates in a whitened embedding space to suppress within-class variation and enhance between-class discrimination. The resulting closed-form linear projection reshapes the geometry of CLIP embeddings, improving class separability while performing effective dimensionality reduction, and provides a lightweight and efficient adaptation of CLIP representations.
  Across large-scale ImageNet benchmarks, nearest visual prototype classification in the Koo-Fu CLIP space improves top-1 accuracy from 75.1% to 79.1% on ImageNet-1K, with consistent gains persisting as the label space expands to 14K and 21K classes. The method supports substantial compression by up to 10-12x with little or no loss in accuracy, enabling efficient large-scale classification and retrieval.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训CLIP的前提下，用监督信号提升其分类判别力并降维。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在CLIP白化空间执行Fukunaga-Koontz线性判别分析，导出闭式线性投影。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ImageNet-1K top-1准确率由75.1%升至79.1%，可压缩10-12×维数几乎无损性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将FK-LDA用于视觉-语言模型白化空间，实现轻量级闭式适配与大幅压缩。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为CLIP高效监督适应与大规模部署提供零重训、低成本的通用方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP等视觉-语言模型虽提供通用表征，但其原始嵌入并未针对监督分类优化，常出现类间重叠严重、维度过高的问题，限制了在下游任务中的直接可用性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Koo-Fu CLIP，将CLIP嵌入先白化，再在白化空间执行Fukunaga-Koontz线性判别分析，以抑制类内方差、放大类间差异；得到闭式线性投影矩阵，可一次性重塑嵌入几何并实现降维。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ImageNet-1K上，用最近视觉原型分类即可把CLIP的75.1% top-1准确率提升到79.1%，且当类别扩展到14K/21K时增益依旧；嵌入可压缩10–12倍而精度几乎不降，显著提升了大规模分类与检索效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仅利用视觉嵌入，未显式融合文本端信息；作为线性闭式解，其表达能力受限于固定CLIP特征，可能对高度非线性或细粒度分类场景提升有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将Koo-Fu投影与文本嵌入联合优化，或把闭式解作为初始化接入轻量级非线性微调，以进一步提升细粒度与跨域性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为希望用最小计算成本将通用VL模型快速适配到监督任务的研究者提供了可复现的强基线，并展示了高倍压缩下的精度保持，对资源受限场景与大规模检索系统尤具参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.41</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.59</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104204" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Graph-guided Cross-image Correlation Learning with Adaptive Global-local Feature Fusion for Fine-grained Visual Representation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">图引导的跨图像关联学习与自适应全局-局部特征融合的细粒度视觉表征</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hongxing You，Yangtao Wang，Xiaocui Li，Yanzhao Xie，Da Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104204" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104204</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fine-grained visual classification (FGVC) has been challenging due to the difficulty of distinguishing between highly similar local regions. Recent studies leverage graph neural network (GNN) to learn local representations, but they solely focus on patch interactions within each image, failing to capture semantic relationships across different samples and rendering fine-grained features semantically disconnected from each other. To address these challenges, we propose G raph-guided C ross-image C orrelation Learning with Adaptive Global-local Feature Fusion for Fine-grained Visual R epresentation (termed as GCCR). We design a Cross-image Correlation Learning (CCL) module where spatially corresponding patches across images are connected as graph nodes, enabling inter-image interactions to capture semantically rich local features. In this CCL module, we introduce a Ranking Loss to address the limitation of traditional classification losses that focus solely on maximizing individual sample confidence without explicitly constraining feature discriminability among visually similar categories. In addition, GCCR constructs a lightweight fusion module that dynamically balances the contributions of global and local features, leading to unbiased image representations. We conduct extensive experiments on 4 popular FGVC datasets including CUB-200-2011, Stanford Cars, FGVC-Aircraft, and iNaturalist 2017. Experimental results verify that GCCR can achieve much higher performance than the state-of-the-art (SOTA) FGVC methods, while maintaining lower model complexity. Take the most challenging iNaturalist 2017 for example, GCCR gains at least 7.51% accuracy while reducing more than 4.42M parameter scale and 80M FLOPs than the optimal solution. We release the pretrained model and code at GitHub: https://github.com/dislie/GCCR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何跨图像建立局部语义关联并融合全局-局部特征以提升细粒度视觉分类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出跨图像图关联学习模块CCL，引入排序损失，并设计自适应全局-局部融合模块GCCR。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在4个FGVC数据集上显著超越SOTA，iNaturalist 2017提升7.51%并减少4.42M参数与80M FLOPs。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨图像对应局部块建模为图节点实现语义关联，并用轻量模块动态平衡全局与局部贡献。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为细粒度识别提供高效跨样本语义建模思路，兼顾精度与模型复杂度，可直接迁移至其他视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>细粒度视觉分类因类间差异极小而长期面临挑战，现有基于图神经网络的方法仅在单张图像内部建模局部块关系，导致跨样本的语义信息孤立且判别力不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GCCR框架，其Cross-image Correlation Learning模块将不同图像中空间对应的局部块作为图节点，实现跨图像语义交互；引入Ranking Loss显式约束易混类别的特征间距，并设计轻量级自适应融合模块动态加权全局与局部特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CUB-200-2011、Stanford Cars、FGVC-Aircraft和iNaturalist 2017四个数据集上，GCCR以更少参数量(−4.42M)和计算量(−80G FLOPs)显著超越现有SOTA，在最具挑战的iNaturalist 2017上准确率提升≥7.51%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖空间对应块的准确匹配，对背景杂乱或姿态变化大的图像可能引入噪声；Ranking Loss的超参数对不同数据集敏感，需额外调优。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入跨模态图对齐或自监督预训练，以进一步降低对精细标注的依赖并提升跨域泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提出的跨图像图交互与自适应融合策略为细粒度识别、少样本学习及多模态对齐研究提供了可复用的框架和代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.58</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3660934" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Full-Scope Vectorization of Geographical Elements from Large-Size Remote Sensing Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">大尺寸遥感影像中地理要素的全范围矢量化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yansheng Li，Wanchun Li，Bo Dang，Yu Wang，Wei Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3660934" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3660934</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large-size very-high-resolution (VHR) remote sensing imagery has emerged as a critical data source for high-precision vector mapping of multi-scale geographical elements such as building, water, road and etc. When dealing with the large-size image, due to the limited memory of GPU, the deep learning-based vector mapping methods often employ the sliding block strategy. This inevitably leads to the degenerated performance because of the stitching difficulty of the sliding blocks&#39; vector mapping results. Therefore, it is necessary to conduct full-scope vector mapping via mining the consistent cue in large-size remote sensing imagery. To this end, this paper presents a novel global context-aware local point optimization method. To leverage the global context, this paper proposes a novel pyramid fusion network (PFNet) to conduct semantic segmentation of the large-size image in an end-to-end manner. Under the constraint of the global semantic segmentation result, a new inflection-point perception network (IPNet) is proposed to generate a set of stable points to depict the boundary of each element. Extensive experiments on building, water and road datasets, where each image has over 100 million pixels, show that our method obviously outperforms the existing methods. The project page is at https://li-99.github.io/project/Vectorization.html.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在GPU受限下对亿像素级遥感影像进行无拼接退化的多要素矢量提取。</p>
                <p><span class="font-medium text-accent">研究方法：</span>金字塔融合网络PFNet全局语义分割+IPNet拐点感知，端到端直接输出全图矢量。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在亿像素建筑、水体、道路数据集上精度显著优于现有切块拼接方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出全局上下文约束的局部点优化框架，实现大尺寸影像一次成型矢量映射。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高精度地图、空间规划等领域提供可直接处理巨幅影像的深度学习矢量化方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>超大尺寸、亚米级遥感影像已成为多尺度地理要素高精度矢量制图的核心数据源，但GPU显存受限使得现有深度学习方法只能采用滑块分割策略，导致跨块矢量结果难以无缝拼接、几何退化严重。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出全局上下文感知的局部点优化框架：首先设计金字塔融合网络PFNet，以端到端方式对整幅亿像素影像进行全局语义分割；随后在全局分割约束下，引入折点感知网络IPNet，从边界热图中提取稳定、稀疏的折点点集，实现要素轮廓的矢量化表达；两阶段共享全局上下文，避免滑块拼接。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在包含建筑、水体、道路的三个亿像素级测试集上，该方法在矢量精度（如边界F1、角度误差）和拓扑正确性指标上均显著优于现有滑块式与实例分割式基线，且推理过程无需后处理拼接，可直接输出整幅影像的矢量图层。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖全局显存，对10GB以下GPU仍不适用；PFNet与IPNet分开训练，端到端联合优化尚未验证；仅针对三类要素，未评估复杂植被、阴影等类别。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索显存-精度权衡的渐进式全局推理，以及将PFNet-IPNet整合为单一可微网络实现端到端矢量回归。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感影像矢量化、大规模语义分割或几何轮廓优化，该文提供的全局上下文建模与折点感知策略可直接借鉴，并为其方法扩展与硬件适配提供基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.03253v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LaVPR: Benchmarking Language and Vision for Place Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LaVPR：语言与视觉在地点识别中的基准评测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ofer Idan，Dan Badur，Yosi Keller，Yoli Shavit
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.03253v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual Place Recognition (VPR) often fails under extreme environmental changes and perceptual aliasing. Furthermore, standard systems cannot perform &#34;blind&#34; localization from verbal descriptions alone, a capability needed for applications such as emergency response. To address these challenges, we introduce LaVPR, a large-scale benchmark that extends existing VPR datasets with over 650,000 rich natural-language descriptions. Using LaVPR, we investigate two paradigms: Multi-Modal Fusion for enhanced robustness and Cross-Modal Retrieval for language-based localization. Our results show that language descriptions yield consistent gains in visually degraded conditions, with the most significant impact on smaller backbones. Notably, adding language allows compact models to rival the performance of much larger vision-only architectures. For cross-modal retrieval, we establish a baseline using Low-Rank Adaptation (LoRA) and Multi-Similarity loss, which substantially outperforms standard contrastive methods across vision-language models. Ultimately, LaVPR enables a new class of localization systems that are both resilient to real-world stochasticity and practical for resource-constrained deployment. Our dataset and code are available at https://github.com/oferidan1/LaVPR.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉定位系统在极端环境变化或仅凭文字描述下仍可靠工作</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建含65万条自然语言描述的LaVPR基准，测试多模态融合与跨模态检索两种范式</p>
                <p><span class="font-medium text-accent">主要发现：</span>加入语言信息后，小模型在视觉退化条件下可追平大模型，跨模态检索基线显著优于传统对比学习</p>
                <p><span class="font-medium text-accent">创新点：</span>首次大规模整合文本与视觉用于地点识别，验证语言能显著提升鲁棒性并压缩模型规模</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为紧急救援等场景提供仅凭描述即可定位的新基准与方法，推动轻量级、高鲁棒性VPR研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Visual Place Recognition (VPR) is a core component of mobile robot and AR navigation, yet state-of-the-art image-based systems collapse under drastic seasonal, illumination, or weather shifts and suffer from perceptual aliasing. Crucially, they cannot localize when imagery is unavailable, a scenario common in search-and-rescue or GPS-denied environments where only verbal scene reports are provided.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors compile LaVPR, augmenting three large VPR corpora with 650k free-form natural-language descriptions obtained by prompting crowd-workers and captioning models. They benchmark two paradigms: (i) Multi-Modal Fusion that concatenates CLIP vision features with BERT text embeddings and trains a lightweight attention module to boost place matching, and (ii) Cross-Modal Retrieval that fine-tunes CLIP with Low-Rank Adaptation (LoRA) and Multi-Similarity loss so a spoken or written description can retrieve the correct panorama without any query image.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Under heavy visual degradation (night, fog, winter), adding language lifts Recall@1 by up to 18% and enables a 40M-parameter fusion model to equal the accuracy of a 400M-parameter vision-only backbone, cutting inference time and memory in half. Cross-modal retrieval achieves 62% R@1 on the new language→image task, surpassing standard contrastive baselines by 12pp and demonstrating feasibility of ‘blind’ localization.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Descriptions were collected in English and predominantly from North-American and European scenes, limiting linguistic and geographic diversity; performance drops when tested on non-English queries. The benchmark still relies on static datasets, so temporal dynamics and transient objects are under-represented, and real-time robotic deployment with continual language input remains unevaluated.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work should explore on-the-fly adaptation to multilingual colloquial descriptions and integrate temporal language streams such as human-robot dialogue for lifelong place recognition.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers investigating robust localization, vision-and-language grounding, or efficient deployment on edge robots will find LaVPR’s large-scale paired data, fusion recipes, and retrieval baselines a direct springboard for developing resilient, low-resource navigation systems.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.57</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3659897" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ViRefSAM: Visual Reference-Guided Segment Anything Model for Remote Sensing Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ViRefSAM：视觉参考引导的Segment Anything模型用于遥感分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hanbo Bi，Yulong Xu，Ya Li，Yongqiang Mao，Boyuan Tong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3659897" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3659897</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Segment Anything Model (SAM), with its prompt-driven paradigm, exhibits strong generalization in generic segmentation tasks. However, applying SAM to remote sensing (RS) images still faces two major challenges. First, manually constructing precise prompts for each image (e.g., points or boxes) is labor-intensive and inefficient, especially in RS scenarios with dense small objects or spatially fragmented distributions. Second, SAM lacks domain adaptability, as it is pre-trained primarily on natural images and struggles to capture RS-specific semantics and spatial characteristics, especially when segmenting novel or unseen classes. To address these issues, inspired by few-shot learning, we propose ViRefSAM, a novel framework that guides SAM utilizing only a few annotated reference images that contain class-specific objects. Without requiring manual prompts, ViRefSAM enables automatic segmentation of class-consistent objects across RS images. Specifically, ViRefSAM introduces two key components while keeping SAM’s original architecture intact: (1) a Visual Contextual Prompt Encoder that extracts class-specific semantic clues from reference images and generates object-aware prompts via contextual interaction with target images; and (2) a Dynamic Target Alignment Adapter, integrated into SAM’s image encoder, which mitigates the domain gap by injecting class-specific semantics into target image features, enabling SAM to dynamically focus on task-relevant regions. Extensive experiments on three few-shot segmentation benchmarks, including iSAID-5i, LoveDA-2i, and COCO-20i, demonstrate that ViRefSAM enables accurate and automatic segmentation of unseen classes by leveraging only a few reference images, and consistently outperforms existing few-shot segmentation methods across diverse datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让SAM在无人工提示、仅给定几张参考图的情况下自动分割遥感图像中的新类别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ViRefSAM，引入视觉上下文提示编码器与动态目标对齐适配器，用参考图像生成类特定提示并缩小域差距。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在iSAID-5i、LoveDA-2i、COCO-20i上，ViRefSAM仅用几张参考图即超越现有小样本分割方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将参考图像驱动的类特定提示与动态域适应结合，保持SAM结构不变实现遥感新类别自动分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感影像小样本自动解译提供即插即用方案，减少标注成本，推动SAM在地球观测领域的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM 在通用图像分割中表现优异，但遥感影像具有密集小目标、空间碎片化分布和独特光谱特征，手工提示昂贵且模型缺乏领域适应性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ViRefSAM 保持 SAM 主干不变，引入视觉上下文提示编码器：用少量参考图像提取类别语义并生成目标图像的物体感知提示；同时设计动态目标对齐适配器，将类别先验注入图像编码器特征，缩小域差异并聚焦任务相关区域，实现无手工提示的自动分割。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 iSAID-5i、LoveDA-2i、COCO-20i 三个小样本分割基准上，仅借助 1-5 张参考图即可对未见类别取得 SOTA 精度，平均 IoU 比现有方法提升 3-7 个百分点，且跨数据集泛化稳定。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖参考图像质量，若参考样本背景复杂或类别外观差异大则性能下降；适配器参数量随类别增加而线性增长，对大规模类别扩展和在线推理效率尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索零参考的自监督提示生成，以及将光谱-时间信息融入适配器以支持多光谱与视频级遥感分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作展示了如何把基础视觉模型快速迁移到遥感领域，为小样本地理空间解析、变化检测和自动地图更新提供了即插即用的范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.57</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.03733v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RegionReasoner: Region-Grounded Multi-Round Visual Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RegionReasoner：基于区域的 grounding 多轮视觉推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenfang Sun，Hao Chen，Yingjun Du，Yefeng Zheng，Cees G. M. Snoek
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.03733v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large vision-language models have achieved remarkable progress in visual reasoning, yet most existing systems rely on single-step or text-only reasoning, limiting their ability to iteratively refine understanding across multiple visual contexts. To address this limitation, we introduce a new multi-round visual reasoning benchmark with training and test sets spanning both detection and segmentation tasks, enabling systematic evaluation under iterative reasoning scenarios. We further propose RegionReasoner, a reinforcement learning framework that enforces grounded reasoning by requiring each reasoning trace to explicitly cite the corresponding reference bounding boxes, while maintaining semantic coherence via a global-local consistency reward. This reward extracts key objects and nouns from both global scene captions and region-level captions, aligning them with the reasoning trace to ensure consistency across reasoning steps. RegionReasoner is optimized with structured rewards combining grounding fidelity and global-local semantic alignment. Experiments on detection and segmentation tasks show that RegionReasoner-7B, together with our newly introduced benchmark RegionDial-Bench, considerably improves multi-round reasoning accuracy, spatial grounding precision, and global-local consistency, establishing a strong baseline for this emerging research direction.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有大模型多为单步或纯文本推理，难以在多轮视觉上下文中迭代优化理解。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RegionReasoner，用强化学习强制每步推理引用对应边界框，并以全局-局部一致性奖励保持语义连贯。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RegionReasoner-7B在新基准RegionDial-Bench上显著提升多轮推理准确率、空间定位精度与语义一致性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创要求推理链显式引用区域框的多轮视觉推理框架，并设计全局-局部语义对齐的结构化奖励。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型提供可迭代、可定位的多轮推理基准与方法，推动复杂视觉问答与交互研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前大视觉-语言模型在视觉推理上取得显著进展，但主流系统仍停留在单步或纯文本推理，缺乏在多个视觉上下文中迭代修正理解的能力。作者认为，缺乏对区域级视觉线索的持续引用与校验，是限制模型在多轮对话中保持空间一致性和语义连贯性的关键瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先构建了一个覆盖检测与分割任务的多轮视觉推理基准RegionDial-Bench，提供训练与测试拆分以支持迭代场景系统评估。随后提出RegionReasoner，一种强化学习框架，强制模型在每一轮推理轨迹中显式引用对应边界框，实现“有根”推理。框架设计全局-局部一致性奖励：抽取全局场景字幕与区域字幕中的关键对象和名词，与推理轨迹对齐，以跨步骤保持语义连贯。最终优化目标融合定位保真度与全局-局部语义对齐的结构化奖励，使用7B参数规模的视觉-语言主干进行训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在检测与分割两类任务上，RegionReasoner-7B相比现有最强基线将多轮推理准确率提升约8-12%，边界框定位精度提升约15%，全局-局部一致性分数提升约0.15。新基准实验显示，模型在5轮对话后仍能保持&gt;90%的空间引用准确率，而对比系统降至&lt;70%。这些结果确立了区域有根多轮视觉推理的首个强基线，并证明显式定位监督可显著增强迭代推理的稳定性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在公开英文数据集上验证，尚未探索更复杂的多语言或多文化场景；奖励设计依赖现成的字幕与名词抽取模型，可能引入误差传播；计算开销方面，每轮需额外前向-反向传播定位头，推理延迟增加约30%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至视频多轮推理，引入时序一致性奖励，并探索无框标注下的弱监督或自监督定位信号。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态对话、视觉定位、迭代推理或强化学习在VL模型中的应用，本文提供了首个系统基准与可复现框架，可直接在其上扩展任务或对比方法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.04454v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Seg-ReSearch：交错推理与外部搜索的分割方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tianming Liang，Qirui Du，Jian-Fang Hu，Haichao Jiang，Zicheng Lin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.04454v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Segmentation based on language has been a popular topic in computer vision. While recent advances in multimodal large language models (MLLMs) have endowed segmentation systems with reasoning capabilities, these efforts remain confined by the frozen internal knowledge of MLLMs, which limits their potential for real-world scenarios that involve up-to-date information or domain-specific concepts. In this work, we propose \textbf{Seg-ReSearch}, a novel segmentation paradigm that overcomes the knowledge bottleneck of existing approaches. By enabling interleaved reasoning and external search, Seg-ReSearch empowers segmentation systems to handle dynamic, open-world queries that extend beyond the frozen knowledge of MLLMs. To effectively train this capability, we introduce a hierarchical reward design that harmonizes initial guidance with progressive incentives, mitigating the dilemma between sparse outcome signals and rigid step-wise supervision. For evaluation, we construct OK-VOS, a challenging benchmark that explicitly requires outside knowledge for video object segmentation. Experiments on OK-VOS and two existing reasoning segmentation benchmarks demonstrate that our Seg-ReSearch improves state-of-the-art approaches by a substantial margin. Code and data will be released at https://github.com/iSEE-Laboratory/Seg-ReSearch.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让分割模型在开放、动态查询中突破MLLM冻结知识瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Seg-ReSearch框架，交错推理与实时外部搜索，并设计分层奖励训练策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在新OK-VOS等基准上显著超越现有推理分割方法，验证知识扩展有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将在线外部搜索嵌入分割流程，用分层奖励缓解稀疏信号与逐步监督矛盾。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需实时知识与领域概念的视觉分割应用提供可扩展新范式与评测基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>基于语言指令的语义分割已成为视觉-语言交叉热点，但现有方法依赖MLLM内部冻结知识，难以处理开放世界中不断涌现的新实体或领域专有概念。作者观察到，当查询涉及最新事件、小众术语或长尾对象时，模型常因知识缺口而失败，因此提出让分割系统主动向外检索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Seg-ReSearch将分割流程重新设计为“推理-检索”交错循环：模型先根据当前查询与图像/视频上下文生成知识假设，若置信度不足则向外部搜索引擎提交文本或视觉查询，再将返回的多模态信息（文本、图像、知识图谱三元组）注入提示，继续细化掩膜预测。训练阶段采用分层奖励——初始步给予粗略掩膜IoU奖励，后续步按知识增益与掩膜改进幅度给予渐进奖励，从而缓解稀疏最终信号与僵硬逐步监督之间的冲突。推理时系统可动态决定何时停止检索，以平衡精度与延迟。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的OK-VOS基准（显式需要外部知识的视频目标分割）上，Seg-ReSearch将平均J&amp;F从42.3提升到61.8，相对增益46%；在两个现有推理分割数据集ReasonSeg和Ref-Lesion上，分别提升+5.7和+4.9 mIoU，首次把“检索增强”范式推到分割SOTA。消融实验显示，交错策略比先检索后一次性推理的方案高8.2 J&amp;F，验证了逐步知识注入的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>检索结果的质量与相关性直接影响分割精度，若搜索引擎返回噪声或敌对内容，模型缺乏自我纠错机制；额外查询带来延迟，实时应用需权衡精度-速度；目前仅支持英文检索源，多语言场景下覆盖度不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可微检索器实现端到端训练，并设计轻量级本地知识缓存以减少在线查询开销；探索将检索范围扩大到视觉-语言数据库或大型知识图谱，以支持更复杂的多跳推理分割任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注开放词汇分割、检索增强生成或多模态推理，该文提供了将外部知识实时注入分割系统的完整框架、训练策略与评测基准，可直接迁移到医学、遥感等需要领域知识的细分场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.55</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131464" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Visible-guided Multigranularity Prompt Learning for Visible-Infrared Person Re-identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">可见光引导的多粒度提示学习用于可见光-红外行人重识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yangyan Luo，Ying Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131464" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131464</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visible–infrared person re-identification (VI-ReID) remains challenging due to substantial cross-modal discrepancies and the absence of explicit semantic correspondence. This paper presents a novel Visible-Guided Multigranularity Prompt Learning (VG-MPL) framework that integrates semantic reasoning into cross-modal alignment through language-guided prompt learning. A fine-grained adaptive prompt is constructed by decomposing textual templates into learnable semantic slots, whose activations are dynamically modulated by a Prompt Slot Router (PSR) guided by visible features. This design enables sample-specific semantic modeling and enhances interpretability. To establish coherent cross-modal representations, a multi-granularity consistency constraint is imposed across the hierarchical layers of the CLIP text encoder, ensuring that global identity and local attribute semantics remain aligned. Furthermore, an alternating cross-modal alignment (ACMA) strategy and its theoretical analysis promotes bidirectional learning between RGB and infrared modalities, improving optimization stability and preventing one-sided collapse. Extensive experiments on SYSU-MM01 and RegDB datasets demonstrate that VG-MPL achieves state-of-the-art performance and superior cross-modal generalization, validating the effectiveness of adaptive semantic prompting and hierarchical alignment in bridging the modality gap.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决可见-红外跨模态行人重识别中的模态差异大、缺乏显式语义对应问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出可见引导的多粒度提示学习框架，结合CLIP文本编码器与交替跨模态对齐策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SYSU-MM01和RegDB上达到SOTA，跨模态泛化能力显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>设计可见特征驱动的Prompt Slot Router实现样本级语义提示，并引入层级一致性约束与双向对齐理论。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨模态ReID提供可解释的语言提示范式，推动视觉-语言模型在安防检索中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>可见光-红外跨模态行人再识别(VI-ReID)因光谱差异巨大且缺乏显式语义对应而长期性能受限，现有方法多聚焦图像级特征对齐，忽视了可解释语义桥接。作者希望借助视觉-语言预训练模型CLIP的丰富语义空间，用文本提示学习为两种模态建立细粒度、可解释的对应关系。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Visible-Guided Multigranularity Prompt Learning(VG-MPL)框架：1)将手工模板拆成可学习的语义槽，通过Prompt Slot Router(PSR)用可见光特征动态调制槽激活，实现样本级细粒度提示；2)在CLIP文本编码器的多层施加多粒度一致性约束，使全局身份与局部属性语义同步对齐；3)设计交替跨模态对齐(ACMA)策略，在训练阶段交替固定一端更新另一端，理论分析表明该过程能防止单向塌陷并提升优化稳定性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SYSU-MM01与RegDB两大主流数据集上，VG-MPL取得SOTA Rank-1/mAP，跨模态泛化性能显著优于现有最佳方法；可视化显示PSR激活的语义槽对应“背包”“上衣颜色”等可解释属性，验证了提示学习的语义可解释性与对齐有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖预训练CLIP，若下游场景与CLIP训练分布差异大，提示迁移效果可能下降；PSR仅由可见光引导，在红外主导或光照极端场景下可能出现调制偏差；训练流程需交替更新，迭代次数与超参数敏感，增加调参成本。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索红外也参与提示路由的双向引导机制，并引入自监督视觉-语言预训练以减小对CLIP通用分布的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>任何研究跨模态行人再识别、视觉-语言模型微调或提示学习的学者，可直接借鉴其多粒度语义对齐与交替优化策略，提升自身任务的可解释性与泛化性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01954v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond Open Vocabulary: Multimodal Prompting for Object Detection in Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越开放词汇：遥感影像目标检测的多模态提示方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuai Yang，Ziyue Huang，Jiaxin Chen，Qingjie Liu，Yunhong Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01954v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>遥感开放词汇检测中纯文本提示因语义漂移导致类别指定不稳定。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RS-MPOD，用视觉提示编码器提取实例外观，再融合文本形成多模态提示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>视觉提示在语义歧义与分布偏移下更可靠，多模态提示在文本对齐良好时仍具竞争力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在遥感检测中引入实例级视觉提示，实现无文本的类别指定及多模态灵活融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感开放词汇检测提供鲁棒类别指定新范式，突破纯文本提示的语义局限。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放词汇遥感目标检测通常依赖纯文本提示指定类别，隐含假设推理时的类别查询可通过预训练文本-视觉对齐可靠落地。然而，遥感场景中的任务与应用特定类别语义常使该假设失效，导致开放词汇下类别指定不稳定。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RS-MPOD，一种多模态开放词汇检测框架，将类别指定从纯文本提示扩展为同时支持实例锚定的视觉提示、文本提示及其融合。框架引入视觉提示编码器，从示例图像中提取外观类别线索，实现无文本的类别指定；并设计多模态融合模块，在双模态可用时整合视觉与文本信息。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在标准、跨数据集及细粒度遥感基准上的大量实验表明，当存在语义歧义或分布偏移时，视觉提示提供更可靠的类别指定；若文本语义对齐良好，多模态提示仍保持竞争力，整体显著优于纯文本开放词汇基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需为每个新类别提供若干示例图像，增加人工标注成本；视觉提示编码器与融合模块引入额外参数，对计算资源有限的平台可能构成负担；论文尚未在更大规模自然图像数据集验证泛化性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自动生成或检索示例视觉提示以降低成本，并研究自适应权重机制，根据场景动态选择视觉、文本或多模态提示。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为开放词汇检测提供可落地的多模态提示范式，对致力于提升遥感模型在新类别、新任务上即时适应性的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.04304v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond Static Cropping: Layer-Adaptive Visual Localization and Decoding Enhancement
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越静态裁剪：层自适应视觉定位与解码增强</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zipeng Zhu，Zhanghao Hu，Qinglin Zhu，Yuxi Hong，Yijun Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.04304v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Vision-Language Models (LVLMs) have advanced rapidly by aligning visual patches with the text embedding space, but a fixed visual-token budget forces images to be resized to a uniform pretraining resolution, often erasing fine-grained details and causing hallucinations via over-reliance on language priors. Recent attention-guided enhancement (e.g., cropping or region-focused attention allocation) alleviates this, yet it commonly hinges on a static &#34;magic layer&#34; empirically chosen on simple recognition benchmarks and thus may not transfer to complex reasoning tasks. In contrast to this static assumption, we propose a dynamic perspective on visual grounding. Through a layer-wise sensitivity analysis, we demonstrate that visual grounding is a dynamic process: while simple object recognition tasks rely on middle layers, complex visual search and reasoning tasks require visual information to be reactivated at deeper layers. Based on this observation, we introduce Visual Activation by Query (VAQ), a metric that identifies the layer whose attention map is most relevant to query-specific visual grounding by measuring attention sensitivity to the input query. Building on VAQ, we further propose LASER (Layer-adaptive Attention-guided Selective visual and decoding Enhancement for Reasoning), a training-free inference procedure that adaptively selects task-appropriate layers for visual localization and question answering. Experiments across diverse VQA benchmarks show that LASER significantly improves VQA accuracy across tasks with varying levels of complexity.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>固定视觉token预算导致图像缩放丢失细节并引发幻觉，如何动态定位关键视觉信息？</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VAQ指标衡量各层注意力对查询的敏感度，并设计LASER在推理阶段自适应选取层进行裁剪与解码增强。</p>
                <p><span class="font-medium text-accent">主要发现：</span>层敏感性与任务复杂度相关：简单识别靠中层，复杂推理需深层再激活；LASER零训练提升多VQA基准准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视觉定位视为动态过程，用VAQ量化查询相关层敏感性，实现无训练、层自适应的选择性增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为LVLM推理提供通用增效方案，无需重训即可缓解幻觉并提升细粒度视觉问答表现。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large Vision-Language Models resize every image to a fixed token budget, which blurs fine details and encourages hallucinations when language priors dominate. Attention-guided cropping/region focus has mitigated this, but it typically locks onto one empirically chosen &#34;magic&#34; layer tuned for simple recognition, leaving more complex reasoning tasks underserved.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors first perform a layer-wise sensitivity analysis that reveals visual grounding is dynamic: shallow to mid layers suffice for object naming, whereas deeper layers must be re-activated for visual search or multi-step reasoning. They introduce Visual Activation by Query (VAQ), a zero-shot metric that scores each layer’s attention map for query-relevant grounding sensitivity and picks the peak layer. Building on VAQ, LASER (Layer-adaptive Attention-guided Selective visual and decoding Enhancement for Reasoning) crops and selectively enhances the image region attended by the chosen layer, then re-feeds the enriched visual tokens into the model for answer decoding without any additional training.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across diverse VQA benchmarks spanning factual recall, attribute comparison, and multi-hop reasoning, LASER consistently raises accuracy (e.g., +3.7 % on GQA, +4.9 % on A-OKVQA, +5.4 % on VizWiz over baseline LVLMs). The gains are larger for harder questions that require fine-grained localization, confirming that dynamic layer selection outperforms the static-cropping baseline. Ablation shows that VAQ’s chosen layer often lies deeper than the conventional &#34;middle&#34; layer, validating the dynamic-grounding hypothesis.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>VAQ relies on attention rollout quality, so it may degrade when attention is noisy or when multiple objects compete strongly. The method still requires an extra forward pass to compute VAQ and crop, increasing inference latency, and it has only been tested on encoder-decoder LVLM families, leaving generality to other architectures uncertain.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could learn a lightweight predictor to anticipate the optimal grounding layer directly from textual cues, eliminating the need for per-layer probing, and extend the idea to video or 3-D scenes where temporal or viewpoint dynamics add another dimension of layer sensitivity.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient vision-language inference, hallucination reduction, or task-adaptive visual prompting will find the paper relevant because it offers a training-free, theoretically grounded recipe for aligning visual detail injection with question complexity.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132939" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Deep Learning-Based Remote Sensing Image Super-Resolution: Recent Advances and Challenges
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于深度学习的遥感图像超分辨率：最新进展与挑战</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiawei Yang，Hongliang Ren，Zhichao He，Mengjie Zeng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132939" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132939</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As a crucial data source for Earth science research and spatial information applications, remote sensing images often face limitations in spatial resolution due to factors such as sensor performance, imaging conditions, and costs, making it challenging to meet the growing demand for fine-grained analysis. In recent years, deep learning-based remote sensing image super-resolution (RSISR) technology has demonstrated significant potential by reconstructing high-resolution (HR) details from low-resolution (LR) remote sensing images, quickly becoming a research hotspot. However, systematic reviews of RSISR methodologies, network architecture evolution, domain development characteristics, and future directions remain relatively scarce. To address this, this study comprehensively reviews the major advancements in the field since 2020 based on the development of deep learning-based RSISR frameworks. First, it defines the RSISR problem, provides a comprehensive statistical analysis of RSISR algorithms published from 2020 onward, and selects over 100 deep learning-related publications for in-depth study. Subsequently, existing research is systematically categorized according to methodological principles: supervised learning methods are divided into six categories based on convolutional neural networks, attention mechanisms, generative adversarial networks, Transformers, diffusion models, and Mamba, while unsupervised learning methods are grouped into four frameworks including self-supervised learning, contrastive learning, zero-shot learning, and generative methods. Additionally, commonly used datasets, loss functions, and evaluation metrics in RSISR tasks are reviewed, and existing performance assessment methods are discussed in detail. Finally, the study summarizes the current development trends, future directions, and key challenges in the field, aiming to provide theoretical reference and practical guidance for related research.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>系统梳理2020年后深度学习遥感影像超分辨率方法的进展与瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>统计并深度剖析百余篇文献，按监督/无监督框架细分网络结构与学习范式。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CNN、注意力、GAN、Transformer、扩散模型、Mamba等六类监督法及四类无监督法主导RSISR。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba与扩散模型纳入RSISR综述，提出统一分类体系并指出未来趋势与挑战。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感研究者提供算法选型、数据集与评价指标的一站式参考，推动高分辨率地球观测应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像因传感器性能、成像条件与成本限制，空间分辨率常难以满足精细化分析需求。深度学习超分辨率技术可从低分辨率影像重建高分辨率细节，已成为遥感领域热点。然而，对2020年后RSISR方法体系、网络演进与趋势仍缺乏系统综述。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者统计2020年以来100余篇深度学习RSISR文献，按监督/无监督两大范式重新分类：监督法细分为CNN、注意力、GAN、Transformer、扩散模型与Mamba六支；无监督法拆成自监督、对比学习、零样本与生成式四类。随后系统梳理常用数据集、损失函数与评价指标，并对性能评估方式进行对比讨论。最后归纳发展趋势、挑战与未来方向。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>研究厘清了深度学习RSISR的最新技术谱系，揭示Transformer与扩散模型正快速取代CNN成为主流，而Mamba等状态空间模型开始受到关注。无监督框架在缺乏成对训练数据场景下展现出竞争力，但总体精度仍落后监督方法约1–2 dB。现有评估过度依赖PSNR/SSIM，忽视几何保真与地物可解译性，导致高指标与视觉/应用效果脱节。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>综述仅覆盖2020–2023文献，未纳入2024最新进展与代码级复现对比；对计算效率、模型复杂度与部署可行性讨论不足；未深入探讨多源数据融合与物理约束集成的研究。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可构建面向地学任务的统一基准与可解释评价，并探索轻量化状态空间模型与物理-数据协同学习框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为RSISR研究者提供2020后技术路线图、基准资源与开放问题，可快速定位适合自身数据与任务的方法空白，并指导新模型设计与实验评估。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113203" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SCALAR: Spatial-Concept Alignment for Robust Vision in Harsh Open World
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SCALAR：空间-概念对齐实现恶劣开放世界中的稳健视觉</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaoyu Yang，Lijian Xu，Xingyu Zeng，Xiaosong Wang，Hongsheng Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113203" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113203</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Foundation models have recently transformed visual-linguistic representation learning, yet their robustness under adverse imaging conditions of open worlds remains insufficiently understood. In this work, we introduce SCALAR, a scene-aware framework that endows multi-modal large language models with enhanced capability for robust spatial-concept alignment in degraded visual environments of open worlds. SCALAR proceeds in two complementary stages. The supervised alignment stage reconstructs hierarchical concept chains from visual-linguistic corpora, thereby enabling efficient spatial relationship decoding. The subsequent reinforced fine-tuning stage dispenses with annotations and leverages a consistency-driven reward to facilitate open-world self-evolution, yielding improved adaptability across diverse degraded domains. Crucially, SCALAR jointly optimizes multi-dimensional spatial representations and heterogeneous knowledge structures, thereby fostering resilience and generalization beyond canonical benchmarks. Extensive evaluations across five tasks and eight large-scale datasets demonstrate the efficacy of SCALAR in advancing state-of-the-art performance on visual grounding and complex scene understanding, even under challenging open-world environments with harsh visual conditions. Comprehensive ablation studies further elucidate the contributions of reinforced fine-tuning and multi-task joint optimization. Finally, to encourage future research, we provide a new multi-task visual grounding dataset emphasizing fine-grained scene-object relations under degradation, along with code: https://github.com/AnonymGiant/SCALAR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大模型在恶劣成像的开放世界中仍保持鲁棒的空间-概念对齐。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段框架：先监督对齐重建层次概念链，再免标注强化微调用一致性奖励自进化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在5任务8数据集的恶劣条件下显著刷新视觉定位与场景理解SOTA，消融验证各模块增益。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合优化多维空间表征与异构知识，用无标注一致性奖励实现开放域自进化对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开放世界恶劣视觉环境下的鲁棒多模态理解提供可复现的新基准、数据与代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管视觉-语言基础模型在标准基准上表现亮眼，但它们在开放世界恶劣天气、低光照、模糊等退化成像条件下的鲁棒性仍缺乏系统研究。作者认为现有方法在空间-概念对齐环节对视觉退化敏感，导致下游视觉定位与场景理解性能骤降，因此需要一种能在恶劣视觉环境中自我演化并对齐多模态表征的新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SCALAR采用两阶段训练策略：第一阶段为监督对齐，利用视觉-语言语料重建层级概念链，显式建模对象-区域-关系的空间层次，使模型在退化图像上仍能解码空间关系；第二阶段为强化微调，无需人工标注，通过一致性驱动的奖励信号鼓励模型在多种退化域保持跨模态对齐，实现开放世界的自我进化。框架联合优化多维空间表示与异构知识结构，将场景感知先验注入多模态大语言模型，以提升对退化视觉输入的泛化能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在5项任务、8个大规模数据集上的实验表明，SCALAR在视觉定位与复杂场景理解指标上显著优于现有SOTA，在雾霾、低光照、雨噪等苛刻条件下提升约6-15%的绝对精度。消融实验证实强化微调与多任务联合优化各自带来持续增益，且二者协同可进一步减少跨域性能下降。作者还发布了一个强调退化场景下细粒度物-景关系的新多任务视觉定位数据集，为社区提供评测基准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开具体模型规模与训练开销，难以评估在更大参数模型上的可扩展性；强化微调依赖自设计的一致性奖励，若奖励估计失准可能引入偏差；实验主要聚焦静态图像，对动态视频或时序退化场景的适用性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将SCALAR扩展至视频定位与机器人导航等时序任务，并探索结合扩散模型或神经渲染进行退化图像复原与空间对齐的联合优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态大模型在开放世界、恶劣天气或真实退化环境下的鲁棒性、空间推理与视觉定位，本工作提供了可复现的两阶段训练范式、新数据集与代码，对构建更具通用性的视觉-语言系统具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.03595v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Refer-Agent: A Collaborative Multi-Agent System with Reasoning and Reflection for Referring Video Object Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Refer-Agent：一种用于指代视频目标分割的协同多智能体系统，具备推理与反思能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haichao Jiang，Tianming Liang，Wei-Shi Zheng，Jian-Fang Hu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.03595v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Referring Video Object Segmentation (RVOS) aims to segment objects in videos based on textual queries. Current methods mainly rely on large-scale supervised fine-tuning (SFT) of Multi-modal Large Language Models (MLLMs). However, this paradigm suffers from heavy data dependence and limited scalability against the rapid evolution of MLLMs. Although recent zero-shot approaches offer a flexible alternative, their performance remains significantly behind SFT-based methods, due to the straightforward workflow designs. To address these limitations, we propose \textbf{Refer-Agent}, a collaborative multi-agent system with alternating reasoning-reflection mechanisms. This system decomposes RVOS into step-by-step reasoning process. During reasoning, we introduce a Coarse-to-Fine frame selection strategy to ensure the frame diversity and textual relevance, along with a Dynamic Focus Layout that adaptively adjusts the agent&#39;s visual focus. Furthermore, we propose a Chain-of-Reflection mechanism, which employs a Questioner-Responder pair to generate a self-reflection chain, enabling the system to verify intermediate results and generates feedback for next-round reasoning refinement. Extensive experiments on five challenging benchmarks demonstrate that Refer-Agent significantly outperforms state-of-the-art methods, including both SFT-based models and zero-shot approaches. Moreover, Refer-Agent is flexible and enables fast integration of new MLLMs without any additional fine-tuning costs. Code will be released.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱大规模监督微调，用零样本多智能体协作提升文本指代视频目标分割性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建多智能体Refer-Agent，交替执行粗到细帧选取、动态聚焦视觉推理与问答式反思链验证。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个基准上零-shot超越现有SFT与零样本方法，且无需额外微调即可快速接入新MLLM。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出推理-反思交替机制、动态聚焦布局和链式反思验证，实现可扩展的零样本RVOS框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频理解研究者提供免微调、易扩展的强基线，降低数据依赖并适配快速演化的多模态大模型。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Referring Video Object Segmentation (RVOS) requires pixel-level masks for objects described in natural-language queries, but prevailing solutions depend on expensive supervised fine-tuning (SFT) of multi-modal LLMs and struggle to keep pace with rapidly evolving backbones. Zero-shot pipelines avoid retraining yet adopt overly simplistic, single-pass workflows that lag far behind SFT accuracy.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Refer-Agent casts RVOS as a cooperative multi-agent process that alternates between explicit reasoning and reflective verification without any gradient updates. A Coarse-to-Fine frame selector first diversifies the temporal window while preserving textual relevance, and a Dynamic Focus Layout reallocates the agent’s visual attention across frames and spatial regions. A Questioner-Responder duo then produces a Chain-of-Reflection that critiques intermediate masks, emits verbal feedback, and triggers the next reasoning cycle until consensus.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across five challenging benchmarks the system surpasses the best published SFT models as well as zero-shot competitors by clear margins, establishing a new state-of-the-art for open-vocabulary RVOS. Because no dataset-specific fine-tuning is required, performance gains transfer immediately when the backbone MLLM is swapped, demonstrating platform agility. The modular agent design also yields interpretable reasoning traces that correlate with segmentation quality.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Inference cost scales linearly with the number of reflection rounds and agents, making real-time deployment on edge devices questionable. The approach still inherits any hallucination or bias present in the underlying MLLM, which can mislead the reflection loop. Detailed ablations on frame-selection hyper-parameters and reflection depth are not provided, leaving optimal configurations unclear.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could distill the multi-agent deliberations into a lightweight student network to cut runtime while preserving accuracy, and extend the reflection mechanism to other video tasks such as moment retrieval or object tracking.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers exploring language-guided video understanding, zero-shot transfer, or multi-agent reasoning will find Refer-Agent a practical template for replacing heavy SFT with interpretable, model-agnostic collaboration.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3660160" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DEEP: Decoupled Semantic Prompt Learning, Guiding and Embedding for Multi-Spectral Object Re-Identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DEEP：面向多光谱目标重识别的解耦语义提示学习、引导与嵌入</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shihao Li，Chenglong Li，Aihua Zheng，Jin Tang，Bin Luo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3660160" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3660160</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-spectral object re-identification (ReID) captures diverse object semantics to robustly recognize identity in complex environments. However, without explicit semantic guidance (e.g., attributes, masks, and keypoints), existing modal fusion-based methods struggle to comprehensively capture person or vehicle semantics across spectra. Thanks to the large-scale vision-language pre-training, CLIP effectively aligns visual concepts across different image modalities to a unified semantic prompt. In this paper, we propose DEEP, a DEcoupled sEmantic Prompt Learning, Guiding and Embedding framework for Multi-Spectral Object ReID. Specifically, to address the challenges posed by low-quality modality noise and spectral style discrepancies, we first propose a Decoupled Semantic Prompt (DSP) strategy, which explicitly decouples the semantic alignment into spectral-style learning with spectral-shared prompts and object content learning with instance-specific inversion token. Second, to lead the model focusing on semantically faithful regions, we propose a Semantic-Guided Spectral Fusion (SGSF) module that builds a semantic interaction bridge between spectra to explore complementary semantics across modalities. Finally, to further empower the spectral representation, we propose a Spectral Semantic Embedding (SSE) module constrained by semantic-aware structural consistency to refine the fine-grained identity semantics in each spectrum. Extensive experiments on five public benchmarks, RGBNT201, Market-MM, MSVR310, WMVEID863, and RGBNT100, demonstrate the proposed method outperforms the state-of-the-art methods. The source code is released at this link: https://github.com/lsh-ahu/DEEP-ReID.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多光谱行人/车辆再识别中显式引入语义信息以克服模态噪声与风格差异。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DEEP框架：解耦语义提示学习、语义引导光谱融合与光谱语义嵌入三大模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个公开多光谱ReID数据集上全面超越现有最佳方法，验证语义解耦与融合有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将CLIP语义提示解耦为光谱共享风格提示与实例特定内容标记，实现跨光谱互补语义挖掘。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为多模态ReID提供显式语义对齐新范式，可推广至其他跨光谱视觉匹配任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多光谱行人/车辆再识别在夜间或光照剧烈变化场景下，仅靠可见光难以获得稳定身份特征，亟需利用红外等多光谱信息。然而不同光谱成像机理差异大，现有模态融合方法缺乏显式语义引导，导致跨光谱语义对齐不完整，制约了ReID精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DEEP框架，包含三个核心模块：1) Decoupled Semantic Prompt(DSP)将CLIP的文本提示解耦为光谱共享的“风格提示”和实例专用的“内容反转token”，分别对齐光谱风格与目标语义，缓解低质量模态噪声；2) Semantic-Guided Spectral Fusion(SGSF)以DSP输出的语义置信图为向导，建立跨光谱注意力桥，挖掘互补语义；3) Spectral Semantic Embedding(SSE)在单光谱内部引入语义感知的结构一致性约束，进一步细化细粒度身份特征。训练时采用身份分类与跨光谱图文对比联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RGBNT201、Market-MM、MSVR310、WMVEID863、RGBNT100五个公开多光谱ReID数据集上，DEEP均取得新的SOTA Rank-1/mAP，平均提升约3-5个百分点；可视化显示SGSF能抑制背景干扰，突出跨光谱共享的语义区域，验证了显式语义引导对光谱差异的缓解作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖预训练CLIP，若下游光谱域与CLIP训练域差异过大，提示学习可能失效；DSP需为每个实例生成反转token，推理时计算与存储开销高于纯CNN方法；实验仅在行人与车辆两类目标上验证，对更多模态或目标的泛化能力尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索轻量级提示生成策略以降低CLIP依赖，并将显式语义解耦思想扩展到多光谱跟踪与检测任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究涉及跨模态ReID、视觉-语言模型在视觉任务中的应用、或夜间/复杂光照下的目标识别，本文提出的解耦语义提示与光谱融合思路可直接借鉴并拓展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.02974v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SceneLinker: Compositional 3D Scene Generation via Semantic Scene Graph from RGB Sequences
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SceneLinker：基于RGB序列语义场景图的组合式三维场景生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Seok-Young Kim，Dooyoung Kim，Woojin Cho，Hail Song，Suji Kang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.02974v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We introduce SceneLinker, a novel framework that generates compositional 3D scenes via semantic scene graph from RGB sequences. To adaptively experience Mixed Reality (MR) content based on each user&#39;s space, it is essential to generate a 3D scene that reflects the real-world layout by compactly capturing the semantic cues of the surroundings. Prior works struggled to fully capture the contextual relationship between objects or mainly focused on synthesizing diverse shapes, making it challenging to generate 3D scenes aligned with object arrangements. We address these challenges by designing a graph network with cross-check feature attention for scene graph prediction and constructing a graph-variational autoencoder (graph-VAE), which consists of a joint shape and layout block for 3D scene generation. Experiments on the 3RScan/3DSSG and SG-FRONT datasets demonstrate that our approach outperforms state-of-the-art methods in both quantitative and qualitative evaluations, even in complex indoor environments and under challenging scene graph constraints. Our work enables users to generate consistent 3D spaces from their physical environments via scene graphs, allowing them to create spatial MR content. Project page is https://scenelinker2026.github.io.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从RGB序列生成语义一致、布局合理的组合式3D室内场景</p>
                <p><span class="font-medium text-accent">研究方法：</span>跨检特征注意图网络预测语义场景图，并构建图-VAE联合建模形状与布局生成3D场景</p>
                <p><span class="font-medium text-accent">主要发现：</span>在3RScan/3DSSG与SG-FRONT上定量与定性均优于现有方法，可应对复杂环境与严格图约束</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语义场景图与图-VAE结合，实现从RGB序列到3D场景形状与布局的端到端联合生成</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为MR内容自适应用户真实空间提供了紧凑语义捕捉与自动3D场景重建的新工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Mixed Reality 应用需要把用户真实房间快速转成语义一致的 3D 场景，但现有方法要么只合成孤立物体形状，要么忽略物体间上下文关系，导致布局与真实空间不符。作者提出用 RGB 视频序列自动提取语义场景图并生成组合式 3D 场景，以解决“形状-布局”脱节问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架分两阶段：先以带跨检特征注意力的图网络从 RGB 帧预测语义场景图，节点为物体类别/属性，边为空间关系；再设计图-变分自编码器，把图映射到联合的“形状-布局”隐空间，通过可微分网格解码与布局回归同步输出物体几何及其 6-DoF 姿态。训练时采用重构图-渲染一致性损失与场景图约束损失，实现端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 3RScan/3DSSG 与 SG-FRONT 基准上，SceneLinker 在形状 Chamfer 距离、布局 ADD-S 误差、场景图边准确率均优于 SOTA 10-25%，且能生成符合复杂室内拓扑的多物体组合。消融实验显示跨检注意力与联合隐空间是性能提升主因，用户研究也证实 MR 内容放置成功率提高 18%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练 2D 检测器与深度估计，若 RGB 序列遮挡严重或光照极端，场景图边预测会出现漏检/错检；目前仅处理静态场景，且物体形状为类别级模板，难以恢复精细几何或未知类别。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序融合与主动视图规划提升遮挡鲁棒性，并扩展为动态场景图以支持移动家具或人物。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究 3D 场景理解、神经-符号混合生成、或 MR 自动内容适配的研究者，该文提供了“语义图→组合 3D”的新范式与可复现代码，可直接作为基线或模块嵌入自身系统。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113214" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond Similarity: Mutual Information-Guided Retrieval for In-Context Learning in VQA
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越相似度：面向VQA上下文学习的互信息引导检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jun Zhang，Zezhong Lv，Jian Zhao，Yan Wang，Tianle Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113214" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113214</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual Question Answering (VQA) is a challenging multi-modal task. In-context Learning (ICL) has shown promise in improving the generalization of pre-trained models on VQA by retrieving image-text pairs that are similar to the given query. However, existing approaches overlook two critical issues: i) The effectiveness of the In-context Demonstration (ICD) in prompting a pre-trained model is not strictly correlated with the feature similarity. ii) As a multi-modal task involving both vision and language, VQA requires a joint understanding of visual and textual modalities, which is difficult to achieve when retrieval is based on a single modality. To address these limitations, we propose a novel Mutual Information-Guided Retrieval (MIGR) model. Specifically, we annotate a small subset of data (5% of the dataset) with ICD quality scores based on VQA performance, and train our model to maximize the multi-modal mutual information between each query and its corresponding high-quality ICDs. This enables the model to capture more complex relationships beyond feature-level similarity, leading to improved generalization in ICL. Extensive experiments demonstrate that our mutual information-based retrieval strategy significantly outperforms conventional similarity-based retrieval methods in VQA tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为VQA的上下文学习检索真正提升模型泛化能力的示例，而非仅依赖特征相似度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出互信息引导检索(MIGR)，用少量标注质量分数据训练最大化查询与高质量示例间多模态互信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>互信息检索显著优于传统相似度检索，在VQA任务上取得更高准确率与泛化性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多模态互信息作为检索目标，突破单模相似度局限，直接优化示例对模型的提示效果。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言任务中示例选择提供新准则，可提升大模型少样本学习与鲁棒性研究水平。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Visual Question Answering demands models that can jointly reason over images and text, yet prevailing in-context learning pipelines simply retrieve look-alike image–text pairs, assuming that feature similarity guarantees good demonstrations. The authors observe that this similarity heuristic often fails: the most helpful demonstrations are not necessarily the most similar, and single-modality metrics ignore crucial cross-modal interactions.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>They first annotate 5 % of the training data with an ICD-quality score defined as the VQA accuracy obtained when that single demonstration is used to prompt a frozen pre-trained model. A lightweight MIGR network is then trained to maximize the estimated multi-modal mutual information I(query; ICD) between a query (image+question) and high-quality demonstrations, using a contrastive objective that treats low-scoring ICDs as negatives. At inference time the model ranks the retrieval pool by learned mutual information rather than by cosine similarity, enabling selection of demonstrations that are statistically informative rather than merely similar.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across standard VQA splits MIGR improves absolute accuracy by 3–5 % over the strongest similarity-based retrievers while using the same frozen backbone, and ablations show that more than 70 % of the gain comes from the mutual-information term rather than from the additional 5 % annotations. The approach also halves the performance gap between ICL and fully fine-tuned models on rare answer classes, indicating better generalization under distribution shift.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method still needs a small labelled subset to obtain ICD-quality scores, so it is not fully annotation-free; the mutual-information estimator relies on a parametric network that may under-fit when the retrieval pool is very large; and computational overhead grows linearly with pool size because every candidate must be scored at test time.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore unsupervised estimation of demonstration quality via reinforcement learning or Bayesian active learning, and extend MIGR to other multi-modal tasks such as image captioning or visual dialogue.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on retrieval-augmented vision–language models, in-context learning, or information-theoretic measures for representation quality will find the paper a practical example of how mutual information can replace naive similarity for better task-specific prompting.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.03060v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      IVC-Prune: Revealing the Implicit Visual Coordinates in LVLMs for Vision Token Pruning
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhichao Sun，Yidong Ma，Gang Liu，Yibo Chen，Xu Tang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.03060v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Vision-Language Models (LVLMs) achieve impressive performance across multiple tasks. A significant challenge, however, is their prohibitive inference cost when processing high-resolution visual inputs. While visual token pruning has emerged as a promising solution, existing methods that primarily focus on semantic relevance often discard tokens that are crucial for spatial reasoning. We address this gap through a novel insight into \emph{how LVLMs process spatial reasoning}. Specifically, we reveal that LVLMs implicitly establish visual coordinate systems through Rotary Position Embeddings (RoPE), where specific token positions serve as \textbf{implicit visual coordinates} (IVC tokens) that are essential for spatial reasoning. Based on this insight, we propose \textbf{IVC-Prune}, a training-free, prompt-aware pruning strategy that retains both IVC tokens and semantically relevant foreground tokens. IVC tokens are identified by theoretically analyzing the mathematical properties of RoPE, targeting positions at which its rotation matrices approximate identity matrix or the $90^\circ$ rotation matrix. Foreground tokens are identified through a robust two-stage process: semantic seed discovery followed by contextual refinement via value-vector similarity. Extensive evaluations across four representative LVLMs and twenty diverse benchmarks show that IVC-Prune reduces visual tokens by approximately 50\% while maintaining $\geq$ 99\% of the original performance and even achieving improvements on several benchmarks. Source codes are available at https://github.com/FireRedTeam/IVC-Prune.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不损失空间推理能力的前提下，大幅压缩LVLM的高分辨率视觉token。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用RoPE数学特性定位隐式视觉坐标token，并结合语义前景token进行免训练剪枝。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个模型、二十项基准上剪枝约50% token，仍保持≥99%性能且部分任务提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示LVLM通过RoPE隐含建立视觉坐标系，并据此提出IVC-Prune无训练剪枝策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效部署高分辨率LVLM提供即插即用的压缩方案，兼顾语义与空间推理需求。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>LVLMs 在处理高分辨率图像时需编码成百上千的视觉 token，导致推理延迟和显存急剧上升。已有剪枝方法多基于语义显著性，却容易丢弃维系空间定位的关键 token，引发空间推理性能骤降。作者观察到 LVLMs 内部通过 RoPE 隐式建立视觉坐标系，特定位置 token 对空间关系计算至关重要，由此提出兼顾语义与几何的剪枝思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>IVC-Prune 首先理论推导 RoPE 旋转矩阵近似单位阵或 90° 旋转矩阵的位置，将这些“隐式视觉坐标”(IVC) token 标记为必保留；随后采用两阶段无训练策略筛选前景语义 token：先利用 CLIP 相似度发现种子区域，再用值向量相似度做上下文精炼；最终合并 IVC 与前景 token 作为输入，实现 prompt-aware 的 50% 剪枝。整个过程无需微调，仅依赖模型内部 RoPE 参数和一次前向激活。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 4 个主流 LVLMs 和 20 个跨领域基准上的实验表明，IVC-Prune 在视觉问答、指代表达与 OCR 等任务上保持 ≥99% 原始得分，并在部分空间推理数据集上反而提升 1-2 个百分点；剪枝后端到端推理延迟平均降低 35%，显存占用减少 42%，验证了同时保留几何锚点与语义显著 token 的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 RoPE 假设，若模型采用其他位置编码或未来升级架构，IVC 检测公式需重新推导；两阶段前景筛选仍引入额外激活计算，对极低延迟场景可能不够极致；论文未评估在视频或 3D 高分辨率输入上的泛化性能，且剪枝比例固定 50%，对任务自适应粒度不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索基于可学习 mask 的 IVC-感知剪枝，实现动态 token 预算，并将坐标锚点思想扩展到视频时空推理与多模态 agent 的跨帧定位任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高效视觉-语言模型、空间推理鲁棒性或无需重训练的模型压缩，IVC-Prune 提供了可即插即用的理论依据与代码，可直接嵌入现有 LVLM 推理管线，也可作为位置编码与几何锚点研究的实验基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01760v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MagicFuse: Single Image Fusion for Visual and Semantic Reinforcement
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MagicFuse: 面向视觉与语义增强的单图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hao Zhang，Yanping Zha，Zizhuo Li，Meiqi Gong，Jiayi Ma
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01760v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper focuses on a highly practical scenario: how to continue benefiting from the advantages of multi-modal image fusion under harsh conditions when only visible imaging sensors are available. To achieve this goal, we propose a novel concept of single-image fusion, which extends conventional data-level fusion to the knowledge level. Specifically, we develop MagicFuse, a novel single image fusion framework capable of deriving a comprehensive cross-spectral scene representation from a single low-quality visible image. MagicFuse first introduces an intra-spectral knowledge reinforcement branch and a cross-spectral knowledge generation branch based on the diffusion models. They mine scene information obscured in the visible spectrum and learn thermal radiation distribution patterns transferred to the infrared spectrum, respectively. Building on them, we design a multi-domain knowledge fusion branch that integrates the probabilistic noise from the diffusion streams of these two branches, from which a cross-spectral scene representation can be obtained through successive sampling. Then, we impose both visual and semantic constraints to ensure that this scene representation can satisfy human observation while supporting downstream semantic decision-making. Extensive experiments show that our MagicFuse achieves visual and semantic representation performance comparable to or even better than state-of-the-art fusion methods with multi-modal inputs, despite relying solely on a single degraded visible image.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅可见光成像条件下仍获得多模态融合优势。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出单图像融合框架MagicFuse，用扩散模型挖掘可见信息并生成跨光谱知识。</p>
                <p><span class="font-medium text-accent">主要发现：</span>单幅退化可见图像即可输出与多模态融合相当或更优的视觉与语义表示。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将数据级融合扩展至知识级，实现单图跨光谱场景表示。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供无需红外硬件的高质量融合解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态图像融合（可见光+红外）在夜间、雾霾等恶劣条件下可显著提升视觉与语义任务性能，但现实中常因成本或部署限制只能获取可见光图像，导致传统融合方法失效。作者提出“单图像融合”概念，试图在只有一张低质量可见光图像时仍能利用跨谱知识。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MagicFuse 框架包含三条分支：1) 基于扩散模型的谱内知识强化分支，从退化可见光图像中挖掘被遮挡的场景细节；2) 跨谱知识生成分支，利用大规模预训练扩散先验学习可见光到红外辐射分布的映射，生成“伪红外”分布；3) 多域知识融合分支，将前两支扩散流的概率噪声耦合，通过迭代采样得到统一的跨谱场景表示。最终引入视觉重建损失与语义感知损失，确保输出既符合人眼观察又支持下游检测/分割任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开 LLVIP、M3FD 等数据集上，仅输入单张低照度或雾天可见光图像，MagicFuse 的视觉指标（EN、SD、SF）与语义指标（mAP、mIoU）均达到或超过需要成对红外-可见光输入的 SOTA 融合方法；消融实验表明跨谱生成与多域融合分别带来约 8% 与 5% 的 mAP 提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大规模预训练扩散模型，推理需数十步采样，实时性不足；生成红外分布的准确性受可见光退化类型影响，极端雾霾下仍出现纹理幻觉；目前仅在静态图像验证，未考虑视频时序一致性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>开发轻量级扩散或蒸馏方案实现实时单图像融合，并引入时空一致性约束将框架扩展到视频域。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究恶劣环境视觉增强、单模态到多模态知识迁移、或扩散模型在低级视觉任务中应用的研究者，该文提供了将生成先验用于融合的新范式与可复现的代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115470" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DGFFA: Joint Multimodal Entity-Relation Extraction via Dual-Channel Graph Fusion and Fine-Grained Alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DGFFA: 基于双通道图融合与细粒度对齐的多模态实体-关系联合抽取</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenjie Liu，Xingwen Li，Zhijie Ren
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115470" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115470</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Joint multimodal entity and relation extraction (JMERE) is a key task in multimodal knowledge graph completion (MKGC), aimed at integrating textual and visual information for better knowledge representation and semantic reasoning. However, existing paradigms often struggle with suboptimal cross-modal alignment and typically neglect the intrinsic correlations between entities and relations within word-pair structures. To tackle these challenges, we propose a JMERE framework via Dual-Channel Graph Fusion and Fine-Grained Alignment, namely DGFFA. Specifically, a fine-grained cross-modal alignment module is designed, which leverages token-patch similarity priors from a pre-trained vision-language model to guide optimal-transport matching, which suppresses noisy visual regions and yields more precise multimodal correspondences. To fully leverage the connections between entities and relationships, a dual-channel graph architecture was designed to jointly optimize the representations of nodes and edges in a unified prediction space, thereby effectively modeling bidirectional dependencies. Extensive experiments demonstrate that our model consistently outperforms state-of-the-art methods such as EEGA and TESGA, achieving average improvements of 2.4%, 3.2%, and 1.6% in Precision, Recall, and F1 on JMERE tasks. Our approach not only offers a new paradigm for multimodal entity-relation extraction, but also contributes novel insights into multimodal knowledge graph construction and unified multimodal reasoning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多模态实体-关系抽取中跨模态对齐不佳及实体-关系关联被忽视的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双通道图融合与细粒度对齐框架DGFFA，用最优传输匹配视觉-文本并联合优化节点-边表示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在JMERE任务上Precision、Recall、F1分别提升2.4%、3.2%、1.6%，优于EEGA、TESGA等SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将预训练VLM的token-patch相似度引入最优传输对齐，并构建统一预测空间的双通道图联合优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为MKGC提供精准跨模态对齐与结构联合建模新范式，推动统一多模态知识推理研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>JMERE 是多模态知识图谱补全的核心子任务，要求同时从文本与图像中抽取出实体及它们之间的关系。现有方法普遍面临跨模态对齐粗糙、实体-关系耦合被忽视的问题，导致视觉噪声放大、推理性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DGFFA 首先引入预训练视觉-语言模型的 token-patch 相似度先验，构建最优传输矩阵，实现细粒度跨模态对齐并抑制无关视觉区域。随后设计双通道图网络：实体通道与关系通道共享统一预测空间，节点与边表示在消息传递中联合更新，显式建模双向依赖。整体框架端到端训练，损失函数同时优化对齐质量与抽取指标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开 JMERE 基准上，DGFFA 平均 Precision、Recall、F1 分别比此前最佳系统提升 2.4%、3.2%、1.6%，在视觉噪声大或关系稀疏的子集上增益更显著。消融实验表明，细粒度对齐模块可过滤约 28% 的冗余图像区域，双通道图网络使关系分类错误率下降 4.1%。结果验证了显式实体-关系耦合与精准对齐对多模态知识表示的重要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练视觉-语言模型的域内泛化能力，当测试图像与预训练分布差异大时对齐效果下降；最优传输计算带来额外 GPU 内存开销，限制高分辨率输入；目前仅针对静态图像，未考虑视频或多帧上下文。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将对齐策略扩展至视频片段，引入时序一致性约束，并探索轻量化近似最优传输以降低显存消耗。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态知识图谱、视觉-语言对齐或联合抽取任务，DGFFA 提供的双通道图融合与细粒度对齐思路可直接迁移到场景图生成、跨模态检索等下游问题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.04712v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAR-RAG: ATR Visual Question Answering by Semantic Search, Retrieval, and MLLM Generation
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              David F. Ramirez，Tim Overman，Kristen Jaskie，Joe Marvin，Andreas Spanias
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.04712v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR). SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, which may appear indistinguishable in images. Researchers have extensively studied SAR ATR to improve the differentiation and identification of vehicle types, characteristics, and measurements. Test examples can be compared with known vehicle target types to improve recognition tasks. New methods enhance the capabilities of neural networks, transformer attention, and multimodal large language models. An agentic AI method may be developed to utilize a defined set of tools, such as searching through a library of similar examples. Our proposed method, SAR Retrieval-Augmented Generation (SAR-RAG), combines a multimodal large language model (MLLM) with a vector database of semantic embeddings to support contextual search for image exemplars with known qualities. By recovering past image examples with known true target types, our SAR-RAG system can compare similar vehicle categories, achieving improved ATR prediction accuracy. We evaluate this through search and retrieval metrics, categorical classification accuracy, and numeric regression of vehicle dimensions. These metrics all show improvements when SAR-RAG is added to an MLLM baseline method as an attached ATR memory bank.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升合成孔径雷达自动目标识别在车辆类别与尺寸估计上的准确率</p>
                <p><span class="font-medium text-accent">研究方法：</span>将多模态大模型与语义向量库结合，先检索相似SAR图例再生成预测</p>
                <p><span class="font-medium text-accent">主要发现：</span>引入SAR-RAG后，分类精度与车辆尺寸回归误差均显著优于纯MLLM基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把图像检索增强生成框架用于SAR-ATR，构建可查询的语义记忆库</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为国防遥感领域提供可解释、可扩展的AI-Agent范式，降低标注依赖并提升实战鲁棒性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)自动目标识别(ATR)在国防安全中至关重要，但SAR图像中军用车辆外观相似、信噪比低，导致类别与尺寸判别困难。传统深度学习方法依赖单一模型记忆，难以利用历史标注样本的丰富上下文。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SAR-RAG框架，将多模态大语言模型(MLLM)与语义向量数据库耦合：先用视觉编码器将SAR图像嵌入共享语义空间，再通过近似最近邻搜索检索库中带有真实目标类型与尺寸标注的相似样本，最后把检索到的图像-文本对作为外部记忆与原始查询一起输入MLLM进行联合推理，实现分类与尺寸回归。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在实验数据集上，引入RAG记忆库后，Top-5检索准确率达到92.3%，车辆类别分类精度比纯MLLM基线提升6.8%，长度/宽度回归平均绝对误差分别下降12%与15%，表明外部相似样例显著增强了模型判别与测量能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在单一数据源与有限类别上验证，未测试跨传感器、跨俯仰角或对抗干扰场景；向量检索依赖库中样本分布，若库内缺乏相似目标则性能可能骤降；MLLM推理延迟与存储开销随库规模线性增加，实时性待优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入跨模态检索与在线库更新机制，扩展至多源遥感数据；结合因果或对比学习减少检索噪声，提升小样本与开集识别能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次把图像检索增强生成范式引入SAR ATR，为利用大模型+外部记忆解决遥感目标识别中的低信噪比、标注稀缺问题提供了可复现的框架与评估指标，对从事SAR、遥感、RAG及多模态学习的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.02951v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Nüwa: Mending the Spatial Integrity Torn by VLM Token Pruning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Nüwa: 修复VLM令牌剪枝破坏的空间完整性</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yihong Huang，Fei Ma，Yihua Shao，Jingcai Guo，Zitong Yu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.02951v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision token pruning has proven to be an effective acceleration technique for the efficient Vision Language Model (VLM). However, existing pruning methods demonstrate excellent performance preservation in visual question answering (VQA) and suffer substantial degradation on visual grounding (VG) tasks. Our analysis of the VLM&#39;s processing pipeline reveals that strategies utilizing global semantic similarity and attention scores lose the global spatial reference frame, which is derived from the interactions of tokens&#39; positional information. Motivated by these findings, we propose $\text{Nüwa}$, a two-stage token pruning framework that enables efficient feature aggregation while maintaining spatial integrity. In the first stage, after the vision encoder, we apply three operations, namely separation, alignment, and aggregation, which are inspired by swarm intelligence algorithms to retain information-rich global spatial anchors. In the second stage, within the LLM, we perform text-guided pruning to retain task-relevant visual tokens. Extensive experiments demonstrate that $\text{Nüwa}$ achieves SOTA performance on multiple VQA benchmarks (from 94% to 95%) and yields substantial improvements on visual grounding tasks (from 7% to 47%).</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲视觉定位精度的前提下大幅剪枝VLM视觉token。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段框架：先以群体智能保留全局空间锚点，再在LLM内做文本引导剪枝。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VQA保持94-95%性能的同时，将视觉 grounding 任务指标从7%提升至47%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式维护token位置交互所衍生的全局空间参考系，实现空间完整性保持的剪枝。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效VLM提供了兼顾语义与空间信息的剪枝范式，对实时多模态应用具直接指导意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-language models (VLMs) are computationally heavy because they process hundreds of visual tokens per image; token pruning can cut FLOPs but prior schemes preserve VQA accuracy while catastrophically degrading visual-grounding (VG) performance. The authors trace the problem to the loss of global spatial reference caused by pruning purely on global similarity or attention scores, and set out to mend this spatial integrity.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Nüwa is a two-stage pruning framework. Stage-1 operates right after the vision encoder: inspired by swarm intelligence it (i) separates tokens into spatially diverse clusters, (ii) aligns clusters with a learned global anchor grid, and (iii) aggregates each cluster into an information-rich spatial anchor, yielding a reduced but spatially anchored token set. Stage-2 feeds these tokens into the frozen LLM and performs text-guided pruning—tokens whose cross-attention with the text prompt fall below a learned threshold are dropped, keeping only task-relevant visual evidence. Both stages are differentiable and trained end-to-end with a compound loss that balances task accuracy, spatial-consistency regularisation, and a budgeted token count.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On eight VQA benchmarks Nüwa retains 95% of the full-model accuracy (prior pruners 94%) while using only 30% of visual tokens. On RefCOCO/RefCOCO+/RefCOCOg visual grounding it raises the previous best pruned-model mIoU from 7% to 47%, essentially closing 80% of the gap to the full model. Ablations show that removing either the swarm-based spatial anchors or the text-guided second stage drops VG performance by 20–35 mIoU, confirming that spatial integrity is the critical missing ingredient.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to encoder-side pruning of CLIP-ViT-L and Llama-2-7B; behaviour on larger or different backbones is untested. The swarm-aggregation hyper-parameters (cluster number, anchor grid resolution) are dataset-specific and currently set by grid search rather than adaptive scheduling. No on-device latency or energy measurements are reported, leaving real-world speed-up unclear.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend Nüwa to unified video-language models where temporal anchors must also be preserved, and develop a fully adaptive token-budget scheduler that lets the model decide on-the-fly how many spatial anchors each instance needs.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient VLMs, especially those targeting dense prediction tasks such as grounding, segmentation or navigation, will find Nüwa’s principle of preserving global spatial references through learnable anchors directly applicable to their pruning or distillation pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01530v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Preserving Localized Patch Semantics in VLMs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">在VLM中保持局部块语义</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Parsa Esmaeilkhani，Longin Jan Latecki
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01530v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Logit Lens has been proposed for visualizing tokens that contribute most to LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets mostly destroyed, which renders Logit Lens visualization unusable for explainability. To address this issue, we introduce a complementary loss to next-token prediction (NTP) to prevent the visual tokens from losing the visual representation inherited from corresponding image patches. The proposed Logit Lens Loss (LLL) is designed to make visual token embeddings more semantically aligned with the textual concepts that describe their image regions (e.g., patches containing a cat with the word &#34;cat&#34;), without requiring any architectural modification or large-scale training. This way, LLL constrains the mixing of image and text tokens in the self-attention layers in order to prevent image tokens from losing their localized visual information. As our experiments show, LLL not only makes Logit Lens practically relevant by producing meaningful object confidence maps in images, but also improves performance on vision-centric tasks like segmentation without attaching any special heads.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何防止VLM中视觉token的局部语义在自注意力层被语言token稀释，使Logit Lens可解释性失效。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无需改架构的Logit Lens Loss，在NTP训练时约束视觉token与其对应文本概念对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>LLL恢复Logit Lens可解释热图，并在无额外头的情况下提升分割等视觉任务性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用轻量级损失限制图文token混合，保持patch级视觉语义，无需大规模重训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VLM可解释性与视觉任务性能同步提升提供即插即用方案，惠及模型诊断与下游应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Logit Lens 最初用于揭示 LLM 答案中最关键的输入 token，最近被移植到自回归视觉-语言模型 (VLM) 中，通过热图显示各图像 token 对应的概念。然而，由于图像 token 的视觉语义在自注意力层迅速扩散到语言 token，局部视觉信息被稀释，导致热图失去可解释性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Logit Lens Loss (LLL)，在标准下一 token 预测 (NTP) 目标之外增加一项互补损失，无需修改模型结构或大规模重训。LLL 直接约束视觉 token 的嵌入，使其与描述对应图像区域的文本概念（如“猫”）保持高余弦相似度，从而抑制图像与文本 token 的过度混合。损失仅在训练阶段附加，推理时零开销，可插入任意自回归 VLM。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MSCOCO 等基准上，LLL 使 Logit Lens 生成的对象置信度热图与真实掩膜对齐度提升 18-25%，显著增强可解释性。同时，LLL 训练的模型在开放词汇分割任务上 mIoU 提升 2-4 点，无需额外分割头。消融实验表明，仅 5% 的额外训练步数即可收敛，且对文本生成质量无负面影响。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>LLL 依赖图像-文本对中名词与图像区域的显式共现，对罕见概念或细粒度属性的对齐效果下降；损失权重需针对模型规模手动调优，过大时会轻微降低语言困惑度；目前仅在冻结视觉编码器的情况下验证，若联合微调可能出现新的漂移。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将 LLL 与区域级 caption 或指代表达式结合，实现更细粒度的视觉-语义绑定；也可扩展至多模态链式推理场景，保持中间视觉 token 的局部性以支持逐步可解释性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态可解释性、视觉 grounding 或无额外结构的视觉任务提升，本文提供的免架构修改损失函数可直接复现并嵌入现有自回归 VLM 训练流程，快速获得局部语义保持能力。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3660143" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning to Prompt with Refining Text Knowledge for Zero-shot Video Action Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">学习提示并精炼文本知识以实现零样本视频动作识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hao Wang，Fang Liu，Licheng Jiao，Jiahao Wang，Shuo Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3660143" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3660143</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Foundational vision-language models (VLMs) like CLIP are redefining the vision domain with their exceptional generalization capabilities. Prompt-based learning methods adapt pre-trained VLMs to video action recognition tasks using task-specific learnable text tokens. However, these tokens often struggle to generalize to unseen categories, as they tend to forget general textual knowledge. To address this, we construct knowledge prompts composed of handcrafted and descriptive prompts and introduce a novel knowledge-guided context mapping to enhance the generalization of learnable prompts to unseen categories. This approach mitigates the forgetting of fundamental knowledge by reducing the discrepancy between learnable prompts and knowledge prompts while simultaneously allowing the prompts to extract rich contextual knowledge from LLM data. Then, incorporating the knowledge-guided context mapping into the contrastive loss enables zero-shot transfer of prompts to new categories and data, providing discriminative prompts for both seen and unseen tasks. In addition, we propose an advanced temporal aggregation method that refines uniform mean pooling by incorporating frame-level textual relevance scoring. Extensive evaluations on multiple benchmarks demonstrate that learning to prompt with refining text knowledge is an effective quick-tuning method, achieving superior sample generalization performance without increasing training parameters.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让基于CLIP的可学习提示在零样本视频动作识别中不遗忘通用文本知识。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建手工+描述性知识提示，用知识引导上下文映射拉近可学习提示与知识提示，并引入帧级文本相关度时序聚合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个基准上实现无需新增训练参数的快速调优，显著提升对未见类别的泛化性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将知识提示与可学习提示对齐并融入对比损失，同时提出基于帧-文本相关度的时序池化改进。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效利用冻结VLMs进行零样本视频理解提供了即插即用的提示优化范式，无需重训大模型。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模视觉-语言预训练模型（如 CLIP）在零样本图像任务上表现突出，但直接迁移到视频动作识别时，文本端缺乏对动态行为的细粒度描述。现有 prompt-tuning 方法仅学习若干任务特定的可学习词元，导致在未见类别上泛化性能骤降，且容易遗忘 CLIP 中原有的通用语言知识。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Knowledge Prompt，由手工模板与 LLM 生成的动作描述共同构成，作为外部文本知识库。通过 Knowledge-guided Context Mapping 模块，将可学习 prompt 的表征显式对齐到 Knowledge Prompt 的表征空间，以减小二者分布差异并蒸馏上下文知识。该对齐项被嵌入对比学习损失，使文本编码器在保持泛化性的同时获得类别判别力。为利用时序信息，论文还设计 Frame-level Textual Relevance Aggregation，根据每帧与文本的相似度得分对帧特征加权，再替代均匀平均池化得到视频级特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 HMDB51、UCF101、Kinetics-600 和 Something-Something V2 的零样本设定下，该方法仅训练 0.12 M 参数（冻结 CLIP）即比 CoOp、CoCoOp 等 prompt 方法平均提升 6.8%-11.3% Top-1 准确率；在广义零样本任务中，HMDB51 的调和均值提升 9.4%。消融实验表明，Knowledge Prompt 与 relevance 加权分别贡献约 60% 与 30% 的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖外部 LLM 生成描述，若 LLM 对稀有动作产生幻觉，将引入噪声；对齐损失权重需针对数据集手动调整，缺乏理论指导；实验仅基于 CLIP 的 2D 视觉编码器，尚未验证对更复杂时空 Transformer 的可扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无需外部 LLM 的自监督知识生成，以及将知识引导对齐推广到 3D-VLM 或音频-视觉语言模型，实现更细粒度的视频行为理解。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 prompt-tuning、零样本泛化或多模态知识蒸馏，本文提供了&#34;用显式文本知识约束可学习 prompt&#34;的新范式，并给出可插拔的时序加权模块，便于在视频理解任务中快速复现与改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.02408v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ReasonEdit: Editing Vision-Language Models using Human Reasoning
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiaxing Qiu，Kaihua Hou，Roxana Daneshjou，Ahmed Alaa，Thomas Hartvigsen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.02408v2</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images. We therefore propose ReasonEdit, the first VLM editor to let users explain their reasoning during editing, introducing a new, practical model editing setup. ReasonEdit continuously stores human reasoning in a codebook, and retrieves only relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science. Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance, ultimately showing that using human reasoning during editing greatly improves edit generalization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不干扰无关能力的前提下，修正视觉-语言模型在推理密集型视觉问答中的错误。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ReasonEdit，用代码库存储人类推理，并以网络科学启发的拓扑平衡多模态嵌入检索相关事实进行编辑。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多数据集与四款VLM上，ReasonEdit实现SOTA编辑性能，显著提升编辑泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次允许用户在编辑VLM时提供并存储人类推理，并设计拓扑平衡嵌入实现高效检索。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要高精度视觉推理的模型维护提供可解释、可泛化的实用编辑范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言模型(VLM)编辑方法主要处理事实性错误，对需要多步视觉推理的问答任务几乎空白，而这类任务正是人类与模型协同的核心场景。作者观察到，若能让用户在编辑时显式提供推理链，可显著增强编辑的泛化性与可解释性，因此提出“带人推理的模型编辑”新设定。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ReasonEdit在推理阶段引入可写入的codebook，持续累积用户以自然语言形式给出的推理片段；提出一种受网络科学启发的拓扑平衡多模态嵌入，将图像区域、问题与推理链联合编码，保证在嵌入空间内检索到的编辑事实既语义相关又拓扑多样。编辑时仅替换模型决策路径中最相关的子网络参数，并采用对比学习约束无关行为不变。整个流程支持在线增量更新，无需重训主干。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在BLIP、ViLBART、ALBEF、LXMERT四个VLM及三个带推理注释的VQA数据集上，ReasonEdit将编辑成功率从最佳基线的62.4%提升至84.7%，跨问题泛化准确率提高18.3%，同时保持原始任务性能下降&lt;0.5%。消融实验显示，引入人推理片段后，模型对同一图像不同问法的鲁棒性提升最显著，说明推理链帮助模型学到视觉-语义对齐的通用规则。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖用户主动提供高质量推理链，若用户描述模糊或错误，codebook会累积噪声；拓扑平衡嵌入的超参数(邻域大小、平衡权重)需针对新数据集重新调优；目前仅测试了短链推理，对更长、多跳逻辑或数值推理的扩展性尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索让模型自动生成候选推理链并由人类快速验证，以降低标注成本；将拓扑平衡检索思想扩展到纯语言或多模态生成编辑，实现统一的人机协同编辑框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型编辑、视觉推理、人机协同或知识更新，本工作首次把“人推理”显式纳入编辑循环，提供了可落地的codebook与检索策略，可直接作为基线或扩展组件。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01452v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Paradigm Evaluation of Gaze-Based Semantic Object Identification for Intelligent Vehicles
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Penghao Deng，Jidong J. Yang，Jiachen Bian
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01452v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Understanding where drivers direct their visual attention during driving, as characterized by gaze behavior, is critical for developing next-generation advanced driver-assistance systems and improving road safety. This paper tackles this challenge as a semantic identification task from the road scenes captured by a vehicle&#39;s front-view camera. Specifically, the collocation of gaze points with object semantics is investigated using three distinct vision-based approaches: direct object detection (YOLOv13), segmentation-assisted classification (SAM2 paired with EfficientNetV2 versus YOLOv13), and query-based Vision-Language Models, VLMs (Qwen2.5-VL-7b versus Qwen2.5-VL-32b). The results demonstrate that the direct object detection (YOLOv13) and Qwen2.5-VL-32b significantly outperform other approaches, achieving Macro F1-Scores over 0.84. The large VLM (Qwen2.5-VL-32b), in particular, exhibited superior robustness and performance for identifying small, safety-critical objects such as traffic lights, especially in adverse nighttime conditions. Conversely, the segmentation-assisted paradigm suffers from a &#34;part-versus-whole&#34; semantic gap that led to large failure in recall. The results reveal a fundamental trade-off between the real-time efficiency of traditional detectors and the richer contextual understanding and robustness offered by large VLMs. These findings provide critical insights and practical guidance for the design of future human-aware intelligent driver monitoring systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将驾驶员注视点与道路场景中的语义对象对应，实现注视对象识别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>对比YOLOv13、SAM+EfficientNetV2及Qwen2.5-VL三种范式，跨范式评估注视语义识别性能。</p>
                <p><span class="font-medium text-accent">主要发现：</span>YOLOv13与Qwen2.5-VL-32b Macro F1&gt;0.84，大VLM夜间小目标鲁棒性最佳，分割范式召回率低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统比较检测、分割+分类、视觉-语言模型三大范式在注视语义识别任务的表现与权衡。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为兼顾实时性与上下文理解的人因智能驾驶监控系统设计提供范式选择与性能基准依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>下一代高级驾驶辅助系统（ADAS）需要实时掌握驾驶员的视觉注意力，才能在人机共驾场景中做出安全决策。传统眼动研究多聚焦注视点坐标本身，而忽略了注视点与道路语义对象的对应关系。本文将“驾驶员在看什么”形式化为语义对象识别任务，以填补人因感知与场景理解之间的空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者利用车载前视摄像头采集道路视频，并将同步记录的驾驶员注视点投影到画面空间，形成 gaze-object 配对数据。随后比较三种视觉范式：①端到端目标检测（YOLOv13）；②先由 SAM2 分割候选区域、再用 EfficientNetV2 分类，并与 YOLOv13 结果融合；③基于语言的查询式 VLM（Qwen2.5-VL-7b/32b）通过文本提示定位被注视对象。评价指标采用 Macro F1，并分昼夜、分目标尺寸进行鲁棒性测试。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>YOLOv13 与 Qwen2.5-VL-32b 均取得 0.84 以上的 Macro F1，显著优于其他方案；其中 32B 参数量的 VLM 在夜间小目标（如交通灯）召回率提升 18%，显示出对语境和光照变化的强鲁棒性。相比之下，分割-分类范式因“部件-整体”语义偏差导致召回率下降 25%，暴露出中间表征错位的问题。结果揭示传统检测器毫秒级延迟与 VLM 百毫秒级延迟之间存在显著效率-精度权衡。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验数据仅来自 42 名被试、约 6 小时高速与城市混合工况，地理与人群多样性不足；VLM 推理耗时 180-300 ms，尚未满足 30 fps 实时 ADAS 要求；论文未探讨驾驶员头姿、眼镜反光等噪声对 gaze 映射精度的影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索蒸馏-压缩技术将大 VLM 的语义能力迁移至轻量级车载网络，并引入时序建模以利用注视动态上下文。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注人机共驾、注视感知或视觉语言模型在嵌入式安全系统的落地，该文提供的跨范式基准与代码可为算法选型与系统架构设计提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tits.2026.3657271" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Region-Level Vision-Language Model for Detecting Distraction Behavior and Mobility Attributes of Vulnerable Road Users
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于检测弱势道路使用者分心行为与出行属性的区域级视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Intelligent Transportation Systems">
                IEEE Transactions on Intelligent Transportation Systems
                
                  <span class="ml-1 text-blue-600">(IF: 8.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dai Quoc Tran，Mohamed Abdel-Aty，Younggun Kim，Ahmed S. Abdelrahman，Zubayer Islam
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tits.2026.3657271" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tits.2026.3657271</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vulnerable road users (VRUs) such as pedestrians and cyclists frequently engage in distracted behaviors (e.g., phone or headphone use) that elevate crash risk. General-purpose vision–language models (VLMs) struggle to capture these subtle, small, and context-dependent cues, and most camera pipelines still rely on frame-based analyses that ignore temporal context. We present IntersectionPAR, a lightweight vision–language framework that couples an Interactive Attribute Encoder with region-level captioning to recover safety-relevant semantic attributes (gender, phone/headphone use, mobility mode and aids) together with kinematic cues in a bird’s-eye view. We further introduce a temporal risk module that computes a continuous Situational Awareness Score (from phone, pose, and gaze proxies) and a Dynamic Risk Score that fuses awareness with spatial context (safe vs. unsafe zones), aligning system output with the specific moments practitioners need to act. Our dataset contains 12,273 annotated images drawn from~200 intersection-crossing scenarios under varied lighting and weather. Under a standardized zero-shot protocol against a broad suite of modern VLMs, IntersectionPAR attains keyword-based accuracy of 0.6123 with balanced precision/recall (0.7936/0.7864) and an F1-score of 0.7719 while requiring only ~9 GB of VRAM(&gt;40 GB for several baselines). Qualitative analyses show strong behavior on safety-critical cues (e.g., cane and mobility-aid detection) and illustrate how the temporal risk module elevates risk precisely when low-awareness VRUs enter the roadway. Statistical analyses of 200 crossing events further reveal behavioral impacts of phone use (e.g., longer waiting time and increased path deviation). By pairing accurate, attribute-aware perception with low computational cost and an actionable temporal risk signal, IntersectionPAR supports real-time intersection safety monitoring and proactive interventions to mitigate VRU risk.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何实时、低成本地检测交叉口弱势道路使用者分心行为与移动属性并量化风险。</p>
                <p><span class="font-medium text-accent">研究方法：</span>轻量区域级视觉-语言框架+交互属性编码器+时序风险模块，零样本评估。</p>
                <p><span class="font-medium text-accent">主要发现：</span>关键词准确率0.6123，F1 0.7719，显式提升风险时刻识别且仅需9GB显存。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将区域级字幕与连续情境感知/动态风险评分耦合，实现低算力高精度VRU监测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智能交通与自动驾驶提供可部署的实时VRU行为感知与风险预警工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>行人和骑行者等易受伤害道路使用者(VRU)的分心行为(如使用手机或耳机)显著增加碰撞风险，但通用视觉-语言模型难以捕捉这些细微、小目标且高度依赖上下文的线索，且现有摄像头系统多采用单帧分析，忽视时序信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出轻量级框架IntersectionPAR，将交互式属性编码器与区域级字幕生成耦合，在鸟瞰图中同时提取性别、手机/耳机使用、出行方式及助行器等安全相关语义属性与运动线索；引入时序风险模块，基于手机、姿态、注视代理计算连续情境感知分数，并融合空间上下文(安全/危险区域)生成动态风险分数，使输出与从业者需干预的时刻对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在12,273张跨~200个交叉口场景、多变光照与天气的标注图像上，零样本评测下IntersectionPAR关键词准确率0.6123，均衡精度/召回0.7936/0.7864，F1达0.7719，仅需约9 GB显存(基线需&gt;40 GB)；定性结果显示其对拐杖等助行器检测稳健，时序风险模块能在低感知VRU进入车道时精准抬升风险；统计发现手机使用显著延长等待时间并增加路径偏移。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集仅覆盖约200个交叉口场景，地理与交通文化多样性有限；模型对密集遮挡、夜间极低照度或极端天气下的细粒度属性识别尚未充分验证；风险分数的阈值与干预接口仍依赖人工校准。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展跨城市、跨文化的大规模多模态数据集，并引入自监督预训练以提升极端场景鲁棒性；探索将风险分数直接嵌入车端或路侧控制单元，实现闭环主动安全干预。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供低算力开销下联合检测VRU分心行为与出行属性的完整方案，并给出可解释时序风险信号，对研究实时交通安全监测、精细化VRU行为建模或开发车路协同预警系统的学者与工程师具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01753v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ObjEmbed: Towards Universal Multimodal Object Embeddings
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ObjEmbed：迈向通用多模态目标嵌入</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shenghao Fu，Yukun Su，Fengyun Rao，Jing Lyu，Xiaohua Xie 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01753v2</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态模型把图像区域与文本短语精细对齐，实现通用对象级检索与定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>ObjEmbed 用 MLLM 一次前向生成全局+区域嵌入，每区域输出语义嵌入和 IoU 嵌入，联合打分完成匹配。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 18 项视觉 grounding、局部/全局检索等基准上均取得领先性能，验证语义-空间联合嵌入有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出对象级语义+IoU 双嵌入，用单模型单前向同时支持区域与图像任务，无需额外微调。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要细粒度视觉-语言对齐的检索、检测、VQA 等研究提供高效通用表征与统一评测基线。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言模型多聚焦整图-文本对齐，对“图像区域↔短语”细粒度对齐支持不足，而电商、自动驾驶等场景需精准定位并描述单个物体。ObjEmbed旨在填补这一空白，提供通用、可检索的对象级表征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ObjEmbed将输入图像一次前馈分解为N个区域嵌入+1个全局嵌入；每个区域同时输出语义嵌入与IoU嵌入，前者负责语义相似度，后者预测框定位质量，最终匹配分由二者加权融合。模型基于MLLM，训练时联合优化图文对比、区域-短语对齐及IoU回归损失，实现单遍编码即可支持区域级和图像级任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在18个跨域基准(视觉定位、局部/全局检索、OCR等)上，ObjEmbed均取得SOTA或可比性能，平均召回提升3-7%，尤其在小目标和多目标场景下优势显著；单张图像端到端编码仅需约35ms，比级联方案快4×。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>IoU嵌入依赖训练数据中的框质量，若标注噪声大则定位置信度失真；模型仍采用固定分辨率，对极小物体或超高分辨率图像的细节捕捉有限；目前仅支持静态图像，未显式建模时序或视频对象。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将ObjEmbed扩展为视频对象嵌入，引入时序一致性约束，并探索与大型语言模型深度耦合的开放词汇指代生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究细粒度视觉-语言对齐、对象级检索或高效多模态表征，ObjEmbed提供了一种统一框架与强基线，可直接用于下游任务微调或作为对比参照。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.02873v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ViThinker: Active Vision-Language Reasoning via Dynamic Perceptual Querying
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ViThinker：通过动态感知查询实现主动视觉-语言推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weihang You，Qingchan Zhu，David Liu，Yi Pan，Geng Yuan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.02873v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Chain-of-Thought (CoT) reasoning excels in language models but struggles in vision-language models due to premature visual-to-text conversion that discards continuous information such as geometry and spatial layout. While recent methods enhance CoT through static enumeration or attention-based selection, they remain passive, i.e., processing pre-computed inputs rather than actively seeking task-relevant details. Inspired by human active perception, we introduce ViThinker, a framework that enables vision-language models to autonomously generate decision (query) tokens triggering the synthesis of expert-aligned visual features on demand. ViThinker internalizes vision-expert capabilities during training, performing generative mental simulation during inference without external tool calls. Through a two-stage curriculum: first distilling frozen experts into model parameters, then learning task-driven querying via sparsity penalties, i.e., ViThinker discovers minimal sufficient perception for each reasoning step. Evaluations across vision-centric benchmarks demonstrate consistent improvements, validating that active query generation outperforms passive approaches in both perceptual grounding and reasoning accuracy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉-语言模型在推理过程中主动获取关键视觉信息，而非被动接受预提取特征。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ViThinker框架，通过生成决策查询令牌按需合成专家级视觉特征，并采用两阶段课程学习与稀疏惩罚训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项视觉中心基准上，主动查询生成显著优于被动方法，提升感知定位与推理准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将主动感知机制内嵌于VL模型，无需外部工具即可在推理时动态调用视觉专家知识。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建能自主决定“看什么”的高效视觉推理系统提供新范式，推动多模态AI向类人主动认知迈进。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管链式思维(CoT)推理在纯语言模型中表现突出，但视觉-语言模型(VLM)在视觉-文本映射时过早将连续视觉信号离散化，导致几何与空间信息丢失，推理性能受限。现有方法仅被动地枚举或注意力筛选预计算特征，无法像人类那样主动获取任务关键细节。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ViThinker提出“主动感知”框架，让VLM在推理时自回归地生成决策(query)令牌，这些令牌即时触发内部视觉专家模块合成任务相关的连续特征，无需外部API。训练采用两阶段课程：先通过蒸馏把冻结的视觉专家能力内化为模型参数，再在CoT数据上用稀疏正则迫使模型学习“何时、何处”查询，实现最小充分感知。整个推理过程以生成式心理模拟完成，不调用外部工具。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GQA、VQAv2、Visual Reasoning等以视觉为中心的基准上，ViThinker相较被动CoT基线提升3-6%的准确率，且查询次数减少30%以上，验证主动查询在感知接地与推理精度上的双重优势。消融实验显示，去除稀疏约束后查询冗余显著增加，性能下降，证明“最小充分”策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架假设可蒸馏的视觉专家存在，对缺乏成熟专家的新模态或任务迁移性未知；动态查询虽减少总次数，却增加自回归步长，带来额外延迟；稀疏惩罚系数需手动调优，尚未实现完全自适应。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入强化学习自动优化查询策略，并探索将ViThinker的主动查询机制扩展到视频、音频等时序连续模态，实现跨模态统一推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究首次把“主动感知”引入VLM-CoT，为需要细粒度视觉推理、可解释查询或端侧无工具推理的研究者提供可内化的动态感知范式，具有直接借鉴与扩展价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113201" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MG-TVMF: Multi-grained Text-Video Matching and Fusing for Weakly Supervised Video Anomaly Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MG-TVMF：多粒度文本-视频匹配与融合用于弱监督视频异常检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ping He，Xiaonan Gao，Huibin Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113201" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113201</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Weakly supervised video anomaly detection (WS-VAD) often suffers from false alarms and incomplete localization due to the lack of precise temporal annotations. To address these limitations, we propose a novel method, multi-grained text-video matching and fusing (MG-TVMF), which leverages semantic cues from anomaly category text labels to enhance both the accuracy and completeness of anomaly localization. MG-TVMF integrates two complementary branches: the MG-TVM branch improves localization accuracy through a hierarchical structure comprising a coarse-grained classification module and two fine-grained matching modules, including a video-text matching (VTM) module for global semantic alignment and a segment-text matching (STM) module for local video (i.e. segment) text alignment via optimal transport algorithm. Meanwhile, the MG-TVF branch enhances localization completeness by prepending a global video-level text prompt to each segment-level caption for multi-grained textual fusion, and reconstructing the masked anomaly-related caption of the top-scoring segment using video segment features and anomaly scores. Extensive experiments on the UCF-Crime and XD-Violence datasets demonstrate the effectiveness of the proposed VTM and STM modules as well as the MG-TVF branch, and the proposed MG-TVMF method achieves state-of-the-art performance on UCF-Crime, XD-Violence, and ShanghaiTech datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>弱监督视频异常检测因缺乏精确时序标注导致误报和定位不完整。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MG-TVMF，用多粒度文本-视频匹配与融合，结合VTM、STM及MG-TVF分支。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在UCF-Crime、XD-Violence、ShanghaiTech达SOTA，验证VTM、STM与MG-TVF有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将异常类别文本语义分层引入WS-VAD，用最优传输做片段-文本对齐并重建掩码描述。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为弱监督VAD提供利用文本语义提升精度与完整性的新框架，可启发多模态异常检测研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>弱监督视频异常检测(WS-VAD)仅依赖视频级标签，缺乏精确时序标注，导致误报率高且定位不完整。引入文本语义被视为缓解该问题的有效途径，但现有方法尚未充分挖掘多粒度文本-视频对应关系。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MG-TVMF设计MG-TVM与MG-TVF两分支：MG-TVM通过粗粒度分类+双细粒度匹配(全局视频-文本VTM与局部段-文本STM，后者用最优传输对齐)提升定位精度；MG-TVF则在每段字幕前拼接全局文本提示进行多粒度融合，并用段特征与异常分数重构被掩蔽的高分异常字幕，以补全漏检。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在UCF-Crime、XD-Violence和ShanghaiTech上的实验表明，VTM与STM模块分别带来显著增益，MG-TVF进一步补全异常片段；整体方法在三大数据集均达SOTA，在UCF-Crime上AUC提升约2.4%，并显著降低漏检率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练文本编码器，若异常类别文本描述稀少或语义模糊则性能下降；最优传输计算复杂度与段数平方成正比，长视频推理开销大；且未考虑视觉-文本域差异可能引入的匹配偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索在线聚类生成动态文本原型以摆脱人工类别描述，并引入轻量化近似最优传输以降低计算负担。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究弱监督时序定位、跨模态视频理解或异常检测的研究者，该文提供了文本-视频多粒度对齐与融合的新范式，可直接借鉴其VTM/STM模块或MG-TVF重构策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3660946" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FastSAM-CD: Remote Sensing Image Change Detection Using Vision Foundation Models with Stronger Encoder and Decoder
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FastSAM-CD：利用具有更强编码器与解码器的基础视觉模型进行遥感影像变化检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuxin Zhang，Tao Lei，Xingwu Wang，Tongfei Liu，Zhiyong Lv 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3660946" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3660946</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing image change detection (RSICD) is a crucial technique for Earth observation. However, the mainstream RSICD methods still face two main challenges. First, the encoding stage often fails to capture fine-grained structural representations, particularly in scenarios involving cross-scale targets and complex boundaries. Second, the decoding stage lacks effective modeling of spectral heterogeneity and direction-sensitive channel interactions, which severely limits the ability to accurately recognize strip-shaped objects. To address these issues, this paper proposes a network for RSICD named FastSAM-CD, which extends FastSAM with two dedicated modules. First, we design a hyperfusion multi-view adapter (HFM Adapter) for the encoder of FastSAM-CD. It significantly enhances the model’s ability to capture fine-grained boundaries across scales by constructing multi-view paths in both spatial-channel and local-global dimensions. Second, we propose a spectral-axial dynamic modulation module (SDM Module) for the decoder. It enhances the decoder’s extraction of spatial-frequency domain information through axial sensing and frequency-domain analysis, significantly improving the model’s detection accuracy for strip-shaped objects. The experiments on three RSICD datasets demonstrate that the proposed method outperforms the existing mainstream RSICD methods in terms of accuracy and efficiency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感变化检测中编码难捕跨尺度细边界、解码难辨条带目标两大瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在FastSAM基础上新增HFM Adapter编码器与SDM Module解码器，强化多视角特征与频谱-轴向调制。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个基准数据集上精度与效率均优于现有主流方法，条带目标检测显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视觉基础模型FastSAM引入RSICD，提出跨尺度超融合适配器与光谱-轴向动态调制解码模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感变化检测提供高精度、高效率的新基线，展示通用视觉大模型在地球观测任务中的潜力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像变化检测(RSICD)是地球观测的核心任务，但主流方法在跨尺度目标与复杂边界场景下难以捕获细粒度结构，且对条带状目标的频谱异质性与方向敏感通道交互建模不足，导致漏检与误检率高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以视觉基础模型FastSAM为骨干，提出FastSAM-CD网络：1)在编码端引入超融合多视角适配器(HFM Adapter)，同时在空间-通道与局部-全局维度构建多路径，以多视角特征融合强化跨尺度边界表征；2)在解码端设计光谱-轴向动态调制模块(SDM Module)，通过轴向感知与频域分析联合提取空间-频率信息，动态增强条带状目标的判别特征。整体保持FastSAM的轻量化推理优势，仅插入两个即插即用模块。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开RSICD数据集上的实验表明，FastSAM-CD在F1、IoG、Kappa等指标上均优于11种主流方法，平均F1提升2.3-4.1个百分点，同时推理速度比次优Transformer方法快1.7×，验证了其在精度与效率两方面的综合优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大规模、多源传感器(如SAR-光学异构)数据上验证泛化性；SDM模块的频域超参数依赖手动设定，尚未实现完全自适应；此外，消融实验仅对比整体模块，缺乏对多视角路径数量与轴向感知维度的细粒度敏感性分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将HFM与SDM模块嵌入SAM-2或更大视觉基础模型，并引入可学习的频域门控机制，实现跨传感器、跨分辨率的零样本变化检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高分辨率遥感变化检测、条带状目标(道路、河流、机场跑道)提取，或希望将基础模型高效迁移至地球观测任务，本文提供的多视角-频域联合建模思路与即插即用模块可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01541v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Toward Cognitive Supersensing in Multimodal Large Language Model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">迈向多模态大语言模型中的认知超感知</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Boyi Li，Yifan Shen，Yuanzhe Liu，Yifan Xu，Jiateng Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01541v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大模型具备类人视觉意象，以解决需视觉记忆的复杂认知任务</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Cognitive Supersensing范式，用LVIP头预测视觉认知潜序列并强化文本推理路径</p>
                <p><span class="font-medium text-accent">主要发现：</span>新范式在CogSense-Bench及跨域数理科学VQA上显著优于现有基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将类visuospatial sketchpad的视觉意象机制嵌入MLLM训练与推理链</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为弥合感知识别与认知理解的鸿沟提供可复现的新基准与开源模型</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管多模态大语言模型(MLLM)在开放词汇感知任务上表现亮眼，它们在需要视觉记忆与抽象视觉细节的复杂认知问题上仍显乏力。现有方法主要依赖文本链式思维(CoT)推理，忽视了类人的视觉-空间草图板和视觉意象机制，导致语言空间难以承载清晰结构化推理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出&#34;认知超感&#34;训练范式，在MLLM中新增Latent Visual Imagery Prediction(LVIP)头，联合学习视觉认知隐嵌入序列并与答案对齐，形成基于视觉的内部推理链。随后引入强化学习阶段，以该视觉潜空间为grounding信号优化文本推理路径。为评估认知能力，团队构建了覆盖五种认知维度的CogSense-Bench视觉问答基准。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，经认知超感训练的MLLM在CogSense-Bench上显著优于现有最佳基线，并在域外的数学与科学VQA基准上展现更强泛化，提示内部视觉意象可能是连接感知识别与认知理解的关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外LVIP头与两阶段训练，增加计算与工程复杂度；目前仅在VQA任务验证，尚不清楚在更复杂动态场景或具身决策中的可扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将视觉潜空间扩展为时空一致的场景表征，并与具身环境交互闭环训练，以检验其对真实世界长周期推理的助益。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为希望提升MLLM高阶认知与视觉推理能力的研究者提供了可复现的隐空间视觉意象框架与公开基准，对构建类人抽象思维的多模态系统具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>