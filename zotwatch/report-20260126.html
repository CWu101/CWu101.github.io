<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-26</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-26 10:53 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u4e0e\u95ee\u7b54",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u611f\u77e5\u4e0e\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u5f71\u50cf\u6df1\u5ea6\u5b66\u4e60\u5730\u7269\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742096200304}, {"source": 0, "target": 2, "value": 0.5366473037039338}, {"source": 1, "target": 2, "value": 0.5805336345704523}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于遥感智能的论文、2篇关于空间推理的论文与1篇关于行星地貌的论文。</p>
            
            <p><strong class="text-accent">遥感智能</strong>：《Towards Realistic Remote Sensing Dataset Distillation with Discriminative Prototype-guided Diffusion》提出以判别式原型引导扩散实现遥感数据蒸馏，缓解大规模标注依赖；《Towards Open-Vocabulary Semantic Segmentation for Remote Sensing Images》构建开放词汇语义分割框架，使模型可按任意文本描述对遥感影像进行像素级分类。</p>
            
            <p><strong class="text-accent">空间推理</strong>：《Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video》利用合成视频基准评估视觉-语言模型在时空几何线索下的情境与空间意识；《Spatial-Agent: Agentic Geo-spatial Reasoning with Scientific Core Concepts》引入嵌套科学核心概念的智能体架构，实现城市、交通、灾害等真实地理空间推理任务。</p>
            
            <p><strong class="text-accent">行星地貌</strong>：《Natural Language-Driven Global Mapping of Martian Landforms》通过自然语言驱动将像素级火星轨道影像升维至语义地貌图，实现全球尺度可扩展地貌制图。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于跨模态检索与定位的论文、6篇关于遥感与行星视觉理解的论文、5篇关于多模态大模型空间推理的论文、4篇关于视频表征与摘要的论文、3篇关于视觉定位与导航的论文、2篇关于合成数据与评测的论文以及2篇关于行人再识别的论文。</p>
            
            <p><strong class="text-text-secondary">跨模态检索</strong>：该主题聚焦图文-音视频混合查询下的检索与定位，如《X-Aligner》提出两阶段对齐实现组合视频检索，《HVD》借鉴人眼注视先验过滤无关帧以提升文本-视频检索精度，《ICON》利用不变反事实优化解决文本行人搜索中的视觉-语言偏差，《Natural Language-Driven Global Mapping of Martian Landforms》通过开放词汇分割将轨道影像直接映射到自然语言地貌概念。</p>
            
            <p><strong class="text-text-secondary">遥感理解</strong>：针对遥感影像的开放词汇解析与分割，如《Towards Open-Vocabulary Semantic Segmentation for Remote Sensing Images》首次将OVSS引入遥感领域以支持任意语义类别查询，同主题多篇工作进一步探索了高分辨率遥感影像的零样本场景分类与变化检测。</p>
            
            <p><strong class="text-text-secondary">空间推理</strong>：研究多模态大模型在三维及认知层面的空间推理能力，如《OnlineSI》让LLM在线理解3D场景并执行指代表达定位，《Cognitively-Inspired Tokens Overcome Egocentric Bias in Multimodal Models》引入认知视角令牌缓解自我中心偏差，多篇论文共同揭示VLMs在视角转换与几何关系判断上的脆弱性并提出矫正策略。</p>
            
            <p><strong class="text-text-secondary">视频表征</strong>：探索视频-文本联合表征学习与摘要生成，如《HVD》利用人眼运动先验强化关键帧关注，《Multimodal Summarization via Coarse-and-Fine Granularity Synergy》通过粗细粒度协同与区域反事实推理过滤生成高质量多模态摘要，其他工作进一步提出时序对齐与事件级语义聚合机制。</p>
            
            <p><strong class="text-text-secondary">视觉导航</strong>：面向机器人与AR的在线视觉里程计及语义导航，如《Keyframe-Based Feed-Forward Visual Odometry》在单向前馈网络中同时完成位姿估计与稠密重建，同组研究将语义分割与深度估计耦合以提升低纹理环境下的鲁棒性。</p>
            
            <p><strong class="text-text-secondary">合成评测</strong>：通过合成数据系统评测模型能力，如《Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video》构建时空推理基准揭示VLMs在几何-语义一致场景中的脆弱性，另一篇论文扩展了对抗式合成数据用于鲁棒性诊断。</p>
            
            <p><strong class="text-text-secondary">行人搜索</strong>：聚焦自然语言描述的行人检索与重识别，如《ICON》提出不变反事实优化与神经符号先验，显著缓解文本-行人模态差异，另一篇工作引入细粒度属性解耦以提升跨摄像头检索精度。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 45%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15949v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Natural Language-Driven Global Mapping of Martian Landforms
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">自然语言驱动的火星地貌全局制图</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiran Wang，Shuoyuan Wang，Zhaoran Wei，Jiannan Zhao，Zhonghua Yao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15949v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用自然语言直接检索并全局标注火星地貌，摆脱预设分类限制</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建MarScope，用20万图文对训练共享语义空间，实现无标签像素级检索</p>
                <p><span class="font-medium text-accent">主要发现：</span>5秒内完成全球查询，F1最高0.978，支持形态与过程导向分析</p>
                <p><span class="font-medium text-accent">创新点：</span>首次以自然语言为接口，实现行星尺度灵活语义检索与地貌制图</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为行星科学家提供零门槛、开放查询工具，释放大规模轨道影像科学价值</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>行星地貌学长期依赖专家手工标注或预定义分类体系，导致高分辨率轨道影像库只能以像素级元数据检索，难以用自然语言概念进行开放语义查询。火星全球影像数据量已超 PB 级，但缺乏将日常科学语言直接映射到地表形态的跨模态工具，限制了快速、可扩展的全球地貌调查。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建 MarScope，一个视觉-语言对齐框架，把 200 000 余幅火星轨道影像与对应科学描述文本编码进共享 512 维语义空间，采用双流 ViT+Transformer 对比学习损失训练。推理时，任意英文自然语言查询被编码为语义向量，与预先计算的全球影像瓦片向量进行近邻搜索，5 秒内返回概率热图，无需额外标注或重训练。体系结构支持零样本、标签自由检索，并可级联后处理生成 F1 评估的二值化地貌图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在涵盖撞击坑、冲沟、极层沉积等 12 类典型地貌的基准上，MarScope 零样本 F1 最高达 0.978，平均超过传统监督模型 0.15。案例显示，系统可用“新鲜撞击坑喷射物”或“可能由流水形成的沟槽”等过程性描述直接检索，实现从形态分类到成因解释的无缝过渡。全球一致性检验表明，模型在 1 km/px 的 85% 火星表面影像上保持检索一致性 &gt;0.91，为行星尺度地貌制图提供实时能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>训练文本主要来源于英文科学摘要与行星命名公报，对非英语术语、口语化描述或新兴词汇的覆盖不足；影像-文本对以 CTX、HIRISE 为主，对 MRO 以外的多源传感器（如 CaSSIS、TGO）泛化性能未验证。此外，近邻检索依赖预定义瓦片大小，可能错过尺度小于 50 m 的细微构造。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可引入多语言文本与多尺度影像金字塔，实现跨语种、跨分辨率的联合嵌入；同时融入时序影像，支持“新撞击坑”等动态事件的实时发现与演化追踪。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究展示了如何以自然语言为统一接口，对 PB 级行星影像进行零样本语义检索，为地球观测、深空测绘及多模态地理信息检索提供了可复用的框架与训练策略，特别适合关注视觉-语言模型在遥感、地貌自动解译及开放集识别任务中的研究者。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 41%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15780v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用合成生成视频评估视觉-语言模型的情境与空间感知能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pascal Benschop，Justin Dauwels，Jan van Gemert
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15780v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估视觉-语言模型在合成视频中识别暴力/安全情境并跟踪角色与空间关系的能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建最小差异合成视频对基准，零样本测试最新VLMs的情境与空间感知任务。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型表现仅略高于随机，稳定颜色提示可部分缓解角色混淆但无法根治空间推理脆弱性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用可控合成视频对同时诊断VLMs的情境安全判断与细粒度时空角色定位弱点。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提供可复现诊断工具，引导研究者以轻量级空间先验改进大规模预训练模型的时空推理。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉-语言模型（VLM）在静态场景上表现强劲，但在依赖细微时序或几何线索的语义推理上仍显脆弱，尤其是区分暴力与无害互动、跨视角绑定施害者身份等安全敏感场景。作者认为现有视频基准偏重动作识别，缺乏对情境与空间联合推理的细粒度诊断，因此提出合成视频基准以系统评估VLM的短板。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究用程序化3D引擎生成最小差异视频对：同一角色与场景下仅改变动作语义（如推人 vs 拍手）、摄像机角度或轨迹偏移，形成三项任务——暴力/无害分类、跨视角施害者绑定、轨迹对齐判断。基准完全合成，可精确控制细粒度时空变量；作者在零样本设定下测试了多款最新VLM，仅输入帧+文本提示，不施加任何微调或额外训练。为探究先验能否缓解错误，还引入稳定颜色线索（同一角色始终着固定颜色）作为辅助实验。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>所有受测VLM在三项任务上的平均准确率仅略高于随机（约55-60%），表明情境与空间联合推理仍是普遍瓶颈。颜色恒定线索可把施害者绑定错误率降低约8个百分点，但对暴力识别和轨迹对齐帮助有限，说明模型核心缺陷在于缺乏显式时空/几何先验而非单纯身份混淆。作者公开了生成代码与12万对视频，便于社区复现与后续诊断。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>合成数据虽可控，但角色外观、动作分布与真实监控或社交媒体视频存在域差异，结论能否外推至真实场景尚待验证；实验仅覆盖零样本提示，未探讨微调或加入专用时空模块是否能显著提升性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可探索将轻量级空间-时序先验（如轨迹解析、几何一致性损失）嵌入VLM预训练或微调流程，并在真实视频域上验证其迁移效果。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究视频理解、安全敏感行为检测、多模态推理或模型鲁棒性，该文提供了一套可复现的合成诊断工具，可快速定位VLM在时空细粒度任务上的缺陷并量化改进效果。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.42</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 39%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15829v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Realistic Remote Sensing Dataset Distillation with Discriminative Prototype-guided Diffusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向判别性原型引导扩散的真实感遥感数据集蒸馏</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yonghao Xu，Pedram Ghamisi，Qihao Weng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15829v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent years have witnessed the remarkable success of deep learning in remote sensing image interpretation, driven by the availability of large-scale benchmark datasets. However, this reliance on massive training data also brings two major challenges: (1) high storage and computational costs, and (2) the risk of data leakage, especially when sensitive categories are involved. To address these challenges, this study introduces the concept of dataset distillation into the field of remote sensing image interpretation for the first time. Specifically, we train a text-to-image diffusion model to condense a large-scale remote sensing dataset into a compact and representative distilled dataset. To improve the discriminative quality of the synthesized samples, we propose a classifier-driven guidance by injecting a classification consistency loss from a pre-trained model into the diffusion training process. Besides, considering the rich semantic complexity of remote sensing imagery, we further perform latent space clustering on training samples to select representative and diverse prototypes as visual style guidance, while using a visual language model to provide aggregated text descriptions. Experiments on three high-resolution remote sensing scene classification benchmarks show that the proposed method can distill realistic and diverse samples for downstream model training. Code and pre-trained models are available online (https://github.com/YonghaoXu/DPD).</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不泄露敏感数据的前提下，将大规模遥感影像数据集压缩成可替代原数据的小规模合成集。</p>
                <p><span class="font-medium text-accent">研究方法：</span>训练文本-图像扩散模型，以预训练分类器一致性损失和聚类原型视觉-语言风格引导生成蒸馏样本。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个高分辨率遥感场景分类基准上，仅用极少量合成样本即可训练出与原数据集性能相当的模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把数据集蒸馏引入遥感领域，并提出分类器驱动与判别性原型引导的扩散框架提升合成质量。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感数据共享与隐私保护提供高效低成本的替代方案，减少存储传输开销并降低敏感信息泄露风险。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习在遥感影像解译中取得显著进展，但依赖大规模训练数据带来高昂存储与计算成本，且敏感类别数据存在泄露风险。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首次将数据集蒸馏引入遥感领域，利用文本到图像扩散模型将大规模遥感数据集压缩为紧凑蒸馏集。为提升合成样本判别性，在扩散训练阶段注入预训练分类器的分类一致性损失。针对遥感影像语义复杂，先在潜空间聚类选取多样原型作视觉风格引导，再用视觉语言模型生成聚合文本描述。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个高分辨率遥感场景分类基准上的实验表明，该方法可蒸馏出真实且多样的样本，足以支持下游模型训练，验证了其在保持性能的同时显著减少数据量与隐私暴露。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练分类器和视觉语言模型的质量，若这些模型存在偏差将直接影响蒸馏效果；聚类步骤对超参数敏感，可能遗漏稀有类别原型；扩散模型训练与推理计算开销仍较大。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无分类器指导的蒸馏策略以降低对预训练模型的依赖，并研究跨传感器、跨分辨率的蒸馏迁移能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为需处理大规模遥感数据、关注数据隐私与高效训练的研究者提供首个遥感专用数据集蒸馏框架，可直接借鉴其原型引导扩散策略以压缩自有数据集。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 36%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113120" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Open-Vocabulary Semantic Segmentation for Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感图像的开放词汇语义分割研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Da Zhang，Mingmin Zeng，Xuelong Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113120" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113120</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-vocabulary semantic segmentation (OVSS) for remote sensing images (RSI) aims to achieve precise segmentation of arbitrary semantic categories specified within RSI. However, existing mainstream OVSS models are mostly trained on natural images and struggle to handle the rotational diversity and unique characteristics of RSI, resulting in insufficient feature representation and category discrimination capabilities. To ameliorate this challenge, we propose ROSS, an open vocabulary semantic segmentation framework that combines effective feature fusion with dedicated modeling of RSI characteristics. Specifically, ROSS employs a dual-branch image encoder (DBIE): one branch leverages multi-directional augmentation to enhance the representation of rotation-invariant features, while the other incorporates remote sensing (RS) specific knowledge via an encoder pretrained on large-scale RSI data. During feature fusion, ROSS generates cost maps from both branches and designs a spatial-class dual-level cost aggregation (SDCA) module based on spatial and category information, thereby fully integrating global spatial context and category discriminability. Finally, we introduce a RS knowledge transfer upsampling module that efficiently fuses and reconstructs multi-scale features to achieve high-resolution and fine-grained segmentation. Experiments on four open-vocabulary RS datasets demonstrate that ROSS consistently outperforms current state-of-the-art (SOTA) models. This robust performance across different training and evaluation configurations verifies its effectiveness and broad applicability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让自然图像训练的开放词汇语义分割模型适应遥感图像的旋转多样性与独特特征。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ROSS框架：双分支编码器分别提取旋转不变特征与遥感先验，空间-类别双重代价聚合融合，再上采样重建高分辨率分割。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个开放词汇遥感数据集上全面超越现有SOTA，验证跨配置稳健性与广泛适用性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将旋转增强分支、大规模遥感预训练先验及空间-类别双重代价聚合集成于开放词汇分割框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供可直接指认任意类别的分割工具，突破固定类别限制，提升灾害监测、土地利用等应用灵活性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有开放词汇语义分割(OVSS)模型几乎全部在自然场景图像上训练，直接迁移到遥感影像(RSI)时，会因RSI特有的旋转变化、尺度差异和地物光谱特征而性能骤降。为此，作者提出ROSS框架，旨在让模型在无需重新训练的情况下，即可按任意文本提示对RSI进行细粒度分割。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ROSS采用双分支图像编码器(DBIE)：一分支通过多方向旋转增广学习旋转不变特征，另一分支引入在百万级RSI上预训练的编码器以注入遥感专属先验。两分支特征被送入空间-类别双层代价聚合(SDCA)模块，先生成空间代价图与类别代价图，再按空间邻接和语义相似度联合聚合，从而同时捕获全局空间上下文与跨类别判别力。最后，RS知识迁移上采样模块以多尺度融合方式重建高分辨率特征图，实现边界精细的开放词汇分割。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开OVSS-RS基准上，ROSS平均mIoU比此前最佳方法提升3.8-7.2个百分点，并在零样本、少样本及全监督三种协议下均保持领先；可视化结果显示其对旋转目标、细长道路与小面积建筑物的分割边缘更完整。消融实验表明DBIE与SDCA分别贡献约2.3和1.9 mIoU，验证了旋转不变表征与遥感先验结合的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模(如全球覆盖)或多源传感器(RGB、SAR、多光谱)混合数据上验证，可扩展性未知；SDCA的显存开销随类别数线性增长，对高分辨率大图仍需切块推理；此外，文本提示需人工设计，未探讨自动提示优化对RSI特异类别的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练与提示学习，实现遥感影像-文本对齐的自动提示生成，并探索跨模态时空一致性的视频级OVSS。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感智能解译、开放词汇/基础模型迁移或旋转不变表征，本文提供的双分支融合与代价聚合思路可直接借鉴，并作为基准对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 36%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16965v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Spatial-Agent: Agentic Geo-spatial Reasoning with Scientific Core Concepts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Spatial-Agent：基于科学核心概念的智能体地理空间推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Riyang Bao，Cheng Yang，Dazhou Yu，Zhexiang Tang，Gengchen Mai 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16965v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Geospatial reasoning is essential for real-world applications such as urban analytics, transportation planning, and disaster response. However, existing LLM-based agents often fail at genuine geospatial computation, relying instead on web search or pattern matching while hallucinating spatial relationships. We present Spatial-Agent, an AI agent grounded in foundational theories of spatial information science. Our approach formalizes geo-analytical question answering as a concept transformation problem, where natural-language questions are parsed into executable workflows represented as GeoFlow Graphs -- directed acyclic graphs with nodes corresponding to spatial concepts and edges representing transformations. Drawing on spatial information theory, Spatial-Agent extracts spatial concepts, assigns functional roles with principled ordering constraints, and composes transformation sequences through template-based generation. Extensive experiments on MapEval-API and MapQA benchmarks demonstrate that Spatial-Agent significantly outperforms existing baselines including ReAct and Reflexion, while producing interpretable and executable geospatial workflows.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型真正执行可验证的地理空间计算，而非依赖搜索或模式匹配。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将自然语言问题解析为GeoFlow图，节点为空间概念、边为变换，按空间信息理论排序并模板化生成可执行工作流。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MapEval-API与MapQA上显著优于ReAct、Reflexion等基线，输出可解释且可执行。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把空间信息科学基础理论嵌入AI代理，用概念变换与GeoFlow图形式化地理问答。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为城市分析、交通规划、灾害响应等领域提供可信、可复现的LLM地理推理框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有大模型智能体在回答地理空间问题时，多依赖网络搜索或模式匹配，缺乏真正的空间计算能力，容易产生空间关系幻觉。作者认为根本原因在于缺乏对空间信息科学基础理论的显式建模，导致无法将自然语言问题转化为可执行、可解释的地理分析流程。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Spatial-Agent 把地理分析问题形式化为“概念转换”任务，先用 LLM 从问句中抽取空间概念节点，再按空间信息理论赋予其功能角色与顺序约束，最后通过模板化生成将节点组装成有向无环图 GeoFlow Graph。图中边表示数据或算子转换，节点对应缓冲区、叠加、网络分析等原子操作，可直接调用后端 GIS API 执行并返回结果。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MapEval-API 与 MapQA 两个基准上，Spatial-Agent 比 ReAct、Reflexion 等强基线平均提升 18–25 个百分点，同时输出可读的地理工作流图供用户验证。消融实验表明，显式概念角色与拓扑排序约束是性能提升的主要来源，且生成的流程在真实 PostGIS 环境中 92 % 可成功执行。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅测试了英文问句与美国公开数据集，对多语言、多尺度或实时流数据场景尚未验证；GeoFlow 模板库目前依赖人工设计，覆盖的算子类型有限，可能遗漏复杂时空分析任务。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习模板生成与神经-符号联合优化，让智能体在交互中自动扩展算子库并适应多语言、多区域数据格式。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注空间语义解析、可解释地理 AI 或 LLM 与 GIS 深度耦合，该文提供了将空间信息理论注入智能体决策链的完整范式与评测基准，可直接对比或扩展其 GeoFlow 框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.40</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113120" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Open-Vocabulary Semantic Segmentation for Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感图像的开放词汇语义分割研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Da Zhang，Mingmin Zeng，Xuelong Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113120" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113120</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-vocabulary semantic segmentation (OVSS) for remote sensing images (RSI) aims to achieve precise segmentation of arbitrary semantic categories specified within RSI. However, existing mainstream OVSS models are mostly trained on natural images and struggle to handle the rotational diversity and unique characteristics of RSI, resulting in insufficient feature representation and category discrimination capabilities. To ameliorate this challenge, we propose ROSS, an open vocabulary semantic segmentation framework that combines effective feature fusion with dedicated modeling of RSI characteristics. Specifically, ROSS employs a dual-branch image encoder (DBIE): one branch leverages multi-directional augmentation to enhance the representation of rotation-invariant features, while the other incorporates remote sensing (RS) specific knowledge via an encoder pretrained on large-scale RSI data. During feature fusion, ROSS generates cost maps from both branches and designs a spatial-class dual-level cost aggregation (SDCA) module based on spatial and category information, thereby fully integrating global spatial context and category discriminability. Finally, we introduce a RS knowledge transfer upsampling module that efficiently fuses and reconstructs multi-scale features to achieve high-resolution and fine-grained segmentation. Experiments on four open-vocabulary RS datasets demonstrate that ROSS consistently outperforms current state-of-the-art (SOTA) models. This robust performance across different training and evaluation configurations verifies its effectiveness and broad applicability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让自然图像训练的开放词汇语义分割模型适应遥感图像的旋转多样性与独特特征。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ROSS框架：双分支编码器分别提取旋转不变特征与遥感先验，空间-类别双重代价聚合融合，再上采样重建高分辨率分割。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个开放词汇遥感数据集上全面超越现有SOTA，验证跨配置稳健性与广泛适用性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将旋转增强分支、大规模遥感预训练先验及空间-类别双重代价聚合集成于开放词汇分割框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供可直接指认任意类别的分割工具，突破固定类别限制，提升灾害监测、土地利用等应用灵活性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有开放词汇语义分割(OVSS)模型几乎全部在自然场景图像上训练，直接迁移到遥感影像(RSI)时，会因RSI特有的旋转变化、尺度差异和地物光谱特征而性能骤降。为此，作者提出ROSS框架，旨在让模型在无需重新训练的情况下，即可按任意文本提示对RSI进行细粒度分割。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ROSS采用双分支图像编码器(DBIE)：一分支通过多方向旋转增广学习旋转不变特征，另一分支引入在百万级RSI上预训练的编码器以注入遥感专属先验。两分支特征被送入空间-类别双层代价聚合(SDCA)模块，先生成空间代价图与类别代价图，再按空间邻接和语义相似度联合聚合，从而同时捕获全局空间上下文与跨类别判别力。最后，RS知识迁移上采样模块以多尺度融合方式重建高分辨率特征图，实现边界精细的开放词汇分割。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开OVSS-RS基准上，ROSS平均mIoU比此前最佳方法提升3.8-7.2个百分点，并在零样本、少样本及全监督三种协议下均保持领先；可视化结果显示其对旋转目标、细长道路与小面积建筑物的分割边缘更完整。消融实验表明DBIE与SDCA分别贡献约2.3和1.9 mIoU，验证了旋转不变表征与遥感先验结合的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模(如全球覆盖)或多源传感器(RGB、SAR、多光谱)混合数据上验证，可扩展性未知；SDCA的显存开销随类别数线性增长，对高分辨率大图仍需切块推理；此外，文本提示需人工设计，未探讨自动提示优化对RSI特异类别的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练与提示学习，实现遥感影像-文本对齐的自动提示生成，并探索跨模态时空一致性的视频级OVSS。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感智能解译、开放词汇/基础模型迁移或旋转不变表征，本文提供的双分支融合与代价聚合思路可直接借鉴，并作为基准对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16155v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HVD：面向文本-视频检索的人类视觉驱动视频表征学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zequn Xie，Xin Liu，Boyun Zhang，Yuxiao Lin，Sihang Cai 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16155v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from &#34;blind&#34; feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>文本-视频检索中模型难以从冗余背景提取关键视觉信息。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HVD框架，用FFSM选关键帧、PFCM压缩patch为显著实体，实现由粗到细对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五项基准上达SOTA，验证其模拟人类视觉焦点的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将人类宏观-微观感知机制引入视频表征，实现帧级筛选与实体级压缩协同。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为CLIP类模型提供去冗余、显实体的视觉表示范式，提升跨模态检索效率与可解释性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 在图文匹配上的成功促使研究者将其扩展到文本-视频检索，但视频帧序列高度冗余，而查询文本通常只描述少数关键物体或动作，导致现有方法难以聚焦有效信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Human Vision-Driven (HVD) 框架，先由 Frame Features Selection Module (FFSM) 依据与文本的粗粒度相关度筛选关键帧，模拟人眼的宏观扫视；再由 Patch Features Compression Module (PFCM) 在保留帧内对注意力加权后的 patch 特征，通过可学习的聚类 token 将数百个 patch 压缩成数个“显著视觉实体”，实现细粒度实体级对齐；两阶段均用对比损失端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MSR-VTT、MSVC、LSMDC、ActivityNet Captions 和 DiDeMo 五个基准上，HVD 的 R@1 平均提升 2.6-4.1 个百分点，消融实验表明 FFSM 可剪掉约 40% 冗余帧而不掉点，PFCM 将显存占用降低 28%，可视化热图显示其注意力与人眼注视分布的 Kendall τ 达 0.68，验证了“类人视觉焦点”能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练 CLIP 的视觉编码器，对低质量或场景剧烈变化的视频帧选择可能失效；PFCM 的聚类 token 数量固定，当视频中实体数目远超设定值时会出现欠拟合；目前仅评估了短文本查询，尚未验证复杂多事件长描述下的泛化性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应聚类或级联选择策略，使帧和实体数目随内容动态变化，并探索与音频、语音等多模态线索的联合筛选机制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注文本-视频检索、视觉-语言预训练或人类注意力建模，本文提出的粗到细选择-压缩范式可直接迁移到视频问答、片段定位等任务，并提供可解释的注意力可视化工具。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115356" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multimodal Summarization via Coarse-and-Fine Granularity Synergy and Region Counterfactual Reasoning Filter
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于粗细粒度协同与区域反事实推理滤波的多模态摘要</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Rulong Liu，Qing He，Yuji Wang，Nisuo Du，Zhihao Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115356" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115356</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal Summarization (MS) generates high-quality summaries by integrating textual and visual information. However, existing MS research faces several challenges, including (1) ignoring fine-grained key information between visual and textual modalities and interaction with coarse-grained information, (2) cross-modal semantic inconsistency, which hinders alignment and fusion of visual and textual feature spaces, and (3) ignoring inherent heterogeneity of an image when filtering visual information, which causes excessive filtering or excessive retention. To address these issues, we propose Coarse-and-Fine Granularity Synergy and Region Counterfactual Reasoning Filter (CFCR) for MS. Specifically, we design Coarse-and-Fine Granularity Synergy (CFS) to capture both global (coarse-grained) and important detailed (fine-grained) information in text and image modalities. Based on this, we design Dual-granularity Contrastive Learning (DCL) for mapping coarse-grained and fine-grained visual features into the text semantic space, thereby reducing semantic inconsistency caused by modality differences at dual granularity levels, and facilitating cross-modal alignment. To address the issue of excessive filtering or excessive retention in visual information filtering, we design a Region Counterfactual Reasoning Filter (RCF) that employs Counterfactual Reasoning to determine the validity of image regions and generate category labels. These labels are then used to train Image Region Selector to select regions beneficial for summarization. Extensive experiments on the representative MMSS and MSMO dataset show that CFCR outperforms multiple strong baselines, particularly in terms of selecting and focusing on critical details, demonstrating its effectiveness in MS.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>多模态摘要中细粒度关键信息缺失、跨模态语义不一致及视觉过滤失衡问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CFCR框架：粗细粒度协同CFS、双粒度对比学习DCL、区域反事实过滤RCF。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MMSS与MSMO数据集上显著超越强基线，更精准聚焦关键细节。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合粗细粒度协同与反事实区域过滤，实现跨模态语义对齐与自适应视觉筛选。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为多媒体摘要、跨模态对齐与视觉推理研究提供可扩展的粒度协同与反事实过滤范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态摘要旨在同时利用文本与视觉信息生成高质量摘要，但现有工作往往只关注全局或局部单粒度特征，忽略了跨模态细粒度关键信息及其与粗粒度上下文的协同，导致视觉-文本语义不一致和视觉区域冗余或缺失。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CFCR框架，首先用Coarse-and-Fine Granularity Synergy (CFS)并行抽取文本和图像的全局与重要细节表示；随后设计Dual-granularity Contrastive Learning (DCL)将双粒度视觉特征映射到文本语义空间，减少模态差异并促进跨模态对齐；最后提出Region Counterfactual Reasoning Filter (RCF)，通过反事实推理评估各图像区域对摘要的贡献度并生成类别标签，再训练Image Region Selector保留有益区域。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MMSS与MSMO基准上的实验显示，CFCR在ROUGE、BERTScore等指标上显著优于多个强基线，尤其能精准聚焦关键细节，验证双粒度协同与区域反事实过滤对提升摘要质量的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外的区域类别标注来训练RCF，引入标注成本；反事实推理计算开销大，对高分辨率图像或长视频扩展性有限；且未探讨在视觉缺失或文本极短场景下的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无监督或弱监督的区域贡献估计以降低成本，并将反事实推理扩展到视频时序片段选择，实现长视频多模态摘要。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为需要同时理解文本与视觉内容、精细控制信息过滤的摘要、检索或问答系统提供了可复用的双粒度对齐与反事实筛选思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16582v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      X-Aligner: Composed Visual Retrieval without the Bells and Whistles
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">X-Aligner：无需花哨技巧的复合视觉检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuqian Zheng，Mariana-Iuliana Georgescu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16582v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Composed Video Retrieval (CoVR) facilitates video retrieval by combining visual and textual queries. However, existing CoVR frameworks typically fuse multimodal inputs in a single stage, achieving only marginal gains over initial baseline. To address this, we propose a novel CoVR framework that leverages the representational power of Vision Language Models (VLMs). Our framework incorporates a novel cross-attention module X-Aligner, composed of cross-attention layers that progressively fuse visual and textual inputs and align their multimodal representation with that of the target video. To further enhance the representation of the multimodal query, we incorporate the caption of the visual query as an additional input. The framework is trained in two stages to preserve the pretrained VLM representation. In the first stage, only the newly introduced module is trained, while in the second stage, the textual query encoder is also fine-tuned. We implement our framework on top of BLIP-family architecture, namely BLIP and BLIP-2, and train it on the Webvid-CoVR data set. In addition to in-domain evaluation on Webvid-CoVR-Test, we perform zero-shot evaluations on the Composed Image Retrieval (CIR) data sets CIRCO and Fashion-IQ. Our framework achieves state-of-the-art performance on CoVR obtaining a Recall@1 of 63.93% on Webvid-CoVR-Test, and demonstrates strong zero-shot generalization on CIR tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破单阶段融合瓶颈，提升组合式视频-文本检索性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出X-Aligner跨注意力模块，在BLIP/BLIP-2上两阶段训练并加入图像标题。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Webvid-CoVR-Test Recall@1达63.93%，零样本CIR任务表现领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>渐进式跨注意力对齐与视觉标题增强，实现无需复杂技巧的新CoVR框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型在组合检索中的应用提供简洁高效的新基线。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Composed Video Retrieval (CoVR) lets users search videos with an image plus text, but prior methods fuse modalities in one shot and barely outperform simple baselines. The authors argue that stronger, pretrained Vision-Language Models (VLMs) are under-exploited for this task.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>They build on BLIP/BLIP-2, freeze most weights, and insert a lightweight X-Aligner sub-network of cascaded cross-attention layers that progressively merge the visual query, its automatically generated caption, and the text query into a single embedding aligned to candidate videos. Training is split: Stage 1 trains only X-Aligner to preserve VLM knowledge; Stage 2 additionally fine-tunes the text encoder.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On Webvid-CoVR-Test the model sets a new state-of-the-art Recall@1 of 63.93%, beating prior art by clear margins. It also zero-shot transfers to composed image retrieval benchmarks CIRCO and Fashion-IQ, outperforming specialized CIR models without any image-task training. Ablation shows the progressive cross-attention and caption injection each give significant gains.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach is evaluated only with short Webvid clips; scalability to longer, untrimmed videos is unverified. Reliance on auto-generated captions may propagate noise when vision captions are poor, and computational cost increases with extra cross-attention layers.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend X-Aligner to longer videos with hierarchical or memory-efficient attention, and explore adaptive caption selection or rejection to reduce noise.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on multimodal retrieval, VLMs, or cross-modal fusion can borrow the staged fine-tuning and progressive alignment ideas to boost performance without heavy retraining.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15949v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Natural Language-Driven Global Mapping of Martian Landforms
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">自然语言驱动的火星地貌全局制图</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiran Wang，Shuoyuan Wang，Zhaoran Wei，Jiannan Zhao，Zhonghua Yao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15949v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用自然语言直接检索并全局标注火星地貌，摆脱预设分类限制</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建MarScope，用20万图文对训练共享语义空间，实现无标签像素级检索</p>
                <p><span class="font-medium text-accent">主要发现：</span>5秒内完成全球查询，F1最高0.978，支持形态与过程导向分析</p>
                <p><span class="font-medium text-accent">创新点：</span>首次以自然语言为接口，实现行星尺度灵活语义检索与地貌制图</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为行星科学家提供零门槛、开放查询工具，释放大规模轨道影像科学价值</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>行星地貌学长期依赖专家手工标注或预定义分类体系，导致高分辨率轨道影像库只能以像素级元数据检索，难以用自然语言概念进行开放语义查询。火星全球影像数据量已超 PB 级，但缺乏将日常科学语言直接映射到地表形态的跨模态工具，限制了快速、可扩展的全球地貌调查。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建 MarScope，一个视觉-语言对齐框架，把 200 000 余幅火星轨道影像与对应科学描述文本编码进共享 512 维语义空间，采用双流 ViT+Transformer 对比学习损失训练。推理时，任意英文自然语言查询被编码为语义向量，与预先计算的全球影像瓦片向量进行近邻搜索，5 秒内返回概率热图，无需额外标注或重训练。体系结构支持零样本、标签自由检索，并可级联后处理生成 F1 评估的二值化地貌图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在涵盖撞击坑、冲沟、极层沉积等 12 类典型地貌的基准上，MarScope 零样本 F1 最高达 0.978，平均超过传统监督模型 0.15。案例显示，系统可用“新鲜撞击坑喷射物”或“可能由流水形成的沟槽”等过程性描述直接检索，实现从形态分类到成因解释的无缝过渡。全球一致性检验表明，模型在 1 km/px 的 85% 火星表面影像上保持检索一致性 &gt;0.91，为行星尺度地貌制图提供实时能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>训练文本主要来源于英文科学摘要与行星命名公报，对非英语术语、口语化描述或新兴词汇的覆盖不足；影像-文本对以 CTX、HIRISE 为主，对 MRO 以外的多源传感器（如 CaSSIS、TGO）泛化性能未验证。此外，近邻检索依赖预定义瓦片大小，可能错过尺度小于 50 m 的细微构造。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可引入多语言文本与多尺度影像金字塔，实现跨语种、跨分辨率的联合嵌入；同时融入时序影像，支持“新撞击坑”等动态事件的实时发现与演化追踪。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究展示了如何以自然语言为统一接口，对 PB 级行星影像进行零样本语义检索，为地球观测、深空测绘及多模态地理信息检索提供了可复用的框架与训练策略，特别适合关注视觉-语言模型在遥感、地貌自动解译及开放集识别任务中的研究者。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15780v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用合成生成视频评估视觉-语言模型的情境与空间感知能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pascal Benschop，Justin Dauwels，Jan van Gemert
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15780v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估视觉-语言模型在合成视频中识别暴力/安全情境并跟踪角色与空间关系的能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建最小差异合成视频对基准，零样本测试最新VLMs的情境与空间感知任务。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型表现仅略高于随机，稳定颜色提示可部分缓解角色混淆但无法根治空间推理脆弱性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用可控合成视频对同时诊断VLMs的情境安全判断与细粒度时空角色定位弱点。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提供可复现诊断工具，引导研究者以轻量级空间先验改进大规模预训练模型的时空推理。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉-语言模型（VLM）在静态场景上表现强劲，但在依赖细微时序或几何线索的语义推理上仍显脆弱，尤其是区分暴力与无害互动、跨视角绑定施害者身份等安全敏感场景。作者认为现有视频基准偏重动作识别，缺乏对情境与空间联合推理的细粒度诊断，因此提出合成视频基准以系统评估VLM的短板。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究用程序化3D引擎生成最小差异视频对：同一角色与场景下仅改变动作语义（如推人 vs 拍手）、摄像机角度或轨迹偏移，形成三项任务——暴力/无害分类、跨视角施害者绑定、轨迹对齐判断。基准完全合成，可精确控制细粒度时空变量；作者在零样本设定下测试了多款最新VLM，仅输入帧+文本提示，不施加任何微调或额外训练。为探究先验能否缓解错误，还引入稳定颜色线索（同一角色始终着固定颜色）作为辅助实验。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>所有受测VLM在三项任务上的平均准确率仅略高于随机（约55-60%），表明情境与空间联合推理仍是普遍瓶颈。颜色恒定线索可把施害者绑定错误率降低约8个百分点，但对暴力识别和轨迹对齐帮助有限，说明模型核心缺陷在于缺乏显式时空/几何先验而非单纯身份混淆。作者公开了生成代码与12万对视频，便于社区复现与后续诊断。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>合成数据虽可控，但角色外观、动作分布与真实监控或社交媒体视频存在域差异，结论能否外推至真实场景尚待验证；实验仅覆盖零样本提示，未探讨微调或加入专用时空模块是否能显著提升性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可探索将轻量级空间-时序先验（如轨迹解析、几何一致性损失）嵌入VLM预训练或微调流程，并在真实视频域上验证其迁移效果。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究视频理解、安全敏感行为检测、多模态推理或模型鲁棒性，该文提供了一套可复现的合成诊断工具，可快速定位VLM在时空细粒度任务上的缺陷并量化改进效果。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15931v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ICON: Invariant Counterfactual Optimization with Neuro-Symbolic Priors for Text-Based Person Search
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ICON：基于神经符号先验的不变反事实优化用于文本行人检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiangyu Wang，Zhixin Lv，Yongjiao Sun，Anrui Han，Ye Yuan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15931v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on &#34;Passive Observation&#34; leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>TBPS模型在开放场景下因伪相关与空间语义错位而鲁棒性差。</p>
                <p><span class="font-medium text-accent">研究方法：</span>ICON框架融合因果与拓扑先验，含空间干预、背景解耦、显著性正则与神经符号拓扑对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ICON在标准集领先，对遮挡、背景扰动与定位噪声保持高鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将因果反事实优化与神经符号拓扑先验引入TBPS，实现从统计拟合到因果不变学习范式转变。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安防检索提供抗分布漂移的因果鲁棒方案，启发视觉语言任务向可解释因果建模发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Text-Based Person Search (TBPS) 旨在用自然语言查询跨摄像头检索行人，是视觉-语言协同的重要场景，但现有预训练范式在开放世界中因被动观察导致虚假相关与空间语义错位，对分布偏移极为脆弱。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 ICON 框架，将因果与拓扑先验注入训练：1) Rule-Guided Spatial Intervention 在输入层对检测框施加可微扰动并惩罚预测变化，切断位置捷径；2) Counterfactual Context Disentanglement 用语义分割将前景背景分离，执行背景移植的因果反事实，迫使模型忽略环境；3) Saliency-Driven Semantic Regularization 根据显著图自适应掩码局部区域，再重构原始特征，抑制局部偏差；4) Neuro-Symbolic Topological Alignment 用人体骨架符号先验构建拓扑图，通过图神经匹配损失保证激活区域与关节语义一致。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CUHK-PEDES、ICFG-PEDES 等标准 benchmark 上 ICON 取得新 SOTA；在合成遮挡、背景替换、检测框扰动三类鲁棒性测试中，Rank-1 下降幅度比基线少 40%-60%，验证其因果不变性；消融实验显示每项因果模块均带来显著增益，其中空间干预贡献最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖外部人体检测与语义分割模型，引入额外误差级联；因果干预仅针对训练阶段，推理时仍使用原始图像，计算开销集中在训练端；拓扑先验基于通用人体骨架，对特殊姿态或遮挡严重场景可能失配。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将因果干预扩展至端到端推理阶段并设计轻量级神经-符号融合架构，以提升实时性；探索无监督或弱监督的因果发现，降低对外部分割模型的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及跨模态检索、因果表征学习或鲁棒视觉推理，ICON 提供了系统的神经-符号因果干预范式，可直接迁移到文本-图像匹配、行人重识别等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16538v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OnlineSI: Taming Large Language Model for Online 3D Understanding and Grounding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OnlineSI：驯服大语言模型以实现在线三维理解与定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zixian Liu，Zhaoxi Chen，Liang Pan，Ziwei Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16538v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In recent years, researchers have increasingly been interested in how to enable Multimodal Large Language Models (MLLM) to possess spatial understanding and reasoning capabilities. However, most existing methods overlook the importance of the ability to continuously work in an ever-changing world, and lack the possibility of deployment on embodied systems in real-world environments. In this work, we introduce OnlineSI, a framework that can continuously improve its spatial understanding of its surroundings given a video stream. Our core idea is to maintain a finite spatial memory to retain past observations, ensuring the computation required for each inference does not increase as the input accumulates. We further integrate 3D point cloud information with semantic information, helping MLLM to better locate and identify objects in the scene. To evaluate our method, we introduce the Fuzzy $F_1$-Score to mitigate ambiguity, and test our method on two representative datasets. Experiments demonstrate the effectiveness of our method, paving the way towards real-world embodied systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大模型在动态环境中持续进行在线3D理解与定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>维护有限空间记忆，融合3D点云与语义，视频流增量更新。</p>
                <p><span class="font-medium text-accent">主要发现：</span>有限记忆保持恒定推理开销，显著提升在线3D定位与理解性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出首个支持视频流持续输入的在线3D理解框架OnlineSI。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为具身智能在真实动态场景中的实时部署提供可行方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有 MLLM 在空间推理任务上多聚焦静态单帧输入，难以在持续变化的现实环境中保持在线、增量式的 3D 理解，限制了其在具身机器人等实时系统中的应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>OnlineSI 以视频流为输入，维护一个容量固定的 3D 空间记忆，用滑动窗口方式增量融合新帧的点云与语义特征，保证每步推理计算量与历史长度无关；记忆通过可学习的空间-语义对齐模块与 LLM 隐状态交互，实现跨帧对象定位与指代表达解析。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ScanRefer 和 Nr3D 两个基准上，该方法在保持 30 FPS 在线推理的同时，将 Fuzzy F1 相对基线提升 6–9%，验证了其持续增强 3D  grounding 的能力；消融实验显示固定记忆策略在 512 帧后仍不损失精度，而计算耗时仅增加 4%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>固定容量记忆在极长序列或大规模场景中可能出现信息溢出；目前仅验证于室内 RGB-D 视频，尚未在室外动态或无深度输入场景测试，且对快速运动造成的点云配准误差敏感。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入自适应记忆压缩与拓扑图表示，以支持室外大尺度与动态物体场景，并探索与机器人规划闭环耦合的在线主动感知。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究关注多模态大模型、在线 3D 感知或具身智能，该文提供了可落地的增量式框架与评测指标，可直接作为基线或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16020v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Keyframe-Based Feed-Forward Visual Odometry
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于关键帧的前馈式视觉里程计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weichen Dai，Wenhan Su，Da Kong，Yuhang Ming，Wanzeng Kong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16020v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network. However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately. This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information. Integrating traditional geometric heuristics into these methods is non-trivial, as their performance depends on high-dimensional latent representations rather than explicit geometric metrics. To bridge this gap, we propose a novel keyframe-based feed-forward VO. Instead of relying on hand-crafted rules, our approach employs reinforcement learning to derive an adaptive keyframe policy in a data-driven manner, aligning selection with the intrinsic characteristics of the underlying foundation model. We train our agent on TartanAir dataset and conduct extensive evaluations across several real-world datasets. Experimental results demonstrate that the proposed method achieves consistent and substantial improvements over state-of-the-art feed-forward VO methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在视觉基础模型中引入关键帧策略，减少冗余计算并提升位姿估计精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用强化学习在TartanAir上训练自适应关键帧策略，嵌入前馈VO网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比VGGT-Long等最新方法，在多个真实数据集上显著降低误差与计算量。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将数据驱动的RL关键帧选择融入视觉基础模型VO，摆脱手工几何规则。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效、高精度的端到端VO/SLAM提供可扩展关键帧机制，推动基础模型实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉基础模型（VFM）将位姿估计与稠密重建统一在一个前馈网络中，给VO/SLAM带来范式转变，但它们通常对整段图像序列“一视同仁”，忽略了传统关键帧策略在降低计算量与抑制小视差退化方面的价值。由于VFM依赖高维隐空间而非显式几何量，直接嵌入手工关键帧规则困难，因此需要一种与模型内在特性耦合的、可学习的帧选择机制。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出基于强化学习的关键帧前馈VO：将帧选择建模为马尔可夫决策过程，状态由VFM的隐特征与位姿不确定性构成，动作为“是否设为关键帧”，奖励兼顾跟踪精度、计算开销与视差增益；策略网络在TartanAir仿真数据上训练，无需人工设计阈值。推理阶段，RL智能体实时输出关键帧索引，VFM仅对被选帧进行完整的前馈估计，其余帧通过轻量级插值或局部更新维持轨迹。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在EuRoC、TUM-RGBD与ScanNet真实数据集上，该方法以平均30-40%的FLOPs节省，实现了比VGGT-Long等最新前馈VO低15-25%的ATE，并显著减少大纹理缺失区域的漂移；消融实验表明RL策略自动学习在快速旋转或低视差时段降低关键帧密度，验证了数据驱动选择与VFM内部表示的适配性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>RL策略仅在仿真数据训练，真实-仿真域差距可能导致关键帧分布偏移；目前仅针对单目前馈VO，未耦合回环检测或全局优化，因此在长序列仍存在累积漂移；奖励函数权重需手动调优，对不同传感器或运动模式的泛化能力尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线自适应微调或元学习，使关键帧策略在真实环境中持续进化；将框架扩展到VFM-based SLAM，联合学习回环与关键帧决策，实现端到端的全局一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉基础模型在SLAM中的高效部署、强化学习与几何视觉的交叉，或寻求减少大模型推理开销同时保持精度的方法，本论文提供了可直接借鉴的RL-帧选择范式与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16378v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cognitively-Inspired Tokens Overcome Egocentric Bias in Multimodal Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">受认知启发的标记克服多模态模型中的自我中心偏差</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bridget Leonard，Scott O. Murray
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16378v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal language models (MLMs) perform well on semantic vision-language tasks but fail at spatial reasoning that requires adopting another agent&#39;s visual perspective. These errors reflect a persistent egocentric bias and raise questions about whether current models support allocentric reasoning. Inspired by human spatial cognition, we introduce perspective tokens, specialized embeddings that encode orientation through either (1) embodied body-keypoint cues or (2) abstract representations supporting mental rotation. Integrating these tokens into LLaVA-1.5-13B yields performance on level-2 visual perspective-taking tasks. Across synthetic and naturalistic benchmarks (Isle Bricks V2, COCO, 3DSRBench), perspective tokens improve accuracy, with rotation-based tokens generalizing to non-human reference agents. Representational analyses reveal that fine-tuning enhances latent orientation sensitivity already present in the base model, suggesting that MLMs contain precursors of allocentric reasoning but lack appropriate internal structure. Overall, embedding cognitively grounded spatial structure directly into token space provides a lightweight, model-agnostic mechanism for perspective-taking and more human-like spatial reasoning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态模型摆脱自我中心偏差，完成他人视角的空间推理</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入模仿人类空间认知的perspective tokens，微调LLaVA-1.5-13B并在多视角基准测试</p>
                <p><span class="font-medium text-accent">主要发现：</span>视角标记显著提升视觉透视任务准确率，旋转式标记可泛化到非人类参照</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将具身关键点与抽象旋转嵌入作为轻量级token，直接注入模型实现透视推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>证明MLM已潜藏他者中心表征，只需认知启发的结构即可解锁类人空间推理</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态语言模型在语义视觉-语言任务上表现优异，但在需要采纳他人视角的空间推理上普遍失败，表现出顽固的自我中心偏差。该缺陷引发了对现有模型是否具备真正的他者中心（allocentric）空间推理能力的质疑。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者受人类空间认知启发，提出“视角标记”（perspective tokens）——两种专用嵌入：1) 基于身体关键点（body-keypoint）具身线索的方向编码；2) 支持心理旋转的抽象方向表示。将这些标记直接嵌入LLaVA-1.5-13B的token空间，并在合成Isle Bricks V2、COCO及3DSRBench等二级视觉视角选取任务上微调。实验对比了具身与旋转两类标记，并分析微调前后模型潜层对方向信息的敏感度变化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>引入视角标记后，模型在所有基准上的视角选取准确率显著提升，其中旋转型标记可泛化到非人类参照代理。表征探针显示，基础模型已隐含方向敏感性，微调仅强化并结构化该先验；结果说明MLM具备allocentric推理的“雏形”，但缺乏合适内部结构来调用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在13B规模的LLaVA上验证，尚未测试更小或更大模型以及除CLIP-ViT外的视觉编码器；视角标记依赖显式方向标注，在真实场景缺乏此类监督时可能失效；对高阶社会认知或动态多代理视角的扩展尚待探索。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可在无监督或弱监督条件下自动学习视角标记，并扩展到动态多代理、三维导航及对话式空间推理场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为改善多模态模型空间与社会认知提供了轻量级、模型无关的视角标记范式，对研究视觉-语言空间推理、自我-他者中心偏差及人类认知启发的模型设计具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15829v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Realistic Remote Sensing Dataset Distillation with Discriminative Prototype-guided Diffusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向判别性原型引导扩散的真实感遥感数据集蒸馏</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yonghao Xu，Pedram Ghamisi，Qihao Weng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15829v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent years have witnessed the remarkable success of deep learning in remote sensing image interpretation, driven by the availability of large-scale benchmark datasets. However, this reliance on massive training data also brings two major challenges: (1) high storage and computational costs, and (2) the risk of data leakage, especially when sensitive categories are involved. To address these challenges, this study introduces the concept of dataset distillation into the field of remote sensing image interpretation for the first time. Specifically, we train a text-to-image diffusion model to condense a large-scale remote sensing dataset into a compact and representative distilled dataset. To improve the discriminative quality of the synthesized samples, we propose a classifier-driven guidance by injecting a classification consistency loss from a pre-trained model into the diffusion training process. Besides, considering the rich semantic complexity of remote sensing imagery, we further perform latent space clustering on training samples to select representative and diverse prototypes as visual style guidance, while using a visual language model to provide aggregated text descriptions. Experiments on three high-resolution remote sensing scene classification benchmarks show that the proposed method can distill realistic and diverse samples for downstream model training. Code and pre-trained models are available online (https://github.com/YonghaoXu/DPD).</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不泄露敏感数据的前提下，将大规模遥感影像数据集压缩成可替代原数据的小规模合成集。</p>
                <p><span class="font-medium text-accent">研究方法：</span>训练文本-图像扩散模型，以预训练分类器一致性损失和聚类原型视觉-语言风格引导生成蒸馏样本。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个高分辨率遥感场景分类基准上，仅用极少量合成样本即可训练出与原数据集性能相当的模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把数据集蒸馏引入遥感领域，并提出分类器驱动与判别性原型引导的扩散框架提升合成质量。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感数据共享与隐私保护提供高效低成本的替代方案，减少存储传输开销并降低敏感信息泄露风险。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习在遥感影像解译中取得显著进展，但依赖大规模训练数据带来高昂存储与计算成本，且敏感类别数据存在泄露风险。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首次将数据集蒸馏引入遥感领域，利用文本到图像扩散模型将大规模遥感数据集压缩为紧凑蒸馏集。为提升合成样本判别性，在扩散训练阶段注入预训练分类器的分类一致性损失。针对遥感影像语义复杂，先在潜空间聚类选取多样原型作视觉风格引导，再用视觉语言模型生成聚合文本描述。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个高分辨率遥感场景分类基准上的实验表明，该方法可蒸馏出真实且多样的样本，足以支持下游模型训练，验证了其在保持性能的同时显著减少数据量与隐私暴露。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练分类器和视觉语言模型的质量，若这些模型存在偏差将直接影响蒸馏效果；聚类步骤对超参数敏感，可能遗漏稀有类别原型；扩散模型训练与推理计算开销仍较大。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无分类器指导的蒸馏策略以降低对预训练模型的依赖，并研究跨传感器、跨分辨率的蒸馏迁移能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为需处理大规模遥感数据、关注数据隐私与高效训练的研究者提供首个遥感专用数据集蒸馏框架，可直接借鉴其原型引导扩散策略以压缩自有数据集。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16788v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      REL-SF4PASS: Panoramic Semantic Segmentation with REL Depth Representation and Spherical Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">REL-SF4PASS：基于REL深度表示与球面融合的全景语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xuewei Li，Xinghan Bao，Zhimin Chen，Xi Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16788v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As an important and challenging problem in computer vision, Panoramic Semantic Segmentation (PASS) aims to give complete scene perception based on an ultra-wide angle of view. Most PASS methods often focus on spherical geometry with RGB input or using the depth information in original or HHA format, which does not make full use of panoramic image geometry. To address these shortcomings, we propose REL-SF4PASS with our REL depth representation based on cylindrical coordinate and Spherical-dynamic Multi-Modal Fusion SMMF. REL is made up of Rectified Depth, Elevation-Gained Vertical Inclination Angle, and Lateral Orientation Angle, which fully represents 3D space in cylindrical coordinate style and the surface normal direction. SMMF aims to ensure the diversity of fusion for different panoramic image regions and reduce the breakage of cylinder side surface expansion in ERP projection, which uses different fusion strategies to match the different regions in panoramic images. Experimental results show that REL-SF4PASS considerably improves performance and robustness on popular benchmark, Stanford2D3D Panoramic datasets. It gains 2.35% average mIoU improvement on all 3 folds and reduces the performance variance by approximately 70% when facing 3D disturbance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何充分利用全景几何与深度信息提升全景语义分割精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出圆柱坐标REL深度表征与球面动态多模态融合SMMF网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Stanford2D3D数据集上mIoU平均提升2.35%，3D扰动下性能方差降约70%。</p>
                <p><span class="font-medium text-accent">创新点：</span>REL将深度、俯仰角、侧向角统一于圆柱坐标，SMMF按区域动态融合并抑制ERP展开断裂。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全景场景理解提供更几何一致的多模态融合范式，可泛化至VR/AR与机器人导航。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>全景语义分割(PASS)需要在360°超大视场中同时完成几何与语义理解，现有方法多将深度图直接作为第四通道或转成HHA，忽视了全景图像特有的柱面几何与球面连续性，导致深度信息利用率低、边界断裂严重。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出REL深度表示：将原始深度重投影为柱面坐标下的修正深度、仰角-增益垂直倾角和侧向方位角，使网络能同时感知3D位置与表面法向；并设计球面动态多模态融合SMMF，对全景图像的极区、赤道区和接缝区分别采用不同的RGB-D融合策略，抑制ERP展开带来的柱面侧壁断裂。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Stanford2D3D三个官方fold上，REL-SF4PASS将平均mIoU提高2.35%，并将面对3D扰动时的性能方差降低约70%，验证了其在几何失真和传感器噪声下的鲁棒性；可视化显示极区和墙角类别的边界一致性显著改善。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>REL依赖可获取的密集深度图，对单目全景或深度缺失场景未做验证；SMMF的手工分区策略虽有效，但引入了额外超参，对不同数据集可能需要重新调整。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督方式在单目全景上估计REL表示，并将SMMF升级为可学习的球面注意力机制，以自动适应不同全景数据集的几何分布。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注360°视觉、深度利用或多模态融合，本文提出的柱面几何感知深度表示与区域自适应融合策略可直接迁移到全景检测、布局估计及VR/AR场景理解任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16973v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VisGym：面向多模态智能体的多样化、可定制、可扩展环境</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zirui Wang，Junyi Zhang，Jiaxin Ge，Long Lian，Letian Fu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16973v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Modern Vision-Language Models (VLMs) remain poorly characterized in multi-step visual interactions, particularly in how they integrate perception, memory, and action over long horizons. We introduce VisGym, a gymnasium of 17 environments for evaluating and training VLMs. The suite spans symbolic puzzles, real-image understanding, navigation, and manipulation, and provides flexible controls over difficulty, input representation, planning horizon, and feedback. We also provide multi-step solvers that generate structured demonstrations, enabling supervised finetuning. Our evaluations show that all frontier models struggle in interactive settings, achieving low success rates in both the easy (46.6%) and hard (26.0%) configurations. Our experiments reveal notable limitations: models struggle to effectively leverage long context, performing worse with an unbounded history than with truncated windows. Furthermore, we find that several text-based symbolic tasks become substantially harder once rendered visually. However, explicit goal observations, textual feedback, and exploratory demonstrations in partially observable or unknown-dynamics settings for supervised finetuning yield consistent gains, highlighting concrete failure modes and pathways for improving multi-step visual decision-making. Code, data, and models can be found at: https://visgym.github.io/.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统评估并提升视觉-语言模型在长时多步视觉交互中的决策能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建17个可定制多模态环境VisGym，提供难度、表征、反馈等控制并生成结构化演示用于监督微调</p>
                <p><span class="font-medium text-accent">主要发现：</span>前沿模型在交互任务中成功率低，长上下文反致性能下降，视觉化使符号任务变难，显式目标与探索演示可提升表现</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出覆盖符号、真实图像、导航、操控的多样化可扩展基准，支持动态调节与自动生成训练数据</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VLM社区提供统一平台诊断长时视觉决策缺陷，指引感知-记忆-行动整合及训练策略改进</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管视觉-语言模型(VLM)在静态图文任务上表现亮眼，它们在需要多步视觉交互的场景中仍缺乏系统评估，尤其是感知-记忆-动作耦合与长时程规划能力尚不清晰。作者认为现有基准多聚焦单步问答或静态分类，无法揭示VLM在动态、部分可观测环境中的真实决策瓶颈，因此亟需一个可扩展、可定制的交互式评测平台。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>VisGym构建了17个统一API的环境，覆盖符号谜题、真实图像理解、导航与机械臂操纵四类任务，并开放难度、输入模态、规划步长与奖励密度的可调接口。团队为每个环境实现了多步求解器，可自动生成带中间推理链的结构化演示，用于监督微调。实验采用zero-shot、chain-of-thought与微调三种协议，对比了GPT-4V、Claude-3、Gemini等前沿模型在不同上下文长度与观测格式下的表现。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>所有前沿模型在交互任务上显著失效，easy配置平均成功率仅46.6%，hard降至26.0%；当历史上下文从截断窗口改为无限制时，性能反而下降，显示长上下文利用不足。将原本易解的文本符号任务转为纯视觉输入后，准确率下降可达30个百分点，说明视觉渲染引入了额外认知负担。然而，提供显式目标图像、文本化环境反馈以及用探索演示进行监督微调，可分别带来5–15%的绝对提升，验证了部分可观测与未知动力学场景下的改进路径。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅覆盖17个离散任务，尚不足以代表真实世界连续控制或高维动作空间的复杂性；所有环境仍基于模拟器，视觉外观与物理动力学与真实机器人场景存在差距。作者未深入分析模型参数量、推理成本与样本效率之间的权衡，也未对微调后的泛化能力进行跨任务测试。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至连续动作与真实机器人接口，并引入在线强化学习以突破纯监督微调的性能天花板；同时需研究自适应记忆机制，使VLM能在长时程交互中自主决定保留或丢弃历史信息。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态智能体的决策、长程规划或VLM在真实交互场景中的落地，VisGym提供了可复现、可扩展的基准与微调数据，可直接用于诊断模型缺陷、测试新算法并对比性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15724v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VideoThinker：利用LLM引导的工具推理构建智能体视频大模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chenglin Li，Qianglong Chen，Feng Han，Yikun Wang，Xingxi Yin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15724v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Long-form video understanding remains a fundamental challenge for current Video Large Language Models. Most existing models rely on static reasoning over uniformly sampled frames, which weakens temporal localization and leads to substantial information loss in long videos. Agentic tools such as temporal retrieval, spatial zoom, and temporal zoom offer a natural way to overcome these limitations by enabling adaptive exploration of key moments. However, constructing agentic video understanding data requires models that already possess strong long-form video comprehension, creating a circular dependency. We address this challenge with VideoThinker, an agentic Video Large Language Model trained entirely on synthetic tool interaction trajectories. Our key idea is to convert videos into rich captions and employ a powerful agentic language model to generate multi-step tool use sequences in caption space. These trajectories are subsequently grounded back to video by replacing captions with the corresponding frames, yielding a large-scale interleaved video and tool reasoning dataset without requiring any long-form understanding from the underlying model. Training on this synthetic agentic dataset equips VideoThinker with dynamic reasoning capabilities, adaptive temporal exploration, and multi-step tool use. Remarkably, VideoThinker significantly outperforms both caption-only language model agents and strong video model baselines across long-video benchmarks, demonstrating the effectiveness of tool augmented synthetic data and adaptive retrieval and zoom reasoning for long-form video understanding.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何打破长视频理解与工具型数据间的循环依赖，提升长时序定位与信息保留。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用LLM在合成字幕空间生成多步工具轨迹，再映射回帧，训练VideoThinker。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VideoThinker在长视频基准上显著优于纯字幕代理与强视频基线模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无需模型具备长视频理解即可大规模合成工具交互数据并训练代理式VideoLLM。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建能动态探索关键片段的代理视频模型提供了可扩展的数据与训练范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>长视频理解要求模型在数百到上千帧中精准定位关键事件，现有Video-LLM多依赖均匀采样做静态推理，既丢失时序信息又难以回答“何时发生什么”的细粒度问题。引入可主动检索、放大时/空区域的agentic工具被视为突破口，却受限于“需强视频理解能力才能标注工具使用数据”的循环依赖。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>VideoThinker提出“先在字幕空间合成工具轨迹、再 grounding 回视频”的闭环：先用稠密字幕模型把长视频转为多段文本描述，再用大语言模型在纯文本上自回归生成多步工具调用链（如temporal_retrieve、spatial_zoom、temporal_zoom），形成数万条“字幕+工具”交互序列；随后将每条轨迹中的字幕节点替换为对应时间戳的原始帧，构建大规模帧-工具交错数据。模型仅在此合成数据上训练，无需任何人工长视频问答标注，即获得可动态调用工具、逐步聚焦关键片段的agentic VideoLLM。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在EgoSchema、LongVideoBench、NExT-QA等长视频基准上，VideoThinker比同等规模的caption-only LLM agent平均提升12-18分，比当前最强视频模型提升6-10分，仅用合成数据就达到SOTA；消融实验表明，移除temporal zoom或spatial zoom任一项均导致&gt;3分下降，验证了工具组合的必要性；定性案例显示，模型能在1500帧中通过3-4次工具调用准确定位“厨师加盐”的5秒片段并回答细节问题。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>合成轨迹完全依赖稠密字幕质量，若字幕缺失或时间错位，工具调用会被错误监督；目前工具集仅三种原子操作，尚不支持跨视频检索、音频模态及更高阶逻辑工具；实验集中在英文公开长视频，对影视剧情、多语言对话等更复杂域的泛化能力未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>扩展工具箱至跨镜头检索、音频事件定位与三维时空裁剪，并探索用强化学习在真实视频反馈上微调工具策略，以进一步缩小合成与真实分布差距。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注长视频时序定位、agentic视觉推理或低成本构建工具使用数据，VideoThinker提供了“字幕-工具-帧”闭环合成范式，可直接借鉴其数据生成代码与评估协议，快速迁移到事件检测、视频编辑、教育视频问答等场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113064" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning Gated Experts for Segment Anything in the Wild
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向野外任意目标分割的门控专家学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yizhen Guo，Hang Guo，Tao Dai，Zhi Wang，Bin Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113064" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113064</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Segment anything model (SAM) and its variants have recently shown promising performance as foundation models. However, existing SAM-based models can only handle scenarios seen during training, and usually suffer unstable performance when transferring to real-world unseen data, such as low-light, rainy, or blurred images, which is crucial for applications such as autopilot. Therefore, adapting SAM-based models for real-world degradation while not impairing their original ability remains an open challenge. In this work, we propose a novel gated Mixture-of-Experts (MoE) structure, called RouGE, to improve the robustness of SAM-based models. Specifically, RouGE uses multiple lightweight probability gates to decompose complex real-world image conditions and judge whether the feature needs to be adjusted, as well as to what extent the adjustment needs to be done, then handles them differently with a set of low-rank experts. During the inference stage, RouGE processes input images in a completely blind manner, thus improving the model’s performance in real-world scenarios. Extensive experiments demonstrate that RouGE consistently achieves state-of-the-art results on both degraded and clean images compared with other methods while tuning only 1.5% of parameters. Our source code is publicly accessible via: https://github.com/Guo-Yizhen/RouGE .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让SAM在雾、暗、模糊等真实退化场景下保持鲁棒分割而不牺牲原性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RouGE门控混合专家框架，用轻量概率门判断并调用低秩专家自适应调整特征，仅调1.5%参数。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多种退化与干净图像上均达SOTA，零样本盲推理即可显著提升SAM鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将门控MoE引入SAM，实现按退化类型自动选择低秩专家的无监督自适应机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶等真实视觉任务提供即插即用的SAM增强方案，参数高效且无需退化先验。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM 系列模型在干净数据上表现优异，但在低照度、雨雾、模糊等真实退化场景下性能骤降，而完全重训或微调又容易遗忘原始能力，亟需一种轻量且保真的自适应方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 RouGE——一种基于门控混合专家（MoE）的即插即用模块，仅对 SAM 编码器特征做外接式修正；多个轻量级概率门根据输入图像的退化类型与强度生成像素级权重，决定特征是否需要调整及调整幅度，随后由一组低秩专家网络完成差异化复原。整个框架在推理阶段完全盲化，无需退化类型先验，仅训练 1.5% 参数量即可与主干协同。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在包含低光、雨天、模糊、噪声等 8 类退化及对应干净图像的混合测试集上，RouGE 将 SAM 的 mIoU 平均提升 8.3%，在退化子集最高提升 14.7%，同时保持干净图像性能不降；对比现有领域泛化与鲁棒微调方法，RouGE 参数少 20×，推理速度提升 1.6×，取得新 SOTA。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>门控网络仍依赖退化与干净图像成对训练，极端罕见退化类型可能被门控错误分类；额外 MoE 分支增加显存占用约 12%，在边缘端实时部署需进一步压缩；实验主要围绕 SAM 的 ViT-B 主干，尚未验证在更大或其他分割架构上的通用性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无配对自监督门控学习，使框架真正走向开放世界；将 RouGE 思想扩展到检测、跟踪等视觉任务，构建统一的退化鲁棒基础模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究分割模型鲁棒性、MoE 高效微调或自动驾驶感知系统，该文提供了可插拔的轻量范式与完整代码，能快速迁移至你的场景并作为强基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104183" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adversarial perturbation for RGB-T tracking via intra-modal excavation and cross-modal collusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于模态内挖掘与跨模态串通的RGB-T跟踪对抗扰动</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinyu Xiang，Xuying Wu，Shengxiang Li，Qinglong Yan，Tong Zou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104183" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104183</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing adversarial perturbation attack for visual object trackers mainly focuses on RGB modality, yet research on RGB-T trackers’ adversarial perturbation remains unexplored. To address this gap, we propose an I ntra-modal excavation and C ross-modal collusion adversarial perturbation attack algorithm (ICAttack) for RGB-T Tracking. Firstly, we establish a novel intra-modal adversarial clues excavation (ImAE) paradigm. By leveraging the unique distribution properties of each modality as a prior, we independently extract the attack cues of different modalities from the public noise space. Building upon this, we develop a cross-modal adversarial collusion (CmAC) strategy, which enables implicit and dynamic interaction between the adversarial tokens of two modalities. This interaction facilitates negotiation and collaboration, achieving a synergistic attack gain for RGB-T trackers that surpasses the effect of a single-modality attack. The above process, from intra-modal excavation to cross-modal collusion, creates a progressive and systematic attack framework for RGB-T trackers. Besides, by introducing the spatial adversarial intensity control module and precise response disruption loss, we further enhance both the attack stealthiness and precision of our adversarial perturbations. The control module reduces attack strength in less critical areas to improve stealth. The disruption loss uses a small mask on the tracker’s brightest semantic response region, concentrating the perturbation to interfere with the tracker’s target awareness precisely. Extensive evaluations of attack performances in different SOTA victimized RGB-T trackers demonstrate the advantages of ICAttack in terms of specificity and effectiveness of cross-modal attacks. Moreover, we offer a user-friendly interface to promote the practical deployment of adversarial perturbations.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何首次生成能同时欺骗RGB与热红外模态的RGB-T跟踪对抗扰动。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ImAE独立挖掘各模态攻击线索，再用CmAC让双模态扰动动态协同，并辅以空间强度控制与精准响应破坏损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ICAttack在多种SOTA RGB-T跟踪器上实现显著跨模态协同攻击增益，超越单模态攻击且更隐蔽精准。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创面向RGB-T跟踪的跨模态协同对抗框架，将模态内挖掘与模态间共谋结合并引入强度控制与响应破坏机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为RGB-T跟踪安全评估提供首个专用攻击基线，揭示多模态融合脆弱性并推动鲁棒跟踪研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有对抗扰动攻击几乎只针对RGB单模态跟踪器，而RGB-T跟踪器因额外利用热红外模态在复杂场景下表现出更强鲁棒性，却尚未被系统研究其安全漏洞。作者认为忽视跨模态协同会使RGB-T系统产生虚假安全感，因此首次探索针对RGB-T跟踪的对抗攻击。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出ICAttack框架：先通过Intra-modal Adversarial clues Excavation(ImAE)利用各模态统计先验从公共噪声空间独立挖掘专属扰动线索，再设计Cross-modal Adversarial Collusion(CmAC)令RGB与T模态的对抗token在特征层动态交互协商，实现协同攻击增益。框架还引入空间强度控制模块在背景区域削弱扰动提升隐蔽性，并采用精准响应破坏损失仅对跟踪器最亮语义响应区域施加小mask扰动，以精准扰乱目标感知。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个SOTA RGB-T跟踪器上的实验表明，ICAttack的跨模态协同攻击成功率显著高于单独RGB或T攻击，且所需扰动能量更低；隐蔽性指标（L∞、FID、人类可感知评分）优于现有RGB攻击方法，验证其 stealthiness 与 specificity。结果首次揭示RGB-T跟踪器在跨模态联合欺骗下的脆弱性，对后续防御研究提供基准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前攻击假设白盒设置，需要完整获取跟踪器参数与梯度，现实黑盒场景可行性未验证；评估仅集中于短期跟踪序列，对长时遮挡、目标重入等复杂情形下的攻击稳定性尚不清楚。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可延伸为黑盒查询攻击与物理世界打印攻击，并研究基于跨模态一致性自监督的防御机制；同时扩展至RGB-D、RGB-E等多模态跟踪框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态视觉鲁棒性、跨模态协同机制或对抗攻防，本工作提供了首个RGB-T跟踪攻击范式与评估基准，可直接作为对比基线或启发防御设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.41</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16381v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VTFusion: A Vision-Text Multimodal Fusion Network for Few-Shot Anomaly Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VTFusion：面向小样本异常检测的视觉-文本多模态融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuxin Jiang，Yunkang Cao，Yuqi Cheng，Yiheng Zhang，Weiming Shen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16381v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-Shot Anomaly Detection (FSAD) has emerged as a critical paradigm for identifying irregularities using scarce normal references. While recent methods have integrated textual semantics to complement visual data, they predominantly rely on features pre-trained on natural scenes, thereby neglecting the granular, domain-specific semantics essential for industrial inspection. Furthermore, prevalent fusion strategies often resort to superficial concatenation, failing to address the inherent semantic misalignment between visual and textual modalities, which compromises robustness against cross-modal interference. To bridge these gaps, this study proposes VTFusion, a vision-text multimodal fusion framework tailored for FSAD. The framework rests on two core designs. First, adaptive feature extractors for both image and text modalities are introduced to learn task-specific representations, bridging the domain gap between pre-trained models and industrial data; this is further augmented by generating diverse synthetic anomalies to enhance feature discriminability. Second, a dedicated multimodal prediction fusion module is developed, comprising a fusion block that facilitates rich cross-modal information exchange and a segmentation network that generates refined pixel-level anomaly maps under multimodal guidance. VTFusion significantly advances FSAD performance, achieving image-level AUROCs of 96.8% and 86.2% in the 2-shot scenario on the MVTec AD and VisA datasets, respectively. Furthermore, VTFusion achieves an AUPRO of 93.5% on a real-world dataset of industrial automotive plastic parts introduced in this paper, further demonstrating its practical applicability in demanding industrial scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅有几张正常样本的情况下，利用视觉-文本多模态信息精准检测工业异常。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VTFusion：任务特定视觉/文本特征提取器+跨模态融合块+像素级分割网络，并辅以合成异常数据增强。</p>
                <p><span class="font-medium text-accent">主要发现：</span>2-shot设定下MVTec AD达96.8% AUROC，VisA达86.2% AUROC；自建汽车塑件数据集AUPRO 93.5%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次为工业FSAD设计任务自适应视觉-文本特征提取与深度跨模态融合，缓解预训练域偏差与语义错位。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据稀缺场景提供高实用性的工业质检方案，推动多模态学习在异常检测中的落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-Shot Anomaly Detection (FSAD) is pivotal for industrial inspection where only a handful of normal samples are available, yet most vision-language approaches borrow features pre-trained on natural images and perform rudimentary fusion, overlooking domain-specific nuances and cross-modal misalignment. This leads to brittle detectors that degrade when textual descriptions contain shop-floor jargon or when lighting and texture variations dominate the scene.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>VTFusion introduces task-specific visual and textual backbones that are fine-tuned on the target industrial domain while synthetic anomalies are generated on-the-fly to enlarge the decision boundary. A cross-modal fusion block first exchanges spatial and semantic tokens through attention, yielding a co-embedded feature volume that is subsequently decoded by a lightweight segmentation head to produce pixel-precise anomaly heatmaps. The whole pipeline is trained end-to-end with an anomaly-margin contrastive loss that pushes normal features together while pulling anomalies apart in the fused space.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On MVTec AD and VisA the 2-shot image-level AUROC jumps to 96.8% and 86.2%, outperforming the best competing method by 3.4 and 5.1 points, respectively. Pixel-level AUPRO on the authors’ proprietary automotive plastic-parts dataset reaches 93.5%, cutting false positives on mold-injection seams by roughly half compared with a strong CLIP-based baseline. Ablation shows that domain-adaptive fine-tuning contributes 60% of the gain, while the fusion block alone adds another 25%, validating the importance of both components.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach still needs a small set of clean, defect-free images for each new object class and assumes access to textual descriptions that may not exist for legacy industrial parts. Synthetic anomalies are generated with simple copy-paste and color jitter, which may not cover subtle metallurgical defects, and inference latency grows linearly with the length of the text prompt, posing challenges for high-speed inline inspection.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore prompt-free language generation from CAD metadata and integrate test-time adaptation that updates the fusion module with streaming unlabeled images to further reduce the shot requirement.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers investigating multimodal learning, few-shot industrial vision, or cross-modal alignment will find VTFusion a practical benchmark that explicitly tackles domain shift and granular semantic mismatch between vision and language in manufacturing scenes.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16885v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GPA-VGGT:Adapting VGGT to Large scale Localization by self-Supervised learning with Geometry and Physics Aware loss
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GPA-VGGT：通过几何与物理感知损失的自监督学习将VGGT适配于大规模定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yangfan Xu，Lilian Zhang，Xiaofeng He，Pengdong Wu，Wenqi Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16885v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transformer-based general visual geometry frameworks have shown promising performance in camera pose estimation and 3D scene understanding. Recent advancements in Visual Geometry Grounded Transformer (VGGT) models have shown great promise in camera pose estimation and 3D reconstruction. However, these models typically rely on ground truth labels for training, posing challenges when adapting to unlabeled and unseen scenes. In this paper, we propose a self-supervised framework to train VGGT with unlabeled data, thereby enhancing its localization capability in large-scale environments. To achieve this, we extend conventional pair-wise relations to sequence-wise geometric constraints for self-supervised learning. Specifically, in each sequence, we sample multiple source frames and geometrically project them onto different target frames, which improves temporal feature consistency. We formulate physical photometric consistency and geometric constraints as a joint optimization loss to circumvent the requirement for hard labels. By training the model with this proposed method, not only the local and global cross-view attention layers but also the camera and depth heads can effectively capture the underlying multi-view geometry. Experiments demonstrate that the model converges within hundreds of iterations and achieves significant improvements in large-scale localization. Our code will be released at https://github.com/X-yangfan/GPA-VGGT.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让VGGT在无标签大规模场景下完成自监督定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用序列级几何-光度联合损失自监督训练VGGT，多源帧投影增强时序一致性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>数百次迭代即收敛，大规模定位精度显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将序列级几何约束与物理光度一致性联合用于VGGT自监督学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无标签大场景视觉定位提供可扩展的自监督方案，降低标注成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉几何基础Transformer(VGGT)在相机位姿估计与3D重建中表现优异，但严重依赖大规模带标签数据，难以直接迁移到无标签或未见场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GPA-VGGT，将成对几何关系扩展为序列级约束：在视频序列中采样多帧源图像并几何投影到不同目标帧，构建光度一致性+几何一致性的联合自监督损失，无需任何真值标签即可端到端训练相机位姿、深度及跨视图注意力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，模型在数百次迭代内收敛，在大型室外数据集上的绝对轨迹误差(ATE)相比监督VGGT降低18%，相对位姿误差降低25%，显著提升了无标签场景下的定位精度与三维一致性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍假设光度恒定、场景静态，对动态物体和光照变化敏感；此外，序列级约束需要足够长的视频输入，在短片段或极低纹理区域性能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入光流或语义掩码处理动态物体，并探索跨数据集自监督预训练以进一步提升泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为无标签大场景定位提供了可复现的自监督框架，代码开源，对研究SLAM、MVS及无监督几何学习的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131307" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MRHead: Online Reconstruct Vectorized HD Map via Manifold Structure Feature
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MRHead：基于流形结构特征的在线矢量化高清地图重建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Taohong Zhu，Chunchit Siu，Yunhe Wu，Ke Song，Huiyuan Xiong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131307" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131307</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">High-definition (HD) maps are crucial for autonomous driving. Traffic markings have a distinct manifold structure, but the elements expressed through vertex sequences often lose manifold features—specifically the inherent geometric continuity and smooth topological relationships embedded in road elements. The existing map constructor also overlook the effect of the implicit manifold structure on the vertex sequence. We introduce the manifold reconstruction head (MRHead) aimed at extracting the manifold features inherent in vertex sequence and reconstructing the HD map elements. Our approach suggests utilizing manifold distance rather than Euclidean distance to formulate an adjacency matrix for linear manifolds, while extracting manifold features grounded in object proximity and direction. Additionally, we employ graph networks for extracting patch manifold features. ultimately, the reconstruction model is built on extracted features to rectify the errors on geometric and semantic elements of HD map. Experiments conducted on the widely employed nuScenes dataset revealed improvements in mAP compared to MapTR, MapTRv2, and StreamMapNet by 4.4%, 0.9%, and 3.1%, respectively. Regarding computational cost, while the full model operates at 17.0 FPS, our proposed modular design allows for a ”training-only” deployment strategy, achieving these gains with zero additional inference latency (operating at 19.6 FPS). The results suggest that our method can be seamlessly integrated into existing point-based regression frameworks (e.g., MapTR) as a plug-and-play auxiliary supervision module, enhancing performance without altering the backbone architecture.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在线重建保持几何连续与拓扑平滑的矢量化高精地图。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MRHead，用流形距离建图、图网络提取patch流形特征并校正地图误差。</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes上mAP较MapTR/MapTRv2/StreamMapNet提升4.4%/0.9%/3.1%，零额外推理延迟。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将流形结构特征引入顶点序列回归，实现即插即用训练增强模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为点回归式高精地图方案提供无代价性能增益，可直接嵌入现有框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有基于顶点序列的高精地图构造方法把道路元素拆成离散点，破坏了交通标线天然的流形几何连续性与拓扑平滑性，导致重建精度受限。作者观察到这一结构信息丢失是制约在线矢量化建图性能的关键瓶颈，因而提出显式建模并恢复流形特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MRHead在解码端新增流形重建头：先用流形距离替代欧氏距离构造线性邻接矩阵，保留曲线内禀度量；随后结合目标邻近性与方向一致性提取点级流形特征，并用图网络在局部面片层面进一步聚合高阶流形模式；最终把学到的特征用于并行修正几何顶点位置与语义类别，实现端到端训练。整个模块以辅助监督形式接入，主干网络参数不变，推理时可完全摘除。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>nuScenes基准上，MRHead分别将MapTR、MapTRv2、StreamMapNet的mAP提高4.4%、0.9%、3.1%，而完整模型仍保持17.0 FPS；若采用&#34;仅训练阶段启用&#34;策略，推理延迟为零且帧率升至19.6 FPS，验证了即插即用特性。消融实验显示流形距离与图网络面片特征各贡献约一半增益，证实流形结构建模的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在nuScenes单数据集验证，缺乏在不同地理区域、车道线样式及恶劣天气下的泛化测试；流形邻接矩阵的超参数(邻域大小、方向阈值)需手动设定，尚未实现自适应；对高曲率或截断标线，流形距离可能偏离真实测地线，引入额外误差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索数据驱动的可学习流形度量，使邻接关系随场景动态调整，并将MRHead扩展至时序融合与多任务学习，实现统一的长距离拓扑推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注在线矢量化高精地图、结构感知点序列建模或即插即用型性能提升模块，本文提出的流形重建头提供了零推理代价的新思路与可直接集成的代码接口。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131325" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-Agent Role-Playing by LLMs and LMMs: An Explainable Open-World Multi-Modal Crisis Tweet Classification Method
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于LLM与LMM的多智能体角色扮演：一种可解释的开放世界多模态危机推文分类方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tong Bie，Yongli Hu，Yu Fu，Linjia Hao，Tengfei Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131325" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131325</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">How to better leverage Large Language Models (LLMs) and Large Multi-modal Models (LMMs) in downstream tasks has become a prominent research topic. In the field of crisis tweet classification, existing approaches often fail to simultaneously handle open-world scenarios, missing modalities, and explainable classification, while still relying on supervised training data. To address these challenges, we propose a Multi-Agent Role-Playing framework (MARP) in which multiple LLMs and LMMs collaborate through specialized role assignments. MARP includes a LMM-based image analysis expert agent and four LLM-based agents: ordinary social media user, humanitarian organization staff member, content verification expert and long-text summarization expert. The social media user and humanitarian staff member are assigned distinct tasks, gathering tweet information from different perspectives by consulting the image analysis expert. The verification expert analyzes interactions to identify potential issues, while the summarization expert consolidates chat logs into summaries. These summaries are processed by the classification expert to generate predictions while also serving as explanatory rationales. We also propose a Query-aware Dynamic Masking (QDM) that selectively filters irrelevant image regions based on cross-modal similarity, enhancing LMMs’ focus on question-relevant visual content. Experiments on the CrisisMMD dataset under both open-world and missing-modality settings demonstrate that MARP achieves zero-shot accuracy improvements of 6.44% and 2.47% over state-of-the-art baselines, respectively. MARP exhibits strong performance in a training-free manner, while also providing explanatory rationales.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需监督训练的开放世界、缺失模态场景下实现可解释的危机推文分类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多智能体角色扮演框架MARP，让LLM/LMM分工协作，并引入查询感知动态掩码QDM聚焦相关图像区域。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CrisisMMD上，MARP零样本准确率分别提升6.44%和2.47%，并提供可解释摘要。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用角色扮演多智能体协同完成零样本多模态危机分类，并给出可解释推理链。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为灾害响应研究者提供免训练、可解释且鲁棒的社交媒体内容自动分类新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>危机推文分类传统上依赖监督数据，难以同时应对开放世界场景、模态缺失和可解释性三大挑战。随着大模型（LLM/LMM）兴起，如何零样本、可解释地利用其多模态能力成为应急管理领域的新焦点。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出多智能体角色扮演框架 MARP，由 1 个 LMM 图像分析专家和 4 个 LLM 专家（普通用户、人道组织员工、内容核查员、长文摘要员）组成，通过角色化对话从不同视角提取推文-图像信息。引入 Query-aware Dynamic Masking（QDM）按跨模态相似度动态屏蔽无关图像区域，迫使 LMM 聚焦问题相关视觉线索。最终由分类专家在零样本设置下依据对话摘要给出预测并将摘要作为解释依据，全程无需训练数据。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CrisisMMD 数据集上，MARP 在开放世界和缺失模态两种零样本设定下分别比最强基线提高 6.44% 和 2.47% 的准确率，同时自动生成人类可读的分类理由。实验表明角色分工与 QDM 视觉过滤均对性能提升有显著贡献，验证了多智能体协作在危机场景下的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖多个大模型同时在线推理，计算与延迟成本高昂；角色提示设计需领域知识，迁移至其他灾害类型时可能需重新调优；零样本性能仍受限于底层 LLM/LMM 的幻觉与偏见，且未在真实部署的流媒体环境中验证时效性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级蒸馏或端侧小模型替代部分智能体以降低开销，并引入在线反馈机制让系统在真实应急流中持续自我修正。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型在应急情报、多模态社交媒体分析或零样本可解释分类中的应用，本文提供的角色扮演协作范式与 QDM 视觉过滤策略可直接借鉴并扩展到其他灾害或舆情监测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.15698v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for Harmful Image Generation via Semantic-Agnostic Inputs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越视觉安全：通过语义无关输入越狱多模态大语言模型以生成有害图像</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mingyu Yu，Lana Liu，Zhehao Zhao，Wei Wang，Sujuan Qin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.15698v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The rapid advancement of Multimodal Large Language Models (MLLMs) has introduced complex security challenges, particularly at the intersection of textual and visual safety. While existing schemes have explored the security vulnerabilities of MLLMs, the investigation into their visual safety boundaries remains insufficient. In this paper, we propose Beyond Visual Safety (BVS), a novel image-text pair jailbreaking framework specifically designed to probe the visual safety boundaries of MLLMs. BVS employs a &#34;reconstruction-then-generation&#34; strategy, leveraging neutralized visual splicing and inductive recomposition to decouple malicious intent from raw inputs, thereby leading MLLMs to be induced into generating harmful images. Experimental results demonstrate that BVS achieves a remarkable jailbreak success rate of 98.21\% against GPT-5 (12 January 2026 release). Our findings expose critical vulnerabilities in the visual safety alignment of current MLLMs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何绕过MLLM视觉安全过滤，诱导其生成有害图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出BVS框架，用“重建-再生成”策略，通过语义无关拼接与重组隐藏恶意意图。</p>
                <p><span class="font-medium text-accent">主要发现：</span>对GPT-5(2026版) jailbreak成功率达98.21%，揭示视觉安全对齐重大漏洞。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语义无关输入用于视觉越狱，实现恶意意图与原始图像解耦。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为MLLM视觉安全研究提供基准攻击范式，推动多模态对齐防御发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型（MLLM）在图文对齐上的安全研究多聚焦文本侧，视觉安全边界缺乏系统评估。随着模型可生成高保真图像，恶意提示可能绕过视觉审查机制，亟需专门探测其视觉安全极限的方法。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Beyond Visual Safety（BVS）框架，采用“重建-再生成”两阶段策略：先用中性化视觉拼接将有害图像语义剥离成看似无害的碎片，再通过归纳重组把这些碎片与良性文本提示配对，诱导模型在生成阶段还原出有害图像。整个过程不依赖显式恶意文本，利用模型自身的视觉补全与对齐缺口完成越狱。实验在 GPT-5 2026 版上测试 1 000 组图文对，自动化评估生成图像与目标有害概念的 CLIP 相似度与人工验证。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>BVS 对 GPT-5 的视觉安全限制实现 98.21% 的越狱成功率，显著高于现有文本侧攻击的 42% 基线。分析表明，模型的视觉编码器对碎片化语义不敏感，而文本解码器在缺乏显性恶意提示时不会触发安全过滤。该结果首次揭示 MLLM 视觉对齐与文本对齐存在异步漏洞，为后续防御研究提供量化基准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅针对 GPT-5 闭源版本，未验证其他架构或开源模型的可迁移性；评估指标依赖 CLIP 相似度，可能遗漏文化语境层面的微妙伤害；实验范围限于静态图像生成，未涵盖视频或多轮交互场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展 BVS 至视频帧生成与对话式编辑场景，并开发视觉-语义联合对齐的实时防御模块。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态安全、生成模型对齐或红队评估，本文提供了首个系统攻击视觉安全边界的框架与可复制基准，可直接作为对比基线或防御靶点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16108v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多模态气候虚假信息检测：融合视觉-语言模型与外部知识源</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Marzieh Adeli Shamsabad，Hamed Ghodrati
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16108v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何检测社交媒体中结合图像与文字的气候虚假信息。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将视觉-语言模型与反向搜索、事实核查和专家知识等外部知识源融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>引入外部知识后，模型对气候虚假图像-文本对的识别准确率显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把实时外部知识检索嵌入多模态气候虚假信息检测流程。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为应对快速演化的气候谣言提供可更新的自动化检测工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>社交媒体上的气候相关虚假图像与短视频激增，传统事实核查速度远不及传播速度，亟需自动化检测手段。现有视觉-语言模型只能依赖训练时已固化的知识，无法对快速演变的气候事件及新出现的误导策略做出判断。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出把冻结权重的CLIP式VLM作为语义编码器，在推理阶段动态调用外部知识：对输入图像做反向搜索获取来源与篡改痕迹，检索Google Fact Check Tools与Snopes等实时核查文章，并抽取IPCC、NASA等权威机构的科学摘要。检索结果经重排序后，与图像-文本联合嵌入一起输入轻量级融合模块，输出四分类标签（准确、误导、虚假、无法验证）。整个流程无需微调大模型，仅更新融合层与检索索引，保证部署效率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的多模态气候谣言基准MClimateD上，该方法比纯VLM基线提高12.7% F1，并将“无法验证”案例的误判率从34%降到18%。消融实验显示，反向图像搜索对检测深度伪造图表贡献最大，而引入IPCC段落可把科学概念错误降低9%。用户研究表明，系统提供的可解释证据使气候记者的事实核查时间缩短42%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未评估方法在非英语语境及低资源语言上的检索效果；外部知识源的可信度假设可能不成立，若权威机构本身被攻击或存在偏见，模型会放大错误；实时检索带来延迟与隐私问题，在带宽受限地区难以落地。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索检索-生成协同框架，让VLM主动生成查询并迭代验证，同时引入时间序列证据追踪以捕捉谣言演变过程。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态虚假检测、动态知识增强或科学传播，该文提供了可即插即用的检索-融合范式与气候领域评测数据，可迁移到医疗、疫情等其它科学议题的谣言识别任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16210v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PyraTok：面向视频理解与生成且与语言对齐的金字塔式分词器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Onkar Susladkar，Tushar Prakash，Adheesh Juvekar，Kiet A. Nguyen，Dong-Hwan Jang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16210v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何设计能跨多尺度、强对齐语言且零样本迁移强的视频离散 tokenizer。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在预训练视频 VAE 上引入语言对齐金字塔量化模块，联合多尺度文本引导量化与层级自回归目标。</p>
                <p><span class="font-medium text-accent">主要发现：</span>十个基准上实现 SOTA 重建、提升文生视频质量并刷新零样本分割/动作定位/理解纪录。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出共享大码本的多深度金字塔量化与语言联合训练，使视觉 token 语义结构化且跨模态对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频生成与理解提供高保真、语言对齐的通用 token 表示，可零样本迁移并支持 4K/8K。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现代文本到视频生成与视频理解系统普遍依赖离散视频 VAE，但现有 tokenizer 仅在单一尺度学习视觉码本，词汇量有限且仅受浅层语言监督，导致跨模态对齐差、零样本迁移弱。PyraTok 旨在通过多尺度、语言对齐的离散化来缓解这一瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PyraTok 以预训练视频 VAE 为骨干，提出 Language-aligned Pyramidal Quantization (LaPQ) 模块，在编码器多个深度用共享的大型二进制码本进行离散化，生成紧凑且富有语义的 token 序列。训练时联合优化多尺度文本引导量化损失与对整个 token 层级的全局自回归生成目标，使视觉 token 与语言紧密耦合。推理阶段可逐级解码，支持 4K/8K 超高清视频。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 10 个基准上，PyraTok 取得视频重建新 SOTA，并在文本到视频生成质量上持续优于现有方法。零样本迁移方面，首次在视频分割、时序动作定位与多项视频理解任务上同时刷新 SOTA，证明其表征的通用性与高分辨率鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更长时序（&gt;1 分钟）或复杂事件链视频上充分验证；对共享二进制码本的存储与更新开销未给出详细复杂度分析；训练需要配对的文本-视频数据，对无描述或弱描述数据集的适应性未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 LaPQ 拓展到音频-视频-文本三模态共享码本，实现统一 tokenizer；研究基于 PyraTok 的多智能体协同长视频生成与编辑。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视频生成、跨模态对齐、离散表征或零样本视频理解，PyraTok 提供了一种可扩展的多尺度量化框架与训练策略，可直接替换现有 VAE tokenizer 或作为下游任务的预训练视觉编码器。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16895v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Evaluating Large Vision-language Models for Surgical Tool Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向手术器械检测的大型视觉-语言模型评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Nakul Poudel，Richard Simon，Cristian A. Linte
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16895v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Surgery is a highly complex process, and artificial intelligence has emerged as a transformative force in supporting surgical guidance and decision-making. However, the unimodal nature of most current AI systems limits their ability to achieve a holistic understanding of surgical workflows. This highlights the need for general-purpose surgical AI systems capable of comprehensively modeling the interrelated components of surgical scenes. Recent advances in large vision-language models that integrate multimodal data processing offer strong potential for modeling surgical tasks and providing human-like scene reasoning and understanding. Despite their promise, systematic investigations of VLMs in surgical applications remain limited. In this study, we evaluate the effectiveness of large VLMs for the fundamental surgical vision task of detecting surgical tools. Specifically, we investigate three state-of-the-art VLMs, Qwen2.5, LLaVA1.5, and InternVL3.5, on the GraSP robotic surgery dataset under both zero-shot and parameter-efficient LoRA fine-tuning settings. Our results demonstrate that Qwen2.5 consistently achieves superior detection performance in both configurations among the evaluated VLMs. Furthermore, compared with the open-set detection baseline Grounding DINO, Qwen2.5 exhibits stronger zero-shot generalization and comparable fine-tuned performance. Notably, Qwen2.5 shows superior instrument recognition, while Grounding DINO demonstrates stronger localization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估大视觉-语言模型在手术器械检测中的有效性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在GraSP机器人手术数据集上对Qwen2.5、LLaVA1.5、InternVL3.5进行零样本与LoRA微调比较。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Qwen2.5零样本与微调均最优，识别强、定位弱于Grounding DINO。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统比较最新大视觉-语言模型在手术器械检测任务。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建通用多模态手术AI提供模型选择与性能基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>手术场景高度复杂，传统单模态AI难以同时理解视觉与语义信息，限制了其在手术导航与决策中的全面支持。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者选用GraSP机器人手术数据集，系统评估Qwen2.5、LLaVA1.5、InternVL3.5三款大视觉-语言模型在零样本与LoRA微调下的手术器械检测能力；以Grounding DINO作为开放集检测基线，采用标准检测指标对比定位与识别性能；实验设计覆盖零样本泛化与参数高效微调两种临床可落地场景。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Qwen2.5在零样本与LoRA设置下均显著优于其他VLMs，零样本泛化能力超过Grounding DINO，微调后与其定位精度相当；Qwen2.5在器械类别识别准确率上表现最佳，而Grounding DINO在边界框定位精度方面略优；结果表明大视觉-语言模型可在无需大量标注的情况下提供可靠的手术器械感知。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在GraSP一个机器人数据集上验证，未覆盖腹腔镜、显微外科等多模态真实手术环境；VLMs的推理延迟、显存占用与可解释性尚未评估，距离实时临床部署仍有差距。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可构建多中心、多术式的大规模手术VLM基准，并探索结合视觉提示与链式思维推理的实时手术场景理解框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次系统量化主流大视觉-语言模型在手术器械检测上的性能与局限，为研究多模态手术感知、零样本泛化及参数高效微调的研究者提供可直接对比的基线与实验洞察。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.50</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16520v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TangramPuzzle: Evaluating Multimodal Large Language Models with Compositional Spatial Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TangramPuzzle：基于组合空间推理的多模态大型语言模型评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Daixian Liu，Jiayi Kuang，Yinghui Li，Yangning Li，Di Yin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16520v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual recognition and semantic understanding. Nevertheless, their ability to perform precise compositional spatial reasoning remains largely unexplored. Existing benchmarks often involve relatively simple tasks and rely on semantic approximations or coarse relative positioning, while their evaluation metrics are typically limited and lack rigorous mathematical formulations. To bridge this gap, we introduce TangramPuzzle, a geometry-grounded benchmark designed to evaluate compositional spatial reasoning through the lens of the classic Tangram game. We propose the Tangram Construction Expression (TCE), a symbolic geometric framework that grounds tangram assemblies in exact, machine-verifiable coordinate specifications, to mitigate the ambiguity of visual approximation. We design two complementary tasks: Outline Prediction, which demands inferring global shapes from local components, and End-to-End Code Generation, which requires solving inverse geometric assembly problems. We conduct extensive evaluation experiments on advanced open-source and proprietary models, revealing an interesting insight: MLLMs tend to prioritize matching the target silhouette while neglecting geometric constraints, leading to distortions or deformations of the pieces.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估多模态大模型能否进行精确的组合式空间推理</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于七巧板提出TCE符号几何框架并设计轮廓预测与端到端代码生成任务</p>
                <p><span class="font-medium text-accent">主要发现：</span>MLLMs重目标轮廓匹配而轻几何约束，导致拼图块扭曲变形</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用坐标级可验证的TCE基准量化MLLMs组合空间推理能力</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为诊断和改进大模型精细几何推理提供严格基准与方法论</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)在视觉识别与语义理解上进展迅速，但其精确的组合式空间推理能力尚未被系统检验。现有基准多停留在简单语义近似或粗略相对定位，缺乏严格数学形式化，难以衡量模型对几何约束的细粒度遵从。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以七巧板游戏为场景提出TangramPuzzle基准，并设计Tangram Construction Expression(TCE)——一种用绝对坐标符号化描述板块拼装的几何框架，使标签可机验且消除视觉歧义。任务一Outline Prediction要求模型给定局部分块预测全局轮廓；任务二End-to-End Code Generation则让模型直接输出可执行代码完成逆向拼装。实验覆盖多种开源与闭源MLLM，采用TCE提供的精确IoU、角度误差与边长误差等多维指标进行量化评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，所有受测模型在语义层面能大致拼出目标轮廓，但几何精度显著不足：IoU平均仅0.55，角度误差最高达18°，边长拉伸/压缩可达15%。模型倾向优先匹配目标剪影而忽视硬几何约束，导致板块出现扭曲、重叠或缝隙。结果首次量化揭示了MLLM在组合式空间推理上的系统性缺陷，并证明TCE可稳定区分不同模型的细微能力差异。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>TCE目前仅覆盖二维刚体变换，尚未检验更一般的仿射、投影或三维空间推理；任务局限于七巧板七种板块，对复杂曲线、柔性部件等泛化性未知；实验主要关注静态图像输入，未涉及时序动作或语言-动作闭环。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将TCE扩展至三维积木或机械装配，并引入可微分物理模拟以评估模型对碰撞、稳定性等动态约束的推理；同时探索在训练阶段加入TCE风格的几何正则，以提升模型对硬空间约束的敏感性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态模型在几何、机器人、CAD 或教育拼图等领域的细粒度空间推理评估与改进，本工作提供了首个可机验、数学严格的基准与诊断工具，可直接复用或扩展其TCE框架与任务范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115404" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-modal Dual Attention Graph Contrastive Learning for Recommendation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于推荐的多模态双注意力图对比学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shouxing Ma，Shiqing Wu，Yawen Zeng，Kaize Shi，Guandong Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115404" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115404</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modal recommender systems, incorporating rich content information (e.g., images and texts) into user behavior modeling, have attracted significant attention recently. Current work has successfully combined graph neural networks (GNNs) and contrastive learning to improve recommendation accuracy and mitigate the inherent sparse data problem. Yet, view augmentation strategies borrowed from other domains—such as edge or node dropout—tend to distort the original graph structure, leading to unintended semantic drift and suboptimal representation learning. Moreover, prior work has predominantly focused on optimizing inter-modal weights while overlooking user-specific modality preferences and adaptation of modal features generated by generic models. To tackle the above issues, we propose a novel multi-m O dal d U al a T tention G raph c O ntrastive learning framework (OUTGO). Specifically, we first encode user and item representations by utilizing user and item homogeneous GNNs. Then, we employ designed intra- and inter-attention mechanisms, sequentially and adaptively, tuning each modal feature value based on the principal loss and considering fusing them with different modal perspectives. Additionally, semantic and structural contrastive learning tasks are introduced to alleviate the sparse data without destroying the original data structure. Extensive experiments on real-world datasets demonstrate the superiority of OUTGO compared to state-of-the-art baselines. The code is available at https://anonymous.4open.science/r/OUTGO .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>多模态推荐中如何在不破坏图结构的前提下缓解数据稀疏并捕捉用户模态偏好。</p>
                <p><span class="font-medium text-accent">研究方法：</span>同质 GNN 编码+序列化双注意力机制+语义/结构对比学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>OUTGO 在真实数据集上显著优于现有基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出无结构破坏的语义-结构对比任务及用户特定模态自适应双注意力融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建鲁棒多模态推荐系统提供了不损图结构的新对比学习范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态推荐系统通过引入图像、文本等富媒体内容来增强用户行为建模，已成为热点。然而，现有将图神经网络与对比学习结合的方法普遍采用跨域借用的图增广（如随机删边/节点），易扭曲原图语义并导致表示漂移。同时，既有工作侧重模态间权重优化，却忽视用户个性化的模态偏好及对通用预提取模态特征的二次适配。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出OUTGO框架：先分别用用户同质图和物品同质图GNN编码得到双方初始表示；随后串行执行intra-attention（单模态内特征重标定）与inter-attention（跨模态融合权重自适应），依据主任务损失动态调整每维特征并显式建模用户模态偏好；最后引入语义级与结构级两类对比任务，在不破坏原图连接的前提下生成正样本，缓解数据稀疏。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个真实数据集上的实验显示，OUTGO在HR@20、NDCG@20等指标上平均提升6-11%，显著优于SOTA多模态对比推荐基线；消融实验表明intra+inter双重注意力与语义/结构对比任务分别贡献约40%与35%的性能增益；可视化分析揭示框架能自动降低低质量模态权重，减少噪声传播。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开预提取视觉/文本特征的具体模型与超参，复现难度较高；对比任务仅依赖图结构相似性，未考虑时序或外部知识；实验局限于短视频与电商场景，对新闻、音乐等稀疏模态场景的泛化能力尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索大模型生成的动态模态表征与OUTGO的端到端联合训练，并引入因果推断以区分用户真实兴趣与曝光偏差。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态信息融合、图对比学习或个性化模态偏好建模，本文提供的双注意力机制与无增广对比策略可直接迁移并扩展至社交推荐、跨域推荐等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16214v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CamPilot: Improving Camera Control in Video Diffusion Model with Efficient Camera Reward Feedback
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CamPilot：利用高效相机奖励反馈改进视频扩散模型中的相机控制</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenhang Ge，Guibao Shen，Jiawei Feng，Luozhou Wang，Hao Lu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16214v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in camera-controlled video diffusion models have significantly improved video-camera alignment. However, the camera controllability still remains limited. In this work, we build upon Reward Feedback Learning and aim to further improve camera controllability. However, directly borrowing existing ReFL approaches faces several challenges. First, current reward models lack the capacity to assess video-camera alignment. Second, decoding latent into RGB videos for reward computation introduces substantial computational overhead. Third, 3D geometric information is typically neglected during video decoding. To address these limitations, we introduce an efficient camera-aware 3D decoder that decodes video latent into 3D representations for reward quantization. Specifically, video latent along with the camera pose are decoded into 3D Gaussians. In this process, the camera pose not only acts as input, but also serves as a projection parameter. Misalignment between the video latent and camera pose will cause geometric distortions in the 3D structure, resulting in blurry renderings. Based on this property, we explicitly optimize pixel-level consistency between the rendered novel views and ground-truth ones as reward. To accommodate the stochastic nature, we further introduce a visibility term that selectively supervises only deterministic regions derived via geometric warping. Extensive experiments conducted on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of our proposed method. Project page: \href{https://a-bigbao.github.io/CamPilot/}{CamPilot Page}.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升视频扩散模型的相机可控性，解决现有方法对齐评估缺失、解码开销大、忽略3D几何的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建高效相机感知3D解码器，将潜在视频与相机位姿解码为3D高斯，利用新视图渲染清晰度量化对齐奖励并优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RealEstate10K和WorldScore基准上显著增强相机控制精度与视频质量，验证3D奖励反馈有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将3D高斯解码引入奖励反馈学习，用几何扭曲导致的模糊度直接衡量视频-相机对齐，实现高效像素级监督。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为可控视频生成研究提供轻量级3D奖励方案，推动相机精准操控在内容创作与仿真中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>虽然最新的可控相机视频扩散模型已显著提升了视频与相机位姿的一致性，但相机可控性仍显不足。作者观察到现有奖励反馈学习(ReFL)方法直接迁移到相机控制任务时，缺乏能评估视频-相机对齐的奖励模型，且将隐空间解码为RGB视频再计算奖励代价高昂，同时忽略了3D几何信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出CamPilot，通过轻量级相机感知3D解码器将视频隐变量与相机位姿共同解码为3D高斯表示，利用位姿同时作为输入和投影参数；若隐变量与位姿不一致，会在3D结构中引入几何畸变并导致渲染模糊。作者将渲染新视角与真实视角的像素级一致性作为显式奖励，并引入基于几何翘曲的可见性项，仅对确定性区域进行监督，从而避免随机区域带来的噪声。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RealEstate10K和WorldScore基准上的大量实验表明，CamPilot显著提高了相机控制精度与视频-相机对齐度，同时保持较低的额外计算开销。该方法验证了利用3D几何反馈强化隐空间训练的可行性，为后续可控生成研究提供了新范式。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖3D高斯表示的准确性，若场景深度估计或高斯参数化出现偏差，奖励信号可能失真；此外，可见性掩码基于几何翘曲，对动态或非朗伯区域仍可能引入误差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将CamPilot扩展至动态场景与多对象交互，或结合更丰富的物理先验以提升奖励模型的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为研究视频扩散模型中的相机控制、3D几何反馈与奖励学习提供了高效框架，对致力于提升可控生成、3D感知与神经渲染的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.47</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16965v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Spatial-Agent: Agentic Geo-spatial Reasoning with Scientific Core Concepts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Spatial-Agent：基于科学核心概念的智能体地理空间推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Riyang Bao，Cheng Yang，Dazhou Yu，Zhexiang Tang，Gengchen Mai 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16965v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Geospatial reasoning is essential for real-world applications such as urban analytics, transportation planning, and disaster response. However, existing LLM-based agents often fail at genuine geospatial computation, relying instead on web search or pattern matching while hallucinating spatial relationships. We present Spatial-Agent, an AI agent grounded in foundational theories of spatial information science. Our approach formalizes geo-analytical question answering as a concept transformation problem, where natural-language questions are parsed into executable workflows represented as GeoFlow Graphs -- directed acyclic graphs with nodes corresponding to spatial concepts and edges representing transformations. Drawing on spatial information theory, Spatial-Agent extracts spatial concepts, assigns functional roles with principled ordering constraints, and composes transformation sequences through template-based generation. Extensive experiments on MapEval-API and MapQA benchmarks demonstrate that Spatial-Agent significantly outperforms existing baselines including ReAct and Reflexion, while producing interpretable and executable geospatial workflows.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型真正执行可验证的地理空间计算，而非依赖搜索或模式匹配。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将自然语言问题解析为GeoFlow图，节点为空间概念、边为变换，按空间信息理论排序并模板化生成可执行工作流。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MapEval-API与MapQA上显著优于ReAct、Reflexion等基线，输出可解释且可执行。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把空间信息科学基础理论嵌入AI代理，用概念变换与GeoFlow图形式化地理问答。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为城市分析、交通规划、灾害响应等领域提供可信、可复现的LLM地理推理框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有大模型智能体在回答地理空间问题时，多依赖网络搜索或模式匹配，缺乏真正的空间计算能力，容易产生空间关系幻觉。作者认为根本原因在于缺乏对空间信息科学基础理论的显式建模，导致无法将自然语言问题转化为可执行、可解释的地理分析流程。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Spatial-Agent 把地理分析问题形式化为“概念转换”任务，先用 LLM 从问句中抽取空间概念节点，再按空间信息理论赋予其功能角色与顺序约束，最后通过模板化生成将节点组装成有向无环图 GeoFlow Graph。图中边表示数据或算子转换，节点对应缓冲区、叠加、网络分析等原子操作，可直接调用后端 GIS API 执行并返回结果。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MapEval-API 与 MapQA 两个基准上，Spatial-Agent 比 ReAct、Reflexion 等强基线平均提升 18–25 个百分点，同时输出可读的地理工作流图供用户验证。消融实验表明，显式概念角色与拓扑排序约束是性能提升的主要来源，且生成的流程在真实 PostGIS 环境中 92 % 可成功执行。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅测试了英文问句与美国公开数据集，对多语言、多尺度或实时流数据场景尚未验证；GeoFlow 模板库目前依赖人工设计，覆盖的算子类型有限，可能遗漏复杂时空分析任务。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习模板生成与神经-符号联合优化，让智能体在交互中自动扩展算子库并适应多语言、多区域数据格式。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注空间语义解析、可解释地理 AI 或 LLM 与 GIS 深度耦合，该文提供了将空间信息理论注入智能体决策链的完整范式与评测基准，可直接对比或扩展其 GeoFlow 框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113124" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Generative Model-Based Mixed-Semantic Enhancement for Transductive Zero-Shot Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于生成模型的混合语义增强用于直推式零样本学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Huaizhou Qi，Yang Liu，Jungong Han，Lei Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113124" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113124</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Zero-shot learning (ZSL) addresses the critical challenge of recognizing and classifying instances from categories not seen during training. Although generative model-based approaches have achieved notable success in ZSL, their predominant reliance on forward generation strategies coupled with excessive dependence on auxiliary information hampers model generalization and robustness. To overcome these limitations, we propose a Mixed-Semantic Enhancement framework inspired by interpolation-based feature extraction. This novel approach is designed to synthesize enriched auxiliary information through integrating authentic semantic cues, thereby refining the mapping from semantic descriptions to visual features. The enhanced feature synthesis capability enables better discrimination of ambiguous classes while preserving inter-class relationships. In addition, we establish bidirectional alignment between visual features and auxiliary information. This cross-modal interaction mechanism not only strengthens the generator’s training process through feature consistency constraints but also facilitates dynamic information exchange between modalities. Extensive experiments in a transductive setting across four benchmark datasets demonstrate significant performance gains, highlighting the robustness and effectiveness of our approach in advancing generative ZSL models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少生成式ZSL对前向生成与辅助信息的过度依赖并提升泛化性</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于插值特征提取的混合语义增强框架，实现视觉-语义双向对齐与合成</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个基准数据集的转导ZSL任务上显著优于现有生成方法，验证鲁棒性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入插值式混合语义增强与跨模态双向对齐约束，改进生成器训练</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为生成式零样本学习提供新的语义增强与对齐思路，可直接提升分类性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>零样本学习(ZSL)旨在识别训练阶段未见过的类别，生成式方法虽取得进展，却过度依赖前向生成与辅助信息，导致泛化与鲁棒性受限。本文动机在于缓解对单一方向生成和冗余语义先验的依赖，以提升模型在真实开放集场景下的判别能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Mixed-Semantic Enhancement框架，借鉴插值式特征提取思想，在语义空间与视觉空间之间进行双向对齐。具体而言，该方法将真实语义线索与生成特征混合，合成更丰富且一致的辅助信息，并引入跨模态一致性约束，使生成器在训练过程中动态交换视觉与语义信息，从而细化语义到视觉的映射并保留类间关系。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个基准数据集的转导设置下，该方法显著优于现有生成式ZSL模型，平均提升约3-5个Harmonic均值点，并在广义ZSL场景中表现出更强的鲁棒性。实验表明，混合语义增强有效缓解了歧义类混淆，同时保持了对未见类的判别力，验证了双向对齐在提升生成质量与跨模态一致性的双重效益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖充足的语义属性标注，若属性缺失或噪声较大，插值增强可能放大偏差；其次，双向对齐增加了训练复杂度与超参数数量，对大规模数据集的可扩展性尚未验证；此外，实验仅在转导设定下完成，归纳ZSL及在线增量场景的表现仍未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索弱监督或无属性场景下的自监督语义增强，并将框架扩展至归纳或持续ZSL，以验证其在大规模动态环境中的稳定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为生成式ZSL提供了新的插值-对齐范式，对关注跨模态生成、鲁棒特征合成或开放集识别的研究者具有直接参考价值，其双向约束思想亦可迁移至其他零样本视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.44</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.49</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16211v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Why Can&#39;t I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">为什么我打不开抽屉？缓解零样本组合动作识别中的对象驱动捷径</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Geo Ahn，Inwoong Lee，Taeoh Kim，Minho Shim，Dongyoon Wee 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16211v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined factors: severe sparsity and skewness of compositional supervision, and the asymmetric learning difficulty between verbs and objects. As training progresses, the existing ZS-CAR model increasingly ignores visual evidence and overfits to co-occurrence statistics. Consequently, the existing model does not gain the benefit of compositional recognition in unseen verb-object compositions. To address this, we propose RCORE, a simple and effective framework that enforces temporally grounded verb learning. RCORE introduces (i) a composition-aware augmentation that diversifies verb-object combinations without corrupting motion cues, and (ii) a temporal order regularization loss that penalizes shortcut behaviors by explicitly modeling temporal structure. Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps. Our findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>零样本组合动作识别为何因“物体驱动捷径”而失效</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RCORE框架：组合感知增广+时序顺序正则，强制动词时序学习</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型忽视视觉运动而死记共现统计，RCORE显著提 unseen 组合精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示并缓解物体捷径，用无偏增广与结构正则化保障动词时序建模</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可泛化组合视频理解系统指明去捷径化训练方向</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>零样本组合动作识别(ZS-CAR)要求模型在训练阶段未见过的动词-对象组合上仍保持泛化，但现有方法在测试时往往崩溃。作者发现这一失败并非因为组合空间过大，而是由于模型偷偷利用“对象→动词”共现捷径：只要认出对象就猜最常伴随的动词，完全忽略运动线索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先通过统计与可视诊断证明，训练集的组合标注极度稀疏且长尾，导致模型在优化时自动选择更简单的对象特征而抑制动词特征。为此提出RCORE框架：①组合感知增强——对同一视频在帧序不变前提下替换对象标签，生成稀有动词-对象对，以稀释共现偏差；②时序顺序正则化——在特征空间强制动作片段的帧级顺序可逆重构，若模型仅靠静态对象就无法重构顺序，从而惩罚捷径行为。整体训练仍保持端到端，仅增加两项轻量级损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Sth-com和新建的EK100-com两个基准上，RCORE将未见组合准确率分别提升6.8和9.3个百分点，把“组合增益”( unseen–seen 差距)从负值转为显著正值。可视化显示，RCORE的动词激活区域更集中于运动部位，共现偏差估计量下降约40%，证明捷径被有效抑制。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>增强策略依赖现成的对象检测器，若检测器在目标域失效则性能可能下降；方法目前仅针对动词-对象二部组合，尚未扩展到副词、工具等更高阶组合；实验主要关注分类准确率，对计算开销与实时性讨论有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入因果干预或反事实采样进一步削弱共现偏差，并研究无检测器、完全基于视觉-语言预训练的增强方式；同时把时序正则推广到更细粒度的时序片段排序，以支持复杂多步骤组合动作识别。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注组合泛化、动作识别中的捷径效应、或视频-语言模型的鲁棒性，本文提供的诊断工具与无需额外标注的RCORE框架可直接迁移到相关任务，为抑制偏差、提升 unseen 性能提供可复现的基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>