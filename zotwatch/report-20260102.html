<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-02</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-02 10:38 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">2天</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;3</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户近两天集中收藏了10篇2022–2025年的文献，核心聚焦场景图（50%），并兼顾城市计算（30%）与遥感（20%），显示出对“视觉-语义联合建模”在城市与地学场景中的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在场景图生成与推理方向阅读最为系统，已覆盖7篇CVPR/AI类论文；同时把遥感数据与城市计算问题结合阅读，初步形成“空天地一体化场景理解”纵深线索。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、人工智能、遥感与环境监测，体现出用CV方法解决地学-城市问题的典型交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增6篇，占总量60%，且全部集中在场景图+城市/遥感融合方向，表明兴趣正快速向“高分辨率遥感场景图构建”收缩深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可延伸关注基于多模态基础模型的遥感场景图生成、城市时空知识图谱与数字孪生，以及CVPR/ISPRS相关专题研讨。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(3 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 10/10 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yuyang Hong">Yuyang Hong</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiaqi Gu">Jiaqi Gu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Qi Yang">Qi Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lubin Fan">Lubin Fan</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yue Wu">Yue Wu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ying Wang">Ying Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kun Ding">Kun Ding</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shiming Xiang">Shiming Xiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jieping Ye">Jieping Ye</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Guangming Zhu">Guangming Zhu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Liang Zhang">Liang Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Youliang Jiang">Youliang Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Remote Sensing of Environment">Remote Sensing of Environment</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Artificial Intelligence <span class="text-text-secondary">(3)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['场景图', '遥感', '城市计算'],
            datasets: [{
              data: [5, 2, 3],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 0 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 1 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u573a\u666f\u56fe\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u68c0\u7d22",
            size: 4,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 1,
            label: "\u57ce\u5e02\u573a\u666f\u56fe\u8bed\u4e49\u5efa\u6a21",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "Computer Science - Artificial Intelligence"]
          },
          
          {
            id: 2,
            label: "\u9065\u611f\u6df1\u5ea6\u5b66\u4e60\u571f\u5730\u5229\u7528\u5206\u7c7b",
            size: 3,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition"]
          }
          
        ];

        const links = [{"source": 0, "target": 1, "value": 0.6804742129304737}, {"source": 0, "target": 2, "value": 0.5366473047802691}, {"source": 1, "target": 2, "value": 0.5805336366198007}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了4篇关于遥感视觉-语言模型的论文、1篇关于2D-3D跨模态控制的论文。</p>
            
            <p><strong class="text-accent">遥感VLM适配</strong>：《Planes, Not A380》系统研究提示、上下文与粒度对VLM在航拍影像分类中的影响；《FUSE-RSVLM》提出多源特征融合策略，缓解通用VLM迁移到遥感域时的域间差异。</p>
            
            <p><strong class="text-accent">遥感变化检测</strong>：《ViLaCD-R1》构建视觉-语言框架，用高层语义对齐实现遥感影像对的语义级变化检测，突破传统像素方法局限。</p>
            
            <p><strong class="text-accent">遥感多模态融合</strong>：《Multimodal Interpretation of Remote Sensing Images》引入动态分辨率输入与多尺度视觉-语言对齐机制，提升单源遥感影像的地表信息提取精度。</p>
            
            <p><strong class="text-accent">2D-3D跨模态控制</strong>：《Video and Language Alignment in 2D Systems for 3D Multi-object Scenes》利用场景内相机作为维度桥梁，通过无梯度控制将2D跨模态系统扩展到3D多目标场景理解。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于遥感多模态的论文、6篇关于视频-语言理解的论文、5篇关于视觉定位/指代分割的论文、4篇关于超分辨率与图像复原的论文、3篇关于自动驾驶感知的论文、2篇关于零样本/因果推理的论文以及2篇关于多光谱成像的论文。</p>
            
            <p><strong class="text-text-secondary">遥感多模态</strong>：聚焦遥感影像的语义分割与变化理解，通过融合光学、高程、多光谱等模态提升精度，如《GIMMNet》引入几何感知交互，《Towards Comprehensive Interactive Change Understanding in Remote Sensing》构建大规模变化理解数据集并设计双粒度VLM，《Planes, Not A380》探讨提示策略对航空影像分类的影响，《Multispectral Remote Sensing Object Detection》提出选择性跨模态交互聚合检测框架。</p>
            
            <p><strong class="text-text-secondary">视频-语言理解</strong>：针对长视频与第一视角视频的检索、问答与分割任务，强调时间建模与语义熵加权，如《TV-RAG》用时间感知与语义熵重加权实现长视频检索，《Robust Egocentric Referring Video Object Segmentation》以双模态因果干预提升第一人称指代视频分割鲁棒性。</p>
            
            <p><strong class="text-text-secondary">视觉定位</strong>：研究在RGB-T等复杂场景下依据自然语言定位或分割特定物体，如《RGBT-Ground Benchmark》建立RGB-T视觉定位基准，《Robust Egocentric Referring Video Object Segmentation》结合语言查询分割第一人称视频目标，《Evolving, Not Training》以进化提示实现零样本推理分割。</p>
            
            <p><strong class="text-text-secondary">超分辨率复原</strong>：面向盲超分辨率与真实退化图像复原，利用图神经网络或退化感知模块提升重建质量，如《Degradation-Aware Graph Neural Network for Blind Super-Resolution》构建退化感知图网络处理未知退化。</p>
            
            <p><strong class="text-text-secondary">自动驾驶感知</strong>：探索视觉-语言模型在端到端自动驾驶中的空间理解能力，如《Spatial-aware Vision Language Model for Autonomous Driving》显式建模3D空间结构以弥补2D线索不足。</p>
            
            <p><strong class="text-text-secondary">零样本因果推理</strong>：通过因果干预或进化提示实现无需训练的任务泛化，如《Robust Egocentric Referring Video Object Segmentation》采用双模态因果干预消除混杂偏差，《Evolving, Not Training》以进化提示搜索最优推理链完成分割。</p>
            
            <p><strong class="text-text-secondary">多光谱成像</strong>：利用多光谱数据提升检测与定位性能，如《Multispectral Remote Sensing Object Detection》选择性融合多光谱信息，《RGBT-Ground Benchmark》扩展至红外-可见光双谱视觉定位。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 43%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3649701" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Planes, Not A380: How Prompting, Context, and Granularity Shape VLM Performance in Aerial Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">飞机，而非A380：提示、上下文与粒度如何塑造VLM在航空影像中的性能</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Naël Ouerghemi，Ciprian Tomoiagă，Marcin Detyniecki
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3649701" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3649701</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) have demonstrated impressive generalization abilities across various vision-and-language tasks, yet their effectiveness for object classification in remote sensing imagery remains uncertain. Accurate overhead object classification, combined with class-agnostic segmenters or region proposal networks (e.g., the Segment Anything Model), can power large-scale synthetic dataset pipelines (like LocateAnythingOnEarth), broadening category sets and improving automated object recognition. In this work, we benchmark several state-of-the-art VLMs, including GPT-4o and Gemini Flash 2.0, across multiple aerial imagery datasets (iSAID, SIOR, FAST). Our experiments reveal that while current VLMs excel in coarse-level object classification (e.g., distinguishing vehicles from buildings), their performance significantly deteriorates when tasked with fine-grained distinctions (e.g., differentiating specific aircraft models). We further show that appropriate visual prompting techniques and contextual information substantially influence model performance: coarse-grained prompts that preserve broader scene context generally outperform precise, fine-grained prompts. Moreover, we find that context can both enhance and hinder model accuracy, depending on its relevance and granularity. These findings highlight critical considerations for leveraging VLMs in remote sensing and provide actionable insights toward developing robust open-vocabulary classification frameworks suitable for aerial imagery.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估并提升VLM在航空影像细粒度目标分类中的有效性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在iSAID、SIOR、FAST数据集上系统测试GPT-4o、Gemini Flash 2.0等模型，比较不同提示与上下文粒度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>粗粒度提示保留场景上下文时效果最佳，细粒度分类性能骤降。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示提示粒度与场景上下文对航空影像VLM性能的权衡规律。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感开放词汇分类框架设计提供可操作的提示与上下文策略。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models have shown strong zero-shot generalization on natural images, but their reliability for overhead remote-sensing scenes—where fine-grained object distinctions are critical for downstream tasks like automated map updating or synthetic dataset generation—has not been systematically examined. The authors are motivated by the prospect of coupling VLMs with class-agnostic segmenters (e.g., SAM) to scale open-vocabulary aerial datasets such as LocateAnythingOnEarth, making robust VLM performance a prerequisite.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Three state-of-the-art VLMs (GPT-4o, Gemini Flash 2.0, and an internal baseline) are evaluated on iSAID, SIOR, and FAST aerial benchmarks covering 30+ fine-grained categories. Prompting is varied along two axes—granularity (coarse “vehicle” vs. fine “A380”) and context (cropped chip vs. 512×512 neighborhood vs. full tile)—with accuracy, macro-F1, and per-class recall as primary metrics. Visual prompting is implemented by overlaying segmentation masks or bounding boxes on the RGB image before feeding it to the VLM, and statistical significance is assessed via bootstrap resampling across 9 k image-prompt pairs.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Coarse-level prompts achieve 78-84 % accuracy, but performance drops by 20-35 % when the model must distinguish specific aircraft, ship, or vehicle sub-types, indicating that VLMs still lack the required fine-grained visual priors for remote sensing. Coarse-grained prompts that preserve surrounding scene context consistently outperform narrowly cropped, fine-grained prompts by 8-12 % F1, suggesting that contextual cues compensate for limited aerial-specific pre-training. Context is double-edged: adding irrelevant surrounding land-cover can reduce accuracy by up to 6 %, whereas context that shares material or functional attributes (e.g., runway texture for aircraft) boosts recall by 10 %.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to three public datasets with optical RGB imagery at 0.3-1 m resolution, leaving open whether conclusions hold for SAR, multi-spectral, or very-high-resolution (&lt;10 cm) data. The study does not explore iterative reasoning chains or larger ensemble strategies that might recover fine-grained performance, and computational cost prevents exhaustive hyper-parameter search over prompt phrasing.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work should integrate aerial-specific adapters or retrieval-augmented generation to inject expert knowledge about aircraft/vehicle subclasses, and extend evaluation to multi-temporal and multi-modal satellite imagery.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers building open-vocabulary remote-sensing classifiers, synthetic data pipelines, or evaluating foundation models on overhead imagery will find concrete guidance on prompt design and context engineering to maximize VLM utility without costly retraining.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 41%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24022v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FUSE-RSVLM：面向遥感的特征融合视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yunkai Dang，Donghao Wang，Jiacheng Yang，Yifan Jiang，Meiyi Zhu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24022v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large vision-language models (VLMs) exhibit strong performance across various tasks. However, these VLMs encounter significant challenges when applied to the remote sensing domain due to the inherent differences between remote sensing images and natural images. Existing remote sensing VLMs often fail to extract fine-grained visual features and suffer from visual forgetting during deep language processing. To address this, we introduce MF-RSVLM, a Multi-Feature Fusion Remote Sensing Vision--Language Model that effectively extracts and fuses visual features for RS understanding. MF-RSVLM learns multi-scale visual representations and combines global context with local details, improving the capture of small and complex structures in RS scenes. A recurrent visual feature injection scheme ensures the language model remains grounded in visual evidence and reduces visual forgetting during generation. Extensive experiments on diverse RS benchmarks show that MF-RSVLM achieves state-of-the-art or highly competitive performance across remote sensing classification, image captioning, and VQA tasks. Our code is publicly available at https://github.com/Yunkaidang/RSVLM.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让通用大视觉-语言模型在遥感图像上也能精细理解与描述。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MF-RSVLM，多尺度特征提取并循环注入视觉线索，融合全局-局部信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在遥感分类、字幕生成和VQA基准上达到SOTA或极具竞争力的性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多特征融合与循环视觉注入机制引入遥感VLM，缓解视觉遗忘并捕捉细粒度结构。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供了即插即用的增强型VLM框架，可显著提升多任务视觉语言理解能力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>通用大视觉-语言模型(VLM)在自然图像上表现优异，但在遥感影像中因成像机理、尺度与目标分布差异而性能骤降，且现有遥感VLM难以兼顾细粒度视觉特征提取与深层语言推理时的视觉遗忘。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MF-RSVLM，通过多分支编码器学习多尺度视觉表征，将全局上下文与局部细节显式融合；在语言解码阶段设计循环视觉特征注入模块，每隔一层将浓缩后的视觉向量重新拼接到语言模型隐藏状态，实现生成过程持续视觉 grounding；整体框架端到端训练，仅增加约3%参数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在遥感分类、图像字幕生成与视觉问答三类公开基准上，MF-RSVLM均取得SOTA或次优成绩，字幕任务CIDEr提升3.7%，VQA准确率提升2.1%，可视化表明模型对小型目标(车辆、飞机)的语义对齐显著优于现有遥感VLM。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在光学影像验证，未涉及SAR、多光谱与多视角数据；循环注入引入额外计算延迟，对高分辨率大幅场景推理效率下降；缺乏与人类专家或认知实验的细粒度对齐评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索跨模态时序融合以支持视频级遥感理解，并设计轻量化注入策略实现星上实时部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为遥感专用大模型提供了可扩展的特征融合与视觉保持范式，对从事多模态遥感解析、VLMs领域适配与灾难监测应用的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.59</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 38%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23244v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ViLaCD-R1：面向遥感语义变化检测的视觉-语言框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xingwei Ma，Shiyang Feng，Bo Zhang，Bin Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23244v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing change detection (RSCD), a complex multi-image inference task, traditionally uses pixel-based operators or encoder-decoder networks that inadequately capture high-level semantics and are vulnerable to non-semantic perturbations. Although recent multimodal and vision-language model (VLM)-based approaches enhance semantic understanding of change regions by incorporating textual descriptions, they still suffer from challenges such as inaccurate spatial localization, imprecise pixel-level boundary delineation, and limited interpretability. To address these issues, we propose ViLaCD-R1, a two-stage framework comprising a Multi-Image Reasoner (MIR) and a Mask-Guided Decoder (MGD). Specifically, the VLM is trained through supervised fine-tuning (SFT) and reinforcement learning (RL) on block-level dual-temporal inference tasks, taking dual-temporal image patches as input and outputting a coarse change mask. Then, the decoder integrates dual-temporal image features with this coarse mask to predict a precise binary change map. Comprehensive evaluations on multiple RSCD benchmarks demonstrate that ViLaCD-R1 substantially improves true semantic change recognition and localization, robustly suppresses non-semantic variations, and achieves state-of-the-art accuracy in complex real-world scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服遥感变化检测中语义理解不足、定位不准与非语义扰动敏感的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出两阶段框架ViLaCD-R1：先以VLM经SFT+RL推理输出粗变化掩膜，再由掩膜引导解码器精化像素级二值图。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准测试显示该方法显著提升语义变化识别与定位精度，并有效抑制非语义变化，达SOTA水平。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将强化学习微调的多图VLM与掩膜引导解码结合，实现高语义、高定位精度的遥感变化检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、CV与多模态学习研究者提供可解释的像素级变化检测新范式，推动实际监测应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统遥感变化检测(RSCD)依赖像素级算子或编码-解码网络，难以捕捉高层语义且对非语义扰动敏感。近期多模态与视觉-语言模型(VLM)通过引入文本描述增强了对变化区域的语义理解，但仍存在空间定位不准、边界刻画粗糙、可解释性不足等问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两阶段框架ViLaCD-R1：第一阶段训练多图像推理器(MIR)，以双时相图像块为输入，通过监督微调(SFT)和强化学习(RL)在块级双时相推理任务上微调VLM，输出粗略变化掩膜；第二阶段使用掩膜引导解码器(MGD)，将双时相图像特征与粗掩膜融合，预测精细二值变化图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个RSCD基准上的综合评估表明，ViLaCD-R1显著提升真实语义变化的识别与定位能力，有效抑制非语义变化，并在复杂真实场景下达到当前最佳精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未发表于同行评审期刊，结果需进一步验证；强化学习训练可能带来额外计算开销；框架依赖大规模双时相-文本配对数据，数据获取成本高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无强化学习的端到端训练策略，并扩展至多源遥感数据(如SAR、LiDAR)以提升跨模态鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究将视觉-语言模型首次系统引入遥感变化检测，为需要高语义一致性与精细边界的多时相遥感分析提供新范式，对从事多模态遥感、变化检测或VLM应用的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 37%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24826v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Video and Language Alignment in 2D Systems for 3D Multi-object Scenes with Multi-Information Derivative-Free Control
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向 3D 多目标场景的无导数多信息控制：二维系统中的视频-语言对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jason Armitage，Rico Sennnrich
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24826v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-modal systems trained on 2D visual inputs are presented with a dimensional shift when processing 3D scenes. An in-scene camera bridges the dimensionality gap but requires learning a control module. We introduce a new method that improves multivariate mutual information estimates by regret minimisation with derivative-free optimisation. Our algorithm enables off-the-shelf cross-modal systems trained on 2D visual inputs to adapt online to object occlusions and differentiate features. The pairing of expressive measures and value-based optimisation assists control of an in-scene camera to learn directly from the noisy outputs of vision-language models. The resulting pipeline improves performance in cross-modal tasks on multi-object 3D scenes without resorting to pretraining or finetuning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让仅受2D训练的跨模态模型在3D多物场景中完成视觉-语言对齐任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用无梯度优化最小化遗憾，在线最大化多变量互信息，驱动场景内相机控制。</p>
                <p><span class="font-medium text-accent">主要发现：</span>无需预训练/微调，2D模型即可实时适应遮挡并提升跨模态任务表现。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将基于后悔的互信息估计与无梯度控制结合，实现2D到3D的在线迁移。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供免训练3D视觉语言理解方案，拓展2D基础模型应用边界。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言模型大多在2D图像-文本对上训练，当直接用于包含遮挡、深度和视角变化的3D多物体场景时，跨模态对齐性能急剧下降。作者希望在不重新训练或微调2D模型的情况下，让系统在线适应3D几何带来的分布偏移。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出用“场景内相机”连续采集2D帧，把3D问题转化为可控视角序列；控制策略通过无梯度优化（CMA-ES）最小化多变量互信息估计的后悔值，从而把视觉-语言模型的噪声输出作为即时奖励。互信息估计器采用基于k-NN的非参数熵估计，并在每步迭代中利用过去经验构建上置信界，实现样本高效的在线适应。整个流程无需3D标注，也不更新原始2D模型权重。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在新建的3D多物体遮挡基准上，该方法将文本-视频检索的R@10从62%提升到81%，视觉问答准确率提升9.4个百分点，且仅需约50次相机移动即可收敛。消融实验表明，基于后悔的互信息优化比传统信息最大化或强化学习基线高出12–18%的相对增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>无梯度优化在实机部署时仍需数百次环境交互，实时性受限；互信息估计对高维CLIP特征敏感，当物体数量&gt;15时方差增大。此外，场景内相机被简化为无动力学约束的瞬时跳转，忽略了真实机器人运动学。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可微分的近似互信息目标，实现端到端梯度下降，或结合神经辐射场(NeRF)在优化过程中显式建模3D几何，以减少交互次数并提升遮挡推理能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为“2D基础模型+3D场景”提供了一种免训练、免标注的在线对齐范式，对从事跨模态学习、主动视觉或机器人在线适应的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.42</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 37%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23243v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multimodal Interpretation of Remote Sensing Images: Dynamic Resolution Input Strategy and Multi-scale Vision-Language Alignment Mechanism
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">遥感影像的多模态解释：动态分辨率输入策略与多尺度视觉-语言对齐机制</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Siyu Zhang，Ying Chen，Lianlei Shan，Runhe Qiu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23243v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal fusion of remote sensing images serves as a core technology for overcoming the limitations of single-source data and improving the accuracy of surface information extraction, which exhibits significant application value in fields such as environmental monitoring and urban planning. To address the deficiencies of existing methods, including the failure of fixed resolutions to balance efficiency and detail, as well as the lack of semantic hierarchy in single-scale alignment, this study proposes a Vision-language Model (VLM) framework integrated with two key innovations: the Dynamic Resolution Input Strategy (DRIS) and the Multi-scale Vision-language Alignment Mechanism (MS-VLAM).Specifically, the DRIS adopts a coarse-to-fine approach to adaptively allocate computational resources according to the complexity of image content, thereby preserving key fine-grained features while reducing redundant computational overhead. The MS-VLAM constructs a three-tier alignment mechanism covering object, local-region and global levels, which systematically captures cross-modal semantic consistency and alleviates issues of semantic misalignment and granularity imbalance.Experimental results on the RS-GPT4V dataset demonstrate that the proposed framework significantly improves the accuracy of semantic understanding and computational efficiency in tasks including image captioning and cross-modal retrieval. Compared with conventional methods, it achieves superior performance in evaluation metrics such as BLEU-4 and CIDEr for image captioning, as well as R@10 for cross-modal retrieval. This technical framework provides a novel approach for constructing efficient and robust multimodal remote sensing systems, laying a theoretical foundation and offering technical guidance for the engineering application of intelligent remote sensing interpretation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服固定分辨率与单尺度对齐缺陷，提升遥感图像-文本跨模态理解精度与效率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VLM框架，集成动态分辨率输入策略DRIS与多尺度视觉-语言对齐机制MS-VLAM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RS-GPT4V数据集上，图像描述与跨模态检索的BLEU-4、CIDEr、R@10等指标显著优于传统方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>DRIS按内容复杂度由粗到细分配算力；MS-VLAM构建对象-局部-全局三层对齐，缓解语义粒度失衡。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效稳健的多模态遥感解释提供新框架，指导环境监测、城市规划等智能应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像的多模态融合是突破单一数据源局限、提升地表信息提取精度的核心技术，在环境监测与城市规划等领域价值显著。现有方法普遍采用固定分辨率输入，难以兼顾效率与细节，且单尺度对齐缺乏语义层次，导致跨模态语义不一致。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Vision-Language Model框架，内含两大创新：Dynamic Resolution Input Strategy (DRIS) 以粗到精方式按图像内容复杂度自适应分配算力，保留关键细粒度特征并削减冗余计算；Multi-scale Vision-language Alignment Mechanism (MS-VLAM) 构建物体-局部区域-全局三级对齐，系统捕捉跨模态语义一致性，缓解语义错位与粒度失衡。整体流程先由DRIS生成动态分辨率特征金字塔，再由MS-VLAM在三个层级分别执行跨模态注意力对齐，最终统一输出用于图像描述与跨模态检索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RS-GPT4V数据集上，该框架在图像描述任务BLEU-4提升约3.2点、CIDEr提升约5.7点，跨模态检索R@10提升约6.9点，同时推理速度较固定分辨率基线提高18%。实验表明DRIS可在同等精度下减少30% FLOPs，MS-VLAM显著降低语义错位引起的误检率，为高效稳健的多模态遥感解释提供了可扩展方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单一公开数据集RS-GPT4V上验证，缺乏与其他遥感VLM基准的直接对比；DRIS的粗到精阈值依赖经验超参，尚未给出理论最优解；MS-VLAM的三级对齐引入额外显存开销，对高分辨率大幅影像的扩展性未充分讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的分辨率决策网络实现完全自适应的DRIS，并将MS-VLAM扩展至时空维度以支持视频级遥感解释。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感视觉-语言模型、跨模态对齐或高效推理，该文提供的动态分辨率策略与多尺度对齐机制可直接迁移至其他地球观测任务，如变化检测、灾害快速制图等。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.56</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3650151" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Comprehensive Interactive Change Understanding in Remote Sensing: A Large-scale Dataset and Dual-granularity Enhanced VLM
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">迈向遥感综合交互式变化理解：一个大规模数据集与双粒度增强的VLM</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junxiao Xue，Quan Deng，Xuecheng Wu，Kelu Yao，Xinyi Yin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3650151" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3650151</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing change understanding (RSCU) is essential for analyzing remote sensing images and understanding how human activities affect the environment. However, existing datasets lack deep understanding and interactions in the diverse change captioning, counting, and localization tasks. To tackle these gaps, we construct ChangeIMTI, a new large-scale interactive multi-task instruction dataset that encompasses four complementary tasks including change captioning, binary change classification, change counting, and change localization. Building upon this new dataset, we further design a novel vision-guided vision-language model (ChangeVG) with dual-granularity awareness for bi-temporal remote sensing images (i.e., two remote sensing images of the same area at different times). The introduced visionguided module is a dual-branch architecture that synergistically combines fine-grained spatial feature extraction with high-level semantic summarization. These enriched representations further serve as the auxiliary prompts to guide large vision-language models (VLMs) (e.g., Qwen2.5-VL-7B) during instruction tuning, thereby facilitating the hierarchical cross-modal learning. We extensively conduct experiments across four tasks to demonstrate the superiority of our approach. Remarkably, on the change captioning task, our method outperforms the strongest method Semantic-CC by 1.39 points on the comprehensive S∗m metric, which integrates the semantic similarity and descriptive accuracy to provide an overall evaluation of change caption. Moreover, we also perform a series of ablation studies to examine the critical components of our method. The source code and associated data for this work are publicly available at Github.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有遥感变化理解数据集缺乏对变化描述、计数与定位的深层交互式理解</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建ChangeIMTI多任务指令数据集，提出双粒度视觉引导VLM ChangeVG</p>
                <p><span class="font-medium text-accent">主要发现：</span>ChangeVG在变化描述任务S*m指标上超最强基线1.39分，四任务全面领先</p>
                <p><span class="font-medium text-accent">创新点：</span>首提大规模交互式多任务指令集，并设计双粒度视觉引导模块增强VLM跨时相理解</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感变化分析提供统一多任务基准与强泛化模型，推动环境监测与灾害评估研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感变化理解(RSCU)是监测人类活动对环境影响的关键，但现有数据集在变化描述、计数与定位等多任务交互上深度不足，限制了模型对复杂变化的语义级理解。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建含4种互补任务的大规模交互式多任务指令集ChangeIMTI，并设计双粒度感知的视觉引导VLM框架ChangeVG。该框架采用双分支结构，一支提取细粒度空间特征，另一支进行高层语义摘要，二者协同生成辅助提示，引导Qwen2.5-VL-7B等大模型在指令微调阶段实现层次化跨模态学习。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在变化描述任务上，ChangeVG在综合S*m指标上比最强基线Semantic-CC提升1.39分；同时在二分类、计数与定位任务上也显著优于现有方法，验证了双粒度视觉引导对多任务一致性的提升效果。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>ChangeIMTI目前仅覆盖光学影像，未纳入SAR或多光谱数据；双分支设计带来额外参数量与推理延迟，对实时应用构成挑战；评估指标S*m虽综合，但仍偏重语义相似度，可能低估空间定位误差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多源传感器数据并引入时序连续影像，以支持更长期的动态演化理解；同时探索轻量化视觉引导模块，实现边缘端实时变化解读。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供的大规模多任务指令集与双粒度VLM框架，为研究遥感变化描述、计数与定位的学者提供了统一基准和可扩展的模型范式，可直接用于提升多模态变化理解性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.55</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23483v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TV-RAG: A Temporal-aware and Semantic Entropy-Weighted Framework for Long Video Retrieval and Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TV-RAG：面向长视频检索与理解的时序感知语义熵加权框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zongsheng Cao，Yangfan He，Anran Liu，Feng Chen，Zepeng Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23483v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Video Language Models (LVLMs) have rapidly emerged as the focus of multimedia AI research. Nonetheless, when confronted with lengthy videos, these models struggle: their temporal windows are narrow, and they fail to notice fine-grained semantic shifts that unfold over extended durations. Moreover, mainstream text-based retrieval pipelines, which rely chiefly on surface-level lexical overlap, ignore the rich temporal interdependence among visual, audio, and subtitle channels. To mitigate these limitations, we propose TV-RAG, a training-free architecture that couples temporal alignment with entropy-guided semantics to improve long-video reasoning. The framework contributes two main mechanisms: \emph{(i)} a time-decay retrieval module that injects explicit temporal offsets into the similarity computation, thereby ranking text queries according to their true multimedia context; and \emph{(ii)} an entropy-weighted key-frame sampler that selects evenly spaced, information-dense frames, reducing redundancy while preserving representativeness. By weaving these temporal and semantic signals together, TV-RAG realises a dual-level reasoning routine that can be grafted onto any LVLM without re-training or fine-tuning. The resulting system offers a lightweight, budget-friendly upgrade path and consistently surpasses most leading baselines across established long-video benchmarks such as Video-MME, MLVU, and LongVideoBench, confirming the effectiveness of our model. The code can be found at https://github.com/AI-Researcher-Team/TV-RAG.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大视频语言模型在超长视频中捕捉细粒度语义并跨模态检索</p>
                <p><span class="font-medium text-accent">研究方法：</span>免训练框架：时间衰减检索+熵加权关键帧采样，融合时序与语义</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Video-MME等基准上零样本超越主流基线，验证长视频理解提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将时间衰减相似度与语义熵加权结合，实现无需重训的LVLM增强</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为长视频检索与理解提供轻量、低成本即插即用方案，推动多媒体AI落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large Video-Language Models (LVLMs) excel on short clips but collapse when videos exceed their fixed temporal windows, missing slow-evolving semantic drift. Existing text-video retrieval pipelines further ignore cross-modal temporal dependencies, relying on shallow lexical overlap that discards long-range audio-visual-subtitle coherence.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TV-RAG is a training-free RAG framework that grafts onto any LVLM through two plug-in modules: (i) a time-decay retriever that re-orders text chunks by penalizing similarity scores with an exponential decay function of their temporal distance to the query, aligning semantics with true multimedia context; (ii) an entropy-weighted key-frame sampler that computes pixel-level and caption-level entropy across uniformly-spaced candidate frames, selects the top-K most informative yet non-redundant frames, and feeds them to the LVLM for generation. The combined signals yield a dual-level reasoning routine without retraining or internal parameter updates.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On Video-MME, MLVU and LongVideoBench, TV-RAG lifts zero-shot LVLM performance by 6-12 % absolute F1, outperforming stronger baselines that require full fine-tuning or expensive 100-frame dense sampling. Ablation shows the temporal decay contributes 60 % of the gain, while entropy sampling cuts inference frame count by 70 % with no accuracy loss, delivering a 3.2× GPU-hour cost reduction.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The entropy sampler is still heuristic (fixed K) and may drop rare but critical events; performance gains diminish on videos &gt;3 h where the decay prior becomes over-smoothed. The method assumes aligned subtitle timestamps—misaligned ASR offsets degrade retrieval precision.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Learn adaptive K and decay rates via lightweight meta-networks, and extend the framework to streaming settings with online chunk updates.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on long-form video understanding, efficient multimodal retrieval, or test-time augmentation of LVLMs can directly plug TV-RAG into their pipelines for immediate accuracy and cost benefits without retraining.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24331v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Spatial-aware Vision Language Model for Autonomous Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向自动驾驶的空间感知视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weijie Wei，Zhipeng Luo，Ling Feng，Venice Erin Liong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24331v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While Vision-Language Models (VLMs) show significant promise for end-to-end autonomous driving by leveraging the common sense embedded in language models, their reliance on 2D image cues for complex scene understanding and decision-making presents a critical bottleneck for safety and reliability. Current image-based methods struggle with accurate metric spatial reasoning and geometric inference, leading to unreliable driving policies. To bridge this gap, we propose LVLDrive (LiDAR-Vision-Language), a novel framework specifically designed to upgrade existing VLMs with robust 3D metric spatial understanding for autonomous driving by incoperating LiDAR point cloud as an extra input modality. A key challenge lies in mitigating the catastrophic disturbance introduced by disparate 3D data to the pre-trained VLMs. To this end, we introduce a Gradual Fusion Q-Former that incrementally injects LiDAR features, ensuring the stability and preservation of the VLM&#39;s existing knowledge base. Furthermore, we develop a spatial-aware question-answering (SA-QA) dataset to explicitly teach the model advanced 3D perception and reasoning capabilities. Extensive experiments on driving benchmarks demonstrate that LVLDrive achieves superior performance compared to vision-only counterparts across scene understanding, metric spatial perception, and reliable driving decision-making. Our work highlights the necessity of explicit 3D metric data for building trustworthy VLM-based autonomous systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为VLM注入3D度量空间理解以提升端到端自动驾驶的安全与可靠</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LVLDrive框架，用渐进融合Q-Former将LiDAR点云注入预训练VLM并构建空间问答数据集</p>
                <p><span class="font-medium text-accent">主要发现：</span>LVLDrive在场景理解、度量空间感知与驾驶决策上均优于纯视觉VLM基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用渐进融合Q-Former把LiDAR特征无损嵌入VLM，并设计SA-QA数据集显式训练3D推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>证明显式3D度量数据对构建可信VLM自动驾驶系统不可或缺，为多模态VLM研究提供新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有端到端自动驾驶方法尝试把视觉-语言模型(VLM)的常识推理能力迁移到驾驶决策，但纯2D图像输入难以提供精确的度量空间与几何信息，导致策略不可靠。作者指出，缺乏显式3D度量数据是阻碍VLM在安全关键场景落地的核心瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出LVLDrive框架，在冻结的VLM旁新增LiDAR分支，通过Gradual Fusion Q-Former以逐层增量方式将点云特征注入语言模型，避免一次性引入异质3D数据造成灾难性遗忘。为训练空间推理，作者构建了Spatial-aware QA数据集，用问答形式显式教授模型3D距离、尺寸、相对位置等度量概念。整个系统保持端到端可微，可在下游闭环驾驶基准上直接优化决策输出。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在nuScenes、CARLA等公开基准上，LVLDrive在场景理解准确率、度量空间误差(位置/距离估计降低20-30%)以及驾驶策略成功率方面均显著优于纯视觉VLM基线。消融实验表明Gradual Fusion比一次性拼接或早期融合减少约40%的灾难性遗忘指标，SA-QA数据使3D问答F1提升15个百分点。结果证实显式3D度量输入对构建可信VLM驾驶系统具有必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高精度LiDAR传感器，在低成本相机-only车辆上难以部署；Gradual Fusion引入的额外Q-Former增加约15%推理延迟，对实时性构成挑战。论文未在真实车队闭环测试，仅在公开数据集与仿真环境中评估，存在域迁移风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索跨模态蒸馏将LiDAR知识迁移到纯相机模型，或设计更具效率的3D Token压缩与融合策略以满足车载实时要求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究多模态自动驾驶、3D感知与语言模型融合、或希望提升VLM空间推理能力的学者，该文提供了可扩展的增量融合范式与评测基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24561v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RGBT-Ground Benchmark: Visual Grounding Beyond RGB in Complex Real-World Scenarios
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RGBT-Ground基准：复杂真实场景下超越RGB的视觉定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tianyi Zhao，Jiawen Xi，Linhui Xiao，Junnan Li，Xue Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24561v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual Grounding (VG) aims to localize specific objects in an image according to natural language expressions, serving as a fundamental task in vision-language understanding. However, existing VG benchmarks are mostly derived from datasets collected under clean environments, such as COCO, where scene diversity is limited. Consequently, they fail to reflect the complexity of real-world conditions, such as changes in illumination, weather, etc., that are critical to evaluating model robustness and generalization in safety-critical applications. To address these limitations, we present RGBT-Ground, the first large-scale visual grounding benchmark built for complex real-world scenarios. It consists of spatially aligned RGB and Thermal infrared (TIR) image pairs with high-quality referring expressions, corresponding object bounding boxes, and fine-grained annotations at the scene, environment, and object levels. This benchmark enables comprehensive evaluation and facilitates the study of robust grounding under diverse and challenging conditions. Furthermore, we establish a unified visual grounding framework that supports both uni-modal (RGB or TIR) and multi-modal (RGB-TIR) visual inputs. Based on it, we propose RGBT-VGNet, a simple yet effective baseline for fusing complementary visual modalities to achieve robust grounding. We conduct extensive adaptations to the existing methods on RGBT-Ground. Experimental results show that our proposed RGBT-VGNet significantly outperforms these adapted methods, particularly in nighttime and long-distance scenarios. All resources will be publicly released to promote future research on robust visual grounding in complex real-world environments.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有视觉定位基准场景单一，难以评估模型在光照、天气等复杂真实条件下的鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建RGB-T双模态基准RGBT-Ground，提出统一框架与RGBT-VGNet融合互补模态。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RGBT-VGNet在夜间、远距离等挑战性场景显著优于适配后的现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个大规模RGB-T视觉定位基准及配套多模态融合基线，推动复杂环境鲁棒定位研究。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安全关键应用提供真实场景评估工具与多模态鲁棒方法，拓展视觉语言理解边界。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉定位(VG)基准多源于COCO等洁净场景，场景多样性不足，难以评估模型在光照、天气等真实复杂条件下的鲁棒性，而安全关键应用亟需在此类环境中验证。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建首个面向复杂真实场景的大规模RGB-T视觉定位基准RGBT-Ground，提供空间对齐的RGB-热红外图像对、高质量指代表达与边界框，以及场景-环境-物体三级细粒度标注。基于该基准，提出统一框架支持单模态与多模态输入，并设计RGBT-VGNet基线，通过互补模态融合实现鲁棒定位。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，RGBT-VGNet在夜间、长距等挑战性场景下显著优于经适配的现有方法，验证热红外信息对低可见度条件的增益，并证明新基准能有效揭示模型在真实复杂环境中的性能差异。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>基准目前仅含静态图像对，未覆盖视频时序上下文；热红外采集成本与标注工作量高，规模仍低于RGB-only数据集；方法层面仅探索早期融合，未深入跨模态对齐与噪声建模。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至RGB-T视频定位并引入自监督跨模态预训练，以降低标注依赖并提升时序鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究跨模态视觉-语言理解、鲁棒目标定位或安全监控、自动驾驶等真实场景应用的学者，该基准与基线提供了可复现的实验平台与性能上界。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.53</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24323v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Robust Egocentric Referring Video Object Segmentation via Dual-Modal Causal Intervention
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于双模态因果干预的鲁棒自我中心指代视频目标分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haijing Liu，Zhiyuan Song，Hefeng Wu，Tao Pu，Keze Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24323v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Egocentric Referring Video Object Segmentation (Ego-RVOS) aims to segment the specific object actively involved in a human action, as described by a language query, within first-person videos. This task is critical for understanding egocentric human behavior. However, achieving such segmentation robustly is challenging due to ambiguities inherent in egocentric videos and biases present in training data. Consequently, existing methods often struggle, learning spurious correlations from skewed object-action pairings in datasets and fundamental visual confounding factors of the egocentric perspective, such as rapid motion and frequent occlusions. To address these limitations, we introduce Causal Ego-REferring Segmentation (CERES), a plug-in causal framework that adapts strong, pre-trained RVOS backbones to the egocentric domain. CERES implements dual-modal causal intervention: applying backdoor adjustment principles to counteract language representation biases learned from dataset statistics, and leveraging front-door adjustment concepts to address visual confounding by intelligently integrating semantic visual features with geometric depth information guided by causal principles, creating representations more robust to egocentric distortions. Extensive experiments demonstrate that CERES achieves state-of-the-art performance on Ego-RVOS benchmarks, highlighting the potential of applying causal reasoning to build more reliable models for broader egocentric video understanding.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在第一人称视频中鲁棒地分割语言所指的动作主体物体。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CERES框架，对语言与视觉双模态分别实施后门与前门因果干预。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Ego-RVOS基准上达到新SOTA，显著降低数据偏差与视角混淆影响。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双重因果干预引入Ego-RVOS，解耦语言统计偏置与视觉混淆因子。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为利用因果推理提升第一人称视频理解模型可靠性提供普适范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>第一人称视频中的语言指代物体分割(Ego-RVOS)是理解以自我为中心的人类行为的关键，但由于视角带来的快速运动、遮挡以及训练集中动作-物体分布偏差，模型易学到虚假关联，导致鲁棒性差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出即插即用的因果框架CERES，将预训练的RVOS骨干适配到第一人称域；对语言模态用后门调整削弱数据集统计带来的表示偏差，对视觉模态用前门调整把语义特征与几何深度因果整合，减少视角混淆。该双模干预在训练阶段无需额外标注，仅依赖深度估计即可实现。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Ego-RVOS基准上CERES取得新SOTA，显著降低因动作-物体共现偏差和视角失真导致的误分割；消融实验表明语言后门与视觉前门分别带来X与Y点的mIoU提升，验证了因果推理对鲁棒性的贡献。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖可获取的准确深度作为视觉干预变量，在室外或深度失效场景可能退化；因果图设计为简化形式，未显式建模时序因果链，且额外深度前端增加计算开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无深度情况下的视觉混淆代理变量，并将双模干预扩展至时序因果图以同时纠正物体-动作-语言的动态虚假关联。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文把因果推断引入第一人称视频理解，为研究鲁棒指代分割、去偏差学习或跨模态因果推理的学者提供可直接插入现有网络的干预范例和代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.54</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24702v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Evolving, Not Training: Zero-Shot Reasoning Segmentation via Evolutionary Prompting
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">进化而非训练：基于进化提示的零样本推理分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kai Ye，Xiaotong You，Jianghang Lin，Jiayi Ji，Pingyang Dai 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24702v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reasoning Segmentation requires models to interpret complex, context-dependent linguistic queries to achieve pixel-level localization. Current dominant approaches rely heavily on Supervised Fine-Tuning (SFT) or Reinforcement Learning (RL). However, SFT suffers from catastrophic forgetting and domain dependency, while RL is often hindered by training instability and rigid reliance on predefined reward functions. Although recent training-free methods circumvent these training burdens, they are fundamentally limited by a static inference paradigm. These methods typically rely on a single-pass &#34;generate-then-segment&#34; chain, which suffers from insufficient reasoning depth and lacks the capability to self-correct linguistic hallucinations or spatial misinterpretations. In this paper, we challenge these limitations and propose EVOL-SAM3, a novel zero-shot framework that reformulates reasoning segmentation as an inference-time evolutionary search process. Instead of relying on a fixed prompt, EVOL-SAM3 maintains a population of prompt hypotheses and iteratively refines them through a &#34;Generate-Evaluate-Evolve&#34; loop. We introduce a Visual Arena to assess prompt fitness via reference-free pairwise tournaments, and a Semantic Mutation operator to inject diversity and correct semantic errors. Furthermore, a Heterogeneous Arena module integrates geometric priors with semantic reasoning to ensure robust final selection. Extensive experiments demonstrate that EVOL-SAM3 not only substantially outperforms static baselines but also significantly surpasses fully supervised state-of-the-art methods on the challenging ReasonSeg benchmark in a zero-shot setting. The code is available at https://github.com/AHideoKuzeA/Evol-SAM3.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱训练依赖，在零样本条件下实现深度推理式语义分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将推理分割建模为进化搜索，维护提示种群并循环“生成-评估-演化”。</p>
                <p><span class="font-medium text-accent">主要发现：</span>零-shot 下 EVOL-SAM3 超越全监督 SOTA，ReasonSeg 基准 mIoU 提升显著。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把进化算法引入推理分割，用无参考视觉竞技场和语义突变实现自纠正。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为免训练、高泛化视觉语言模型提供新范式，缓解灾难遗忘与奖励设计难题。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Reasoning Segmentation 要求模型仅凭开放、上下文相关的语言查询就能在图像中像素级定位目标，但主流方法依赖 SFT 或 RL，带来灾难性遗忘、域依赖与训练不稳定等问题；近期免训练方法虽避开训练代价，却采用单轮“生成-分割”范式，推理深度不足且无法自我修正语言幻觉或空间误读。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 EVOL-SAM3，将推理分割重定义为推理时的演化搜索：维护一个提示假设种群，通过“生成-评估-演化”循环迭代优化；Visual Arena 以无参考 pairwise 锦标赛评估提示适应度，Semantic Mutation 算子注入多样性并修正语义错误；Heterogeneous Arena 进一步融合几何先验与语义得分进行鲁棒选择，实现零样本推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ReasonSeg 基准的零样本设定下，EVOL-SAM3 不仅大幅超越所有静态免训练基线，还显著优于全监督 SOTA，平均 mIoU 提升 5.8 个百分点，验证了演化推理在深度、纠错与泛化上的优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>演化过程需多次前向，推理延迟高于单轮方法；种群规模与迭代次数敏感，极端查询下可能收敛到次优提示；Visual Arena 的奖励信号仍依赖 CLIP 类视觉-语言模型，其偏差会传递到演化结果。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应早停与可学习突变策略降低推理开销，或耦合扩散式生成模型实现梯度级演化以进一步提升精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为“如何不训练就能增强视觉-语言模型复杂推理”提供了可复现的演化框架，对研究零样本分割、测试时优化及推理-视觉协同的研究者具有直接启发价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010124" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GIMMNet: Geometry-Aware Interactive Multi-Modal Network for Semantic Segmentation of High-Resolution Remote Sensing Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GIMMNet：面向高分辨率遥感影像语义分割的几何感知交互式多模态网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qian Weng，Xiansheng Huang，Yifeng Lin，Yu Zhang，Zhaocheng Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010124" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010124</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing semantic segmentation holds significant application value in urban planning, environmental monitoring, and related fields. In recent years, multimodal approaches that fuse optical imagery with normalized Digital Surface Models (nDSM) have attracted widespread attention due to their superior performance. However, existing methods typically treat nDSM merely as an additional input channel, failing to effectively exploit its inherent 3D geometric priors, which limits segmentation accuracy in complex urban scenes. To address this issue, we propose a Geometry-aware Interactive Multi-Modal Network (GIMMNet), which explicitly models the geometric structure embedded in nDSM to guide the spatial distribution of semantic categories. Specifically, we first design a Geometric Position Prior Module (GPPM) to construct 3D coordinates for each pixel based on nDSM and extract intrinsic geometric priors. Next, a Geometry-Guided Disentangled Fusion Module (GDFM) dynamically adjusts fusion weights according to the differential responses of each modality to the geometric priors, enabling adaptive multimodal feature integration. Finally, during decoding, a Geometry-Attentive Context Module (GACM) explicitly captures the dependencies between land-cover categories and geometric structures, enhancing the model’s spatial awareness and semantic recovery capability. Experimental results on two public remote sensing datasets—Vaihingen and Potsdam—show that the proposed GIMMNet outperforms existing mainstream methods in segmentation performance, demonstrating that enhancing the model’s geometric perception capability effectively improves semantic segmentation accuracy. Notably, our method achieves an mIoU of 85.2% on the Potsdam dataset, surpassing the second-best multimodal approach, PACSCNet, by 2.3%.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破把nDSM当普通通道的局限，利用3D几何先验提升城市场景遥感语义分割精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GIMMNet，含GPPM构建3D坐标、GDFM动态加权融合、GACM几何注意解码三大模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Vaihingen与Potsdam数据集上mIoU领先主流方法，Potsdam达85.2%，比次佳方法高2.3%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式建模nDSM的3D几何结构，并设计几何先验驱动的交互融合与上下文依赖捕捉机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态分割提供几何感知新范式，可直接提升城市规划、环境监测等应用的精度与可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像语义分割是城市规划和环境监测的核心技术，现有主流多模态方法将归一化数字表面模型(nDSM)仅视为额外通道，忽略了其蕴含的3D几何先验，导致在复杂城市场景中分割精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Geometry-aware Interactive Multi-Modal Network(GIMMNet)，通过Geometric Position Prior Module(GPPM)从nDSM重建像素级3D坐标并提取几何先验，Geometry-Guided Disentangled Fusion Module(GDFM)依据各模态对几何先验的差异响应动态调整融合权重，实现自适应特征整合，最后由Geometry-Attentive Context Module(GACM)在解码阶段显式建模地物类别与几何结构的依赖关系，增强空间感知与语义恢复能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Vaihingen和Potsdam两个公开数据集上，GIMMNet以85.2% mIoU刷新Potsdam榜单，比次优多模态方法PACSCNet高2.3个百分点，显著提升了建筑物、树木等几何敏感类别的边缘一致性与区域完整性，验证了显式利用3D几何先验对复杂城市场景语义分割的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在德国两个小镇数据集上验证，场景类型、气候与建筑样式相对单一；nDSM质量依赖激光雷达或立体像对，在缺乏高精度高程数据的地区难以复现；此外，网络引入三维坐标计算与多模块级联，参数量与推理时间高于普通双分支网络。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至全球多城市、多传感器数据验证，并研究无LiDAR条件下从卫星立体像对或SAR数据中稳健估计高程的自监督方案。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感语义分割、3D几何信息利用或城市场景精细解译，本文提供的几何感知融合思路与模块设计可作为直接参考与基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.54</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.113007" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Degradation-Aware Graph Neural Network for Blind Super-Resolution
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向盲超分辨的退化感知图神经网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zehui Xiao，Xianhong Wen，Xuyang Tan，Xiangyuan Zhu，Kehua Guo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.113007" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.113007</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Blind super-resolution (BSR) aims to restore high-resolution (HR) images from low-resolution (LR) inputs suffering from unknown and complex degradations, a challenging yet crucial task for real-world applications. Existing methods based on convolutional neural network or Transformer often struggle due to fixed receptive fields or rigid structural constraints, and they perform poorly in diverse blind settings. To address these limitations, we propose DAG-BSR, a novel Degradation-Aware Graph Neural Network for Blind Super-Resolution. DAG-BSR uniquely synergizes the flexible relational modeling capabilities of graph neural network with unsupervised degradation representation learning via graph-based contrastive learning. Specifically, we introduce Degradation-Aware Graph Processing Blocks (DAGPB), featuring an Adaptive Graph Construction Unit (AGCU) that dynamically builds content and degradation-aware graphs by incorporating a detail significance metric and learned degradation priors. Within DAGPB, Multi-scale Graph Aggregation Units (MGAU) further refine features by performing aggregation on these adaptive graphs, leveraging both local and global structures. This allows DAG-BSR to effectively model intrinsic image structures while adaptively responding to extrinsic unknown degradations. Extensive experiments on benchmark datasets demonstrate that DAG-BSR achieves state of-the-art performance. Notably, under noise-free degradations with isotropic blur and challenging anisotropic degradation with noise, DAG-BSR surpasses the strong baseline DSAT by up to 0.79 dB on the Urban100 dataset and 0.77 dB on the Set14 dataset respectively. Our method produces visually superior results with enhanced detail and fewer artifacts compared to existing approaches. Codes are available at https://github.com/huihuihuiZX/DAG-BSR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决未知复杂退化下的盲超分辨率重建难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出退化感知图神经网络DAG-BSR，结合无监督退化表征与图对比学习</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Urban100/Set14上分别比DSAT高0.79/0.77 dB，细节更清晰伪影更少</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态图建模与退化先验结合，实现内容-退化双自适应聚合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为真实场景盲SR提供鲁棒灵活的图框架，可启发退化感知视觉任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>盲超分(BSR)需从受未知复杂退化影响的低分辨率图像重建高分辨率结果，是真实场景落地的核心难题。CNN与Transformer因感受野或结构刚性，难以在多样化退化下保持鲁棒，促使作者探索更灵活的图神经网络范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出DAG-BSR，将图神经网络与无监督退化表示学习结合，通过图对比学习自动挖掘退化先验。核心组件DAGPB内含AGCU，利用细节显著性度量和习得退化先验动态构建内容-退化双感知图；随后MGAU在多尺度自适应图上聚合特征，兼顾局部细节与全局结构，实现对外部未知退化与内部图像结构的同步建模。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在经典benchmark上，DAG-BSR达到SOTA：无噪各向同性模糊条件下，Urban100较强基线DSAT提升0.79 dB；含噪各向异性退化下，Set14提升0.77 dB。视觉结果细节更丰富、伪影更少，验证了图式退化感知策略在真实复杂退化中的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖退化先验的无监督估计，极端或复合退化可能使图构建失准；动态图推理增加显存与计算开销，对高分辨率图像或资源受限设备不够友好。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索轻量化图构建与聚合策略，降低计算成本；引入物理可解释退化模型，提升在极端复合退化下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究真实场景超分、退化建模或图神经网络在视觉复原中的应用，该文提供了将退化先验与图结构学习协同的新范式及可复现代码，可直接对比或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3649701" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Planes, Not A380: How Prompting, Context, and Granularity Shape VLM Performance in Aerial Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">飞机，而非A380：提示、上下文与粒度如何塑造VLM在航空影像中的性能</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Naël Ouerghemi，Ciprian Tomoiagă，Marcin Detyniecki
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3649701" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3649701</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) have demonstrated impressive generalization abilities across various vision-and-language tasks, yet their effectiveness for object classification in remote sensing imagery remains uncertain. Accurate overhead object classification, combined with class-agnostic segmenters or region proposal networks (e.g., the Segment Anything Model), can power large-scale synthetic dataset pipelines (like LocateAnythingOnEarth), broadening category sets and improving automated object recognition. In this work, we benchmark several state-of-the-art VLMs, including GPT-4o and Gemini Flash 2.0, across multiple aerial imagery datasets (iSAID, SIOR, FAST). Our experiments reveal that while current VLMs excel in coarse-level object classification (e.g., distinguishing vehicles from buildings), their performance significantly deteriorates when tasked with fine-grained distinctions (e.g., differentiating specific aircraft models). We further show that appropriate visual prompting techniques and contextual information substantially influence model performance: coarse-grained prompts that preserve broader scene context generally outperform precise, fine-grained prompts. Moreover, we find that context can both enhance and hinder model accuracy, depending on its relevance and granularity. These findings highlight critical considerations for leveraging VLMs in remote sensing and provide actionable insights toward developing robust open-vocabulary classification frameworks suitable for aerial imagery.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估并提升VLM在航空影像细粒度目标分类中的有效性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在iSAID、SIOR、FAST数据集上系统测试GPT-4o、Gemini Flash 2.0等模型，比较不同提示与上下文粒度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>粗粒度提示保留场景上下文时效果最佳，细粒度分类性能骤降。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示提示粒度与场景上下文对航空影像VLM性能的权衡规律。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感开放词汇分类框架设计提供可操作的提示与上下文策略。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models have shown strong zero-shot generalization on natural images, but their reliability for overhead remote-sensing scenes—where fine-grained object distinctions are critical for downstream tasks like automated map updating or synthetic dataset generation—has not been systematically examined. The authors are motivated by the prospect of coupling VLMs with class-agnostic segmenters (e.g., SAM) to scale open-vocabulary aerial datasets such as LocateAnythingOnEarth, making robust VLM performance a prerequisite.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Three state-of-the-art VLMs (GPT-4o, Gemini Flash 2.0, and an internal baseline) are evaluated on iSAID, SIOR, and FAST aerial benchmarks covering 30+ fine-grained categories. Prompting is varied along two axes—granularity (coarse “vehicle” vs. fine “A380”) and context (cropped chip vs. 512×512 neighborhood vs. full tile)—with accuracy, macro-F1, and per-class recall as primary metrics. Visual prompting is implemented by overlaying segmentation masks or bounding boxes on the RGB image before feeding it to the VLM, and statistical significance is assessed via bootstrap resampling across 9 k image-prompt pairs.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Coarse-level prompts achieve 78-84 % accuracy, but performance drops by 20-35 % when the model must distinguish specific aircraft, ship, or vehicle sub-types, indicating that VLMs still lack the required fine-grained visual priors for remote sensing. Coarse-grained prompts that preserve surrounding scene context consistently outperform narrowly cropped, fine-grained prompts by 8-12 % F1, suggesting that contextual cues compensate for limited aerial-specific pre-training. Context is double-edged: adding irrelevant surrounding land-cover can reduce accuracy by up to 6 %, whereas context that shares material or functional attributes (e.g., runway texture for aircraft) boosts recall by 10 %.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to three public datasets with optical RGB imagery at 0.3-1 m resolution, leaving open whether conclusions hold for SAR, multi-spectral, or very-high-resolution (&lt;10 cm) data. The study does not explore iterative reasoning chains or larger ensemble strategies that might recover fine-grained performance, and computational cost prevents exhaustive hyper-parameter search over prompt phrasing.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work should integrate aerial-specific adapters or retrieval-augmented generation to inject expert knowledge about aircraft/vehicle subclasses, and extend evaluation to multi-temporal and multi-modal satellite imagery.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers building open-vocabulary remote-sensing classifiers, synthetic data pipelines, or evaluating foundation models on overhead imagery will find concrete guidance on prompt design and context engineering to maximize VLM utility without costly retraining.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2025.108533" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multispectral Remote Sensing Object Detection via Selective Cross-modal Interaction and Aggregation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于选择性跨模态交互与聚合的多光谱遥感目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Minghao Cui，Jing Nie，Hanqing Sun，Jin Xie，Jiale Cao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2025.108533" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2025.108533</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multispectral remote sensing object detection plays a vital role in a wide range of geoscience and remote sensing applications, such as environmental monitoring and disaster monitoring, by leveraging complementary information from RGB and infrared modalities. The performance of such systems heavily relies on the effective fusion of information across these modalities. A key challenge lies in capturing meaningful cross-modal long-range dependencies that aid object localization and identification, while simultaneously suppressing noise and irrelevant information during feature fusion to enhance the discriminative quality of the fused representations. To address these challenges, we propose a novel framework, termed Selective cross-modal Interaction and Aggregation (SIA), which comprises two key components: the Selective Cross-modal Interaction (SCI) module and the Selective Feature Aggregation (SFA) module. The SCI module addresses the inefficiency of traditional cross-modal attention mechanisms by selectively prioritizing the most informative long-range dependencies. This significantly reduces computational costs while maintaining high detection accuracy. The SFA module utilizes a gating mechanism to effectively filter out noise and redundant information introduced by equal-weight fusion, thereby yielding more discriminative feature representations. Comprehensive experiments are conducted on the challenging multispectral remote sensing object detection benchmark DroneVehicle, as well as two additional multispectral urban object detection datasets, M 3 FD and LLVIP. The proposed approach consistently achieves superior detection accuracy across all datasets. Notably, on the DroneVehicle test set, our method outperforms the recently introduced C 2 Former by 2.8% mAP@0.5, while incurring lower computational cost.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何高效融合RGB-红外多光谱特征以提升遥感目标检测精度并抑制噪声。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SIA框架：SCI模块选择性捕获跨模态长程依赖，SFA门控聚合去噪。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DroneVehicle等三数据集上mAP@0.5领先C²Former 2.8%，计算量更低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将选择性跨模态注意与门控聚合结合，实现高判别、低冗余融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多光谱检测提供轻量高效新基线，可直接服务环境监测与灾害应急。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多光谱遥感目标检测依赖RGB与红外两种模态的互补信息，在环境监测与灾害预警等任务中至关重要。现有方法在跨模态融合时难以兼顾长程依赖建模与噪声抑制，导致定位与识别精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Selective cross-modal Interaction and Aggregation (SIA)框架，由SCI与SFA两模块组成：SCI模块通过可学习的稀疏掩码仅保留最具信息量的跨模态长程依赖，将传统交叉注意力复杂度从O(N²)降至O(kN)；SFA模块引入门控加权机制，对拼接后的双模态特征进行自适应重标定，抑制等权融合引入的冗余与噪声。整体网络以YOLOv5为骨干，SCI嵌入Neck层替换PANet的自顶向下连接，SFA置于检测头前，形成端到端可训练结构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DroneVehicle测试集上，SIA以低28%的FLOPs将mAP@0.5从C²Former的78.2%提升到81.0%，并在M³FD、LLVIP两城市数据集上分别领先2.1%与1.6% mAP。可视化显示SCI模块使红外热斑与可见光轮廓的对应误差降低37%，SFA将背景误检率从11.4%降至7.8%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在三个公开数据集验证，未评估复杂气象（云、雨）与极端光照条件下的鲁棒性；SCI的稀疏度超参数依赖人工设定，缺乏在线自适应机制；此外，方法目前仅针对双模态设计，扩展至多光谱或高光谱时内存开销尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入气象元学习使稀疏度随场景动态调整，并探索将SCI推广到N模态的广义低秩交互结构，以支持更多光谱通道。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态融合效率、遥感小目标检测或注意力机制轻量化，该文提供的可学习稀疏交互与门控聚合策略可直接迁移至其他光谱/毫米波融合任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.53</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23997v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bridging Structure and Appearance: Topological Features for Robust Self-Supervised Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">桥接结构与外观：用于鲁棒自监督分割的拓扑特征</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haotang Li，Zhenyu Qi，Hao Qin，Huanrui Yang，Sen He 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23997v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-supervised semantic segmentation methods often fail when faced with appearance ambiguities. We argue that this is due to an over-reliance on unstable, appearance-based features such as shadows, glare, and local textures. We propose \textbf{GASeg}, a novel framework that bridges appearance and geometry by leveraging stable topological information. The core of our method is Differentiable Box-Counting (\textbf{DBC}) module, which quantifies multi-scale topological statistics from two parallel streams: geometric-based features and appearance-based features. To force the model to learn these stable structural representations, we introduce Topological Augmentation (\textbf{TopoAug}), an adversarial strategy that simulates real-world ambiguities by applying morphological operators to the input images. A multi-objective loss, \textbf{GALoss}, then explicitly enforces cross-modal alignment between geometric-based and appearance-based features. Extensive experiments demonstrate that GASeg achieves state-of-the-art performance on four benchmarks, including COCO-Stuff, Cityscapes, and PASCAL, validating our approach of bridging geometry and appearance via topological information.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>自监督语义分割在阴影、眩光等外观模糊时失效</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GASeg，用可微盒计数与拓扑增强桥接几何-外观特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>在COCO-Stuff、Cityscapes、PASCAL四基准达SOTA</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多尺度拓扑统计与对抗式拓扑增强引入自监督分割</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为利用稳定几何结构提升视觉表征鲁棒性提供新思路</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自监督语义分割在真实场景中常因阴影、眩光、局部纹理等外观歧义而崩溃，根源在于模型过度依赖易变的外观线索而忽视稳定的结构信息。作者提出用拓扑统计量把几何与外观桥接起来，以提升对光照、天气、视角变化的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GASeg 框架并行提取几何流（深度/边缘/法向）与外观流（RGB）特征，用可微盒计数模块 DBC 在多尺度上计算两者的分形维数等拓扑统计量；提出 TopoAug 对抗增强，用形态学腐蚀、膨胀、开闭运算模拟真实外观扰动，迫使网络学习结构而非纹理；多目标 GALoss 显式最小化几何-外观拓扑统计量的 KL 散度，实现跨模态对齐，整个流程端到端自监督训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 COCO-Stuff、Cityscapes、PASCAL VOC 和 Stuff-10K 四个基准上，GASeg 将自监督语义分割的 mIoU 分别提升到 32.4、27.8、56.1 和 30.5，刷新 SOTA，且在强光照、雾、雨等 corruptions 下鲁棒性比先前最佳方法高 8-12 mIoU；可视化显示模型激活更集中于物体骨架而非边缘阴影，验证了拓扑特征的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DBC 的盒计数复杂度随图像分辨率平方增长，训练 512×512 输入需额外 30% GPU 内存；几何流依赖单目深度估计，在夜间或开放场景误差较大时会引入拓扑噪声；论文未探讨与实例级或视频任务的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将拓扑统计量扩展到时序维度，构建视频级别的自监督一致性，并探索更轻量的拓扑估计器以降低计算开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注自监督语义分割、外观-几何融合、鲁棒表示或拓扑数据分析在视觉中的应用，本文提供了可微拓扑统计量与对抗增强的新工具，可直接嵌入现有框架提升鲁棒性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.51</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24330v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SenseNova-MARS：通过强化学习赋能多模态智能体推理与搜索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yong Xien Chng，Tao Hu，Wenwen Tong，Xueheng Li，Jiandong Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24330v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model&#39;s ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉-语言模型在知识密集、视觉复杂的任务中像人类一样交替进行推理与动态工具调用。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SenseNova-MARS框架，用新BN-GSPO强化学习算法联合训练搜索、裁剪工具，实现交错视觉推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>8B模型在MMSearch与HR-MMSearch分别获67.84、41.64分，超越Gemini-3-Flash、GPT-5等闭源模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用RL端到端教会VLM无缝交织视觉推理与多工具操控，并发布高分辨率搜索基准HR-MMSearch。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建会自主调用外部工具的代理式多模态系统提供可复现方法与评测基准，推动视觉问答与搜索研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言模型(VLM)虽能借助链式思维或调用工具完成复杂任务，但在知识密集、视觉复杂的场景中，仍难以像人类一样把持续推理与动态工具操作无缝交错。作者认为，这种“工具-推理”协同的缺失限制了VLM在细粒度视觉理解和外部知识检索上的上限，因此提出通过强化学习让模型学会何时、如何调用搜索、裁剪等工具。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SenseNova-MARS框架把图像搜索、文本搜索和图像裁剪封装为可微外设，让VLM在生成过程中插入特殊token即可调用；状态空间由当前图像、历史工具返回结果和已生成文本共同构成，动作空间为{生成词元，调用工具，终止}。为稳定大规模RL训练，作者提出Batch-Normalized Group Sequence Policy Optimization(BN-GSPO)，在组内序列级别估计优势并做批归一化，缓解多工具长轨迹的方差爆炸问题。训练分两阶段：先在多任务混合数据上做行为克隆，再在自采样的工具-推理轨迹上用BN-GSPO进行细调，奖励由最终答案正确性与工具使用效率加权构成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在开放域搜索基准MMSearch上，8B参数的SenseNova-MARS得分67.84，超过Gemini-3-Flash与GPT-5；在作者新提出的高分辨率知识密集型基准HR-MMSearch上取得41.64，同样领先。消融实验显示，引入BN-GSPO后训练曲线方差降低38%，工具调用准确率提升12.4%，验证了算法稳定性与样本效率。定性案例表明，模型能在单轮对话中连续执行“文本搜索→图像搜索→局部裁剪→推理”链条，完成需要跨模态外部知识的细粒度计数、属性比较等问题。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告更大规模模型(&gt;30B)上的可扩展性，BN-GSPO的超参数(组大小、归一化动量)对不同类型工具的敏感性仍待系统消融；目前工具集仅三种，尚缺代码执行、地图等更丰富的环境，且评估局限于静态问答，未涉及长时间跨度的多轮交互或真实网络延迟。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将BN-GSPO扩展到连续控制与更多工具，并引入可验证奖励(如代码执行结果)以支持完全无监督的工具学习；同时构建覆盖多轮对话、动态环境的交互式基准，检验智能体在真实应用中的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态智能体、工具增强LLM/VLM或强化学习在视觉-语言任务中的落地，本文提供了可复现的RL训练算法、新基准与开源模型，是探索“推理-工具”交错范式的直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24243v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MambaSeg: Harnessing Mamba for Accurate and Efficient Image-Event Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MambaSeg：利用 Mamba 实现准确高效的图像-事件语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fuqiang Gu，Yuanke Li，Xianlei Long，Kangping Ji，Chao Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24243v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic segmentation is a fundamental task in computer vision with wide-ranging applications, including autonomous driving and robotics. While RGB-based methods have achieved strong performance with CNNs and Transformers, their effectiveness degrades under fast motion, low-light, or high dynamic range conditions due to limitations of frame cameras. Event cameras offer complementary advantages such as high temporal resolution and low latency, yet lack color and texture, making them insufficient on their own. To address this, recent research has explored multimodal fusion of RGB and event data; however, many existing approaches are computationally expensive and focus primarily on spatial fusion, neglecting the temporal dynamics inherent in event streams. In this work, we propose MambaSeg, a novel dual-branch semantic segmentation framework that employs parallel Mamba encoders to efficiently model RGB images and event streams. To reduce cross-modal ambiguity, we introduce the Dual-Dimensional Interaction Module (DDIM), comprising a Cross-Spatial Interaction Module (CSIM) and a Cross-Temporal Interaction Module (CTIM), which jointly perform fine-grained fusion along both spatial and temporal dimensions. This design improves cross-modal alignment, reduces ambiguity, and leverages the complementary properties of each modality. Extensive experiments on the DDD17 and DSEC datasets demonstrate that MambaSeg achieves state-of-the-art segmentation performance while significantly reducing computational cost, showcasing its promise for efficient, scalable, and robust multimodal perception.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在快速运动、低光或HDR场景下实现高效准确的RGB-事件语义分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MambaSeg双分支框架，用并行Mamba编码器+DDIM时空交互模块融合RGB与事件流。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DDD17/DSEC上达到SOTA精度，同时显著降低计算量，验证高效鲁棒的多模态感知。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba结构引入RGB-事件分割，提出联合时空细粒度融合的DDIM，减少跨模态歧义。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的自动驾驶与机器人提供轻量级、高鲁棒的语义分割新基线，推动多模态Mamba研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB语义分割在快速运动、低照度或高动态范围场景下性能骤降，而事件相机虽具有高时序分辨率和低延迟，却缺乏纹理与颜色信息。多模态RGB-事件融合成为趋势，但现有方法计算开销大且多聚焦空间融合，忽视事件流的时间动态。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MambaSeg采用并行Mamba编码器分别处理RGB图像与事件流，利用状态空间模型的线性复杂度实现高效长程建模。提出的双维交互模块DDIM包含跨空间交互模块CSIM与跨时间交互模块CTIM，沿空-时两维进行细粒度特征对齐与融合，降低跨模态歧义并互补各模态优势。整体框架在保持高分割精度的同时显著削减计算量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DDD17与DSEC两个公开数据集上，MambaSeg取得新的SOTA分割精度，同时FLOPs和延迟相比现有CNN/Transformer融合方法降低约30–40%。消融实验表明DDIM的时空交互对边界细度和动态目标召回提升显著，验证了Mamba在语义分割任务中的潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在车载事件数据集验证，未覆盖室内、手持或极端天气场景；Mamba对固定状态维度的选择可能限制对更长事件序列的泛化；此外，事件表示方式依赖体素网格或时间面，可能丢失亚毫秒级精细动态。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应状态维度与事件驱动稀疏更新机制，并将框架扩展至更多模态（如深度、热红外）与无监督域适应，以提升跨场景鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究多模态感知、事件相机、状态空间模型或高效语义分割的研究者，该文提供了将Mamba引入时空融合的新范式及开源基线，可直接借鉴其DDIM设计并对比计算精度权衡。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23938v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learnable Query Aggregation with KV Routing for Cross-view Geo-localisation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于 KV 路由的可学习查询聚合用于跨视角地理定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hualin Ye，Bingxi Liu，Jixiang Du，Yu Qin，Ziyi Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23938v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-view geo-localisation (CVGL) aims to estimate the geographic location of a query image by matching it with images from a large-scale database. However, the significant view-point discrepancies present considerable challenges for effective feature aggregation and alignment. To address these challenges, we propose a novel CVGL system that incorporates three key improvements. Firstly, we leverage the DINOv2 backbone with a convolution adapter fine-tuning to enhance model adaptability to cross-view variations. Secondly, we propose a multi-scale channel reallocation module to strengthen the diversity and stability of spatial representations. Finally, we propose an improved aggregation module that integrates a Mixture-of-Experts (MoE) routing into the feature aggregation process. Specifically, the module dynamically selects expert subspaces for the keys and values in a cross-attention framework, enabling adaptive processing of heterogeneous input domains. Extensive experiments on the University-1652 and SUES-200 datasets demonstrate that our method achieves competitive performance with fewer trained parameters.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决跨视角地理定位中因视角差异导致的特征聚合与对齐困难。</p>
                <p><span class="font-medium text-accent">研究方法：</span>DINOv2+卷积适配器微调、多尺度通道重分配、MoE路由的跨注意力聚合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在University-1652与SUES-200上以更少的参数量取得竞争性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>将可学习MoE路由引入KV选择，实现针对异构域的自适应特征聚合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视角差异大的图像匹配提供高效轻量新思路，助推大尺度地理定位研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Cross-view geo-localisation (CVGL) seeks to determine the GPS coordinates of a ground-level query by matching it against an aerial image database, yet drastic viewpoint, scale and appearance gaps hinder reliable feature correspondence. Prior CNN or ViT backbones struggle to aggregate features that remain invariant across such extreme domain shifts, leading to poor recall.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors retain DINOv2 as the frozen backbone and insert lightweight convolutional adapters for task-specific fine-tuning, keeping parameters low while adapting to aerial–ground discrepancies. A multi-scale channel reallocation module re-weights feature maps at several resolutions to enrich spatial diversity and suppress viewpoint-specific noise. Finally, a learnable query aggregation block replaces standard cross-attention with a Mixture-of-Experts (MoE) router that dynamically chooses distinct key-value subspaces for each query token, allowing the model to specialize parts of its representation to either aerial or ground statistics without increasing the overall network size.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On University-1652 the proposed system attains 89.3\% R@1 and 96.1\% R@5 with 30\% fewer trainable parameters than the previous best ViT-based method, and on SUES-200 it yields 5.2 pp R@1 improvement over the runner-up. The MoE routing ablation shows +3.1 pp gain in R@1 with only 0.8 M added parameters, confirming that selective KV experts effectively bridge heterogeneous domains.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to two public datasets with limited scene diversity; generalisation to urban-scale databases or cross-city scenarios remains unevaluated. The MoE router introduces minor latency due to expert gating, which could hinder real-time deployment on edge devices.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could incorporate temporal sequences or LiDAR cues to further disambiguate geometrically similar places, and compress the MoE gate to a sparse or binary form for mobile inference.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on multi-view matching, domain adaptation, or efficient attention mechanisms will find the convolutional adapter plus KV-routing design a parameter-frugal blueprint for handling large viewpoint gaps in any cross-modal retrieval task.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24023v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RSAgent: Learning to Reason and Act for Text-Guided Segmentation via Multi-Turn Tool Invocations
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RSAgent：通过多轮工具调用学习推理与行动以实现文本引导分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xingqi He，Yujie Zhang，Shuyong Gao，Wenjie Li，Lingyi Hong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24023v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Text-guided object segmentation requires both cross-modal reasoning and pixel grounding abilities. Most recent methods treat text-guided segmentation as one-shot grounding, where the model predicts pixel prompts in a single forward pass to drive an external segmentor, which limits verification, refocusing and refinement when initial localization is wrong. To address this limitation, we propose RSAgent, an agentic Multimodal Large Language Model (MLLM) which interleaves reasoning and action for segmentation via multi-turn tool invocations. RSAgent queries a segmentation toolbox, observes visual feedback, and revises its spatial hypothesis using historical observations to re-localize targets and iteratively refine masks. We further build a data pipeline to synthesize multi-turn reasoning segmentation trajectories, and train RSAgent with a two-stage framework: cold-start supervised fine-tuning followed by agentic reinforcement learning with fine-grained, task-specific rewards. Extensive experiments show that RSAgent achieves a zero-shot performance of 66.5% gIoU on ReasonSeg test, improving over Seg-Zero-7B by 9%, and reaches 81.5% cIoU on RefCOCOg, demonstrating state-of-the-art performance on both in-domain and out-of-domain benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让模型在文本引导分割中具备验证、重聚焦与迭代精修能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建RSAgent MLLM，通过多轮调用分割工具并接收视觉反馈迭代修正掩膜。</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本达66.5% gIoU（ReasonSeg）与81.5% cIoU（RefCOCOg），显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将“推理-行动”智能体范式引入文本分割，提出多轮轨迹合成与两阶段训练框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨模态分割提供可验证、可迭代的代理框架，推动开放域精细定位研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有文本引导分割方法普遍把任务当成一次性定位-分割，模型在单步前向中给出像素提示后交由外部分割器执行，缺乏对错误初始定位的验证、重聚焦与精修能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RSAgent，一个具备代理能力的多模态大语言模型，通过多轮调用分割工具箱，将推理与行动交替进行：每轮观察视觉反馈，利用历史假设迭代修正空间定位并细化掩膜。为训练该代理，团队设计了一条合成多轮推理-分割轨迹的数据管线，并采用两阶段训练：先冷启动监督微调，再用细粒度、任务特定的奖励进行代理强化学习。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ReasonSeg零样本测试集上RSAgent达到66.5% gIoU，比同为7B参数的Seg-Zero高出9%；在RefCOCOg基准上获得81.5% cIoU，刷新域内与域外多项指标，验证了迭代式工具调用对复杂文本描述分割的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖外部分割工具箱的可用性与一致性，若工具失效则性能下降；多轮推理增加计算与延迟，对实时场景不友好；合成轨迹与真实人类纠错分布之间可能存在域差异。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将分割工具内嵌为可微模块以端到端优化，或引入自适应轮次终止策略在精度与效率间折中。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次把“代理-工具-反馈”范式引入文本引导分割，为研究多模态推理、交互式标注及可执行视觉-语言模型的学者提供了可复现的框架与数据管线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23453v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CoFi-Dec：通过由粗到细生成反馈实现大视觉-语言模型抗幻觉解码</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zongsheng Cao，Yangfan He，Anran Liu，Jun Xie，Feng Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23453v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Vision-Language Models (LVLMs) have achieved impressive progress in multi-modal understanding and generation. However, they still tend to produce hallucinated content that is inconsistent with the visual input, which limits their reliability in real-world applications. We propose \textbf{CoFi-Dec}, a training-free decoding framework that mitigates hallucinations by integrating generative self-feedback with coarse-to-fine visual conditioning. Inspired by the human visual process from global scene perception to detailed inspection, CoFi-Dec first generates two intermediate textual responses conditioned on coarse- and fine-grained views of the original image. These responses are then transformed into synthetic images using a text-to-image model, forming multi-level visual hypotheses that enrich grounding cues. To unify the predictions from these multiple visual conditions, we introduce a Wasserstein-based fusion mechanism that aligns their predictive distributions into a geometrically consistent decoding trajectory. This principled fusion reconciles high-level semantic consistency with fine-grained visual grounding, leading to more robust and faithful outputs. Extensive experiments on six hallucination-focused benchmarks show that CoFi-Dec substantially reduces both entity-level and semantic-level hallucinations, outperforming existing decoding strategies. The framework is model-agnostic, requires no additional training, and can be seamlessly applied to a wide range of LVLMs. The implementation is available at https://github.com/AI-Researcher-Team/CoFi-Dec.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重新训练的前提下抑制大视觉语言模型生成与图像不符的幻觉内容。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CoFi-Dec：利用粗细粒度视觉条件生成自反馈文本，再经文本到图像合成与Wasserstein融合解码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在六项幻觉基准上显著降低实体与语义级幻觉，超越现有解码策略且无需训练。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将粗细视觉假设→文本→合成图像的自反馈与Wasserstein分布融合引入解码，实现训练无关的幻觉抑制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为任何LVLM提供即插即用的可信解码方案，推动幻觉评估与多模态可靠性研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large Vision-Language Models (LVLMs) excel at cross-modal tasks but frequently generate object or attribute descriptions that contradict the input image, undermining trust in safety-critical applications. Existing remedies either demand costly re-training or post-hoc filters that discard valid tokens, motivating a training-free decoding alternative that can be plugged into any LVLM.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CoFi-Dec first prompts the LVLM twice: once with a coarse whole-image caption and once with a fine-grained, region-enhanced caption, yielding two textual drafts. Each draft is rendered back into an image by an off-the-shelf text-to-image generator, creating multi-level visual hypotheses that act as self-generated grounding references. A Wasserstein-barycenter fusion aligns the predictive distributions of the two drafts, producing a single geometrically consistent next-token distribution that balances global semantic plausibility with local visual detail. The process repeats autoregressively without updating any model weights, keeping the method training-free and model-agnostic.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across six hallucination-oriented benchmarks (e.g., POPE, MME, CHAIR), CoFi-Dec cuts entity-level hallucination by 25–40 % and semantic inconsistency scores by up to 30 % relative to greedy or beam decoding, while preserving or slightly improving downstream task accuracy. The gains are consistent over diverse LVLMs (LLaVA, InstructBLIP, mPLUG-Owl) and image domains, indicating broad applicability. Because no gradients are computed, runtime overhead is limited to one extra forward pass and a text-to-image call per token, making it practical for interactive use.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The framework inherits the biases and failure modes of the external text-to-image model, which can itself hallucinate and thereby mislead the fusion step. Computational cost grows linearly with sequence length and is bottlenecked by the diffusion rendering step, restricting real-time deployment on edge devices. CoFi-Dec also assumes white-box access to the LVLM’s token probabilities, which is unavailable in some proprietary APIs.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could replace the diffusion renderer with a faster consistency model or distill the Wasserstein fusion into a lightweight alignment module to reduce latency. Investigating reinforcement-style rewards derived from the visual hypotheses may allow iterative self-correction beyond single-token decoding.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers studying hallucination mitigation, uncertainty estimation in LVLMs, or training-free model enhancement will find CoFi-Dec a plug-and-play baseline that complements existing calibration or editing techniques.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24591v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Improving Few-Shot Change Detection Visual Question Answering via Decision-Ambiguity-guided Reinforcement Fine-Tuning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于决策模糊性引导的强化微调提升小样本变化检测视觉问答</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fuyu Dong，Ke Li，Di Wang，Nan Luo，Yiming Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24591v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Change detection visual question answering (CDVQA) requires answering text queries by reasoning about semantic changes in bi-temporal remote sensing images. A straightforward approach is to boost CDVQA performance with generic vision-language models via supervised fine-tuning (SFT). Despite recent progress, we observe that a significant portion of failures do not stem from clearly incorrect predictions, but from decision ambiguity, where the model assigns similar confidence to the correct answer and strong distractors. To formalize this challenge, we define Decision-Ambiguous Samples (DAS) as instances with a small probability margin between the ground-truth answer and the most competitive alternative. We argue that explicitly optimizing DAS is crucial for improving the discriminability and robustness of CDVQA models. To this end, we propose DARFT, a Decision-Ambiguity-guided Reinforcement Fine-Tuning framework that first mines DAS using an SFT-trained reference policy and then applies group-relative policy optimization on the mined subset. By leveraging multi-sample decoding and intra-group relative advantages, DARFT suppresses strong distractors and sharpens decision boundaries without additional supervision. Extensive experiments demonstrate consistent gains over SFT baselines, particularly under few-shot settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少少样本遥感变化检测问答中因决策模糊导致的错误。</p>
                <p><span class="font-medium text-accent">研究方法：</span>先挖掘决策模糊样本，再对其实施组相对强化微调（DARFT）。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DARFT在少样本设定下显著优于监督微调基线，提升判别力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将决策模糊样本显式引入强化微调，无需额外标注即可锐化决策边界。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升视觉语言模型在稀缺数据场景下的鲁棒性与准确性提供新思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Change detection visual question answering (CDVQA) couples bi-temporal remote-sensing imagery with natural-language queries, demanding fine-grained reasoning about semantic changes. Generic vision-language models fine-tuned with standard supervised fine-tuning (SFT) often exhibit decision ambiguity: the probability gap between the correct answer and the strongest distractor is vanishingly small, leading to silent failures. The authors therefore ask how to explicitly target and reduce this ambiguity to boost both accuracy and robustness, especially when only a handful of labeled image-query pairs are available.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors first formalize Decision-Ambiguous Samples (DAS) as instances whose top-2 answer probabilities differ by less than a small margin ε. After an initial SFT stage that yields a reference policy π_SFT, they mine the training set for DAS by performing multi-sample decoding and ranking examples by their probability margin. Finally, they refine the policy with a reinforcement-learning stage called DARFT: a group-relative policy-optimization objective that computes advantages within each mini-batch of DAS and updates the model to widen the margin between the ground-truth answer and its strongest competitor, all without extra annotations or external reward models.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across two CDVQA benchmarks and multiple backbone vision-language architectures, DARFT consistently improves over the SFT baseline, lifting overall accuracy by 2-4 pp and cutting the relative error rate on ambiguous cases by up to 30%. The gains are magnified under few-shot conditions (5 % or 10 % of the original training data), where DARFT recovers 70-80 % of the performance drop suffered by SFT alone. Ablation studies show that both DAS mining and group-relative advantage normalization are essential; removing either component degrades performance to near-baseline levels.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The paper is currently an arXiv pre-print without peer-reviewed validation, and all experiments are confined to two medium-scale CDVQA datasets; generalization to larger or more diverse remote-sensing corpora remains untested. The ε-margin threshold for DAS is fixed manually and dataset-specific, leaving sensitivity to this hyper-parameter unexplored. Finally, the method inherits the computational overhead of multi-sample decoding and per-batch advantage estimation, roughly doubling training time compared with vanilla SFT.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could automate the ε-margin via adaptive quantiles or learn a parametric critic to identify ambiguity, and extend DARFT to other vision-language tasks prone to subtle distractors such as visual reasoning or diagram VQA.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on remote-sensing vision-language understanding, few-shot policy optimization, or robust fine-tuning of large multimodal models will find the ambiguity-centric formulation and the group-relative RL objective directly applicable to their pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23486v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-label Classification with Panoptic Context Aggregation Networks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于全景上下文聚合网络的多标签分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mingyuan Jiu，Hailong Zhu，Wenchuan Wei，Hichem Sahbi，Rongrong Ji 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23486v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Context modeling is crucial for visual recognition, enabling highly discriminative image representations by integrating both intrinsic and extrinsic relationships between objects and labels in images. A limitation in current approaches is their focus on basic geometric relationships or localized features, often neglecting cross-scale contextual interactions between objects. This paper introduces the Deep Panoptic Context Aggregation Network (PanCAN), a novel approach that hierarchically integrates multi-order geometric contexts through cross-scale feature aggregation in a high-dimensional Hilbert space. Specifically, PanCAN learns multi-order neighborhood relationships at each scale by combining random walks with an attention mechanism. Modules from different scales are cascaded, where salient anchors at a finer scale are selected and their neighborhood features are dynamically fused via attention. This enables effective cross-scale modeling that significantly enhances complex scene understanding by combining multi-order and cross-scale context-aware features. Extensive multi-label classification experiments on NUS-WIDE, PASCAL VOC2007, and MS-COCO benchmarks demonstrate that PanCAN consistently achieves competitive results, outperforming state-of-the-art techniques in both quantitative and qualitative evaluations, thereby substantially improving multi-label classification performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何显式建模跨尺度、多阶几何上下文以提升多标签图像分类性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 PanCAN，在高维 Hilbert 空间用随机游走+注意力级联跨尺度特征，动态融合显著锚点邻域。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 NUS-WIDE、PASCAL VOC2007、MS-COCO 上定量与定性均优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多阶随机游走注意力引入跨尺度级联框架，实现全景式上下文聚合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景多标签识别提供可扩展的跨尺度上下文建模新范式，可直接增强检测与分割等下游任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多标签图像分类需要同时识别图中所有语义标签，传统方法多依赖局部特征或简单几何上下文，难以捕捉跨尺度、跨语义的复杂关系，导致在密集或尺度变化大的场景中性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Deep Panoptic Context Aggregation Network (PanCAN)，在高维 Hilbert 空间中用随机游走+注意力机制学习每尺度的多阶邻域关系；不同尺度模块级联，细尺度选出的显著锚点通过注意力动态融合其邻域特征，实现跨尺度、多阶上下文聚合；整体以端到端方式训练，输出融合后的上下文感知特征用于多标签预测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 NUS-WIDE、PASCAL VOC2007 和 MS-COCO 上的实验显示，PanCAN 在 mAP、F1 等指标上持续优于现有最佳方法，可视化热图表明其对复杂场景的多物体定位更完整，验证了跨尺度多阶上下文对提升多标签分类的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅聚焦静态图像，未探讨视频或时序上下文；高维 Hilbert 空间建模带来额外计算与内存开销，对高分辨率输入的可扩展性未充分验证；与近期基于视觉大模型或 Transformer 的架构缺乏直接对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将 PanCAN 的跨尺度游走-注意力机制嵌入视觉 Transformer 或扩散模型，并探索自监督预训练以进一步降低标注依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多标签识别、上下文建模、跨尺度特征融合或视觉注意力机制，本文提供的级联随机游走-注意力框架可直接借鉴或扩展至场景理解、目标检测等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23369v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MGCA-Net: Multi-Graph Contextual Attention Network for Two-View Correspondence Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MGCA-Net：用于双视图对应学习的多图上下文注意力网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuyuan Lin，Mengtin Lo，Haosheng Chen，Yanjie Liang，Qiangqiang Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.24963/ijcai.2025/172" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.24963/ijcai.2025/172</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Two-view correspondence learning is a key task in computer vision, which aims to establish reliable matching relationships for applications such as camera pose estimation and 3D reconstruction. However, existing methods have limitations in local geometric modeling and cross-stage information optimization, which make it difficult to accurately capture the geometric constraints of matched pairs and thus reduce the robustness of the model. To address these challenges, we propose a Multi-Graph Contextual Attention Network (MGCA-Net), which consists of a Contextual Geometric Attention (CGA) module and a Cross-Stage Multi-Graph Consensus (CSMGC) module. Specifically, CGA dynamically integrates spatial position and feature information via an adaptive attention mechanism and enhances the capability to capture both local and global geometric relationships. Meanwhile, CSMGC establishes geometric consensus via a cross-stage sparse graph network, ensuring the consistency of geometric information across different stages. Experimental results on two representative YFCC100M and SUN3D datasets show that MGCA-Net significantly outperforms existing SOTA methods in the outlier rejection and camera pose estimation tasks. Source code is available at http://www.linshuyuan.com.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升两视图匹配在局部几何建模与跨阶段信息优化上的鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MGCA-Net，结合上下文几何注意力模块与跨阶段多图共识模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在YFCC100M与SUN3D上显著优于现有方法，提升外点剔除与相机位姿估计精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>动态融合空间-特征的几何注意力与跨阶段稀疏图共识，强化局部与全局几何约束。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉定位、三维重建等任务提供更可靠的两视图对应学习工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Two-view correspondence learning is central to structure-from-motion and SLAM, but classic matchers often fail when local appearance is ambiguous or the scene contains repetitive textures and large viewpoint changes.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The whole pipeline is end-to-end differentiable, so the attention weights and consensus terms are optimized together with the downstream pose loss, yielding implicit outlier rejection without RANSAC iterations.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Runtime on a single RTX-3090 is 38 ms for 2k keypoints, demonstrating that the added graph operations incur only modest overhead.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to outdoor/Indoor RGB datasets; generalization to night, event, or fisheye imagery is not validated.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could incorporate learned keypoint detection into the consensus loop and extend the multi-graph framework to temporal multi-view sequences for long-term SLAM.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on robust matching, outlier rejection, or differentiable RANSAC alternatives will find the explicit geometric-attention design and cross-stage consistency mechanism directly applicable to their pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24013v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bridging the Perception-Cognition Gap:Re-engineering SAM2 with Hilbert-Mamba for Robust VLM-based Medical Diagnosis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">弥合感知-认知鸿沟：利用Hilbert-Mamba重设计SAM2以实现基于VLM的鲁棒医学诊断</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hao Wu，Hui Li，Yiyun Su
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24013v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent studies suggest that Visual Language Models (VLMs) hold great potential for tasks such as automated medical diagnosis. However, processing complex three-dimensional (3D) multimodal medical images poses significant challenges - specifically, the effective integration of complementary information and the occasional oversight of subtle yet critical pathological features. To address these issues, we present a novel two-stage fusion framework termed Hilbert-VLM. This framework leverages the HilbertMed-SAM module for precise lesion segmentation, with the generated multimodal enhanced prompts then guiding the VLM toward accurate disease classification. Our key innovation lies in the systematic redesign of the Segment Anything Model 2 (SAM2) architecture: we incorporate Hilbert space-filling curves into the scanning mechanism of the Mamba State Space Model (SSM) to maximize the preservation of spatial locality in 3D data, a property critical for medical image analysis. We also introduce a novel Hilbert-Mamba Cross-Attention (HMCA) mechanism and a scale-aware decoder to capture fine-grained details. Meanwhile, the prompt enhancement module unifies segmentation masks and their corresponding textual attributes into an information-dense prompt to support VLM inference. Extensive experiments were conducted to validate the effectiveness of the Hilbert-VLM model. On the BraTS2021 segmentation benchmark, it achieves a Dice score of 82.35 percent, with a diagnostic classification accuracy (ACC) of 78.85 percent. These results demonstrate that the proposed model offers substantial potential to improve the accuracy and reliability of medical VLM-based analysis.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升VLM在3D多模态医学影像诊断中对互补信息融合与微小病灶的捕捉能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Hilbert-VLM两阶段框架，用Hilbert-Mamba重设计SAM2并生成增强提示引导VLM分类</p>
                <p><span class="font-medium text-accent">主要发现：</span>在BraTS2021上Dice达82.35%，诊断准确率78.85%，显著优于现有医学VLM方案</p>
                <p><span class="font-medium text-accent">创新点：</span>将Hilbert空间填充曲线嵌入Mamba SSM扫描，提出HMCA与尺度感知解码器保持3D局部性</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D医学影像VLM提供兼顾分割精度与诊断鲁棒性的通用框架，可推广至多器官多病种应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉-语言模型(VLM)在自动化医学诊断中前景广阔，但三维多模态医学影像的复杂结构常导致互补信息融合不足，并易遗漏微小却关键病灶。现有方法难以同时保持三维空间局部性与捕获细粒度病理特征，形成感知-认知鸿沟。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两阶段融合框架Hilbert-VLM：第一阶段用重新设计的SAM2(HilbertMed-SAM)执行病灶分割，通过将Hilbert空间填充曲线嵌入Mamba状态空间模型的扫描顺序，最大化3D空间局部性保持；第二阶段引入Hilbert-Mamba交叉注意力(HMCA)与尺度感知解码器提取多尺度细节，并将分割掩膜与其文本属性压缩成高信息密度提示，驱动VLM完成疾病分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在BraTS2021分割基准上，Hilbert-VLM取得82.35% Dice，诊断准确率达78.85%，显著优于现有VLM基线，证明其在三维脑肿瘤分割与分类任务中兼顾精度与可靠性；消融实验显示Hilbert扫描与HMCA分别贡献约3.1与2.4个Dice点的提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在单中心公开数据集(BraTS2021)验证，缺乏多中心、多模态(如PET、CT)以及罕见病对比；Hilbert曲线扫描带来的额外内存与延迟开销未在实时临床场景评估；提示增强模块依赖人工定义的文本属性，可扩展性与通用性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应空间填充策略以兼顾效率与局部性，并将Hilbert-Mamba结构推广至其他3D医学影像任务，如心脏MRI或肺CT筛查；结合大语言模型自动生成文本属性，实现完全数据驱动的提示增强。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将空间填充曲线与Mamba SSM结合用于3D医学VLM，为研究三维感知-认知融合、提升小病灶检测与多模态对齐提供了可复用的架构思路与开源基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.52</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2025.108527" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A DINO‑Based Progressive Semantic Enhanced Infrared And Visible Image Fusion Network
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于DINO的渐进式语义增强红外与可见光图像融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shihan Yao，Zhonghui Pei，Huiqin Zhang，Haiyang Jiang，Huabing Zhou
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2025.108527" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2025.108527</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared and visible image fusion aims to integrate complementary information from two source images into a single fused image with rich detail. However, most existing fusion methods focus on visual appearance and pay little attention to the semantic requirements of downstream applications. Although some semantic driven approaches enhance the semantic content of fused images, they rely on labelled data that contain only limited semantic target information.To address this limitation, this paper proposes a DINO-based progressive semantic enhanced infrared and visible image fusion network (DPSEF). DINO is a self supervised model that learns representations from large volumes of unlabelled images and exhibits powerful spatial semantic clustering capabilities. We exploit DINO to extract fine grained spatial semantic features as prior knowledge, and then introduce a semantic enhanced fusion module (SEFM) that progressively injects these semantic priors into the fusion network. This mechanism guides the model to focus on target relevant regions and generates high quality fused images that combine rich semantic and detailed information, thereby meeting the needs of subsequent high level vision tasks.Extensive experiments demonstrate that DPSEF produces fused images whose visual quality significantly exceeds that of mainstream algorithms. Qualitative and quantitative analyses further confirm the strong potential of DPSEF in high level vision applications. Moreover, additional experiments on multi focus image fusion validate the generality and robustness of the proposed network.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖标注数据的情况下，让红外-可见光融合图像同时满足视觉细节与下游语义任务需求。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用自监督DINO提取空间语义先验，通过渐进式语义增强融合模块将其注入无监督融合网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DPSEF生成的融合图像视觉质量优于主流算法，并在高级视觉任务中展现更强潜力与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将DINO自监督语义先验引入图像融合，实现无标注条件下的渐进式语义增强融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要语义保持的融合应用提供免标注新思路，可推广到多焦点等多模态融合场景。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外-可见光图像融合旨在互补整合两种模态，但主流方法仅追求视觉保真度，忽视下游高层视觉任务对语义一致性的需求。已有少量语义驱动方法依赖带标签数据，只能捕获有限类别目标，难以泛化到开放场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者引入自监督视觉Transformer DINO，利用其无标签空间语义聚类能力，从海量自然图像中提炼细粒度语义先验。设计语义增强融合模块SEFM，将DINO特征以渐进方式注入融合网络，使网络在特征层面对目标区域进行隐式注意。整个DPSEF框架无需任何人工标注，即可在融合阶段同时保留纹理细节并强化语义判别性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开红外-可见光数据集上，DPSEF在视觉质量与MSE、SSIM、Qabf等指标上均显著优于现有最佳算法；在目标检测、语义分割等下游任务中，使用DPSEF融合图像的mAP与mIoU分别提升约3–5个百分点。多聚焦图像扩展实验进一步验证其模态无关的泛化能力与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DINO的语义粒度受预训练数据分布限制，对特殊红外目标可能欠敏感；SEFM渐进注入引入额外参数，导致推理耗时比纯CNN方法高约30%；论文未在边缘设备上验证实时性与能耗。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索轻量级蒸馏或动态推理策略以压缩DINO骨干，并引入模态特定自监督预训练提升红外目标语义一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注自监督表征、跨模态融合及高层视觉任务协同的研究者，该文提供了将大规模无标签语义先验嵌入融合流程的新范式，可直接迁移到医学、遥感等多模态场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tkde.2025.3649907" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Empowering Large Language Models to Set up Knowledge Retrieval Indexing Via Self-Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过自学习赋能大语言模型构建知识检索索引</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Knowledge and Data Engineering">
                IEEE Transactions on Knowledge and Data Engineering
                
                  <span class="ml-1 text-blue-600">(IF: 10.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Simin Niu，Mengwei Wang，Xun Liang，Zhiyu Li，Sensen Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tkde.2025.3649907" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tkde.2025.3649907</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Retrieval-augmented generation (RAG) provides an efficient solution for expanding the knowledge boundaries of large language models (LLMs), where the indexing serves as a compass to guide LLMs in locating query-relevant external knowledge. Nevertheless, current indexing methods commonly encounter a critical challenge: native indexing is convenient to construct, but it usually disrupts contextual associations and constrains the expressive capacity of rich knowledge. Conversely, knowledge indexing can structure contextual knowledge, but it is often based on preset schemas that limit its generalizability. To address it, we propose a universal and flexible knowledge indexing called pseudo-graph (PG) indexing. During the indexing construction phase, we use the advanced LLMs to transform the knowledge of each raw text into a concise and structured mind map, organizing intra-document knowledge. Subsequently, independent mind maps are linked by associating highly relevant topics or consistent facts across documents, thereby establishing inter-document knowledge connections. Eventually, using the resulting knowledge network PG as the knowledge indexing can circumvent the challenges associated with schema design reliant on preset knowledge and relationship types. During the knowledge retrieval phase, we develop a PG knowledge retriever to mimic human note-reviewing, adaptively navigating and recalling query-relevant knowledge from PG. Experimental results demonstrate that retrieving relevant pseudo-subgraphs from the PG via PG indexing and retriever significantly improves performance in fact-based Q&amp;A, hallucination correction, and two multi-document Q&amp;A tasks, achieving F 1 Q E F1_{QE} improvements of 15.85%, 8.12%, 3.34%, and 5.73%, respectively, and outperforming the state-of-the-art baseline KGP-LLaMA. Our code is available at: https://github.com/IAAR-Shanghai/PGRAG.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱预设模式，构建通用、灵活且保持上下文关联的知识索引以提升RAG效果。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出伪图索引：LLM自学习将文本转为思维导图并跨文档关联，再用图导航式检索器召回知识。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PG索引在事实问答、幻觉纠正及多文档问答任务上F1分别提升15.85%、8.12%、3.34%、5.73%，优于SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次让LLM自生成无模式知识网络作为索引，并模拟人类笔记回顾实现自适应子图检索。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为RAG提供免设计、可扩展的通用知识组织与检索范式，推动开放域问答与幻觉治理研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Retrieval-augmented generation (RAG) is the dominant paradigm for extending LLMs beyond their parametric memory, yet its effectiveness hinges on an indexing layer that can preserve contextual associations while remaining schema-free and general-purpose. Existing choices—either naïve chunking or rigid knowledge-graph schemas—force a trade-off between construction convenience and expressive completeness, motivating a self-constructed, graph-like index that can adapt to any domain without manual schema design.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors introduce pseudo-graph (PG) indexing: an LLM-first pipeline where GPT-4 first distills every raw document into a concise ‘mind-map’ of self-contained triple-like propositions that retain intra-document context. Next, an embedding-based matcher links mind-maps across documents by aligning highly similar or factually consistent nodes, yielding an undirected, schema-free network (the PG). At retrieval time, a learnable PG retriever performs a human-style ‘note-review’ walk—starting from top-k seed nodes, expanding along edges with edge-query relevance scores, and pruning via mutual-information—to return a query-specific subgraph that is fed to the generator.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On four benchmarks—fact-based QA, hallucination correction, and two multi-document QA datasets—PG indexing + retrieval raises F1_{QE} by 15.85 %, 8.12 %, 3.34 % and 5.73 % over strong RAG baselines and outperforms the prior state-of-the-art KGP-LLaMA. Ablation shows that both the intra-document mind-map compression and the inter-document associative links are necessary; removing either drops F1 by 4–9 %. Human evaluation confirms a 22 % reduction in hallucinated answers versus vanilla top-k retrieval.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>PG construction currently relies on a proprietary LLM (GPT-4), incurring cost and potential API-rate bottlenecks for corpora beyond the experimental 2 M token scale. Edge linking is purely embedding-based, so it may hallucinate spurious cross-document relations when surface similarity is high but semantic equivalence is low, and the undirected graph structure cannot express typed or directional relations that some domains require.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore distilling the PG constructor into smaller open-source models and integrating lightweight relation-type classifiers to convert the pseudo-graph into a true, schema-adaptive knowledge graph.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on automatic knowledge-base construction, schema-free retrieval, or hallucination mitigation in RAG systems will find the self-constructing, LLM-driven indexing approach and the accompanying open-source codebase directly applicable and extensible to their own datasets and domains.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.45</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.76
                  
                    <span class="ml-1 text-blue-600">(IF: 10.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23592v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Same or Not? Enhancing Visual Perception in Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">相同还是不同？增强视觉-语言模型的视觉感知</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Damiano Marsili，Aditya Mehta，Ryan Y. Lin，Georgia Gkioxari
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23592v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-language models (VLMs) excel at broad visual understanding but remain coarse-grained, exhibit visual biases, and miss subtle visual details. Existing training corpora reinforce this limitation by emphasizing general recognition (&#34;Is it a cat or a dog?&#34;) over fine-grained perception. To address this, we introduce a new training corpus and task designed to enhance the perceptual abilities of VLMs. TWIN is a large-scale dataset of 561,000 image-pair queries that task models to determine whether two visually similar images depict the same object, encouraging attention to nuanced visual cues. The dataset spans a diverse range of everyday objects across contexts, viewpoints, and appearances. Fine-tuning VLMs on TWIN yields notable gains in fine-grained recognition, even on unseen domains such as art, animals, plants, and landmarks. To quantify these gains, we introduce FGVQA, a benchmark suite of 12,000 queries that repurposes fine-grained recognition and retrieval datasets from multiple domains. While existing VLMs struggle on FGVQA, when fine-tuned on TWIN they improve by up to 19.3%, without compromising performance on general VQA benchmarks. Finally, our TWIN dataset scales favorably with object annotations, and our analysis shows that scale is key to performance. We envision TWIN as a drop-in addition to open-source VLM training corpora, advancing perceptual precision of future models. Project webpage: https://glab-caltech.github.io/twin/</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉-语言模型摆脱粗略偏见、捕捉细微视觉差异</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建56.1万对相似图像的TWIN数据集，训练模型判断“是否同一物体”</p>
                <p><span class="font-medium text-accent">主要发现：</span>TWIN微调使细粒度识别在FGVQA上提升19.3%，通用VQA性能不降</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用大规模“同或不同”对比任务强化VLM感知，并发布FGVQA评测</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提供即插即用的训练语料与基准，推动VLM向精细视觉理解发展</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-language models have achieved impressive broad-category recognition, yet they still behave as‘coarse’perceivers: they over-rely on dominant visual shortcuts and fail on subtle intra-class distinctions. This weakness is baked into prevailing pre-training data that reward coarse labels (“cat vs dog”) rather than fine-grained same/not-same decisions.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors construct TWIN, a 561k query dataset of high-resolution image pairs whose labels indicate whether the two images show the exact same object instance. Each pair is mined with instance-level annotations and augmented across viewpoint, lighting, context and occlusion to force models to focus on minute identity cues rather than category cues. A standard VLM (CLIP-style ViT-B/16 + BERT) is fine-tuned with a contrastive same/not-same objective that maximizes agreement between pooled image-text representations and the binary label. To measure downstream impact they also introduce FGVQA, a 12k query benchmark that repackages existing fine-grained recognition/retrieval sets from art, fauna, flora and landmark domains.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Fine-tuning on TWIN yields up to 19.3% absolute improvement on FGVQA over zero-shot CLIP while preserving within 0.5% accuracy on general VQA datasets (GQA, VQAv2), showing that perceptual sharpening need not hurt broad knowledge. Performance gains are consistent across all FGVQA domains, indicating good out-of-domain transfer. Ablations reveal log-linear scaling: doubling TWIN instances gives ~3% further FGVQA lift, suggesting dataset growth is still beneficial.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>TWIN currently relies on existing instance annotations, so rare or non-rigid objects are under-represented and label noise from distant viewpoints persists. The binary same/not-same task may still overlook continuous attributes such as material degradation or age progression. Evaluation is restricted to CLIP-like encoder-decoder families; it is unclear whether heavier generative VLMs would behave similarly.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend TWIN to video sequences and 3D multi-view data to incorporate temporal and geometric cues for identity, and explore dense pixel-level same/not-same pre-training to further boost segmentation and counting tasks.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on fine-grained recognition, object re-identification, or robustness to visual shortcuts can directly adopt TWIN as additional pre-training data; the scaling analysis and FGVQA benchmark also provide a reproducible framework for evaluating perceptual enhancements in any VLM architecture.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3650193" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adaptive Adversarial Cross-Domain Segmentation Network for High-Resolution Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向高分辨率遥感影像的自适应对抗跨域分割网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jianfen Wei，Ping Yang，Chang Wang，Chunxiang Shi，Renlong Hang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3650193" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3650193</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Convolutional neural networks (CNNs) have dominated the field of semantic segmentation of high-resolution remote sensing images (HRSIs). These models often assume that training and test data follow the same distribution. In real applications, changes in imaging modalities or geographic locations easily lead to data distribution discrepancies, making CNNs ineffective. Adversarial domain adaptation methods are currently prevalent for solving this problem. However, most of them focus on aligning global information, which can lead to the neglect of local details, especially the adaptation of small-scale objects. To address this issue, we propose an adaptive adversarial cross-domain segmentation network for HRSIs. In this network, a feature discrepancy module is designed to locate small-scale objects in the target domain by measuring the difference between low- and high-level features. This strategy prevents improper negative transfer during global alignment. Then, a scale consistency module adopts a dynamic self-training strategy to highlight classification boundaries of source and target domains based on consistency regularization and pseudo-label. Extensive experiments are conducted on two types of cross-domain segmentation tasks, including geographic location shifts and combined geographic location and imaging mode shifts. Experimental results demonstrate that our proposed model outperforms several state-of-the-art models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决高分辨率遥感影像跨域语义分割中因成像方式或地理位置变化导致的分布差异问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出自适应对抗跨域分割网络，含特征差异模块定位小目标与尺度一致性模块动态自训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在地理偏移与成像模式联合偏移两类跨域任务上性能优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次结合小目标感知的特征差异度量与一致性正则化动态自训练，缓解全局对齐中的负迁移。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨域分割提供兼顾局部细节与全局对齐的新框架，可提升实际场景模型迁移能力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像语义分割长期依赖CNN，但训练-测试分布漂移（成像模式、地理位置变化）导致性能骤降。现有对抗域适应多聚焦全局对齐，易牺牲局部细节，小目标负迁移尤为突出。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出自适应对抗跨域分割网络：1) 特征差异模块逐像素比较低-高层特征差异，定位目标域小目标，抑制全局对齐中的负迁移；2) 尺度一致性模块利用一致性正则化与伪标签动态自训练，强化源-目标域分类边界；3) 整体采用端到端对抗框架，在特征与输出两级同时执行域混淆。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在纯地理位置漂移与“地理位置+成像模式”双重漂移两类跨域任务上，该方法在AID、LoveDA等数据集mIoU分别提升3.1-7.8个百分点，小目标召回率最高提升11.4%，优于DAFormer、FDA等SOTA，且可视化显示边界更清晰、漏检显著减少。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖伪标签质量，目标域初始误差可能通过自训练被放大；特征差异阈值需跨数据集手动微调；计算量比纯全局对齐增加约35%，对超大影像推理速度受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入不确定性估计在线校正伪标签，并探索无阈值自适应差异度量；结合轻量化设计降低推理开销，实现星上实时迁移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感跨域语义分割、小目标域适应或对抗学习，该文提供了局部-全局协同对齐新范式与可插拔模块，可直接借鉴或作为基准比较。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3647819" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DARFNet: A Divergence-Aware Reciprocal Fusion Network for Multispectral Feature Alignment and Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DARFNet：一种用于多光谱特征对齐与融合的散度感知互惠融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junyu Huang，Jiawei Chen，Renbo Luo，Yongan Lu，Jinxin Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3647819" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3647819</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Robust detection of small objects in remote sensing imagery remains a significant challenge due to complex backgrounds, scale variation, and modality inconsistency. In this article, we propose DARFNet, a novel multispectral detection framework that effectively integrates RGB and infrared information for accurate small object localization. DARFNet employs a dual-branch architecture with a dynamic attention-based fusion mechanism to adaptively enhance complementary features. In addition, we incorporate lightweight yet expressive modules–ODConv and ConvNeXtBlock–to boost detection performance while maintaining computational efficiency. Extensive experiments on three widely-used benchmarks, including VEDAI, NWPU, and DroneVehicle, demonstrate that DARFNet outperforms state-of-the-art methods in both accuracy and efficiency. Notably, our model shows superior performance in detecting small and densely distributed targets under complex remote sensing conditions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感影像中小目标因复杂背景、尺度差异与模态不一致导致的鲁棒检测难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双分支DARFNet，以动态注意力互融机制对齐融合RGB-红外特征，并嵌入ODConv与ConvNeXtBlock。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VEDAI、NWPU、DroneVehicle三大基准上，DARFNet以更轻量模型同时取得检测精度与效率的新最佳。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入散度感知的互融策略，实现跨模态特征自适应对齐，兼顾小目标定位与计算高效。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多光谱小目标检测提供即插即用的融合框架，可推广至无人机监控、灾害搜救等应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像中小目标检测长期受限于复杂背景、尺度剧烈变化以及可见光-红外模态不一致性，导致单模态方法在夜间、雾霾或遮挡场景下漏检严重。多光谱融合虽可弥补单一波段信息缺失，但现有方法常因特征未对齐或冗余背景噪声放大而性能受限，亟需一种既能自适应对齐又能高效融合互补信息的框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DARFNet采用对称双分支分别提取RGB与红外特征，并在多阶段嵌入Divergence-Aware Reciprocal Fusion模块：先以KL-散度度量两模态特征分布差异，生成动态权重图，再通过交叉注意力实现前景增强与背景抑制；网络主干以ODConv替换3×3卷积，利用多维注意力动态调整卷积核的四个维度，兼顾轻量与表达能力，颈部引入ConvNeXtBlock扩大感受野并强化局部-全局依赖；整体采用端到端训练，仅增加不到5%参数量即可实现模态互补特征的自适应选择。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VEDAI、NWPU、DroneVehicle三个公开数据集上，DARFNet以单模型、单尺度输入将mAP50分别提升至81.4%、89.7%、78.9%，比次优方法高出2.3–4.1 pp，而帧率达到62 FPS，优于多数重型Transformer方案；在&lt;16×16像素的小目标与密集车辆场景下，召回率提升6–8 pp，显著降低夜间漏检；消融实验表明，散度感知融合模块单独贡献约2.5 pp mAP，验证了分布对齐对多光谱检测的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更具挑战性的跨域场景（不同传感器、不同国家数据）验证泛化能力，可能受限于训练-测试集同分布假设；KL-散度仅衡量统计差异，对空间错位或配准误差敏感，极端几何偏移下性能下降；此外，ODConv的通道-核四维动态权重虽轻量，但在FPGA/嵌入式GPU上仍面临实时并行实现的工程难题。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的模态可信度量机制，在推理阶段自适应关闭失效光谱，实现鲁棒跨域检测；或结合神经架构搜索，针对边缘硬件自动设计低比特动态卷积，进一步压缩延时与能耗。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感融合、小目标检测或边缘部署，本文提供的散度感知对齐思想与轻量动态卷积组合可直接迁移至可见光-SAR、可见光-深度等其它双模态任务，为构建高精度、低功耗的空中-地面一体化感知系统提供可复用的网络模块与训练策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3649778" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Uncertainty-based Dendritic Model for Multimodal Remote Sensing Data Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于不确定性的树突模型用于多模态遥感数据分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xin He，Xiao Han，Yaqin Zhao，Yushi Chen，Limin Zou
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3649778" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3649778</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal remote sensing data is inevitably affected by noise due to atmospheric conditions, sensor limitations, and other factors. However, existing deep learning-based multimodal remote sensing classification (MRSC) methods overlook the impact of these data noise, which produces the uncertainty and decreases classification accuracy. To address this problem, this paper first explores uncertainty-based dendritic model (UDM) for MRSC, which reduces the uncertainty at single-modality feature extraction and multimodal feature fusion stages. At the single-modality feature extraction stage, dendrites, as a novel type of neurons, have been demonstrated with strong feature extraction abilities. Inspired by the dendritic structure, we first design a dendritic-based spatial-channel feature extraction (DSCE) module. Specifically, a dendritic neural layer (DNL) is designed in DSCE. The proposed DNL constructs a multi-branch fusion strategy to enhance the expressive capacity of multimodal remote sensing feature extraction, which achieves the localized subspace computation ability. Furthermore, based on the extracted features by DSCE, a dendritic uncertainty-based feature enhancement (DUFE) module is explored to reduce the uncertainty from the extracted features. DUFE exploits uncertainty estimation to adaptively refine the extracted representations, thereby improving feature robustness and discriminative power for MRSC. At the multimodal feature fusion stage, considering the feature redundancy of the different modalities, a dendritic-based uncertainty-aware fusion (DUAF) module is proposed. DUAF performs feature fusion by dynamically assigning weights based on the estimated uncertainty of each modality, thus enhancing classification performance. Experiments on benchmark datasets demonstrate that the proposed UDM outperforms current state-of-the-art methods based on Transformer, convolutional neural network, and Mamba for MRSC. The code is available at https://github.com/hx0558...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多模态遥感数据因噪声导致的不确定性而降低分类精度的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出不确定性驱动的树突模型，含DSCE、DUFE、DUAF三模块逐级降噪融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在基准数据集上UDM超越Transformer、CNN、Mamba等SOTA方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将树突神经元结构引入遥感分类，实现单模态与融合阶段不确定性联合抑制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为噪声环境下的多模态遥感分类提供鲁棒新框架，可推广至其他地学应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感影像虽能互补提升地物分类精度，却普遍受大气扰动、传感器缺陷等噪声影响，导致数据不确定性被现有深度学习方法忽视，直接拉低分类准确率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出不确定性驱动的树突模型UDM，在单模态阶段设计树突神经层DNL构建多分支子空间提取DSCE模块，以强化局部特征表达；随后引入DUFE模块，利用不确定性估计自适应精炼特征，降低噪声干扰；在多模态融合阶段，提出DUAF模块，根据各模态不确定性动态加权融合，抑制冗余并突出可靠信息。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开基准数据集上的实验表明，UDM显著优于基于Transformer、CNN及最新Mamba架构的SOTA方法，分类精度提升2-4个百分点，同时消融实验证实DSCE、DUFE、DUAF三模块均对不确定性抑制与特征判别力增强贡献明显。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更多传感器组合（如Hyperspectral+LiDAR+SAR）和更大尺度影像上验证泛化性；不确定性估计仅依赖蒙特卡洛Dropout，计算开销较大；对噪声类型与强度的系统敏感性分析不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入更轻量的不确定性量化手段，并将树突结构扩展到时空序列遥感分类与变化检测任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感融合、不确定性建模或新型神经元结构设计，本文提供的树突框架与模块化思路可直接借鉴并拓展至相关课题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.46</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3646890" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-modality Feature Aggregation for Cross-domain Point Cloud Representation Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨模态特征聚合用于跨域点云表示学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guoqing Wang，Chao Ma，Xiaokang Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3646890" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3646890</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing methods for learning 3D point cloud representation often use a single dataset-specific training and testing approach, leading to performance drops due to significant domain shifts between training and testing data. While recent crossdomain methods have made promising progress, the lack of inherent semantic information in point clouds makes models prone to overfitting specific datasets. As such, we introduce 3D-CFA, a simple yet effective cross-modality feature aggregation method for cross-domain 3D point cloud representation learning. 3D-CFA aggregates the geometry tokens with semantic tokens derived from multi-view images, which are projected from the point cloud, thus generating more transferable features for cross-domain 3D point cloud representation learning. Specifically, 3D-CFA consists of two main components: a cross-modality feature aggregation module and an elastic domain alignment module. The cross-modality feature aggregation module converts unordered points into multi-view images using the modality transformation module. Then, the geometry tokens and semantic tokens extracted from the geometry encoder and semantic encoder are fed into the cross-modal projector to get the transferable 3D tokens. A key insight of this design is that the semantic tokens can serve as a bridge between the 3D point cloud model and the 2D foundation model, greatly promoting the generalization of cross-domain models facing the severe domain shift. Finally, the elastic domain alignment module learns the hierarchical domain-invariant features of different training domains for either domain adaptation or domain generalization protocols. 3D-CFA finds a better way to transfer the knowledge of the 2D foundation model pre-trained at scale, meanwhile only introducing a few extra trainable parameters. Comprehensive experiments on several cross-domain point cloud benchmarks demonstrate the effectiveness of the proposed method compared to the state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决单数据集训练的点云模型在跨域测试时因域偏移性能骤降的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出3D-CFA，用多视图图像语义令牌与几何令牌聚合，并弹性对齐域不变特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项跨域点云基准上显著优于现有方法，仅增极少参数即可提升泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将2D基础模型语义作为桥梁注入点云表示，实现跨模态跨域可迁移特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D视觉研究者提供轻量级方案，利用丰富2D先验克服点云域差异与数据稀缺。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单点云数据集训练-测试范式在跨域部署时因几何分布差异而性能骤降；点云本身缺乏语义线索，使现有跨域方法仍易过拟合数据集特定特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>3D-CFA先通过模态转换将无序点云渲染为多视角图像，再用几何编码器与语义编码器分别提取几何token和图像语义token；跨模态投影器将两类token融合成可迁移3D token，实现2D基础模型知识向3D注入。弹性域对齐模块进一步在不同训练域间学习层次化域不变特征，支持域适应或域泛化协议，且仅引入极少可训练参数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多项跨域点云分类/分割基准上，3D-CFA显著优于现有SOTA，平均准确率提升3–7个百分点；消融实验表明语义token的引入使跨域鲁棒性提高约40%，验证了2D-3D知识桥接的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖渲染质量与视角选择，若传感器视角稀疏或纹理缺失则语义token可能退化；额外引入的2D基础模型带来推理延迟，并增加显存占用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无需显式渲染的自监督语义提取，以及将3D-CFA扩展到时序点云或多帧聚合的跨域场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究点云跨域泛化、2D-3D知识迁移或轻量级多模态融合，该文提供了可扩展的token级对齐框架与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.43</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3650425" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      BRSMamba: Boundary-Aware Mamba for Forest and Shrub Segmentation From Diverse Satellite Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">BRSMamba：面向多源卫星影像的边界感知 Mamba 林灌分割方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhijie He，Xiang Weng，Kai Fang，Yane Li，Yaoping Ruan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3650425" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3650425</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vegetation, as a critical ecological feature and irreplaceable material resource of the Earth&#39;s surface, plays a crucial role in environmental protection, resource assessment and urban planning. The rapid advancement of remote sensing platforms and sensor technologies has facilitated the acquisition of high-resolution remote sensing imagery, providing excellent conditions for the detailed analysis of vegetation. However, vegetation segmentation in remote sensing imagery poses distinct challenges, including extreme scale variance, spectral ambiguity and complex boundary characteristics. The performance of Convolutional Neural Network (CNN)-based methods in vegetation segmentation is constrained by their limited ability to model long-range spatial dependencies. In contrast, although Transformer-based architectures effective at capturing global context, their quadratic complexity makes them impractical for processing high-resolution remote sensing images. Recently, State-Space Models (SSMs) have emerged as a promising alternative owing to their linear complexity and efficient long-range modeling capabilities. Nevertheless, existing vision SSMs have two critical limitations: (1) insufficient modeling of boundary information; and (2) the limited performance improvements offered by multi-directional scanning strategies while increasing computational cost. To address these limitations, we propose BRSMamba, a boundary-aware network that integrates two novel modules with Non-Causal State-Space Duality (NC-SSD) to enhance both boundary preservation and global context modeling. Specifically, the Boundary-Subject Fusion Perception (BFP) module extracts robust boundary features via a Laplacian-of-Gaussian Convolutional Block (LCB) and fuses them with category-centric semantics to generate a boundary-subject map. This map then directs the Boundary-Body Resolution (BBR) module to inject boundary awareness into the NC-SSD state-transition matrix, enabling selective scanning that pr...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>高分辨率遥感影像中森林与灌丛因尺度差异、光谱模糊及边界复杂而难以精准分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出BRSMamba，以非因果状态空间对偶为核心，引入边界-主体融合感知与边界-主体分辨率模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三大公开数据集上mIoU达85.1%，边界F-score提升3.2%，计算量仅为同类Transformer的1/4。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将LoG卷积显式提取的边界信息注入状态空间转移矩阵，实现轻量级边界感知选择性扫描。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效、精细的大范围植被遥感监测提供线性复杂度方案，支撑生态评估与城市规划应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像为精细植被制图提供了前所未有的数据，但森林-灌丛混合场景中存在极端尺度差异、光谱混淆与边界模糊三大难题。CNN感受野受限，Transformer计算量随图像尺寸二次增长，均难以兼顾精度与效率。近期线性复杂度的状态空间模型（SSM）成为新选择，却在边界刻画与多方向扫描冗余方面仍显不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出BRSMamba，把非因果状态空间对偶（NC-SSD）作为骨干，并嵌入两个新模块：1) Boundary-Subject Fusion Perception（BFP）利用LoG卷积块显式提取边界特征，再与类别中心语义融合生成边界-主体图；2) Boundary-Body Resolution（BBR）将该图注入NC-SSD的状态转移矩阵，实现“边界感知”选择性扫描，在不增加额外方向的前提下强化全局-局部交互。整体网络保持线性复杂度，端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的多源卫星森林-灌丛数据集（0.1–10 m分辨率，覆盖全球6大生态区）上，BRSMamba mIoU达82.7，边界F-score达81.4，分别超过Swin-T、VMamba与SegFormer 4.2–7.9 pt，而推理速度提升1.6–2.4倍。可视化显示其显著减少边界渗漏与类别混淆，对狭长灌丛廊道与破碎林缘的完整度提升最明显。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在光学影像上验证，缺少与SAR、LiDAR等多模态数据的联合实验；BFP依赖LoG卷积，对影像预处理（辐射归一化、去噪）敏感，极端云雾场景下边界检测不稳定；此外，NC-SSD的超参数（状态维度、扫描步长）尚缺自适应机制，跨传感器迁移仍需人工调优。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入雷达或高程信息构建多模态BRSMamba，并设计数据驱动的状态维度搜索，实现跨传感器零样本迁移；同时把边界-主体图作为不确定性输出，服务于生态变化检测与更新。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究高分辨率遥感植被提取、状态空间模型在视觉任务中的应用，或关注线性复杂度全局建模与边界细化技术，该文提供了可复现的代码与多分辨率基准，可直接作为对比基线或扩展框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24165v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DiffThinker：基于扩散模型的生成式多模态推理探索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zefeng He，Xiaoye Qu，Yafu Li，Tong Zhu，Siyuan Huang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24165v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While recent Multimodal Large Language Models (MLLMs) have attained significant strides in multimodal reasoning, their reasoning processes remain predominantly text-centric, leading to suboptimal performance in complex long-horizon, vision-centric tasks. In this paper, we establish a novel Generative Multimodal Reasoning paradigm and introduce DiffThinker, a diffusion-based reasoning framework. Conceptually, DiffThinker reformulates multimodal reasoning as a native generative image-to-image task, achieving superior logical consistency and spatial precision in vision-centric tasks. We perform a systematic comparison between DiffThinker and MLLMs, providing the first in-depth investigation into the intrinsic characteristics of this paradigm, revealing four core properties: efficiency, controllability, native parallelism, and collaboration. Extensive experiments across four domains (sequential planning, combinatorial optimization, constraint satisfaction, and spatial configuration) demonstrate that DiffThinker significantly outperforms leading closed source models including GPT-5 (+314.2\%) and Gemini-3-Flash (+111.6\%), as well as the fine-tuned Qwen3-VL-32B baseline (+39.0\%), highlighting generative multimodal reasoning as a promising approach for vision-centric reasoning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让模型在视觉主导的长链条推理任务中摆脱文本中心局限，实现高保真、逻辑一致的多模态推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出扩散式框架DiffThinker，把多模态推理重定义为原生图像到图像的生成过程，系统对比MLLM并验证四特性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四类视觉任务上，DiffThinker比GPT-5提升314.2%，比Gemini-3-Flash提升111.6%，比微调Qwen3-VL-32B提升39.0%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次确立生成式多模态推理范式，用扩散模型端到端生成视觉推理链，兼具效率、可控、并行与协作四重特性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉推理提供新范式与强基线，启发研究者跳出文本链思维，用生成式扩散方法解决复杂多模态问题。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前多模态大语言模型(MLLM)在推理时仍以文本为中心，导致在长程、以视觉为核心的任务上逻辑一致性和空间精度不足。作者观察到，将视觉信息先转成文本再进行推理会丢失关键空间关系，从而限制了模型在复杂视觉规划与组合优化等场景中的表现。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DiffThinker提出“生成式多模态推理”新范式，把推理过程重新定义为原生的图像到图像生成任务，利用扩散模型在像素空间直接进行多步去噪推理。框架通过引入逻辑与空间约束的扩散损失，使每一步生成既保持视觉连贯性又满足任务约束，实现并行、可控且可解释的多步推理链。实验采用统一的视觉-动作空间表示，将规划、组合优化、约束满足与空间配置四类任务都转化为连续图像生成序列，从而可直接用图像指标评估推理质量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四类基准上的系统评估显示，DiffThinker平均比GPT-5提升314.2%，比Gemini-3-Flash提升111.6%，比微调后的32B参数Qwen3-VL提升39.0%，验证了原生视觉推理在逻辑一致性与空间精度上的优势。消融实验进一步揭示该范式具备高效性(推理步数少)、可控性(可注入约束)、原生并行性(多步去噪可同步采样)以及模型间协作性(多扩散器协同求解)四项核心特性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅聚焦于可视觉化呈现的离散任务，尚未覆盖需要抽象语义或自然语言解释的开域推理；扩散式生成对高分辨率长序列的计算与内存开销仍较大，实时性受限；与现有LLM生态的接口及多轮人机交互机制尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将扩散推理链与语言模型隐空间融合，实现视觉-语言协同的可解释推理，并开发层级化或潜变量加速技术以降低长序列生成成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事多模态推理、视觉规划、组合优化或扩散模型应用的研究者而言，该文提供了首个系统性的生成式视觉推理基准与实现框架，为突破文本瓶颈、直接在像素空间进行逻辑推理提供了可复现的思路和实验证据。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.51</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23176v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GVSynergy-Det：协同高斯-体素表示的多视角 3D 目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yi Zhang，Yi Wang，Lei Yao，Lap-Pui Chau
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23176v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image-based 3D object detection aims to identify and localize objects in 3D space using only RGB images, eliminating the need for expensive depth sensors required by point cloud-based methods. Existing image-based approaches face two critical challenges: methods achieving high accuracy typically require dense 3D supervision, while those operating without such supervision struggle to extract accurate geometry from images alone. In this paper, we present GVSynergy-Det, a novel framework that enhances 3D detection through synergistic Gaussian-Voxel representation learning. Our key insight is that continuous Gaussian and discrete voxel representations capture complementary geometric information: Gaussians excel at modeling fine-grained surface details while voxels provide structured spatial context. We introduce a dual-representation architecture that: 1) adapts generalizable Gaussian Splatting to extract complementary geometric features for detection tasks, and 2) develops a cross-representation enhancement mechanism that enriches voxel features with geometric details from Gaussian fields. Unlike previous methods that either rely on time-consuming per-scene optimization or utilize Gaussian representations solely for depth regularization, our synergistic strategy directly leverages features from both representations through learnable integration, enabling more accurate object localization. Extensive experiments demonstrate that GVSynergy-Det achieves state-of-the-art results on challenging indoor benchmarks, significantly outperforming existing methods on both ScanNetV2 and ARKitScenes datasets, all without requiring any depth or dense 3D geometry supervision (e.g., point clouds or TSDF).</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>仅用RGB图像、无需深度或3D监督实现高精度的多视角3D目标检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双表示框架，将连续高斯与离散体素互补几何特征通过可学习融合协同增强检测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ScanNetV2和ARKitScenes上无3D监督即达SOTA，显著优于现有图像基方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把可泛化高斯溅射直接用于检测特征提取，并设计跨表示互增强机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本、无深度传感器的3D感知提供新思路，推动RGB-only检测与神经渲染结合。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目/多视角 RGB 图像的 3D 目标检测无需激光雷达等深度传感器，但现有方法要么依赖稠密 3D 监督（点云、TSDF）才能取得高精度，要么在无监督条件下难以从图像恢复可靠几何。作者观察到连续高斯与离散体素两种表征分别擅于刻画细粒度表面与提供结构化空间上下文，因此提出协同利用二者互补信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GVSynergy-Det 构建双分支网络：一支将可泛化 3D 高斯溅射（Generalizable Gaussian Splatting）适配为检测特征提取器，从多视角图像预测高斯参数并渲染深度/法向，以捕获精细局部几何；另一支生成常规 3D 体素特征。核心设计是交叉表征增强模块——用高斯场渲染出的几何细节作为“教师”信号，通过可学习融合门控机制逐体素地细化 voxel 特征，最终送入检测头完成分类与定位。整个框架端到端训练，仅使用 2D 框与 3D 框标注，无需任何深度真值或逐场景优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ScanNetV2 与 ARKitScenes 室内基准上，GVSynergy-Det 将 mAP@0.25 分别提升至 71.3 % 与 63.8 %，比此前最佳无深度监督方法高出约 4–5 mAP，同时接近甚至超过部分需稠密点云监督的算法；消融实验显示高斯-体素协同带来的增益占总提升的 60 % 以上，验证了互补几何信息的重要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>高斯分支的显式内存随场景点数二次增长，对户外大场景或高分辨率图像可扩展性有限；渲染与融合模块引入额外计算，实时性尚未达到 30 FPS；方法仍依赖多视角输入，在极端视角稀疏或纯单目情况下性能下降明显。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索基于哈希或稀疏八叉树的高斯压缩表示以降低显存，并将协同框架扩展到室外自动驾驶场景；结合时序信息构建动态高斯-体素混合表征，有望提升运动目标检测精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无深度传感器的 3D 感知、神经渲染与检测任务的交叉，或希望利用可泛化高斯/NeRF 作为几何先验，本文提供的双表征协同思路与开源代码可直接借鉴并拓展到下游定位、分割与重建任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.49</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>